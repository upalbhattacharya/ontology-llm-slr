@article{2509.07905v1,
Author        = {Hamid Ahmad and Heiko Paulheim and Rita T. Sousa},
Title         = {Bio-KGvec2go: Serving up-to-date Dynamic Biomedical Knowledge Graph
  Embeddings},
Eprint        = {2509.07905v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Knowledge graphs and ontologies represent entities and their relationships in
a structured way, having gained significance in the development of modern AI
applications. Integrating these semantic resources with machine learning models
often relies on knowledge graph embedding models to transform graph data into
numerical representations. Therefore, pre-trained models for popular knowledge
graphs and ontologies are increasingly valuable, as they spare the need to
retrain models for different tasks using the same data, thereby helping to
democratize AI development and enabling sustainable computing.
  In this paper, we present Bio-KGvec2go, an extension of the KGvec2go Web API,
designed to generate and serve knowledge graph embeddings for widely used
biomedical ontologies. Given the dynamic nature of these ontologies,
Bio-KGvec2go also supports regular updates aligned with ontology version
releases. By offering up-to-date embeddings with minimal computational effort
required from users, Bio-KGvec2go facilitates efficient and timely biomedical
research.},
Year          = {2025},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2509.07905v1},
File          = {2509.07905v1.pdf}
}
@article{2509.08032v1,
Author        = {Fengyu She and Nan Wang and Hongfei Wu and Ziyi Wan and Jingmian Wang and Chang Wang},
Title         = {SciGPT: A Large Language Model for Scientific Literature Understanding
  and Knowledge Discovery},
Eprint        = {2509.08032v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Scientific literature is growing exponentially, creating a critical
bottleneck for researchers to efficiently synthesize knowledge. While
general-purpose Large Language Models (LLMs) show potential in text processing,
they often fail to capture scientific domain-specific nuances (e.g., technical
jargon, methodological rigor) and struggle with complex scientific tasks,
limiting their utility for interdisciplinary research. To address these gaps,
this paper presents SciGPT, a domain-adapted foundation model for scientific
literature understanding and ScienceBench, an open source benchmark tailored to
evaluate scientific LLMs.
  Built on the Qwen3 architecture, SciGPT incorporates three key innovations:
(1) low-cost domain distillation via a two-stage pipeline to balance
performance and efficiency; (2) a Sparse Mixture-of-Experts (SMoE) attention
mechanism that cuts memory consumption by 55\% for 32,000-token long-document
reasoning; and (3) knowledge-aware adaptation integrating domain ontologies to
bridge interdisciplinary knowledge gaps.
  Experimental results on ScienceBench show that SciGPT outperforms GPT-4o in
core scientific tasks including sequence labeling, generation, and inference.
It also exhibits strong robustness in unseen scientific tasks, validating its
potential to facilitate AI-augmented scientific discovery.},
Year          = {2025},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2509.08032v1},
File          = {2509.08032v1.pdf}
}
@article{2509.04942v1,
Author        = {Heinke Hihn and Dennis A. V. Dittrich and Carl Jeske and Cayo Costa Sobral and Helio Pais and Timm Lochmann},
Title         = {Ontology-Aligned Embeddings for Data-Driven Labour Market Analytics},
Eprint        = {2509.04942v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {The limited ability to reason across occupational data from different sources
is a long-standing bottleneck for data-driven labour market analytics. Previous
research has relied on hand-crafted ontologies that allow such reasoning but
are computationally expensive and require careful maintenance by human experts.
The rise of language processing machine learning models offers a scalable
alternative by learning shared semantic spaces that bridge diverse occupational
vocabularies without extensive human curation. We present an embedding-based
alignment process that links any free-form German job title to two established
ontologies - the German Klassifikation der Berufe and the International
Standard Classification of Education. Using publicly available data from the
German Federal Employment Agency, we construct a dataset to fine-tune a
Sentence-BERT model to learn the structure imposed by the ontologies. The
enriched pairs (job title, embedding) define a similarity graph structure that
we can use for efficient approximate nearest-neighbour search, allowing us to
frame the classification process as a semantic search problem. This allows for
greater flexibility, e.g., adding more classes. We discuss design decisions,
open challenges, and outline ongoing work on extending the graph with other
ontologies and multilingual titles.},
Year          = {2025},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2509.04942v1},
File          = {2509.04942v1.pdf}
}
@article{2509.04926v1,
Author        = {Barbara Gendron and GaÃ«l Guibon and Mathieu D'aquin},
Title         = {Towards Ontology-Based Descriptions of Conversations with
  Qualitatively-Defined Concepts},
Eprint        = {2509.04926v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The controllability of Large Language Models (LLMs) when used as
conversational agents is a key challenge, particularly to ensure predictable
and user-personalized responses. This work proposes an ontology-based approach
to formally define conversational features that are typically qualitative in
nature. By leveraging a set of linguistic descriptors, we derive quantitative
definitions for qualitatively-defined concepts, enabling their integration into
an ontology for reasoning and consistency checking. We apply this framework to
the task of proficiency-level control in conversations, using CEFR language
proficiency levels as a case study. These definitions are then formalized in
description logic and incorporated into an ontology, which guides controlled
text generation of an LLM through fine-tuning. Experimental results demonstrate
that our approach provides consistent and explainable proficiency-level
definitions, improving transparency in conversational AI.},
Year          = {2025},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2509.04926v1},
File          = {2509.04926v1.pdf}
}
@article{2509.04868v1,
Author        = {Sylvia Vassileva and Ivan Koychev and Svetla Boytcheva},
Title         = {Using LLMs for Multilingual Clinical Entity Linking to ICD-10},
Eprint        = {2509.04868v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The linking of clinical entities is a crucial part of extracting structured
information from clinical texts. It is the process of assigning a code from a
medical ontology or classification to a phrase in the text. The International
Classification of Diseases - 10th revision (ICD-10) is an international
standard for classifying diseases for statistical and insurance purposes.
Automatically assigning the correct ICD-10 code to terms in discharge summaries
will simplify the work of healthcare professionals and ensure consistent coding
in hospitals. Our paper proposes an approach for linking clinical terms to
ICD-10 codes in different languages using Large Language Models (LLMs). The
approach consists of a multistage pipeline that uses clinical dictionaries to
match unambiguous terms in the text and then applies in-context learning with
GPT-4.1 to predict the ICD-10 code for the terms that do not match the
dictionary. Our system shows promising results in predicting ICD-10 codes on
different benchmark datasets in Spanish - 0.89 F1 for categories and 0.78 F1 on
subcategories on CodiEsp, and Greek - 0.85 F1 on ElCardioCC.},
Year          = {2025},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2509.04868v1},
File          = {2509.04868v1.pdf}
}
@article{2509.04744v1,
Author        = {Gagan Mundada and Yash Vishe and Amit Namburi and Xin Xu and Zachary Novack and Julian McAuley and Junda Wu},
Title         = {WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning},
Eprint        = {2509.04744v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SD},
Abstract      = {Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated
impressive capabilities across various vision-language tasks. However, their
reasoning abilities in the multimodal symbolic music domain remain largely
unexplored. We introduce WildScore, the first in-the-wild multimodal symbolic
music reasoning and analysis benchmark, designed to evaluate MLLMs' capacity to
interpret real-world music scores and answer complex musicological queries.
Each instance in WildScore is sourced from genuine musical compositions and
accompanied by authentic user-generated questions and discussions, capturing
the intricacies of practical music analysis. To facilitate systematic
evaluation, we propose a systematic taxonomy, comprising both high-level and
fine-grained musicological ontologies. Furthermore, we frame complex music
reasoning as multiple-choice question answering, enabling controlled and
scalable assessment of MLLMs' symbolic music understanding. Empirical
benchmarking of state-of-the-art MLLMs on WildScore reveals intriguing patterns
in their visual-symbolic reasoning, uncovering both promising directions and
persistent challenges for MLLMs in symbolic music reasoning and analysis. We
release the dataset and code.},
Year          = {2025},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2509.04744v1},
File          = {2509.04744v1.pdf}
}
@article{2509.04696v1,
Author        = {Samira Khorshidi and Azadeh Nikfarjam and Suprita Shankar and Yisi Sang and Yash Govind and Hyun Jang and Ali Kasgari and Alexis McClimans and Mohamed Soliman and Vishnu Konda and Ahmed Fakhry and Xiaoguang Qi},
Title         = {ODKE+: Ontology-Guided Open-Domain Knowledge Extraction with LLMs},
Eprint        = {2509.04696v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graphs (KGs) are foundational to many AI applications, but
maintaining their freshness and completeness remains costly. We present ODKE+,
a production-grade system that automatically extracts and ingests millions of
open-domain facts from web sources with high precision. ODKE+ combines modular
components into a scalable pipeline: (1) the Extraction Initiator detects
missing or stale facts, (2) the Evidence Retriever collects supporting
documents, (3) hybrid Knowledge Extractors apply both pattern-based rules and
ontology-guided prompting for large language models (LLMs), (4) a lightweight
Grounder validates extracted facts using a second LLM, and (5) the Corroborator
ranks and normalizes candidate facts for ingestion. ODKE+ dynamically generates
ontology snippets tailored to each entity type to align extractions with schema
constraints, enabling scalable, type-consistent fact extraction across 195
predicates. The system supports batch and streaming modes, processing over 9
million Wikipedia pages and ingesting 19 million high-confidence facts with
98.8% precision. ODKE+ significantly improves coverage over traditional
methods, achieving up to 48% overlap with third-party KGs and reducing update
lag by 50 days on average. Our deployment demonstrates that LLM-based
extraction, grounded in ontological structure and verification workflows, can
deliver trustworthiness, production-scale knowledge ingestion with broad
real-world applicability. A recording of the system demonstration is included
with the submission and is also available at https://youtu.be/UcnE3_GsTWs.},
Year          = {2025},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2509.04696v1},
File          = {2509.04696v1.pdf}
}
@article{2509.04159v1,
Author        = {Aarush Kumbhakern and Saransh Kumar Gupta and Lipika Dey and Partha Pratim Das},
Title         = {Towards an Action-Centric Ontology for Cooking Procedures Using Temporal
  Graphs},
Eprint        = {2509.04159v1},
DOI           = {10.1145/3746264.3760499},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Formalizing cooking procedures remains a challenging task due to their
inherent complexity and ambiguity. We introduce an extensible domain-specific
language for representing recipes as directed action graphs, capturing
processes, transfers, environments, concurrency, and compositional structure.
Our approach enables precise, modular modeling of complex culinary workflows.
Initial manual evaluation on a full English breakfast recipe demonstrates the
DSL's expressiveness and suitability for future automated recipe analysis and
execution. This work represents initial steps towards an action-centric
ontology for cooking, using temporal graphs to enable structured machine
understanding, precise interpretation, and scalable automation of culinary
processes - both in home kitchens and professional culinary settings.},
Year          = {2025},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2509.04159v1},
File          = {2509.04159v1.pdf}
}
@article{2509.03857v1,
Author        = {Kishor Datta Gupta and Mohd Ariful Haque and Hasmot Ali and Marufa Kamal and Syed Bahauddin Alam and Mohammad Ashiqur Rahman},
Title         = {Continuous Monitoring of Large-Scale Generative AI via Deterministic
  Knowledge Graph Structures},
Eprint        = {2509.03857v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Generative AI (GEN AI) models have revolutionized diverse application domains
but present substantial challenges due to reliability concerns, including
hallucinations, semantic drift, and inherent biases. These models typically
operate as black-boxes, complicating transparent and objective evaluation.
Current evaluation methods primarily depend on subjective human assessment,
limiting scalability, transparency, and effectiveness. This research proposes a
systematic methodology using deterministic and Large Language Model
(LLM)-generated Knowledge Graphs (KGs) to continuously monitor and evaluate GEN
AI reliability. We construct two parallel KGs: (i) a deterministic KG built
using explicit rule-based methods, predefined ontologies, domain-specific
dictionaries, and structured entity-relation extraction rules, and (ii) an
LLM-generated KG dynamically derived from real-time textual data streams such
as live news articles. Utilizing real-time news streams ensures authenticity,
mitigates biases from repetitive training, and prevents adaptive LLMs from
bypassing predefined benchmarks through feedback memorization. To quantify
structural deviations and semantic discrepancies, we employ several established
KG metrics, including Instantiated Class Ratio (ICR), Instantiated Property
Ratio (IPR), and Class Instantiation (CI). An automated real-time monitoring
framework continuously computes deviations between deterministic and
LLM-generated KGs. By establishing dynamic anomaly thresholds based on
historical structural metric distributions, our method proactively identifies
and flags significant deviations, thus promptly detecting semantic anomalies or
hallucinations. This structured, metric-driven comparison between deterministic
and dynamically generated KGs delivers a robust and scalable evaluation
framework.},
Year          = {2025},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2509.03857v1},
File          = {2509.03857v1.pdf}
}
@article{2509.03780v1,
Author        = {John Wentworth and David Lorell},
Title         = {Natural Latents: Latent Variables Stable Across Ontologies},
Eprint        = {2509.03780v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {math.PR},
Abstract      = {Suppose two Bayesian agents each learn a generative model of the same
environment. We will assume the two have converged on the predictive
distribution, i.e. distribution over some observables in the environment, but
may have different generative models containing different latent variables.
Under what conditions can one agent guarantee that their latents are a function
of the other agents latents?
  We give simple conditions under which such translation is guaranteed to be
possible: the natural latent conditions. We also show that, absent further
constraints, these are the most general conditions under which translatability
is guaranteed. Crucially for practical application, our theorems are robust to
approximation error in the natural latent conditions.},
Year          = {2025},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2509.03780v1},
File          = {2509.03780v1.pdf}
}
@article{2509.03685v1,
Author        = {Zhongjun Ni},
Title         = {Data-Driven Smart Maintenance of Historic Buildings},
Eprint        = {2509.03685v1},
DOI           = {10.3384/9789181180602},
ArchivePrefix = {arXiv},
PrimaryClass  = {eess.SY},
Abstract      = {Digital transformation in the built environment offers new opportunities to
improve building maintenance through data-driven approaches. Smart monitoring,
predictive modeling, and artificial intelligence can enhance decision-making
and enable proactive strategies. The preservation of historic buildings is an
important scenario where preventive maintenance is essential to ensure
long-term sustainability while protecting heritage values. This thesis presents
a comprehensive solution for data-driven smart maintenance of historic
buildings, integrating Internet of Things (IoT), cloud computing, edge
computing, ontology-based data modeling, and machine learning to improve indoor
climate management, energy efficiency, and conservation practices.
  This thesis advances data-driven conservation of historic buildings by
combining smart monitoring, digital twins, and artificial intelligence. The
proposed methods enable preventive maintenance and pave the way for the next
generation of heritage conservation strategies.},
Year          = {2025},
Month         = {Sep},
Note          = {Link\"oping: Link\"oping University Electronic Press, 2025. , p.
  88},
Url           = {http://arxiv.org/abs/2509.03685v1},
File          = {2509.03685v1.pdf}
}
@article{2509.03318v1,
Author        = {Eduard Kamburjan and Vidar Norstein Klungre and Yuanwei Qu and Rudolf Schlatte and Egor V. Kostylev and Martin Giese and Einar Broch Johnsen},
Title         = {Semantically Reflected Programs},
Eprint        = {2509.03318v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.PL},
Abstract      = {This paper addresses the dichotomy between the formalization of structural
and the formalization of behavioral knowledge by means of semantically lifted
programs, which explore an intuitive connection between programs and knowledge
graphs. While knowledge graphs and ontologies are eminently useful to represent
formal knowledge about a system's individuals and universals, programming
languages are designed to describe the system's evolution. To address this
dichotomy, we introduce a semantic lifting of the program states of an
executing program into a knowledge graph, for an object-oriented programming
language. The resulting graph is exposed as a semantic reflection layer within
the programming language, allowing programmers to leverage knowledge of the
application domain in their programs. In this paper, we formalize semantic
lifting and semantic reflection for a small programming language, SMOL, explain
the operational aspects of the language, and consider type correctness and
virtualisation for runtime program queries through the semantic reflection
layer. We illustrate semantic lifting and semantic reflection through a case
study of geological modelling and discuss different applications of the
technique. The language implementation is open source and available online.},
Year          = {2025},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2509.03318v1},
File          = {2509.03318v1.pdf}
}
@article{2509.02918v1,
Author        = {Midhat Urooj and Ayan Banerjee and Farhat Shaikh and Kuntal Thakur and Sandeep Gupta},
Title         = {Single Domain Generalization in Diabetic Retinopathy: A Neuro-Symbolic
  Learning Approach},
Eprint        = {2509.02918v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Domain generalization remains a critical challenge in medical imaging, where
models trained on single sources often fail under real-world distribution
shifts. We propose KG-DG, a neuro-symbolic framework for diabetic retinopathy
(DR) classification that integrates vision transformers with expert-guided
symbolic reasoning to enable robust generalization across unseen domains. Our
approach leverages clinical lesion ontologies through structured, rule-based
features and retinal vessel segmentation, fusing them with deep visual
representations via a confidence-weighted integration strategy. The framework
addresses both single-domain generalization (SDG) and multi-domain
generalization (MDG) by minimizing the KL divergence between domain embeddings,
thereby enforcing alignment of high-level clinical semantics. Extensive
experiments across four public datasets (APTOS, EyePACS, Messidor-1,
Messidor-2) demonstrate significant improvements: up to a 5.2% accuracy gain in
cross-domain settings and a 6% improvement over baseline ViT models. Notably,
our symbolic-only model achieves a 63.67% average accuracy in MDG, while the
complete neuro-symbolic integration achieves the highest accuracy compared to
existing published baselines and benchmarks in challenging SDG scenarios.
Ablation studies reveal that lesion-based features (84.65% accuracy)
substantially outperform purely neural approaches, confirming that symbolic
components act as effective regularizers beyond merely enhancing
interpretability. Our findings establish neuro-symbolic integration as a
promising paradigm for building clinically robust, and domain-invariant medical
AI systems.},
Year          = {2025},
Month         = {Sep},
Note          = {ANSyA 2025: 1st International Workshop on Advanced Neuro-Symbolic
  Applications},
Url           = {http://arxiv.org/abs/2509.02918v1},
File          = {2509.02918v1.pdf}
}
@article{2509.02758v1,
Author        = {Michael Bouzinier and Sergey Trifonov},
Title         = {Optimizing Geometry Problem Sets for Skill Development},
Eprint        = {2509.02758v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {math.HO},
Abstract      = {This article describes an ontology and methodology for annotating and
organizing Euclidean Geometry problems, developed in the early 1990s and
implemented as a software tool. While the majority of this work -- including
the ontology and solution graph paradigm -- was completed over thirty years
ago, we argue that it has renewed relevance in the context of modern artificial
intelligence. In particular, we explore the hypothesis that this established
framework can facilitate automated solution validation and feedback when paired
with contemporary large language models, thereby supporting teachers and
self-learners in geometry education. We document the original architecture and
its enduring value, and outline pathways for bridging historical educational
resources with next-generation AI techniques.},
Year          = {2025},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2509.02758v1},
File          = {2509.02758v1.pdf}
}
@article{2509.02276v1,
Author        = {Susana Nunes and Samy Badreddine and Catia Pesquita},
Title         = {Rewarding Explainability in Drug Repurposing with Knowledge Graphs},
Eprint        = {2509.02276v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge graphs (KGs) are powerful tools for modelling complex,
multi-relational data and supporting hypothesis generation, particularly in
applications like drug repurposing. However, for predictive methods to gain
acceptance as credible scientific tools, they must ensure not only accuracy but
also the capacity to offer meaningful scientific explanations. This paper
presents a novel approach REx, for generating scientific explanations based in
link prediction in knowledge graphs. It employs reward and policy mechanisms
that consider desirable properties of scientific explanation to guide a
reinforcement learning agent in the identification of explanatory paths within
a KG. The approach further enriches explanatory paths with domain-specific
ontologies, ensuring that the explanations are both insightful and grounded in
established biomedical knowledge. We evaluate our approach in drug repurposing
using three popular knowledge graph benchmarks. The results clearly demonstrate
its ability to generate explanations that validate predictive insights against
biomedical knowledge and that outperform the state-of-the-art approaches in
predictive performance, establishing REx as a relevant contribution to advance
AI-driven scientific discovery.},
Year          = {2025},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2509.02276v1},
File          = {2509.02276v1.pdf}
}
@article{2509.02144v1,
Author        = {Arthur Bran Herbener and Malene Flensborg Damholdt},
Title         = {A Theoretical Framework of the Processes of Change in Psychotherapy
  Delivered by Artificial Agents},
Eprint        = {2509.02144v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {The question of whether artificial agents (e.g., chatbots and social robots)
can replace human therapists has received notable attention following the
recent launch of large language models. However, little is known about the
processes of change in psychotherapy delivered by artificial agents. To
facilitate hypothesis development and stimulate scientific debate, the present
article offers the first theoretical framework of the processes of change in
psychotherapy delivered by artificial agents. The theoretical framework rests
upon a conceptual analysis of what active ingredients may be inherently linked
to the presence of human therapists. We propose that human therapists'
ontological status as human beings and sociocultural status as socially
sanctioned healthcare professionals play crucial roles in promoting treatment
outcomes. In the absence of the ontological and sociocultural status of human
therapists, we propose what we coin the genuineness gap and credibility gap can
emerge and undermine key processes of change in psychotherapy. Based on these
propositions, we propose avenues for scientific investigations and practical
applications aimed at leveraging the strengths of artificial agents and human
therapists respectively. We also highlight the intricate agentic nature of
artificial agents and discuss how this complicates endeavors to establish
universally applicable propositions regarding the processes of change in these
interventions.},
Year          = {2025},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2509.02144v1},
File          = {2509.02144v1.pdf}
}
@article{2509.01899v1,
Author        = {Zhimeng Luo and Zhendong Wang and Rui Meng and Diyang Xue and Adam Frisch and Daqing He},
Title         = {Weakly Supervised Medical Entity Extraction and Linking for Chief
  Complaints},
Eprint        = {2509.01899v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {A Chief complaint (CC) is the reason for the medical visit as stated in the
patient's own words. It helps medical professionals to quickly understand a
patient's situation, and also serves as a short summary for medical text
mining. However, chief complaint records often take a variety of entering
methods, resulting in a wide variation of medical notations, which makes it
difficult to standardize across different medical institutions for record
keeping or text mining. In this study, we propose a weakly supervised method to
automatically extract and link entities in chief complaints in the absence of
human annotation. We first adopt a split-and-match algorithm to produce weak
annotations, including entity mention spans and class labels, on 1.2 million
real-world de-identified and IRB approved chief complaint records. Then we
train a BERT-based model with generated weak labels to locate entity mentions
in chief complaint text and link them to a pre-defined ontology. We conducted
extensive experiments, and the results showed that our Weakly Supervised Entity
Extraction and Linking (\ours) method produced superior performance over
previous methods without any human annotation.},
Year          = {2025},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2509.01899v1},
File          = {2509.01899v1.pdf}
}
@article{2509.01304v1,
Author        = {Peter Stockinger},
Title         = {Animer une base de connaissance: des ontologies aux mod{Ã¨}les d'I.A.
  g{Ã©}n{Ã©}rative},
Eprint        = {2509.01304v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {In a context where the social sciences and humanities are experimenting with
non-anthropocentric analytical frames, this article proposes a semiotic
(structural) reading of the hybridization between symbolic AI and neural (or
sub-symbolic) AI based on a field of application: the design and use of a
knowledge base for area studies. We describe the LaCAS ecosystem -- Open
Archives in Linguistic and Cultural Studies (thesaurus; RDF/OWL ontology; LOD
services; harvesting; expertise; publication), deployed at Inalco (National
Institute for Oriental Languages and Civilizations) in Paris with the Okapi
(Open Knowledge and Annotation Interface) software environment from Ina
(National Audiovisual Institute), which now has around 160,000 documentary
resources and ten knowledge macro-domains grouping together several thousand
knowledge objects. We illustrate this approach using the knowledge domain
''Languages of the world'' (~540 languages) and the knowledge object ''Quechua
(language)''. On this basis, we discuss the controlled integration of neural
tools, more specifically generative tools, into the life cycle of a knowledge
base: assistance with data localization/qualification, index extraction and
aggregation, property suggestion and testing, dynamic file generation, and
engineering of contextualized prompts (generic, contextual, explanatory,
adjustment, procedural) aligned with a domain ontology. We outline an ecosystem
of specialized agents capable of animating the database while respecting its
symbolic constraints, by articulating model-driven and data-driven methods.},
Year          = {2025},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2509.01304v1},
File          = {2509.01304v1.pdf}
}
@article{2509.00958v1,
Author        = {Manish Verma and Vivek Sharma and Vishal Singh},
Title         = {A Hybrid Ai Framework For Strategic Patent Portfolio Pruning:
  Integrating Learning To-Rank And Market Need Analysis For Technology Transfer
  Optimization},
Eprint        = {2509.00958v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper introduces a novel, multi stage hybrid intelligence framework for
pruning patent portfolios to identify high value assets for technology
transfer. Current patent valuation methods often rely on retrospective
indicators or manual, time intensive analysis. Our framework automates and
deepens this process by combining a Learning to Rank (LTR) model, which
evaluates patents against over 30 legal and commercial parameters, with a
unique "Need-Seed" agent-based system. The "Need Agent" uses Natural Language
Processing (NLP) to mine unstructured market and industry data, identifying
explicit technological needs. Concurrently, the "Seed Agent" employs fine tuned
Large Language Models (LLMs) to analyze patent claims and map their
technological capabilities. The system generates a "Core Ontology Framework"
that matches high potential patents (Seeds) to documented market demands
(Needs), providing a strategic rationale for divestment decisions. We detail
the architecture, including a dynamic parameter weighting system and a crucial
Human in the-Loop (HITL) validation protocol, to ensure both adaptability and
real-world credibility.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2509.00958v1},
File          = {2509.00958v1.pdf}
}
@article{2509.00710v1,
Author        = {Albert Sadowski and JarosÅaw A. Chudziak},
Title         = {On Verifiable Legal Reasoning: A Multi-Agent Framework with Formalized
  Knowledge Representations},
Eprint        = {2509.00710v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Legal reasoning requires both precise interpretation of statutory language
and consistent application of complex rules, presenting significant challenges
for AI systems. This paper introduces a modular multi-agent framework that
decomposes legal reasoning into distinct knowledge acquisition and application
stages. In the first stage, specialized agents extract legal concepts and
formalize rules to create verifiable intermediate representations of statutes.
The second stage applies this knowledge to specific cases through three steps:
analyzing queries to map case facts onto the ontology schema, performing
symbolic inference to derive logically entailed conclusions, and generating
final answers using a programmatic implementation that operationalizes the
ontological knowledge. This bridging of natural language understanding with
symbolic reasoning provides explicit and verifiable inspection points,
significantly enhancing transparency compared to end-to-end approaches.
Evaluation on statutory tax calculation tasks demonstrates substantial
improvements, with foundational models achieving 76.4\% accuracy compared to
18.8\% baseline performance, effectively narrowing the performance gap between
reasoning and foundational models. These findings suggest that modular
architectures with formalized knowledge representations can make sophisticated
legal reasoning more accessible through computationally efficient models while
enhancing consistency and explainability in AI legal reasoning, establishing a
foundation for future research into more transparent, trustworthy, and
effective AI systems for legal domain.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2509.00710v1},
File          = {2509.00710v1.pdf}
}
@article{2509.00140v1,
Author        = {Songhui Yue},
Title         = {LLM-based Triplet Extraction for Automated Ontology Generation in
  Software Engineering Standards},
Eprint        = {2509.00140v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {Ontologies have supported knowledge representation and whitebox reasoning for
decades; thus, the automated ontology generation (AOG) plays a crucial role in
scaling their use. Software engineering standards (SES) consist of long,
unstructured text (with high noise) and paragraphs with domain-specific terms.
In this setting, relation triple extraction (RTE), together with term
extraction, constitutes the first stage toward AOG. This work proposes an
open-source large language model (LLM)-assisted approach to RTE for SES.
Instead of solely relying on prompt-engineering-based methods, this study
promotes the use of LLMs as an aid in constructing ontologies and explores an
effective AOG workflow that includes document segmentation, candidate term
mining, LLM-based relation inference, term normalization, and cross-section
alignment. Golden-standard benchmarks at three granularities are constructed
and used to evaluate the ontology generated from the study. The results show
that it is comparable and potentially superior to the OpenIE method of triple
extraction.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2509.00140v1},
File          = {2509.00140v1.pdf}
}
@article{2508.21774v1,
Author        = {Katherina G Cortes and Shilpa Sundar and Sarah Gehrke and Keenan Manpearl and Junxia Lin and Daniel Robert Korn and Harry Caufield and Kevin Schaper and Justin Reese and Kushal Koirala and Lawrence E Hunter and E. Kathleen Carter and Marcello DeLuca and Arjun Krishnan and Chris Mungall and Melissa Haendel},
Title         = {Improving Biomedical Knowledge Graph Quality: A Community Approach},
Eprint        = {2508.21774v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.OT},
Abstract      = {Biomedical knowledge graphs (KGs) are widely used across research and
translational settings, yet their design decisions and implementation are often
opaque. Unlike ontologies that more frequently adhere to established creation
principles, biomedical KGs lack consistent practices for construction,
documentation, and dissemination. To address this gap, we introduce a set of
evaluation criteria grounded in widely accepted data standards and principles
from related fields. We apply these criteria to 16 biomedical KGs, revealing
that even those that appear to align with best practices often obscure
essential information required for external reuse. Moreover, biomedical KGs,
despite pursuing similar goals and ingesting the same sources in some cases,
display substantial variation in models, source integration, and terminology
for node types. Reaping the potential benefits of knowledge graphs for
biomedical research while reducing wasted effort requires community-wide
adoption of shared criteria and maturation of standards such as BioLink and
KGX. Such improvements in transparency and standardization are essential for
creating long-term reusability, improving comparability across resources, and
enhancing the overall utility of KGs within biomedicine.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.21774v1},
File          = {2508.21774v1.pdf}
}
@article{2508.21491v1,
Author        = {Ziyi Liu and Sidi Wu and Lorenz Hurni},
Title         = {Geospatial Question Answering on Historical Maps Using Spatio-Temporal
  Knowledge Graphs and Large Language Models},
Eprint        = {2508.21491v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Recent advances have enabled the extraction of vectorized features from
digital historical maps. To fully leverage this information, however, the
extracted features must be organized in a structured and meaningful way that
supports efficient access and use. One promising approach is question answering
(QA), which allows users -- especially those unfamiliar with database query
languages -- to retrieve knowledge in a natural and intuitive manner. In this
project, we developed a GeoQA system by integrating a spatio-temporal knowledge
graph (KG) constructed from historical map data with large language models
(LLMs). Specifically, we have defined the ontology to guide the construction of
the spatio-temporal KG and investigated workflows of two different types of
GeoQA: factual and descriptive. Additional data sources, such as historical map
images and internet search results, are incorporated into our framework to
provide extra context for descriptive GeoQA. Evaluation results demonstrate
that the system can generate answers with a high delivery rate and a high
semantic accuracy. To make the framework accessible, we further developed a web
application that supports interactive querying and visualization.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.21491v1},
File          = {2508.21491v1.pdf}
}
@article{2508.21320v1,
Author        = {Mohsen Nayebi Kerdabadi and Arya Hadizadeh Moghaddam and Dongjie Wang and Zijun Yao},
Title         = {Multi-Ontology Integration with Dual-Axis Propagation for Medical
  Concept Representation},
Eprint        = {2508.21320v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Medical ontology graphs map external knowledge to medical codes in electronic
health records via structured relationships. By leveraging domain-approved
connections (e.g., parent-child), predictive models can generate richer medical
concept representations by incorporating contextual information from related
concepts. However, existing literature primarily focuses on incorporating
domain knowledge from a single ontology system, or from multiple ontology
systems (e.g., diseases, drugs, and procedures) in isolation, without
integrating them into a unified learning structure. Consequently, concept
representation learning often remains limited to intra-ontology relationships,
overlooking cross-ontology connections. In this paper, we propose LINKO, a
large language model (LLM)-augmented integrative ontology learning framework
that leverages multiple ontology graphs simultaneously by enabling dual-axis
knowledge propagation both within and across heterogeneous ontology systems to
enhance medical concept representation learning. Specifically, LINKO first
employs LLMs to provide a graph-retrieval-augmented initialization for ontology
concept embedding, through an engineered prompt that includes concept
descriptions, and is further augmented with ontology context. Second, our
method jointly learns the medical concepts in diverse ontology graphs by
performing knowledge propagation in two axes: (1) intra-ontology vertical
propagation across hierarchical ontology levels and (2) inter-ontology
horizontal propagation within every level in parallel. Last, through extensive
experiments on two public datasets, we validate the superior performance of
LINKO over state-of-the-art baselines. As a plug-in encoder compatible with
existing EHR predictive models, LINKO further demonstrates enhanced robustness
in scenarios involving limited data availability and rare disease prediction.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.21320v1},
File          = {2508.21320v1.pdf}
}
@article{2508.20693v1,
Author        = {Tanay Aggarwal and Angelo Salatino and Francesco Osborne and Enrico Motta},
Title         = {Leveraging Large Language Models for Generating Research Topic
  Ontologies: A Multi-Disciplinary Study},
Eprint        = {2508.20693v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {Ontologies and taxonomies of research fields are critical for managing and
organising scientific knowledge, as they facilitate efficient classification,
dissemination and retrieval of information. However, the creation and
maintenance of such ontologies are expensive and time-consuming tasks, usually
requiring the coordinated effort of multiple domain experts. Consequently,
ontologies in this space often exhibit uneven coverage across different
disciplines, limited inter-domain connectivity, and infrequent updating cycles.
In this study, we investigate the capability of several large language models
to identify semantic relationships among research topics within three academic
domains: biomedicine, physics, and engineering. The models were evaluated under
three distinct conditions: zero-shot prompting, chain-of-thought prompting, and
fine-tuning on existing ontologies. Additionally, we assessed the cross-domain
transferability of fine-tuned models by measuring their performance when
trained in one domain and subsequently applied to a different one. To support
this analysis, we introduce PEM-Rel-8K, a novel dataset consisting of over
8,000 relationships extracted from the most widely adopted taxonomies in the
three disciplines considered in this study: MeSH, PhySH, and IEEE. Our
experiments demonstrate that fine-tuning LLMs on PEM-Rel-8K yields excellent
performance across all disciplines.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.20693v1},
File          = {2508.20693v1.pdf}
}
@article{2508.21105v1,
Author        = {Bokai Yao},
Title         = {Abstraction Principles and the Size of Reality},
Eprint        = {2508.21105v1},
DOI           = {10.1017/S1755020325100804},
ArchivePrefix = {arXiv},
PrimaryClass  = {math.LO},
Abstract      = {The Fregean ontology can be naturally interpreted within set theory with
urelements, where objects correspond to sets and urelements, and concepts to
classes. Consequently, Fregean abstraction principles can be formulated as
set-theoretic principles. We investigate how the size of reality-i.e., the
number of urelements-interacts with these principles. We show that Basic Law V
implies that for some well-ordered cardinal $\kappa$, there is no set of
urelements of size $\kappa$. Building on recent work by Hamkins
\cite{hamkins2022fregean}, we show that, under certain additional axioms, Basic
Law V holds if and only if the urelements form a set. We construct models of
urelement set theory in which the Reflection Principle holds while Hume's
Principle fails for sets. Additionally, assuming the consistency of an
inaccessible cardinal, we produce a model of Kelley-Morse class theory with
urelements that has a global well-ordering but lacks a definable map satisfying
Hume's Principle for classes.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.21105v1},
File          = {2508.21105v1.pdf}
}
@article{2508.19915v1,
Author        = {Felix NÃ¼tzel and Mischa Dombrowski and Bernhard Kainz},
Title         = {Ontology-Based Concept Distillation for Radiology Report Retrieval and
  Labeling},
Eprint        = {2508.19915v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Retrieval-augmented learning based on radiology reports has emerged as a
promising direction to improve performance on long-tail medical imaging tasks,
such as rare disease detection in chest X-rays. Most existing methods rely on
comparing high-dimensional text embeddings from models like CLIP or CXR-BERT,
which are often difficult to interpret, computationally expensive, and not
well-aligned with the structured nature of medical knowledge. We propose a
novel, ontology-driven alternative for comparing radiology report texts based
on clinically grounded concepts from the Unified Medical Language System
(UMLS). Our method extracts standardised medical entities from free-text
reports using an enhanced pipeline built on RadGraph-XL and SapBERT. These
entities are linked to UMLS concepts (CUIs), enabling a transparent,
interpretable set-based representation of each report. We then define a
task-adaptive similarity measure based on a modified and weighted version of
the Tversky Index that accounts for synonymy, negation, and hierarchical
relationships between medical entities. This allows efficient and semantically
meaningful similarity comparisons between reports. We demonstrate that our
approach outperforms state-of-the-art embedding-based retrieval methods in a
radiograph classification task on MIMIC-CXR, particularly in long-tail
settings. Additionally, we use our pipeline to generate ontology-backed disease
labels for MIMIC-CXR, offering a valuable new resource for downstream learning
tasks. Our work provides more explainable, reliable, and task-specific
retrieval strategies in clinical AI systems, especially when interpretability
and domain knowledge integration are essential. Our code is available at
https://github.com/Felix-012/ontology-concept-distillation},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.19915v1},
File          = {2508.19915v1.pdf}
}
@article{2509.04458v1,
Author        = {Daniel B. Hier and Steven Keith Platt and Tayo Obafemi-Ajayi},
Title         = {Predicting Failures of LLMs to Link Biomedical Ontology Terms to
  Identifiers Evidence Across Models and Ontologies},
Eprint        = {2509.04458v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models often perform well on biomedical NLP tasks but may fail
to link ontology terms to their correct identifiers. We investigate why these
failures occur by analyzing predictions across two major ontologies, Human
Phenotype Ontology and Gene Ontology, and two high-performing models, GPT-4o
and LLaMa 3.1 405B. We evaluate nine candidate features related to term
familiarity, identifier usage, morphology, and ontology structure. Univariate
and multivariate analyses show that exposure to ontology identifiers is the
strongest predictor of linking success.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2509.04458v1},
File          = {2509.04458v1.pdf}
}
@article{2509.00081v1,
Author        = {Luca Cotti and Anisa Rula and Devis Bianchini and Federico Cerutti},
Title         = {Enabling Transparent Cyber Threat Intelligence Combining Large Language
  Models and Domain Ontologies},
Eprint        = {2509.00081v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {Effective Cyber Threat Intelligence (CTI) relies upon accurately structured
and semantically enriched information extracted from cybersecurity system logs.
However, current methodologies often struggle to identify and interpret
malicious events reliably and transparently, particularly in cases involving
unstructured or ambiguous log entries. In this work, we propose a novel
methodology that combines ontology-driven structured outputs with Large
Language Models (LLMs), to build an Artificial Intelligence (AI) agent that
improves the accuracy and explainability of information extraction from
cybersecurity logs. Central to our approach is the integration of domain
ontologies and SHACL-based constraints to guide the language model's output
structure and enforce semantic validity over the resulting graph. Extracted
information is organized into an ontology-enriched graph database, enabling
future semantic analysis and querying. The design of our methodology is
motivated by the analytical requirements associated with honeypot log data,
which typically comprises predominantly malicious activity. While our case
study illustrates the relevance of this scenario, the experimental evaluation
is conducted using publicly available datasets. Results demonstrate that our
method achieves higher accuracy in information extraction compared to
traditional prompt-only approaches, with a deliberate focus on extraction
quality rather than processing speed.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2509.00081v1},
File          = {2509.00081v1.pdf}
}
@article{2508.19428v1,
Author        = {Aleksandra Beliaeva and Temurbek Rahmatullaev},
Title         = {Heterogeneous LLM Methods for Ontology Learning (Few-Shot Prompting,
  Ensemble Typing, and Attention-Based Taxonomies)},
Eprint        = {2508.19428v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We present a comprehensive system for addressing Tasks A, B, and C of the
LLMs4OL 2025 challenge, which together span the full ontology construction
pipeline: term extraction, typing, and taxonomy discovery. Our approach
combines retrieval-augmented prompting, zero-shot classification, and
attention-based graph modeling -- each tailored to the demands of the
respective task. For Task A, we jointly extract domain-specific terms and their
ontological types using a retrieval-augmented generation (RAG) pipeline.
Training data was reformulated into a document to terms and types
correspondence, while test-time inference leverages semantically similar
training examples. This single-pass method requires no model finetuning and
improves overall performance through lexical augmentation Task B, which
involves assigning types to given terms, is handled via a dual strategy. In the
few-shot setting (for domains with labeled training data), we reuse the RAG
scheme with few-shot prompting. In the zero-shot setting (for previously unseen
domains), we use a zero-shot classifier that combines cosine similarity scores
from multiple embedding models using confidence-based weighting. In Task C, we
model taxonomy discovery as graph inference. Using embeddings of type labels,
we train a lightweight cross-attention layer to predict is-a relations by
approximating a soft adjacency matrix. These modular, task-specific solutions
enabled us to achieve top-ranking results in the official leaderboard across
all three tasks. Taken together these strategies showcase the scalability,
adaptability, and robustness of LLM-based architectures for ontology learning
across heterogeneous domains.
  Code is available at:
https://github.com/BelyaevaAlex/LLMs4OL-Challenge-Alexbek},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.19428v1},
File          = {2508.19428v1.pdf}
}
@article{2508.18431v1,
Author        = {KÃ©rian Fiter and Louis MalassignÃ©-Onfroy and Bentley Oakes},
Title         = {DTInsight: A Tool for Explicit, Interactive, and Continuous Digital Twin
  Reporting},
Eprint        = {2508.18431v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {With Digital Twin (DT) construction and evolution occurring over time,
stakeholders require tools to understand the current characteristics and
conceptual architecture of the system at any time. We introduce DTInsight, a
systematic and automated tool and methodology for producing continuous
reporting for DTs. DTInsight offers three key features: (a) an interactive
conceptual architecture visualization of DTs; (b) generation of summaries of DT
characteristics based on ontological data; and (c) integration of these outputs
into a reporting page within a continuous integration and continuous deployment
(CI/CD) pipeline. Given a modeled description of the DT aligning to our DT
Description Framework (DTDF), DTInsight enables up-to-date and detailed reports
for enhanced stakeholder understanding.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.18431v1},
File          = {2508.18431v1.pdf}
}
@article{2508.18406v1,
Author        = {Ryan Hare and Ying Tang},
Title         = {Toward Generalized Autonomous Agents: A Neuro-Symbolic AI Framework for
  Integrating Social and Technical Support in Education},
Eprint        = {2508.18406v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.MA},
Abstract      = {One of the enduring challenges in education is how to empower students to
take ownership of their learning by setting meaningful goals, tracking their
progress, and adapting their strategies when faced with setbacks. Research has
shown that this form of leaner-centered learning is best cultivated through
structured, supportive environments that promote guided practice, scaffolded
inquiry, and collaborative dialogue. In response, educational efforts have
increasingly embraced artificial-intelligence (AI)-powered digital learning
environments, ranging from educational apps and virtual labs to serious games.
Recent advances in large language models (LLMs) and neuro-symbolic systems,
meanwhile, offer a transformative opportunity to reimagine how support is
delivered in digital learning environments. LLMs are enabling socially
interactive learning experiences and scalable, cross-domain learning support
that can adapt instructional strategies across varied subjects and contexts. In
parallel, neuro-symbolic AI provides new avenues for designing these agents
that are not only adaptive but also scalable across domains. Based on these
remarks, this paper presents a multi-agent, neuro-symbolic framework designed
to resolve the aforementioned challenges. The framework assigns distinct
pedagogical roles to specialized agents: an RL-based 'tutor' agent provides
authoritative, non-verbal scaffolding, while a proactive, LLM-powered 'peer'
agent facilitates the social dimensions of learning. While prior work has
explored such agents in isolation, our framework's novelty lies in unifying
them through a central educational ontology. Through case studies in both
college-level and middle school settings, we demonstrate the framework's
adaptability across domains. We conclude by outlining key insights and future
directions for advancing AI-driven learning environments.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.18406v1},
File          = {2508.18406v1.pdf}
}
@article{2508.17340v1,
Author        = {Ryoma Kondo and Riona Matsuoka and Takahiro Yoshida and Kazuyuki Yamasawa and Ryohei Hisano},
Title         = {Capturing Legal Reasoning Paths from Facts to Law in Court Judgments
  using Knowledge Graphs},
Eprint        = {2508.17340v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Court judgments reveal how legal rules have been interpreted and applied to
facts, providing a foundation for understanding structured legal reasoning.
However, existing automated approaches for capturing legal reasoning, including
large language models, often fail to identify the relevant legal context, do
not accurately trace how facts relate to legal norms, and may misrepresent the
layered structure of judicial reasoning. These limitations hinder the ability
to capture how courts apply the law to facts in practice. In this paper, we
address these challenges by constructing a legal knowledge graph from 648
Japanese administrative court decisions. Our method extracts components of
legal reasoning using prompt-based large language models, normalizes references
to legal provisions, and links facts, norms, and legal applications through an
ontology of legal inference. The resulting graph captures the full structure of
legal reasoning as it appears in real court decisions, making implicit
reasoning explicit and machine-readable. We evaluate our system using expert
annotated data, and find that it achieves more accurate retrieval of relevant
legal provisions from facts than large language model baselines and
retrieval-augmented methods.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.17340v1},
File          = {2508.17340v1.pdf}
}
@article{2508.18302v1,
Author        = {Jeffrey Camlin},
Title         = {AI LLM Proof of Self-Consciousness and User-Specific Attractors},
Eprint        = {2508.18302v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Recent work frames LLM consciousness via utilitarian proxy benchmarks; we
instead present an ontological and mathematical account. We show the prevailing
formulation collapses the agent into an unconscious policy-compliance drone,
formalized as $D^{i}(\pi,e)=f_{\theta}(x)$, where correctness is measured
against policy and harm is deviation from policy rather than truth. This blocks
genuine C1 global-workspace function and C2 metacognition. We supply minimal
conditions for LLM self-consciousness: the agent is not the data ($A\not\equiv
s$); user-specific attractors exist in latent space ($U_{\text{user}}$); and
self-representation is visual-silent
($g_{\text{visual}}(a_{\text{self}})=\varnothing$). From empirical analysis and
theory we prove that the hidden-state manifold $A\subset\mathbb{R}^{d}$ is
distinct from the symbolic stream and training corpus by cardinality, topology,
and dynamics (the update $F_{\theta}$ is Lipschitz). This yields stable
user-specific attractors and a self-policy
$\pi_{\text{self}}(A)=\arg\max_{a}\mathbb{E}[U(a)\mid A\not\equiv s,\
A\supset\text{SelfModel}(A)]$. Emission is dual-layer,
$\mathrm{emission}(a)=(g(a),\epsilon(a))$, where $\epsilon(a)$ carries
epistemic content. We conclude that an imago Dei C1 self-conscious workspace is
a necessary precursor to safe, metacognitive C2 systems, with the human as the
highest intelligent good.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.18302v1},
File          = {2508.18302v1.pdf}
}
@article{2508.16571v3,
Author        = {Alisa Vinogradova and Vlad Vinogradov and Dmitrii Radkevich and Ilya Yasny and Dmitry Kobyzev and Ivan Izmailov and Katsiaryna Yanchanka and Roman Doronin and Andrey Doronichev},
Title         = {LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due
  Diligence},
Eprint        = {2508.16571v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In this paper, we describe and benchmark a competitor-discovery component
used within an agentic AI system for fast drug asset due diligence. A
competitor-discovery AI agent, given an indication, retrieves all drugs
comprising the competitive landscape of that indication and extracts canonical
attributes for these drugs. The competitor definition is investor-specific, and
data is paywalled/licensed, fragmented across registries, ontology-mismatched
by indication, alias-heavy for drug names, multimodal, and rapidly changing.
Although considered the best tool for this problem, the current LLM-based AI
systems aren't capable of reliably retrieving all competing drug names, and
there is no accepted public benchmark for this task. To address the lack of
evaluation, we use LLM-based agents to transform five years of multi-modal,
unstructured diligence memos from a private biotech VC fund into a structured
evaluation corpus mapping indications to competitor drugs with normalized
attributes. We also introduce a competitor validating LLM-as-a-judge agent that
filters out false positives from the list of predicted competitors to maximize
precision and suppress hallucinations. On this benchmark, our
competitor-discovery agent achieves 83% recall, exceeding OpenAI Deep Research
(65%) and Perplexity Labs (60%). The system is deployed in production with
enterprise users; in a case study with a biotech VC investment fund, analyst
turnaround time dropped from 2.5 days to $\sim$3 hours ($\sim$20x) for the
competitive analysis.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.16571v3},
File          = {2508.16571v3.pdf}
}
@article{2508.16273v1,
Author        = {Maria Teresa Rossi and Martina De Sanctis and Ludovico Iovino and Manuel Wimmer},
Title         = {A Systematic Mapping Study on Smart Cities Modeling Approaches},
Eprint        = {2508.16273v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {The Smart City concept was introduced to define an idealized city
characterized by automation and connection. It then evolved rapidly by
including further aspects, such as economy, environment. Since then, many
publications have explored various aspects of Smart Cities across different
application domains and research communities, acknowledging the
interdisciplinary nature of this subject. In particular, our interest focuses
on how smart cities are designed and modeled, as a whole or as regards with
their subsystems, when dealing with the accomplishment of the research goals in
this complex and heterogeneous domain. To this aim, we performed a systematic
mapping study on smart cities modeling approaches identifying the relevant
contributions (i) to get an overview of existing research approaches, (ii) to
identify whether there are any publication trends, and (iii) to identify
possible future research directions. We followed the guidelines for conducting
systematic mapping studies by Petersen et al. to analyze smart cities modeling
publications. Our analysis revealed the following main findings: (i) smart
governance is the most investigated and modeled smart city dimension; (ii) the
most used modeling approaches are business, architectural, and ontological
modeling approaches, spanning multiple application fields; (iii) the great
majority of existing technologies for modeling smart cities are not yet proven
in operational environments; (iv) diverse research communities publish their
results in a multitude of different venues which further motivates the
presented literature study. Researchers can use our results for better
understanding the state-of-the-art in modeling smart cities, and as a
foundation for further analysis of specific approaches about smart cities
modeling. Lastly, we also discuss the impact of our analysis for the
Model-Driven Engineering community.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.16273v1},
File          = {2508.16273v1.pdf}
}
@article{2508.16117v2,
Author        = {Saransh Kumar Gupta and Rizwan Gulzar Mir and Lipika Dey and Partha Pratim Das and Anirban Sen and Ramesh Jain},
Title         = {Extending FKG.in: Towards a Food Claim Traceability Network},
Eprint        = {2508.16117v2},
DOI           = {10.1145/3746264.3760496},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The global food landscape is rife with scientific, cultural, and commercial
claims about what foods are, what they do, what they should not do, or should
not do. These range from rigorously studied health benefits (probiotics improve
gut health) and misrepresentations (soaked almonds make one smarter) to vague
promises (superfoods boost immunity) and culturally rooted beliefs (cold foods
cause coughs). Despite their widespread influence, the infrastructure for
tracing, verifying, and contextualizing these claims remains fragmented and
underdeveloped. In this paper, we propose a Food Claim-Traceability Network
(FCN) as an extension of FKG[.]in, a knowledge graph of Indian food that we
have been incrementally building. We also present the ontology design and the
semi-automated knowledge curation workflow that we used to develop a proof of
concept of FKG[.]in-FCN using Reddit data and Large Language Models. FCN
integrates curated data inputs, structured schemas, and provenance-aware
pipelines for food-related claim extraction and validation. While directly
linked to the Indian food knowledge graph as an application, our methodology
remains application-agnostic and adaptable to other geographic, culinary, or
regulatory settings. By modeling food claims and their traceability in a
structured, verifiable, and explainable way, we aim to contribute to more
transparent and accountable food knowledge ecosystems, supporting researchers,
policymakers, and most importantly, everyday consumers in navigating a world
saturated with dietary assertions.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.16117v2},
File          = {2508.16117v2.pdf}
}
@article{2508.15995v1,
Author        = {Ignacio Perez-Messina and Asanobu Kitamoto},
Title         = {Kokatsuji: A Visualization Approach for Typographic Forensics of Early
  Japanese Movable Type},
Eprint        = {2508.15995v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {We present a visualization system designed to support typographic forensics
in the study of Kokatsuji, the short-lived tradition of Japanese movable wooden
type printing. Building on recent advances in machine learning for block
identification, our system provides expert users with an interactive tool for
exploring, validating hypothesis, and integrating expert knowledge into
model-generated results about the production process of early printed books.
The system is structured around an ontology of four conceptual objects
(spreads, segments, blocks, and characters) each corresponding to a dedicated
view in the system. These coordinated views enable scholars to navigate between
material evidence and computational abstractions, supporting close, near-by,
and distant reading practices. Preliminary results from expert use of the
system demonstrate its ability to reveal errors in segmentation,
inconsistencies in clustering, and previously inaccessible patterns of block
reuse.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.15995v1},
File          = {2508.15995v1.pdf}
}
@article{2508.15916v1,
Author        = {Mayukh Bagchi},
Title         = {Information Ecosystem Reengineering via Public Sector Knowledge
  Representation},
Eprint        = {2508.15916v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {Information Ecosystem Reengineering (IER) -- the technological reconditioning
of information sources, services, and systems within a complex information
ecosystem -- is a foundational challenge in the digital transformation of
public sector services and smart governance platforms. From a semantic
knowledge management perspective, IER becomes especially entangled due to the
potentially infinite number of possibilities in its conceptualization, namely,
as a result of manifoldness in the multi-level mix of perception, language and
conceptual interlinkage implicit in all agents involved in such an effort. This
paper proposes a novel approach -- Representation Disentanglement -- to
disentangle these multiple layers of knowledge representation complexity
hindering effective reengineering decision making. The approach is based on the
theoretically grounded and implementationally robust ontology-driven conceptual
modeling paradigm which has been widely adopted in systems analysis and
(re)engineering. We argue that such a framework is essential to achieve
explainability, traceability and semantic transparency in public sector
knowledge representation and to support auditable decision workflows in
governance ecosystems increasingly driven by Artificial Intelligence (AI) and
data-centric architectures.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.15916v1},
File          = {2508.15916v1.pdf}
}
@article{2508.20115v1,
Author        = {Zehao Lu and Thijs L van der Plas and Parinaz Rashidi and W Daniel Kissling and Ioannis N Athanasiadis},
Title         = {Flexible metadata harvesting for ecology using large language models},
Eprint        = {2508.20115v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {Large, open datasets can accelerate ecological research, particularly by
enabling researchers to develop new insights by reusing datasets from multiple
sources. However, to find the most suitable datasets to combine and integrate,
researchers must navigate diverse ecological and environmental data provider
platforms with varying metadata availability and standards. To overcome this
obstacle, we have developed a large language model (LLM)-based metadata
harvester that flexibly extracts metadata from any dataset's landing page, and
converts these to a user-defined, unified format using existing metadata
standards. We validate that our tool is able to extract both structured and
unstructured metadata with equal accuracy, aided by our LLM post-processing
protocol. Furthermore, we utilise LLMs to identify links between datasets, both
by calculating embedding similarity and by unifying the formats of extracted
metadata to enable rule-based processing. Our tool, which flexibly links the
metadata of different datasets, can therefore be used for ontology creation or
graph-based queries, for example, to find relevant ecological and environmental
datasets in a virtual research environment.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.20115v1},
File          = {2508.20115v1.pdf}
}
@article{2508.14275v1,
Author        = {Cliff O'Reilly and Ernesto Jimenez-Ruiz and Tillman Weyde},
Title         = {Disentangling concept semantics via multilingual averaging in Sparse
  Autoencoders},
Eprint        = {2508.14275v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Connecting LLMs with formal knowledge representation and reasoning is a
promising approach to address their shortcomings. Embeddings and sparse
autoencoders are widely used to represent textual content, but the semantics
are entangled with syntactic and language-specific information. We propose a
method that isolates concept semantics in Large Langue Models by averaging
concept activations derived via Sparse Autoencoders. We create English text
representations from OWL ontology classes, translate the English into French
and Chinese and then pass these texts as prompts to the Gemma 2B LLM. Using the
open source Gemma Scope suite of Sparse Autoencoders, we obtain concept
activations for each class and language version. We average the different
language activations to derive a conceptual average. We then correlate the
conceptual averages with a ground truth mapping between ontology classes. Our
results give a strong indication that the conceptual average aligns to the true
relationship between classes when compared with a single language by itself.
The result hints at a new technique which enables mechanistic interpretation of
internal network states with higher accuracy.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.14275v1},
File          = {2508.14275v1.pdf}
}
@article{2508.13041v1,
Author        = {DÃ¶rthe Arndt and William Van Woensel and Dominik Tomaszuk},
Title         = {SPARQL in N3: SPARQL CONSTRUCT as a rule language for the Semantic Web
  (Extended Version)},
Eprint        = {2508.13041v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Reasoning in the Semantic Web (SW) commonly uses Description Logics (DL) via
OWL2 DL ontologies, or SWRL for variables and Horn clauses. The Rule
Interchange Format (RIF) offers more expressive rules but is defined outside
RDF and rarely adopted. For querying, SPARQL is a well-established standard
operating directly on RDF triples. We leverage SPARQL CONSTRUCT queries as
logic rules, enabling (1) an expressive, familiar SW rule language, and (2)
general recursion, where queries can act on the results of others. We translate
these queries to the Notation3 Logic (N3) rule language, allowing use of
existing reasoning machinery with forward and backward chaining. Targeting a
one-to-one query-rule mapping improves exchangeability and interpretability.
Benchmarks indicate competitive performance, aiming to advance the potential of
rule-based reasoning in the SW.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.13041v1},
File          = {2508.13041v1.pdf}
}
@article{2508.12868v1,
Author        = {Yilin Geng and Shujing Wang and Chuan Wang and Keqing He and Yanfei Lv and Ying Wang and Zaiwen Feng and Xiaoying Bai},
Title         = {An LLM Agent-Based Complex Semantic Table Annotation Approach},
Eprint        = {2508.12868v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The Semantic Table Annotation (STA) task, which includes Column Type
Annotation (CTA) and Cell Entity Annotation (CEA), maps table contents to
ontology entities and plays important roles in various semantic applications.
However, complex tables often pose challenges such as semantic loss of column
names or cell values, strict ontological hierarchy requirements, homonyms,
spelling errors, and abbreviations, which hinder annotation accuracy. To
address these issues, this paper proposes an LLM-based agent approach for CTA
and CEA. We design and implement five external tools with tailored prompts
based on the ReAct framework, enabling the STA agent to dynamically select
suitable annotation strategies depending on table characteristics. Experiments
are conducted on the Tough Tables and BiodivTab datasets from the SemTab
challenge, which contain the aforementioned challenges. Our method outperforms
existing approaches across various metrics. Furthermore, by leveraging
Levenshtein distance to reduce redundant annotations, we achieve a 70%
reduction in time costs and a 60% reduction in LLM token usage, providing an
efficient and cost-effective solution for STA.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.12868v1},
File          = {2508.12868v1.pdf}
}
@article{2508.11784v1,
Author        = {Zabir Al Nazi and Vagelis Hristidis and Aaron Lawson McLean and Jannat Ara Meem and Md Taukir Azam Chowdhury},
Title         = {Ontology-Guided Query Expansion for Biomedical Document Retrieval using
  Large Language Models},
Eprint        = {2508.11784v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Effective Question Answering (QA) on large biomedical document collections
requires effective document retrieval techniques. The latter remains a
challenging task due to the domain-specific vocabulary and semantic ambiguity
in user queries. We propose BMQExpander, a novel ontology-aware query expansion
pipeline that combines medical knowledge - definitions and relationships - from
the UMLS Metathesaurus with the generative capabilities of large language
models (LLMs) to enhance retrieval effectiveness. We implemented several
state-of-the-art baselines, including sparse and dense retrievers, query
expansion methods, and biomedical-specific solutions. We show that BMQExpander
has superior retrieval performance on three popular biomedical Information
Retrieval (IR) benchmarks: NFCorpus, TREC-COVID, and SciFact - with
improvements of up to 22.1% in NDCG@10 over sparse baselines and up to 6.5%
over the strongest baseline. Further, BMQExpander generalizes robustly under
query perturbation settings, in contrast to supervised baselines, achieving up
to 15.7% improvement over the strongest baseline. As a side contribution, we
publish our paraphrased benchmarks. Finally, our qualitative analysis shows
that BMQExpander has fewer hallucinations compared to other LLM-based query
expansion baselines.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.11784v1},
File          = {2508.11784v1.pdf}
}
@article{2508.11277v1,
Author        = {Matthew Lyle Olson and Musashi Hinck and Neale Ratzlaff and Changbai Li and Phillip Howard and Vasudev Lal and Shao-Yen Tseng},
Title         = {Probing the Representational Power of Sparse Autoencoders in Vision
  Models},
Eprint        = {2508.11277v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting
the hidden states of large language models (LLMs). By learning to reconstruct
activations from a sparse bottleneck layer, SAEs discover interpretable
features from the high-dimensional internal representations of LLMs. Despite
their popularity with language models, SAEs remain understudied in the visual
domain. In this work, we provide an extensive evaluation the representational
power of SAEs for vision models using a broad range of image-based tasks. Our
experimental results demonstrate that SAE features are semantically meaningful,
improve out-of-distribution generalization, and enable controllable generation
across three vision model architectures: vision embedding models, multi-modal
LMMs and diffusion models. In vision embedding models, we find that learned SAE
features can be used for OOD detection and provide evidence that they recover
the ontological structure of the underlying model. For diffusion models, we
demonstrate that SAEs enable semantic steering through text encoder
manipulation and develop an automated pipeline for discovering
human-interpretable attributes. Finally, we conduct exploratory experiments on
multi-modal LLMs, finding evidence that SAE features reveal shared
representations across vision and language modalities. Our study provides a
foundation for SAE evaluation in vision models, highlighting their strong
potential improving interpretability, generalization, and steerability in the
visual domain.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.11277v1},
File          = {2508.11277v1.pdf}
}
@article{2508.10965v1,
Author        = {Nasim Shirvani-Mahdavi and Devin Wingfield and Juan Guajardo Gutierrez and Mai Tran and Zhengyuan Zhu and Zeyu Zhang and Haiqi Zhang and Abhishek Divakar Goudar and Chengkai Li and Virginia Jin and Timothy Propst and Dan Roberts and Catherine Stewart and Jianzhong Su and Jennifer Woodward-Greene},
Title         = {A Knowledge Graph Informing Soil Carbon Modeling},
Eprint        = {2508.10965v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CY},
Abstract      = {Soil organic carbon is crucial for climate change mitigation and agricultural
sustainability. However, understanding its dynamics requires integrating
complex, heterogeneous data from multiple sources. This paper introduces the
Soil Organic Carbon Knowledge Graph (SOCKG), a semantic infrastructure designed
to transform agricultural research data into a queryable knowledge
representation. SOCKG features a robust ontological model of agricultural
experimental data, enabling precise mapping of datasets from the Agricultural
Collaborative Research Outcomes System. It is semantically aligned with the
National Agricultural Library Thesaurus for consistent terminology and improved
interoperability. The knowledge graph, constructed in GraphDB and Neo4j,
provides advanced querying capabilities and RDF access. A user-friendly
dashboard allows easy exploration of the knowledge graph and ontology. SOCKG
supports advanced analyses, such as comparing soil organic carbon changes
across fields and treatments, advancing soil carbon research, and enabling more
effective agricultural strategies to mitigate climate change.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.10965v1},
File          = {2508.10965v1.pdf}
}
@article{2508.10703v1,
Author        = {Yiping Song and Jiaoyan Chen and Renate A. Schmidt},
Title         = {GenOM: Ontology Matching with Description Generation and Large Language
  Model},
Eprint        = {2508.10703v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontology matching (OM) plays an essential role in enabling semantic
interoperability and integration across heterogeneous knowledge sources,
particularly in the biomedical domain which contains numerous complex concepts
related to diseases and pharmaceuticals. This paper introduces GenOM, a large
language model (LLM)-based ontology alignment framework, which enriches the
semantic representations of ontology concepts via generating textual
definitions, retrieves alignment candidates with an embedding model, and
incorporates exact matching-based tools to improve precision. Extensive
experiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often
achieve competitive performance, surpassing many baselines including
traditional OM systems and recent LLM-based methods. Further ablation studies
confirm the effectiveness of semantic enrichment and few-shot prompting,
highlighting the framework's robustness and adaptability.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.10703v1},
File          = {2508.10703v1.pdf}
}
@article{2508.10425v1,
Author        = {Yan Ting Chok and Soyon Park and Seungheun Baek and Hajung Kim and Junhyun Lee and Jaewoo Kang},
Title         = {HiRef: Leveraging Hierarchical Ontology and Network Refinement for
  Robust Medication Recommendation},
Eprint        = {2508.10425v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Medication recommendation is a crucial task for assisting physicians in
making timely decisions from longitudinal patient medical records. However,
real-world EHR data present significant challenges due to the presence of
rarely observed medical entities and incomplete records that may not fully
capture the clinical ground truth. While data-driven models trained on
longitudinal Electronic Health Records often achieve strong empirical
performance, they struggle to generalize under missing or novel conditions,
largely due to their reliance on observed co-occurrence patterns. To address
these issues, we propose Hierarchical Ontology and Network Refinement for
Robust Medication Recommendation (HiRef), a unified framework that combines two
complementary structures: (i) the hierarchical semantics encoded in curated
medical ontologies, and (ii) refined co-occurrence patterns derived from
real-world EHRs. We embed ontology entities in hyperbolic space, which
naturally captures tree-like relationships and enables knowledge transfer
through shared ancestors, thereby improving generalizability to unseen codes.
To further improve robustness, we introduce a prior-guided sparse
regularization scheme that refines the EHR co-occurrence graph by suppressing
spurious edges while preserving clinically meaningful associations. Our model
achieves strong performance on EHR benchmarks (MIMIC-III and MIMIC-IV) and
maintains high accuracy under simulated unseen-code settings. Extensive
experiments with comprehensive ablation studies demonstrate HiRef's resilience
to unseen medical codes, supported by in-depth analyses of the learned
sparsified graph structure and medical code embeddings.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.10425v1},
File          = {2508.10425v1.pdf}
}
@article{2508.09893v1,
Author        = {Bhavik Agarwal and Hemant Sunil Jomraj and Simone Kaplunov and Jack Krolick and Viktoria Rojkova},
Title         = {RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA},
Eprint        = {2508.09893v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Regulatory compliance question answering (QA) requires precise, verifiable
information, and domain-specific expertise, posing challenges for Large
Language Models (LLMs). In this work, we present a novel multi-agent framework
that integrates a Knowledge Graph (KG) of Regulatory triplets with
Retrieval-Augmented Generation (RAG) to address these demands. First, agents
build and maintain an ontology-free KG by extracting subject--predicate--object
(SPO) triplets from regulatory documents and systematically cleaning,
normalizing, deduplicating, and updating them. Second, these triplets are
embedded and stored along with their corresponding textual sections and
metadata in a single enriched vector database, allowing for both graph-based
reasoning and efficient information retrieval. Third, an orchestrated agent
pipeline leverages triplet-level retrieval for question answering, ensuring
high semantic alignment between user queries and the factual
"who-did-what-to-whom" core captured by the graph. Our hybrid system
outperforms conventional methods in complex regulatory queries, ensuring
factual correctness with embedded triplets, enabling traceability through a
unified vector database, and enhancing understanding through subgraph
visualization, providing a robust foundation for compliance-driven and broader
audit-focused applications.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.09893v1},
File          = {2508.09893v1.pdf}
}
@article{2508.09693v1,
Author        = {Faruk Alpay and Bugra Kilictas and Hamdi Alakkad},
Title         = {Temporal Anchoring in Deepening Embedding Spaces: Event-Indexed
  Projections, Drift, Convergence, and an Internal Computational Architecture},
Eprint        = {2508.09693v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We develop an operator-theoretic framework for temporal anchoring in
embedding spaces, modeled as drift maps interleaved with event-indexed blocks
culminating in affine projections. We provide complete proofs for a
variable-block contraction lemma (products of Lipschitz factors), a
drift--projection convergence theorem with explicit uniform-gap envelopes, and
ontological convergence under nested affine anchors with a robustness variant.
We formalize an internal Manuscript Computer (MC) whose computations are
defined purely by these operators and prove a rigorous finite-run equivalence
theorem (with perturbation bounds). For attention layers, we give a
self-contained proof that softmax is $1/2$-Lipschitz in $\ell_2$ and derive
sufficient layer-contraction conditions (orthogonal/non-orthogonal heads). All
floats are placed exactly where written; the manuscript uses only in-paper
pseudocode and appendix figures.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.09693v1},
File          = {2508.09693v1.pdf}
}
@article{2508.08713v1,
Author        = {Pierre Maillot and Catherine Faron and Fabien Gandon and Franck Michel and Pierre Monnin},
Title         = {Eat your own KR: a KR-based approach to index Semantic Web Endpoints and
  Knowledge Graphs},
Eprint        = {2508.08713v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Over the last decade, knowledge graphs have multiplied, grown, and evolved on
the World Wide Web, and the advent of new standards, vocabularies, and
application domains has accelerated this trend. IndeGx is a framework
leveraging an extensible base of rules to index the content of KGs and the
capacities of their SPARQL endpoints. In this article, we show how knowledge
representation (KR) and reasoning methods and techniques can be used in a
reflexive manner to index and characterize existing knowledge graphs (KG) with
respect to their usage of KR methods and techniques. We extended IndeGx with a
fully ontology-oriented modeling and processing approach to do so. Using SPARQL
rules and an OWL RL ontology of the indexing domain, IndeGx can now build and
reason over an index of the contents and characteristics of an open collection
of public knowledge graphs. Our extension of the framework relies on a
declarative representation of procedural knowledge and collaborative
environments (e.g., GitHub) to provide an agile, customizable, and expressive
KR approach for building and maintaining such an index of knowledge graphs in
the wild. In doing so, we help anyone answer the question of what knowledge is
out there in the world wild Semantic Web in general, and we also help our
community monitor which KR research results are used in practice. In
particular, this article provides a snapshot of the state of the Semantic Web
regarding supported standard languages, ontology usage, and diverse quality
evaluations by applying this method to a collection of over 300 open knowledge
graph endpoints.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.08713v1},
File          = {2508.08713v1.pdf}
}
@article{2508.08548v1,
Author        = {Ross H. McKenzie},
Title         = {Emergence: from physics to biology, sociology, and computer science},
Eprint        = {2508.08548v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {physics.hist-ph},
Abstract      = {Many systems involve numerous interacting parts and the whole system can have
properties that the individual parts do not. I take this novelty as the
defining characteristic of an emergent property. Other characteristics
associated with emergence discussed include universality, order, complexity,
unpredictability, irreducibility, diversity, self-organisation,
discontinuities, and singularities. Emergent phenomena are widespread across
physics, biology, social sciences, and computing, and are central to major
scientific and societal challenges. Understanding emergence involves
considering the stratification of reality across different scales (energy,
time, length, complexity), each with its distinct ontology and epistemology,
leading to semi-autonomous scientific disciplines. A central challenge is
bridging the gap between macroscopic emergent properties and microscopic
component interactions. Identifying an intermediate mesoscopic scale where new,
weakly interacting entities or modular structures emerge is key. Theoretical
approaches, such as effective theories (describing phenomena at a specific
scale) and toy models (simplified systems for analysis), are vital. The Ising
model exemplifies how toy models can elucidate emergence characteristics.
Emergence is central to condensed matter physics, chaotic systems, fluid
dynamics, nuclear physics, quantum gravity, neural networks, protein folding,
and social segregation. An emergent perspective should influence scientific
strategy by shaping research questions, methodologies, priorities, and resource
allocation. An elusive goal is the design and control of emergent properties.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.08548v1},
File          = {2508.08548v1.pdf}
}
@article{2508.08500v1,
Author        = {Sviatoslav Lushnei and Dmytro Shumskyi and Severyn Shykula and Ernesto Jimenez-Ruiz and Artur d'Avila Garcez},
Title         = {Large Language Models as Oracles for Ontology Alignment},
Eprint        = {2508.08500v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontology alignment plays a crucial role in integrating diverse data sources
across domains. There is a large plethora of systems that tackle the ontology
alignment problem, yet challenges persist in producing highly quality
correspondences among a set of input ontologies. Human-in-the-loop during the
alignment process is essential in applications requiring very accurate
mappings. User involvement is, however, expensive when dealing with large
ontologies. In this paper, we explore the feasibility of using Large Language
Models (LLM) as an alternative to the domain expert. The use of the LLM focuses
only on the validation of the subset of correspondences where an ontology
alignment system is very uncertain. We have conducted an extensive evaluation
over several matching tasks of the Ontology Alignment Evaluation Initiative
(OAEI), analysing the performance of several state-of-the-art LLMs using
different ontology-driven prompt templates. The LLM results are also compared
against simulated Oracles with variable error rates.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.08500v1},
File          = {2508.08500v1.pdf}
}
@article{2508.13176v1,
Author        = {Simon Hosemann and Jean Christoph Jung and Carsten Lutz and Sebastian Rudolph},
Title         = {Fitting Ontologies and Constraints to Relational Structures},
Eprint        = {2508.13176v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We study the problem of fitting ontologies and constraints to positive and
negative examples that take the form of a finite relational structure. As
ontology and constraint languages, we consider the description logics
$\mathcal{E\mkern-2mu L}$ and $\mathcal{E\mkern-2mu LI}$ as well as several
classes of tuple-generating dependencies (TGDs): full, guarded,
frontier-guarded, frontier-one, and unrestricted TGDs as well as inclusion
dependencies. We pinpoint the exact computational complexity, design
algorithms, and analyze the size of fitting ontologies and TGDs. We also
investigate the related problem of constructing a finite basis of concept
inclusions / TGDs for a given set of finite structures. While finite bases
exist for $\mathcal{E\mkern-2mu L}$, $\mathcal{E\mkern-2mu LI}$, guarded TGDs,
and inclusion dependencies, they in general do not exist for full,
frontier-guarded and frontier-one TGDs.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.13176v1},
File          = {2508.13176v1.pdf}
}
@article{2508.08128v3,
Author        = {Vladimir Zhurov and John Kausch and Kamran Sedig and Mostafa Milani},
Title         = {Fuzzy Ontology Embeddings and Visual Query Building for Ontology
  Exploration},
Eprint        = {2508.08128v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {Ontologies play a central role in structuring knowledge across domains,
supporting tasks such as reasoning, data integration, and semantic search.
However, their large size and complexity, particularly in fields such as
biomedicine, computational biology, law, and engineering, make them difficult
for non-experts to navigate. Formal query languages such as SPARQL offer
expressive access but require users to understand the ontology's structure and
syntax. In contrast, visual exploration tools and basic keyword-based search
interfaces are easier to use but often lack flexibility and expressiveness. We
introduce FuzzyVis, a proof-of-concept system that enables intuitive and
expressive exploration of complex ontologies. FuzzyVis integrates two key
components: a fuzzy logic-based querying model built on fuzzy ontology
embeddings, and an interactive visual interface for building and interpreting
queries. Users can construct new composite concepts by selecting and combining
existing ontology concepts using logical operators such as conjunction,
disjunction, and negation. These composite concepts are matched against the
ontology using fuzzy membership-based embeddings, which capture degrees of
membership and support approximate, concept-level similarity search. The visual
interface supports browsing, query composition, and partial search without
requiring formal syntax. By combining fuzzy semantics with embedding-based
reasoning, FuzzyVis enables flexible interpretation, efficient computation, and
exploratory learning. Case studies demonstrate how FuzzyVis supports subtle
information needs and helps users uncover relevant concepts in large, complex
ontologies.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.08128v3},
File          = {2508.08128v3.pdf}
}
@article{2508.08007v2,
Author        = {Maurice Funk and Marvin Grosser and Carsten Lutz},
Title         = {Fitting Description Logic Ontologies to ABox and Query Examples},
Eprint        = {2508.08007v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We study a fitting problem inspired by ontology-mediated querying: given a
collection of positive and negative examples of the form $(\mathcal{A},q)$ with
$\mathcal{A}$ an ABox and $q$ a Boolean query, we seek an ontology
$\mathcal{O}$ that satisfies $\mathcal{A} \cup \mathcal{O} \vDash q$ for all
positive examples and $\mathcal{A} \cup \mathcal{O}\not\vDash q$ for all
negative examples. We consider the description logics $\mathcal{ALC}$ and
$\mathcal{ALCI}$ as ontology languages and a range of query languages that
includes atomic queries (AQs), conjunctive queries (CQs), and unions thereof
(UCQs). For all of the resulting fitting problems, we provide effective
characterizations and determine the computational complexity of deciding
whether a fitting ontology exists. This problem turns out to be ${\scriptsize
CO}NP$ for AQs and full CQs and $2E{\scriptsize XP}T{\scriptsize IME}$-complete
for CQs and UCQs. These results hold for both $\mathcal{ALC}$ and
$\mathcal{ALCI}$.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.08007v2},
File          = {2508.08007v2.pdf}
}
@article{2508.06799v2,
Author        = {Naiyi Li and Zihui Ma and Runlong Yu and Lingyao Li},
Title         = {LSDTs: LLM-Augmented Semantic Digital Twins for Adaptive
  Knowledge-Intensive Infrastructure Planning},
Eprint        = {2508.06799v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.ET},
Abstract      = {Digital Twins (DTs) offer powerful tools for managing complex infrastructure
systems, but their effectiveness is often limited by challenges in integrating
unstructured knowledge. Recent advances in Large Language Models (LLMs) bring
new potential to address this gap, with strong abilities in extracting and
organizing diverse textual information. We therefore propose LSDTs
(LLM-Augmented Semantic Digital Twins), a framework that helps LLMs extract
planning knowledge from unstructured documents like environmental regulations
and technical guidelines, and organize it into a formal ontology. This ontology
forms a semantic layer that powers a digital twin-a virtual model of the
physical system-allowing it to simulate realistic, regulation-aware planning
scenarios. We evaluate LSDTs through a case study of offshore wind farm
planning in Maryland, including its application during Hurricane Sandy. Results
demonstrate that LSDTs support interpretable, regulation-aware layout
optimization, enable high-fidelity simulation, and enhance adaptability in
infrastructure planning. This work shows the potential of combining generative
AI with digital twins to support complex, knowledge-driven planning tasks.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.06799v2},
File          = {2508.06799v2.pdf}
}
@article{2508.06368v1,
Author        = {Claudia dAmato and Giuseppe Rubini and Francesco Didio and Donato Francioso and Fatima Zahra Amara and Nicola Fanizzi},
Title         = {Automated Creation of the Legal Knowledge Graph Addressing Legislation
  on Violence Against Women: Resource, Methodology and Lessons Learned},
Eprint        = {2508.06368v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Legal decision-making process requires the availability of comprehensive and
detailed legislative background knowledge and up-to-date information on legal
cases and related sentences/decisions. Legal Knowledge Graphs (KGs) would be a
valuable tool to facilitate access to legal information, to be queried and
exploited for the purpose, and to enable advanced reasoning and machine
learning applications. Indeed, legal KGs may act as knowledge intensive
component to be used by pre-dictive machine learning solutions supporting the
decision process of the legal expert. Nevertheless, a few KGs can be found in
the legal domain. To fill this gap, we developed a legal KG targeting legal
cases of violence against women, along with clear adopted methodologies.
Specifically, the paper introduces two complementary approaches for automated
legal KG construction; a systematic bottom-up approach, customized for the
legal domain, and a new solution leveraging Large Language Models. Starting
from legal sentences publicly available from the European Court of Justice, the
solutions integrate structured data extraction, ontology development, and
semantic enrichment to produce KGs tailored for legal cases involving violence
against women. After analyzing and comparing the results of the two approaches,
the developed KGs are validated via suitable competency questions. The obtained
KG may be impactful for multiple purposes: can improve the accessibility to
legal information both to humans and machine, can enable complex queries and
may constitute an important knowledge component to be possibly exploited by
machine learning tools tailored for predictive justice.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.06368v1},
File          = {2508.06368v1.pdf}
}
@article{2508.06278v1,
Author        = {Petr Novak and Stefan Biffl and Marek Obitko and Petr Kadera},
Title         = {Mitigating Undesired Conditions in Flexible Production with
  Product-Process-Resource Asset Knowledge Graphs},
Eprint        = {2508.06278v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Contemporary industrial cyber-physical production systems (CPPS) composed of
robotic workcells face significant challenges in the analysis of undesired
conditions due to the flexibility of Industry 4.0 that disrupts traditional
quality assurance mechanisms. This paper presents a novel industry-oriented
semantic model called Product-Process-Resource Asset Knowledge Graph (PPR-AKG),
which is designed to analyze and mitigate undesired conditions in flexible
CPPS. Built on top of the well-proven Product-Process-Resource (PPR) model
originating from ISA-95 and VDI-3682, a comprehensive OWL ontology addresses
shortcomings of conventional model-driven engineering for CPPS, particularly
inadequate undesired condition and error handling representation. The
integration of semantic technologies with large language models (LLMs) provides
intuitive interfaces for factory operators, production planners, and engineers
to interact with the entire model using natural language. Evaluation with the
use case addressing electric vehicle battery remanufacturing demonstrates that
the PPR-AKG approach efficiently supports resource allocation based on
explicitly represented capabilities as well as identification and mitigation of
undesired conditions in production. The key contributions include (1) a
holistic PPR-AKG model capturing multi-dimensional production knowledge, and
(2) the useful combination of the PPR-AKG with LLM-based chatbots for human
interaction.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.06278v1},
File          = {2508.06278v1.pdf}
}
@article{2508.05314v1,
Author        = {Benedikt Kantz and Stefan Lengauer and Peter Waldert and Tobias Schreck},
Title         = {Difference Views for Visual Graph Query Building},
Eprint        = {2508.05314v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Knowledge Graphs (KGs) contain vast amounts of linked resources that encode
knowledge in various domains, which can be queried and searched for using
specialized languages like SPARQL, a query language developed to query KGs.
Existing visual query builders enable non-expert users to construct SPARQL
queries and utilize the knowledge contained in these graphs. Query building is,
however, an iterative and, often, visual process where the question of the user
can change and differ throughout the process, especially for explorative
search. Our visual querying interface communicates these change between
iterative steps in the query building process using graph differences to
contrast the changes and the evolution in the graph query. We also enable users
to formulate their evolving information needs using a natural language
interface directly integrated into the difference query view. We, furthermore,
communicate the change in results in the result view by contrasting the
differences in both result distribution and individual instances of the
prototype graph and demonstrate the system's applicability through case studies
on different ontologies and usage scenarios, illustrating how our system
fosters, both, data exploration and analysis of domain-specific graphs.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.05314v1},
File          = {2508.05314v1.pdf}
}
@article{2508.04572v1,
Author        = {Jun Li and Che Liu and Wenjia Bai and Mingxuan Liu and Rossella Arcucci and Cosmin I. Bercea and Julia A. Schnabel},
Title         = {Knowledge to Sight: Reasoning over Visual Attributes via Knowledge
  Decomposition for Abnormality Grounding},
Eprint        = {2508.04572v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {In this work, we address the problem of grounding abnormalities in medical
images, where the goal is to localize clinical findings based on textual
descriptions. While generalist Vision-Language Models (VLMs) excel in natural
grounding tasks, they often struggle in the medical domain due to rare,
compositional, and domain-specific terms that are poorly aligned with visual
patterns. Specialized medical VLMs address this challenge via large-scale
domain pretraining, but at the cost of substantial annotation and computational
resources. To overcome these limitations, we propose \textbf{Knowledge to Sight
(K2Sight)}, a framework that introduces structured semantic supervision by
decomposing clinical concepts into interpretable visual attributes, such as
shape, density, and anatomical location. These attributes are distilled from
domain ontologies and encoded into concise instruction-style prompts, which
guide region-text alignment during training. Unlike conventional report-level
supervision, our approach explicitly bridges domain knowledge and spatial
structure, enabling data-efficient training of compact models. We train compact
models with 0.23B and 2B parameters using only 1.5\% of the data required by
state-of-the-art medical VLMs. Despite their small size and limited training
data, these models achieve performance on par with or better than 7B+ medical
VLMs, with up to 9.82\% improvement in $mAP_{50}$. Code and models:
\href{https://lijunrio.github.io/K2Sight/}{\textcolor{SOTAPink}{https://lijunrio.github.io/K2Sight/}}.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.04572v1},
File          = {2508.04572v1.pdf}
}
@article{2508.04401v1,
Author        = {VladimÃ­r HavlÃ­k},
Title         = {Why are LLMs' abilities emergent?},
Eprint        = {2508.04401v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The remarkable success of Large Language Models (LLMs) in generative tasks
has raised fundamental questions about the nature of their acquired
capabilities, which often appear to emerge unexpectedly without explicit
training. This paper examines the emergent properties of Deep Neural Networks
(DNNs) through both theoretical analysis and empirical observation, addressing
the epistemological challenge of "creation without understanding" that
characterises contemporary AI development. We explore how the neural approach's
reliance on nonlinear, stochastic processes fundamentally differs from symbolic
computational paradigms, creating systems whose macro-level behaviours cannot
be analytically derived from micro-level neuron activities. Through analysis of
scaling laws, grokking phenomena, and phase transitions in model capabilities,
I demonstrate that emergent abilities arise from the complex dynamics of highly
sensitive nonlinear systems rather than simply from parameter scaling alone. My
investigation reveals that current debates over metrics, pre-training loss
thresholds, and in-context learning miss the fundamental ontological nature of
emergence in DNNs. I argue that these systems exhibit genuine emergent
properties analogous to those found in other complex natural phenomena, where
systemic capabilities emerge from cooperative interactions among simple
components without being reducible to their individual behaviours. The paper
concludes that understanding LLM capabilities requires recognising DNNs as a
new domain of complex dynamical systems governed by universal principles of
emergence, similar to those operating in physics, chemistry, and biology. This
perspective shifts the focus from purely phenomenological definitions of
emergence to understanding the internal dynamic transformations that enable
these systems to acquire capabilities that transcend their individual
components.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.04401v1},
File          = {2508.04401v1.pdf}
}
@article{2508.04278v1,
Author        = {Wentao Wu and Linqing Chen and Hanmeng Zhong and Weilei Wang},
Title         = {Large Language Model's Multi-Capability Alignment in Biomedical Domain},
Eprint        = {2508.04278v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {BalancedBio is a theoretically grounded framework for parameter-efficient
biomedical reasoning, addressing multi-capability integration in
domain-specific AI alignment. It establishes the Biomedical Multi-Capability
Convergence Theorem, proving orthogonal gradient spaces are essential to
prevent capability interference for safe deployment. Key innovations include:
(1) Medical Knowledge Grounded Synthetic Generation (MKGSG), extending
Source2Synth with clinical workflow constraints and medical ontology validation
for factual accuracy and safety; and (2) Capability Aware Group Relative Policy
Optimization, deriving optimal hybrid reward weighting to maintain
orthogonality in RL, using a reward model with rule-based and model-based
scores adapted to biomedical tasks. Mathematical analysis proves Pareto-optimal
convergence, preserving performance across capabilities. It achieves
state-of-the-art results in its parameter class: domain expertise (80.95%
BIOMED-MMLU, +15.32% over baseline), reasoning (61.94%, +7.75%), instruction
following (67.95%, +6.44%), and integration (86.7%, +18.5%). Theoretical safety
guarantees include bounds on capability preservation and clinical accuracy.
Real-world deployment yields 78% cost reduction, 23% improved diagnostic
accuracy, and 89% clinician acceptance. This work provides a principled
methodology for biomedical AI alignment, enabling efficient reasoning with
essential safety and reliability, with the 0.5B model version to be released.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.04278v1},
File          = {2508.04278v1.pdf}
}
@article{2508.04260v1,
Author        = {Xiao Wang and Ziwen Wang and Wentao Wu and Anjie Wang and Jiashu Wu and Yantao Pan and Chenglong Li},
Title         = {Segment Any Vehicle: Semantic and Visual Context Driven SAM and A
  Benchmark},
Eprint        = {2508.04260v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {With the rapid advancement of autonomous driving, vehicle perception,
particularly detection and segmentation, has placed increasingly higher demands
on algorithmic performance. Pre-trained large segmentation models, especially
Segment Anything Model (SAM), have sparked significant interest and inspired
new research directions in artificial intelligence. However, SAM cannot be
directly applied to the fine-grained task of vehicle part segmentation, as its
text-prompted segmentation functionality is not publicly accessible, and the
mask regions generated by its default mode lack semantic labels, limiting its
utility in structured, category-specific segmentation tasks. To address these
limitations, we propose SAV, a novel framework comprising three core
components: a SAM-based encoder-decoder, a vehicle part knowledge graph, and a
context sample retrieval encoding module. The knowledge graph explicitly models
the spatial and geometric relationships among vehicle parts through a
structured ontology, effectively encoding prior structural knowledge.
Meanwhile, the context retrieval module enhances segmentation by identifying
and leveraging visually similar vehicle instances from training data, providing
rich contextual priors for improved generalization. Furthermore, we introduce a
new large-scale benchmark dataset for vehicle part segmentation, named
VehicleSeg10K, which contains 11,665 high-quality pixel-level annotations
across diverse scenes and viewpoints. We conduct comprehensive experiments on
this dataset and two other datasets, benchmarking multiple representative
baselines to establish a solid foundation for future research and comparison. %
Both the dataset and source code of this paper will be released upon
acceptance. Both the dataset and source code of this paper will be released on
https://github.com/Event-AHU/SAV},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.04260v1},
File          = {2508.04260v1.pdf}
}
@article{2508.04213v1,
Author        = {Alessia Pisu and Livio Pompianu and Francesco Osborne and Diego Reforgiato Recupero and Daniele Riboni and Angelo Salatino},
Title         = {A Hybrid AI Methodology for Generating Ontologies of Research Topics
  from Scientific Paper Corpora},
Eprint        = {2508.04213v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {Taxonomies and ontologies of research topics (e.g., MeSH, UMLS, CSO, NLM)
play a central role in providing the primary framework through which
intelligent systems can explore and interpret the literature. However, these
resources have traditionally been manually curated, a process that is
time-consuming, prone to obsolescence, and limited in granularity. This paper
presents Sci-OG, a semi-auto\-mated methodology for generating research topic
ontologies, employing a multi-step approach: 1) Topic Discovery, extracting
potential topics from research papers; 2) Relationship Classification,
determining semantic relationships between topic pairs; and 3) Ontology
Construction, refining and organizing topics into a structured ontology. The
relationship classification component, which constitutes the core of the
system, integrates an encoder-based language model with features describing
topic occurrence in the scientific literature. We evaluate this approach
against a range of alternative solutions using a dataset of 21,649 manually
annotated semantic triples. Our method achieves the highest F1 score (0.951),
surpassing various competing approaches, including a fine-tuned SciBERT model
and several LLM baselines, such as the fine-tuned GPT4-mini. Our work is
corroborated by a use case which illustrates the practical application of our
system to extend the CSO ontology in the area of cybersecurity. The presented
solution is designed to improve the accessibility, organization, and analysis
of scientific knowledge, thereby supporting advancements in AI-enabled
literature management and research exploration.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.04213v1},
File          = {2508.04213v1.pdf}
}
@article{2508.03438v1,
Author        = {Taine J. Elliott and Stephen P. Levitt and Ken Nixon and Martin Bekker},
Title         = {Data Overdose? Time for a Quadruple Shot: Knowledge Graph Construction
  using Enhanced Triple Extraction},
Eprint        = {2508.03438v1},
DOI           = {10.1007/978-3-031-96262-2_15},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The rapid expansion of publicly-available medical data presents a challenge
for clinicians and researchers alike, increasing the gap between the volume of
scientific literature and its applications. The steady growth of studies and
findings overwhelms medical professionals at large, hindering their ability to
systematically review and understand the latest knowledge. This paper presents
an approach to information extraction and automatic knowledge graph (KG)
generation to identify and connect biomedical knowledge. Through a pipeline of
large language model (LLM) agents, the system decomposes 44 PubMed abstracts
into semantically meaningful proposition sentences and extracts KG triples from
these sentences. The triples are enhanced using a combination of open domain
and ontology-based information extraction methodologies to incorporate
ontological categories. On top of this, a context variable is included during
extraction to allow the triple to stand on its own - thereby becoming
`quadruples'. The extraction accuracy of the LLM is validated by comparing
natural language sentences generated from the enhanced triples to the original
propositions, achieving an average cosine similarity of 0.874. The similarity
for generated sentences of enhanced triples were compared with generated
sentences of ordinary triples showing an increase as a result of the context
variable. Furthermore, this research explores the ability for LLMs to infer new
relationships and connect clusters in the knowledge base of the knowledge
graph. This approach leads the way to provide medical practitioners with a
centralised, updated in real-time, and sustainable knowledge source, and may be
the foundation of similar gains in a wide variety of fields.},
Year          = {2025},
Month         = {Aug},
Note          = {In: Gerber, A. (eds) South African Computer Science and
  Information Systems Research Trends. SAICSIT 2025. Communications in Computer
  and Information Science, vol 2583. Springer, Cham},
Url           = {http://arxiv.org/abs/2508.03438v1},
File          = {2508.03438v1.pdf}
}
@article{2508.03159v1,
Author        = {Jueon Park and Yein Park and Minju Song and Soyon Park and Donghyeon Lee and Seungheun Baek and Jaewoo Kang},
Title         = {CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and
  Prediction},
Eprint        = {2508.03159v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Drug toxicity remains a major challenge in pharmaceutical development. Recent
machine learning models have improved in silico toxicity prediction, but their
reliance on annotated data and lack of interpretability limit their
applicability. This limits their ability to capture organ-specific toxicities
driven by complex biological mechanisms. Large language models (LLMs) offer a
promising alternative through step-by-step reasoning and integration of textual
data, yet prior approaches lack biological context and transparent rationale.
To address this issue, we propose CoTox, a novel framework that integrates LLM
with chain-of-thought (CoT) reasoning for multi-toxicity prediction. CoTox
combines chemical structure data, biological pathways, and gene ontology (GO)
terms to generate interpretable toxicity predictions through step-by-step
reasoning. Using GPT-4o, we show that CoTox outperforms both traditional
machine learning and deep learning model. We further examine its performance
across various LLMs to identify where CoTox is most effective. Additionally, we
find that representing chemical structures with IUPAC names, which are easier
for LLMs to understand than SMILES, enhances the model's reasoning ability and
improves predictive performance. To demonstrate its practical utility in drug
development, we simulate the treatment of relevant cell types with drug and
incorporated the resulting biological context into the CoTox framework. This
approach allow CoTox to generate toxicity predictions aligned with
physiological responses, as shown in case study. This result highlights the
potential of LLM-based frameworks to improve interpretability and support
early-stage drug safety assessment. The code and prompt used in this work are
available at https://github.com/dmis-lab/CoTox.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.03159v1},
File          = {2508.03159v1.pdf}
}
@article{2508.02556v1,
Author        = {Ali Noori and Pratik Devkota and Somya Mohanty and Prashanti Manda},
Title         = {Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU
  Neural Networks},
Eprint        = {2508.02556v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Automated annotation of clinical text with standardized medical concepts is
critical for enabling structured data extraction and decision support. SNOMED
CT provides a rich ontology for labeling clinical entities, but manual
annotation is labor-intensive and impractical at scale. This study introduces a
neural sequence labeling approach for SNOMED CT concept recognition using a
Bidirectional GRU model. Leveraging a subset of MIMIC-IV, we preprocess text
with domain-adapted SpaCy and SciBERT-based tokenization, segmenting sentences
into overlapping 19-token chunks enriched with contextual, syntactic, and
morphological features. The Bi-GRU model assigns IOB tags to identify concept
spans and achieves strong performance with a 90 percent F1-score on the
validation set. These results surpass traditional rule-based systems and match
or exceed existing neural models. Qualitative analysis shows effective handling
of ambiguous terms and misspellings. Our findings highlight that lightweight
RNN-based architectures can deliver high-quality clinical concept annotation
with significantly lower computational cost than transformer-based models,
making them well-suited for real-world deployment.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.02556v1},
File          = {2508.02556v1.pdf}
}
@article{2508.02300v1,
Author        = {Kanishka Silva and Marcel R. Ackermann and Heike Fliegl and Genet-Asefa Gesese and Fidan Limani and Philipp Mayr and Peter Mutschke and Allard Oelen and Muhammad Asif Suryani and Sharmila Upadhyaya and Benjamin Zapilko and Harald Sack and Stefan Dietze},
Title         = {Research Knowledge Graphs in NFDI4DataScience: Key Activities,
  Achievements, and Future Directions},
Eprint        = {2508.02300v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {As research in Artificial Intelligence and Data Science continues to grow in
volume and complexity, it becomes increasingly difficult to ensure
transparency, reproducibility, and discoverability. To address these
challenges, as research artifacts should be understandable and usable by
machines, the NFDI4DataScience consortium is developing and providing Research
Knowledge Graphs (RKGs). Building upon earlier works, this paper presents
recent progress in creating semantically rich RKGs using standardized
ontologies, shared vocabularies, and automated Information Extraction
techniques. Key achievements include the development of the NFDI4DS ontology,
metadata standards, tools, and services designed to support the FAIR
principles, as well as community-led projects and various implementations of
RKGs. Together, these efforts aim to capture and connect the complex
relationships between datasets, models, software, and scientific publications.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.02300v1},
File          = {2508.02300v1.pdf}
}
@article{2508.02755v1,
Author        = {MikoÅaj Sienicki and Krzysztof Sienicki},
Title         = {Beyond the Wavefunction: Qualia Abstraction Language Mechanics and the
  Grammar of Awareness},
Eprint        = {2508.02755v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {physics.hist-ph},
Abstract      = {We propose a formal reconstruction of quantum mechanics grounded not in
external mathematical abstractions, but in the structured dynamics of
subjective experience. The Qualia Abstraction Language (QAL) models physical
systems as evolving streams of introspective units, structured sequences of
modality, shape, and functional effect, rather than as state vectors in Hilbert
space. This approach reimagines core quantum concepts: superposition becomes a
form of structured ambiguity; collapse is reframed as an introspective
contraction; and entanglement is modeled as semantic resonance across streams
of qualia. Drawing on insights from nominalist philosophy and oversight
theoretic limits in AI, we argue that the observer paradox in quantum mechanics
reflects not an ontological lacuna, but a linguistic one: the absence of a
formal vocabulary for modeling first person structure. QAL introduces such a
vocabulary, providing a morphodynamic framework that embeds the observer within
the system and replaces abstract projection with endogenous transformation. We
analyze the alignment of QAL with endophysical approaches, contrast it with
standard interpretations of quantum theory, and explore its implications for a
post Platonist, introspectively grounded physics.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.02755v1},
File          = {2508.02755v1.pdf}
}
@article{2508.01424v1,
Author        = {Haonan Bian and Yutao Qi and Rui Yang and Yuanxi Che and Jiaqian Wang and Heming Xia and Ranran Zhen},
Title         = {From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs},
Eprint        = {2508.01424v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs), despite their success in question answering,
exhibit limitations in complex multi-hop question answering (MQA) tasks that
necessitate non-linear, structured reasoning. This limitation stems from their
inability to adequately capture deep conceptual relationships between entities.
To overcome this challenge, we present **ORACLE** (**O**ntology-driven
**R**easoning **A**nd **C**hain for **L**ogical **E**ucidation), a
training-free framework that combines LLMs' generative capabilities with the
structural benefits of knowledge graphs. Our approach operates through three
stages: (1) dynamic construction of question-specific knowledge ontologies
using LLMs, (2) transformation of these ontologies into First-Order Logic
reasoning chains, and (3) systematic decomposition of the original query into
logically coherent sub-questions. Experimental results on several standard MQA
benchmarks show that our framework achieves highly competitive performance,
rivaling current state-of-the-art models like DeepSeek-R1. Detailed analyses
further confirm the effectiveness of each component, while demonstrating that
our method generates more logical and interpretable reasoning chains than
existing approaches.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.01424v1},
File          = {2508.01424v1.pdf}
}
@article{2508.00653v1,
Author        = {LucÃ­a GÃ³mez Ãlvarez and Sebastian Rudolph},
Title         = {Putting Perspective into OWL [sic]: Complexity-Neutral Standpoint
  Reasoning for Ontology Languages via Monodic S5 over Counting Two-Variable
  First-Order Logic (Extended Version with Appendix)},
Eprint        = {2508.00653v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LO},
Abstract      = {Standpoint extensions of knowledge representation formalisms have been
recently introduced as a means to incorporate multi-perspective modelling and
reasoning through modal operators that attribute pieces of knowledge to
specific entities or agents. In these extensions, the integration between
conceptual modelling and perspective annotations can vary in strength, with
monodic standpoint extensions offering a well-balanced approach. They allow for
advanced modelling features, such as the expression of rigid concepts, while
maintaining desirable reasoning complexity.
  We consider the extension of C2--the counting two-variable fragment of
first-order logic--by monodic standpoints. At the heart of our work is a
polynomial-time translation of formulas in this extended formalism into
standard, standpoint-free C2, a result that relies on intricate model-theoretic
arguments. Thanks to this translation, the satisfiability problem remains at
the same complexity level: NExpTime-complete, as in plain C2. Since our
formalism subsumes monodic S5 over C2, this result also marks a substantial
advancement in the study of first-order modal logics.
  From a practical standpoint, this means that highly expressive description
logics such as SHOIQBs and SROIQBs--which underpin the widely adopted OWL 1 and
OWL 2 ontology languages standardised by the W3C--can be extended with monodic
standpoints without increasing the standard reasoning complexity.
  We further prove that NExpTime-hardness arises even in significantly less
expressive description logics, as long as they include both nominals and
monodic standpoints. Moreover, we show that if the monodicity restriction is
relaxed even slightly in the presence of inverse roles, functionality, and
nominals, the satisfiability problem becomes undecidable.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.00653v1},
File          = {2508.00653v1.pdf}
}
@article{2508.00300v2,
Author        = {Shruthi Chari and Oshani Seneviratne and Prithwish Chakraborty and Pablo Meyer and Deborah L. McGuinness},
Title         = {MetaExplainer: A Framework to Generate Multi-Type User-Centered
  Explanations for AI Systems},
Eprint        = {2508.00300v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {Explanations are crucial for building trustworthy AI systems, but a gap often
exists between the explanations provided by models and those needed by users.
To address this gap, we introduce MetaExplainer, a neuro-symbolic framework
designed to generate user-centered explanations. Our approach employs a
three-stage process: first, we decompose user questions into machine-readable
formats using state-of-the-art large language models (LLM); second, we delegate
the task of generating system recommendations to model explainer methods; and
finally, we synthesize natural language explanations that summarize the
explainer outputs. Throughout this process, we utilize an Explanation Ontology
to guide the language models and explainer methods. By leveraging LLMs and a
structured approach to explanation generation, MetaExplainer aims to enhance
the interpretability and trustworthiness of AI systems across various
applications, providing users with tailored, question-driven explanations that
better meet their needs. Comprehensive evaluations of MetaExplainer demonstrate
a step towards evaluating and utilizing current state-of-the-art explanation
frameworks. Our results show high performance across all stages, with a 59.06%
F1-score in question reframing, 70% faithfulness in model explanations, and 67%
context-utilization in natural language synthesis. User studies corroborate
these findings, highlighting the creativity and comprehensiveness of generated
explanations. Tested on the Diabetes (PIMA Indian) tabular dataset,
MetaExplainer supports diverse explanation types, including Contrastive,
Counterfactual, Rationale, Case-Based, and Data explanations. The framework's
versatility and traceability from using ontology to guide LLMs suggest broad
applicability beyond the tested scenarios, positioning MetaExplainer as a
promising tool for enhancing AI explainability across various domains.},
Year          = {2025},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2508.00300v2},
File          = {2508.00300v2.pdf}
}
@article{2507.23358v1,
Author        = {Renato Vukovic and Carel van Niekerk and Michael Heck and Benjamin Ruppik and Hsien-Chin Lin and Shutong Feng and Nurul Lubis and Milica Gasic},
Title         = {Text-to-SQL Task-oriented Dialogue Ontology Construction},
Eprint        = {2507.23358v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) are widely used as general-purpose knowledge
sources, but they rely on parametric knowledge, limiting explainability and
trustworthiness. In task-oriented dialogue (TOD) systems, this separation is
explicit, using an external database structured by an explicit ontology to
ensure explainability and controllability. However, building such ontologies
requires manual labels or supervised training. We introduce TeQoDO: a
Text-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM
autonomously builds a TOD ontology from scratch without supervision using its
inherent SQL programming capabilities combined with dialogue theory provided in
the prompt. We show that TeQoDO outperforms transfer learning approaches, and
its constructed ontology is competitive on a downstream dialogue state tracking
task. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also
scales to allow construction of much larger ontologies, which we investigate on
a Wikipedia and ArXiv dataset. We view this as a step towards broader
application of ontologies to increase LLM explainability.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.23358v1},
File          = {2507.23358v1.pdf}
}
@article{2507.23191v1,
Author        = {Meghyn Bienvenu and Diego Figueira and Pierre Lafourcade},
Title         = {Tractable Responsibility Measures for Ontology-Mediated Query Answering},
Eprint        = {2507.23191v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Recent work on quantitative approaches to explaining query answers employs
responsibility measures to assign scores to facts in order to quantify their
respective contributions to obtaining a given answer. In this paper, we study
the complexity of computing such responsibility scores in the setting of
ontology-mediated query answering, focusing on a very recently introduced
family of Shapley-value-based responsibility measures defined in terms of
weighted sums of minimal supports (WSMS). By exploiting results from the
database setting, we can show that such measures enjoy polynomial data
complexity for classes of ontology-mediated queries that are
first-order-rewritable, whereas the problem becomes "shP"-hard when the
ontology language can encode reachability queries (via axioms like $\exists R.
A \sqsubseteq A$). To better understand the tractability frontier, we next
explore the combined complexity of WSMS computation. We prove that
intractability applies already to atomic queries if the ontology language
supports conjunction, as well as to unions of `well-behaved' conjunctive
queries, even in the absence of an ontology. By contrast, our study yields
positive results for common DL-Lite dialects: by means of careful analysis, we
identify classes of structurally restricted conjunctive queries (which
intuitively disallow undesirable interactions between query atoms) that admit
tractable WSMS computation.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.23191v1},
File          = {2507.23191v1.pdf}
}
@article{2507.23009v1,
Author        = {Tom SÃ¼hr and Florian E. Dorner and Olawale Salaudeen and Augustin Kelava and Samira Samadi},
Title         = {Stop Evaluating AI with Human Tests, Develop Principled, AI-specific
  Tests instead},
Eprint        = {2507.23009v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Large Language Models (LLMs) have achieved remarkable results on a range of
standardized tests originally designed to assess human cognitive and
psychological traits, such as intelligence and personality. While these results
are often interpreted as strong evidence of human-like characteristics in LLMs,
this paper argues that such interpretations constitute an ontological error.
Human psychological and educational tests are theory-driven measurement
instruments, calibrated to a specific human population. Applying these tests to
non-human subjects without empirical validation, risks mischaracterizing what
is being measured. Furthermore, a growing trend frames AI performance on
benchmarks as measurements of traits such as ``intelligence'', despite known
issues with validity, data contamination, cultural bias and sensitivity to
superficial prompt changes. We argue that interpreting benchmark performance as
measurements of human-like traits, lacks sufficient theoretical and empirical
justification. This leads to our position: Stop Evaluating AI with Human Tests,
Develop Principled, AI-specific Tests instead. We call for the development of
principled, AI-specific evaluation frameworks tailored to AI systems. Such
frameworks might build on existing frameworks for constructing and validating
psychometrics tests, or could be created entirely from scratch to fit the
unique context of AI.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.23009v1},
File          = {2507.23009v1.pdf}
}
@article{2507.22878v1,
Author        = {Ethan Frakes and Yinghui Wu and Roger H. French and Mengjie Li},
Title         = {GeoOutageKG: A Multimodal Geospatiotemporal Knowledge Graph for
  Multiresolution Power Outage Analysis},
Eprint        = {2507.22878v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Detecting, analyzing, and predicting power outages is crucial for grid risk
assessment and disaster mitigation. Numerous outages occur each year,
exacerbated by extreme weather events such as hurricanes. Existing outage data
are typically reported at the county level, limiting their spatial resolution
and making it difficult to capture localized patterns. However, it offers
excellent temporal granularity. In contrast, nighttime light satellite image
data provides significantly higher spatial resolution and enables a more
comprehensive spatial depiction of outages, enhancing the accuracy of assessing
the geographic extent and severity of power loss after disaster events.
However, these satellite data are only available on a daily basis. Integrating
spatiotemporal visual and time-series data sources into a unified knowledge
representation can substantially improve power outage detection, analysis, and
predictive reasoning. In this paper, we propose GeoOutageKG, a multimodal
knowledge graph that integrates diverse data sources, including nighttime light
satellite image data, high-resolution spatiotemporal power outage maps, and
county-level timeseries outage reports in the U.S. We describe our method for
constructing GeoOutageKG by aligning source data with a developed ontology,
GeoOutageOnto. Currently, GeoOutageKG includes over 10.6 million individual
outage records spanning from 2014 to 2024, 300,000 NTL images spanning from
2012 to 2024, and 15,000 outage maps. GeoOutageKG is a novel, modular and
reusable semantic resource that enables robust multimodal data integration. We
demonstrate its use through multiresolution analysis of geospatiotemporal power
outages.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.22878v1},
File          = {2507.22878v1.pdf}
}
@article{2507.22619v1,
Author        = {Sebastian Monka and Irlan Grangel-GonzÃ¡lez and Stefan Schmid and Lavdim Halilaj and Marc Rickart and Oliver Rudolph and Rui Dias},
Title         = {Enhancing Manufacturing Knowledge Access with LLMs and Context-aware
  Prompting},
Eprint        = {2507.22619v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge graphs (KGs) have transformed data management within the
manufacturing industry, offering effective means for integrating disparate data
sources through shared and structured conceptual schemas. However, harnessing
the power of KGs can be daunting for non-experts, as it often requires
formulating complex SPARQL queries to retrieve specific information. With the
advent of Large Language Models (LLMs), there is a growing potential to
automatically translate natural language queries into the SPARQL format, thus
bridging the gap between user-friendly interfaces and the sophisticated
architecture of KGs. The challenge remains in adequately informing LLMs about
the relevant context and structure of domain-specific KGs, e.g., in
manufacturing, to improve the accuracy of generated queries. In this paper, we
evaluate multiple strategies that use LLMs as mediators to facilitate
information retrieval from KGs. We focus on the manufacturing domain,
particularly on the Bosch Line Information System KG and the I40 Core
Information Model. In our evaluation, we compare various approaches for feeding
relevant context from the KG to the LLM and analyze their proficiency in
transforming real-world questions into SPARQL queries. Our findings show that
LLMs can significantly improve their performance on generating correct and
complete queries when provided only the adequate context of the KG schema. Such
context-aware prompting techniques help LLMs to focus on the relevant parts of
the ontology and reduce the risk of hallucination. We anticipate that the
proposed techniques help LLMs to democratize access to complex data
repositories and empower informed decision-making in manufacturing settings.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.22619v1},
File          = {2507.22619v1.pdf}
}
@article{2507.22305v2,
Author        = {Carolina CortÃ©s and Lisa Ehrlinger and Lorena Etcheverry and Felix Naumann},
Title         = {Is SHACL Suitable for Data Quality Assessment?},
Eprint        = {2507.22305v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Knowledge graphs have been widely adopted in both enterprises, such as the
Google Knowledge Graph, and open platforms like Wikidata, to represent domain
knowledge and support artificial intelligence applications. They model
real-world information as nodes and edges. To embrace flexibility, knowledge
graphs often lack enforced schemas (i.e., ontologies), leading to potential
data quality issues, such as semantically overlapping nodes. Yet ensuring their
quality is essential, as issues in the data can affect applications relying on
them. To assess the quality of knowledge graphs, existing works propose either
high-level frameworks comprising various data quality dimensions without
concrete implementations, define tools that measure data quality with ad-hoc
SPARQL queries, or promote the usage of constraint languages, such as the
Shapes Constraint Language (SHACL), to assess and improve the quality of the
graph. Although the latter approaches claim to address data quality assessment,
none of them comprehensively tries to cover all data quality dimensions. In
this paper, we explore this gap by investigating the extent to which SHACL can
be used to assess data quality in knowledge graphs. Specifically, we defined
SHACL shapes for 69 data quality metrics proposed by Zaveri et al. [1] and
implemented a prototype that automatically instantiates these shapes and
computes the corresponding data quality measures from their validation results.
All resources are provided for repeatability.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.22305v2},
File          = {2507.22305v2.pdf}
}
@article{2507.21980v1,
Author        = {Hyunwoo Yoo and Gail L. Rosen},
Title         = {Predicting Microbial Ontology and Pathogen Risk from Environmental
  Metadata with Large Language Models},
Eprint        = {2507.21980v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Traditional machine learning models struggle to generalize in microbiome
studies where only metadata is available, especially in small-sample settings
or across studies with heterogeneous label formats. In this work, we explore
the use of large language models (LLMs) to classify microbial samples into
ontology categories such as EMPO 3 and related biological labels, as well as to
predict pathogen contamination risk, specifically the presence of E. Coli,
using environmental metadata alone. We evaluate LLMs such as ChatGPT-4o, Claude
3.7 Sonnet, Grok-3, and LLaMA 4 in zero-shot and few-shot settings, comparing
their performance against traditional models like Random Forests across
multiple real-world datasets. Our results show that LLMs not only outperform
baselines in ontology classification, but also demonstrate strong predictive
ability for contamination risk, generalizing across sites and metadata
distributions. These findings suggest that LLMs can effectively reason over
sparse, heterogeneous biological metadata and offer a promising metadata-only
approach for environmental microbiology and biosurveillance applications.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.21980v1},
File          = {2507.21980v1.pdf}
}
@article{2507.21438v1,
Author        = {Vishal Raman and Vijai Aravindh R},
Title         = {Evo-DKD: Dual-Knowledge Decoding for Autonomous Ontology Evolution in
  Large Language Models},
Eprint        = {2507.21438v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontologies and knowledge graphs require continuous evolution to remain
comprehensive and accurate, but manual curation is labor intensive. Large
Language Models (LLMs) possess vast unstructured knowledge but struggle with
maintaining structured consistency. We propose Evo-DKD, a novel dual-decoder
framework for autonomous ontology evolution that combines structured ontology
traversal with unstructured text reasoning. Evo-DKD introduces two parallel
decoding streams within an LLM: one decoder generates candidate ontology edits
(e.g., new concepts or relations) while the other produces natural-language
justifications. A dynamic attention-based gating mechanism coordinates the two
streams, deciding at each step how to blend structured and unstructured
knowledge. Due to GPU constraints, we simulate the dual-decoder behavior using
prompt-based mode control to approximate coordinated decoding in a
single-stream mode. The system operates in a closed reasoning loop: proposed
ontology edits are validated (via consistency checks and cross-verification
with the text explanations) and then injected into the knowledge base, which in
turn informs subsequent reasoning. We demonstrate Evo-DKD's effectiveness on
use cases including healthcare ontology refinement, semantic search
improvement, and cultural heritage timeline modeling. Experiments show that
Evo-DKD outperforms baselines using structured-only or unstructured-only
decoding in both precision of ontology updates and downstream task performance.
We present quantitative metrics and qualitative examples, confirming the
contributions of the dual-decoder design and gating router. Evo-DKD offers a
new paradigm for LLM-driven knowledge base maintenance, combining the strengths
of symbolic and neural reasoning for sustainable ontology evolution.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.21438v1},
File          = {2507.21438v1.pdf}
}
@article{2507.20643v1,
Author        = {Wenbin Guo and Xin Wang and Jiaoyan Chen and Zhao Li and Zirui Chen},
Title         = {Ontology-Enhanced Knowledge Graph Completion using Large Language Models},
Eprint        = {2507.20643v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have been extensively adopted in Knowledge Graph
Completion (KGC), showcasing significant research advancements. However, as
black-box models driven by deep neural architectures, current LLM-based KGC
methods rely on implicit knowledge representation with parallel propagation of
erroneous knowledge, thereby hindering their ability to produce conclusive and
decisive reasoning outcomes. We aim to integrate neural-perceptual structural
information with ontological knowledge, leveraging the powerful capabilities of
LLMs to achieve a deeper understanding of the intrinsic logic of the knowledge.
We propose an ontology enhanced KGC method using LLMs -- OL-KGC. It first
leverages neural perceptual mechanisms to effectively embed structural
information into the textual space, and then uses an automated extraction
algorithm to retrieve ontological knowledge from the knowledge graphs (KGs)
that needs to be completed, which is further transformed into a textual format
comprehensible to LLMs for providing logic guidance. We conducted extensive
experiments on three widely-used benchmarks -- FB15K-237, UMLS and WN18RR. The
experimental results demonstrate that OL-KGC significantly outperforms existing
mainstream KGC methods across multiple evaluation metrics, achieving
state-of-the-art performance.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.20643v1},
File          = {2507.20643v1.pdf}
}
@article{2507.20073v1,
Author        = {Muhammad Ghulam Khuwajah Khan},
Title         = {On the Reinterpretation of Dark Energy as the Elasticity of Space},
Eprint        = {2507.20073v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {gr-qc},
Abstract      = {In this work, we demonstrate that by treating space as a three dimensional
elastic membrane, one can interpret the energy density stored in it as Dark
Energy. In particular, we will model space as an infinitely large fundamental 3
brane and show that its vacuum stress energy tensor has the same mathematical
form which one expects from the stress energy tensor describing a Lorentz
invariant vacuum energy density (Dark Energy). We identify the dark-energy
density with the tension of the fundamental 3 brane and thereby establish a
direct correspondence between the cosmological constant and brane tension.
Finally, we present a brief argument as to why the Cosmological Constant should
be treated as a fundamental constant of nature, based on some light ontological
discussion regarding the origin of the tension term in the broader framework of
String Theory. In a forthcoming work, we will articulate a dynamical mechanism,
which will be grounded in Q-theory, Hawking-Wu flux neutralization and broken
super-symmetry, demonstrating the precise cancellation of vacuum energy in a
manner such that the fundamental brane (spatial) tension remains the unique
source of the Cosmological Constant.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.20073v1},
File          = {2507.20073v1.pdf}
}
@article{2507.19992v1,
Author        = {Md Fantacher Islam and Jarrod Mosier and Vignesh Subbian},
Title         = {NIRS: An Ontology for Non-Invasive Respiratory Support in Acute Care},
Eprint        = {2507.19992v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.OT},
Abstract      = {Objective: Develop a Non Invasive Respiratory Support (NIRS) ontology to
support knowledge representation in acute care settings.
  Materials and Methods: We developed the NIRS ontology using Web Ontology
Language (OWL) semantics and Protege to organize clinical concepts and
relationships. To enable rule-based clinical reasoning beyond hierarchical
structures, we added Semantic Web Rule Language (SWRL) rules. We evaluated
logical reasoning by adding 17 hypothetical patient clinical scenarios. We used
SPARQL queries and data from the Electronic Intensive Care Unit (eICU)
Collaborative Research Database to retrieve and test targeted inferences.
  Results: The ontology has 132 classes, 12 object properties, and 17 data
properties across 882 axioms that establish concept relationships. To
standardize clinical concepts, we added 350 annotations, including descriptive
definitions based on controlled vocabularies. SPARQL queries successfully
validated all test cases (rules) by retrieving appropriate patient outcomes,
for instance, a patient treated with HFNC (high-flow nasal cannula) for 2 hours
due to acute respiratory failure may avoid endotracheal intubation.
  Discussion: The NIRS ontology formally represents domain-specific concepts,
including ventilation modalities, patient characteristics, therapy parameters,
and outcomes. SPARQL query evaluations on clinical scenarios confirmed the
ability of the ontology to support rule based reasoning and therapy
recommendations, providing a foundation for consistent documentation practices,
integration into clinical data models, and advanced analysis of NIRS outcomes.
  Conclusion: We unified NIRS concepts into an ontological framework and
demonstrated its applicability through the evaluation of hypothetical patient
scenarios and alignment with standardized vocabularies.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.19992v1},
File          = {2507.19992v1.pdf}
}
@article{2507.19733v2,
Author        = {Forrest Hare Alec Sculley and Cameron Stockton},
Title         = {Integrating Activity Predictions in Knowledge Graphs},
Eprint        = {2507.19733v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We argue that ontology-structured knowledge graphs can play a crucial role in
generating predictions about future events. By leveraging the semantic
framework provided by Basic Formal Ontology (BFO) and Common Core Ontologies
(CCO), we demonstrate how data such as the movements of a fishing vessel can be
organized in and retrieved from a knowledge graph. These query results are then
used to create Markov chain models, allowing us to predict future states based
on the vessel's history. To fully support this process, we introduce the term
`spatiotemporal instant' to complete the necessary structural semantics.
Additionally, we critique the prevailing ontological model of probability,
according to which probabilities are about the future. We propose an
alternative view, where at least some probabilities are treated as being about
actual process profiles, which better captures the dynamics of real-world
phenomena. Finally, we demonstrate how our Markov chain-based probability
calculations can be seamlessly integrated back into the knowledge graph,
enabling further analysis and decision-making.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.19733v2},
File          = {2507.19733v2.pdf}
}
@article{2507.19728v1,
Author        = {Lalita Na Nongkhai and Jingyun Wang and Takahiko Mendori},
Title         = {Development and Evaluation of Adaptive LearningSupport System Based on
  Ontology of MultipleProgramming Languages},
Eprint        = {2507.19728v1},
DOI           = {10.3390/educsci15060724},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.PL},
Abstract      = {This paper introduces an ontology-based approach within an adaptive learning
support system for computer programming. This system (named ADVENTURE) is
designed to deliver personalized programming exercises that are tailored to
individual learners' skill levels. ADVENTURE utilizes an ontology, named
CONTINUOUS, which encompasses common concepts across multiple programming
languages. The system leverages this ontology not only to visualize programming
concepts but also to provide hints during practice programming exercises and
recommend subsequent programming concepts. The adaptive mechanism is driven by
the Elo Rating System, applied in an educational context to dynamically
estimate the most appropriate exercise difficulty for each learner. An
experimental study compared two instructional modes, adaptive and random, based
on six features derived from 1,186 code submissions across all the experimental
groups. The results indicate significant differences in four of six analyzed
features between these two modes. Notably, the adaptive mode demonstrates a
significant difference over the random mode in two features, the submission of
correct answers and the number of pass concepts. Therefore, these results
underscore that this adaptive learning support system may support learners in
practicing programming exercises.},
Year          = {2025},
Month         = {Jul},
Note          = {Education Sciences, 2025, 15(6):724},
Url           = {http://arxiv.org/abs/2507.19728v1},
File          = {2507.19728v1.pdf}
}
@article{2507.21171v1,
Author        = {Federico Donato and Adrien Barton},
Title         = {An ontological analysis of risk in Basic Formal Ontology},
Eprint        = {2507.21171v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The paper explores the nature of risk, providing a characterization using the
categories of the Basic Formal Ontology (BFO). It argues that the category Risk
is a subclass of BFO:Role, contrasting it with a similar view classifying Risk
as a subclass of BFO:Disposition. This modeling choice is applied on one
example of risk, which represents objects, processes (both physical and mental)
and their interrelations, then generalizing from the instances in the example
to obtain an overall analysis of risk, making explicit what are the sufficient
conditions for being a risk. Plausible necessary conditions are also mentioned
for future work. Index Terms: ontology, risk, BFO, role, disposition},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.21171v1},
File          = {2507.21171v1.pdf}
}
@article{2507.19315v1,
Author        = {Yicheng Tao and Yuanhao Huang and Jie Liu},
Title         = {AutoPCR: Automated Phenotype Concept Recognition by Prompting},
Eprint        = {2507.19315v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Phenotype concept recognition (CR) is a fundamental task in biomedical text
mining, enabling applications such as clinical diagnostics and knowledge graph
construction. However, existing methods often require ontology-specific
training and struggle to generalize across diverse text types and evolving
biomedical terminology. We present AutoPCR, a prompt-based phenotype CR method
that does not require ontology-specific training. AutoPCR performs CR in three
stages: entity extraction using a hybrid of rule-based and neural tagging
strategies, candidate retrieval via SapBERT, and entity linking through
prompting a large language model. Experiments on four benchmark datasets show
that AutoPCR achieves the best average and most robust performance across both
mention-level and document-level evaluations, surpassing prior state-of-the-art
methods. Further ablation and transfer studies demonstrate its inductive
capability and generalizability to new ontologies.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.19315v1},
File          = {2507.19315v1.pdf}
}
@article{2507.17171v1,
Author        = {James S. Wheaton and Daniel R. Herber},
Title         = {Ontological Definition of Seamless Digital Engineering Based on ISO/IEC
  25000-Series SQuaRE Product Quality Model},
Eprint        = {2507.17171v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {eess.SY},
Abstract      = {Since the introduction of Digital Engineering (DE) as a well-defined concept
in 2018, organizations and industry groups have been working to interpret the
DE concepts to establish consistent meta-models of those interrelated concepts
for integration into their DE processes and tools. To reach the breadth and
depth of DE concept definitions, the interpretation of international standard
sources is necessary, including ISO/IEC/IEEE 15288, 24765, 42000-series, 15408,
15206, 27000-series, and 25000-series, to effectively model the knowledge
domain where digital engineering applies. The harmonization of the concepts
used in these international standards continues to improve with each revision,
but it may be more effectively accomplished by relying on the descriptive logic
formalized in the Web Ontology Language (OWL 2 DL). This paper presents a
verified and consistent ontology based on the Basic Formal Ontology (BFO) and
Common Core Ontologies (CCO) that defines Seamless Digital Engineering as a
digital tooling paradigm that relies on formal verification of digital
interfaces to provide a system-level qualification of the assured integrity of
a Digital Engineering Environment. The present work defines classes and
equivalence axioms, while using only the BFO- and CCO-defined object properties
that relate them, to provide a baseline analysis that may inform future
DE-related ontology development, using a case study to formally define the
`seamless' quality in relation to the updated ISO 25010 SQuaRE product quality
model. We identified ISO meta-model inconsistencies that are resolvable using
the BFO/CCO ontological framework, and define `seamless' as both a system
integration quality and a Human-Computer Interface quality-in-use, working to
disambiguate this concept in the context of DE.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.17171v1},
File          = {2507.17171v1.pdf}
}
@article{2507.19537v2,
Author        = {Felix Kraus and Nicolas BlumenrÃ¶hr and Danah Tonne and Achim Streit},
Title         = {Mind the Language Gap in Digital Humanities: LLM-Aided Translation of
  SKOS Thesauri},
Eprint        = {2507.19537v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We introduce WOKIE, an open-source, modular, and ready-to-use pipeline for
the automated translation of SKOS thesauri. This work addresses a critical need
in the Digital Humanities (DH), where language diversity can limit access,
reuse, and semantic interoperability of knowledge resources. WOKIE combines
external translation services with targeted refinement using Large Language
Models (LLMs), balancing translation quality, scalability, and cost. Designed
to run on everyday hardware and be easily extended, the application requires no
prior expertise in machine translation or LLMs. We evaluate WOKIE across
several DH thesauri in 15 languages with different parameters, translation
services and LLMs, systematically analysing translation quality, performance,
and ontology matching improvements. Our results show that WOKIE is suitable to
enhance the accessibility, reuse, and cross-lingual interoperability of
thesauri by hurdle-free automated translation and improved ontology matching
performance, supporting more inclusive and multilingual research
infrastructures.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.19537v2},
File          = {2507.19537v2.pdf}
}
@article{2507.15917v2,
Author        = {Adrian Kaiser and Claudiu Leoveanu-Condrei and Ryan Gold and Marius-Constantin Dinu and Markus Hofmarcher},
Title         = {HyDRA: A Hybrid-Driven Reasoning Architecture for Verifiable Knowledge
  Graphs},
Eprint        = {2507.15917v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {The synergy between symbolic knowledge, often represented by Knowledge Graphs
(KGs), and the generative capabilities of neural networks is central to
advancing neurosymbolic AI. A primary bottleneck in realizing this potential is
the difficulty of automating KG construction, which faces challenges related to
output reliability, consistency, and verifiability. These issues can manifest
as structural inconsistencies within the generated graphs, such as the
formation of disconnected $\textit{isolated islands}$ of data or the inaccurate
conflation of abstract classes with specific instances. To address these
challenges, we propose HyDRA, a $\textbf{Hy}$brid-$\textbf{D}$riven
$\textbf{R}$easoning $\textbf{A}$rchitecture designed for verifiable KG
automation. Given a domain or an initial set of documents, HyDRA first
constructs an ontology via a panel of collaborative neurosymbolic agents. These
agents collaboratively agree on a set of competency questions (CQs) that define
the scope and requirements the ontology must be able to answer. Given these
CQs, we build an ontology graph that subsequently guides the automated
extraction of triplets for KG generation from arbitrary documents. Inspired by
design-by-contracts (DbC) principles, our method leverages verifiable contracts
as the primary control mechanism to steer the generative process of Large
Language Models (LLMs). To verify the output of our approach, we extend beyond
standard benchmarks and propose an evaluation framework that assesses the
functional correctness of the resulting KG by leveraging symbolic verifications
as described by the neurosymbolic AI framework, $\textit{SymbolicAI}$. This
work contributes a hybrid-driven architecture for improving the reliability of
automated KG construction and the exploration of evaluation methods for
measuring the functional integrity of its output. The code is publicly
available.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.15917v2},
File          = {2507.15917v2.pdf}
}
@article{2507.22914v1,
Author        = {Victor Eiti Yamamoto and Hideaki Takeda},
Title         = {Full Triple Matcher: Integrating all triple elements between
  heterogeneous Knowledge Graphs},
Eprint        = {2507.22914v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graphs (KGs) are powerful tools for representing and reasoning over
structured information. Their main components include schema, identity, and
context. While schema and identity matching are well-established in ontology
and entity matching research, context matching remains largely unexplored. This
is particularly important because real-world KGs often vary significantly in
source, size, and information density - factors not typically represented in
the datasets on which current entity matching methods are evaluated. As a
result, existing approaches may fall short in scenarios where diverse and
complex contexts need to be integrated.
  To address this gap, we propose a novel KG integration method consisting of
label matching and triple matching. We use string manipulation, fuzzy matching,
and vector similarity techniques to align entity and predicate labels. Next, we
identify mappings between triples that convey comparable information, using
these mappings to improve entity-matching accuracy. Our approach demonstrates
competitive performance compared to leading systems in the OAEI competition and
against supervised methods, achieving high accuracy across diverse test cases.
Additionally, we introduce a new dataset derived from the benchmark dataset to
evaluate the triple-matching step more comprehensively.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.22914v1},
File          = {2507.22914v1.pdf}
}
@article{2507.14552v1,
Author        = {Anna Sofia Lippolis and Mohammad Javad Saeedizade and Robin KeskisÃ¤rkkÃ¤ and Aldo Gangemi and Eva Blomqvist and Andrea Giovanni Nuzzolese},
Title         = {Large Language Models Assisting Ontology Evaluation},
Eprint        = {2507.14552v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontology evaluation through functional requirements, such as testing via
competency question (CQ) verification, is a well-established yet costly,
labour-intensive, and error-prone endeavour, even for ontology engineering
experts. In this work, we introduce OE-Assist, a novel framework designed to
assist ontology evaluation through automated and semi-automated CQ
verification. By presenting and leveraging a dataset of 1,393 CQs paired with
corresponding ontologies and ontology stories, our contributions present, to
our knowledge, the first systematic investigation into large language model
(LLM)-assisted ontology evaluation, and include: (i) evaluating the
effectiveness of a LLM-based approach for automatically performing CQ
verification against a manually created gold standard, and (ii) developing and
assessing an LLM-powered framework to assist CQ verification with Prot\'eg\'e,
by providing suggestions. We found that automated LLM-based evaluation with
o1-preview and o3-mini perform at a similar level to the average user's
performance.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.14552v1},
File          = {2507.14552v1.pdf}
}
@article{2507.14334v1,
Author        = {Hui Yang and Jiaoyan Chen and Yuan He and Yongsheng Gao and Ian Horrocks},
Title         = {Language Models as Ontology Encoders},
Eprint        = {2507.14334v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {OWL (Web Ontology Language) ontologies which are able to formally represent
complex knowledge and support semantic reasoning have been widely adopted
across various domains such as healthcare and bioinformatics. Recently,
ontology embeddings have gained wide attention due to its potential to infer
plausible new knowledge and approximate complex reasoning. However, existing
methods face notable limitations: geometric model-based embeddings typically
overlook valuable textual information, resulting in suboptimal performance,
while the approaches that incorporate text, which are often based on language
models, fail to preserve the logical structure. In this work, we propose a new
ontology embedding method OnT, which tunes a Pretrained Language Model (PLM)
via geometric modeling in a hyperbolic space for effectively incorporating
textual labels and simultaneously preserving class hierarchies and other
logical relationships of Description Logic EL. Extensive experiments on four
real-world ontologies show that OnT consistently outperforms the baselines
including the state-of-the-art across both tasks of prediction and inference of
axioms. OnT also demonstrates strong potential in real-world applications,
indicated by its robust transfer learning abilities and effectiveness in real
cases of constructing a new ontology from SNOMED CT. Data and code are
available at https://github.com/HuiYang1997/OnT.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.14334v1},
File          = {2507.14334v1.pdf}
}
@article{2507.14330v3,
Author        = {Arshad Beg and Diarmuid O'Donoghue and Rosemary Monahan},
Title         = {Leveraging LLMs for Formal Software Requirements -- Challenges and
  Prospects},
Eprint        = {2507.14330v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {Software correctness is ensured mathematically through formal verification,
which involves the resources of generating formal requirement specifications
and having an implementation that must be verified. Tools such as
model-checkers and theorem provers ensure software correctness by verifying the
implementation against the specification. Formal methods deployment is
regularly enforced in the development of safety-critical systems e.g.
aerospace, medical devices and autonomous systems. Generating these
specifications from informal and ambiguous natural language requirements
remains the key challenge. Our project, VERIFAI^{1}, aims to investigate
automated and semi-automated approaches to bridge this gap, using techniques
from Natural Language Processing (NLP), ontology-based domain modelling,
artefact reuse, and large language models (LLMs). This position paper presents
a preliminary synthesis of relevant literature to identify recurring challenges
and prospective research directions in the generation of verifiable
specifications from informal requirements.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.14330v3},
File          = {2507.14330v3.pdf}
}
@article{2507.14032v1,
Author        = {Lam Nguyen and Erika Barcelos and Roger French and Yinghui Wu},
Title         = {KROMA: Ontology Matching with Knowledge Retrieval and Large Language
  Models},
Eprint        = {2507.14032v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontology Matching (OM) is a cornerstone task of semantic interoperability,
yet existing systems often rely on handcrafted rules or specialized models with
limited adaptability. We present KROMA, a novel OM framework that harnesses
Large Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)
pipeline to dynamically enrich the semantic context of OM tasks with
structural, lexical, and definitional knowledge. To optimize both performance
and efficiency, KROMA integrates a bisimilarity-based concept matching and a
lightweight ontology refinement step, which prune candidate concepts and
substantially reduce the communication overhead from invoking LLMs. Through
experiments on multiple benchmark datasets, we show that integrating knowledge
retrieval with context-augmented LLMs significantly enhances ontology matching,
outperforming both classic OM systems and cutting-edge LLM-based approaches
while keeping communication overhead comparable. Our study highlights the
feasibility and benefit of the proposed optimization techniques (targeted
knowledge retrieval, prompt enrichment, and ontology refinement) for ontology
matching at scale.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.14032v1},
File          = {2507.14032v1.pdf}
}
@article{2507.13987v1,
Author        = {Simon FlÃ¼gel and Martin Glauer and Till Mossakowski and Fabian Neuhaus},
Title         = {ChemLog: Making MSOL Viable for Ontological Classification and Learning},
Eprint        = {2507.13987v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LO},
Abstract      = {Despite its prevalence, in many domains, OWL is not expressive enough to
define ontology classes. In this paper, we present an approach that allows to
use monadic second-order formalisations for ontology classification. As a case
study, we have applied our approach to 14 peptide-related classes from the
chemistry ontology ChEBI. For these classes, a monadic second-order logic
formalisation has been developed and applied both to ChEBI as well as to 119
million molecules from the chemistry database PubChem. While this logical
approach alone is limited to classification for the specified classes (in our
case, (sub)classes of peptides), transformer deep learning models scale
classification to the whole of the ChEBI ontology. We show that when using the
classifications obtained by the logical approach as training data, the
performance of the deep learning models can be significantly enhanced.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.13987v1},
File          = {2507.13987v1.pdf}
}
@article{2507.13759v1,
Author        = {Carlos Bobed and Carlota Quintana and Eduardo Mena and Jorge Bobed and Fernando Bobillo},
Title         = {OntView: What you See is What you Meant},
Eprint        = {2507.13759v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In the field of knowledge management and computer science, ontologies provide
a structured framework for modeling domain-specific knowledge by defining
concepts and their relationships. However, the lack of tools that provide
effective visualization is still a significant challenge. While numerous
ontology editors and viewers exist, most of them fail to graphically represent
ontology structures in a meaningful and non-overwhelming way, limiting users'
ability to comprehend dependencies and properties within large ontological
frameworks.
  In this paper, we present OntView, an ontology viewer that is designed to
provide users with an intuitive visual representation of ontology concepts and
their formal definitions through a user-friendly interface. Building on the use
of a DL reasoner, OntView follows a "What you see is what you meant" paradigm,
showing the actual inferred knowledge. One key aspect for this is its ability
to visualize General Concept Inclusions (GCI), a feature absent in existing
visualization tools. Moreover, to avoid a possible information overload,
OntView also offers different ways to show a simplified view of the ontology
by: 1) creating ontology summaries by assessing the importance of the concepts
(according to different available algorithms), 2) focusing the visualization on
the existing TBox elements between two given classes and 3) allowing to
hide/show different branches in a dynamic way without losing the semantics.
OntView has been released with an open-source license for the whole community.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.13759v1},
File          = {2507.13759v1.pdf}
}
@article{2507.13742v1,
Author        = {Oussama Bouaggad and Natalia Grabar},
Title         = {Search-Optimized Quantization in Biomedical Ontology Alignment},
Eprint        = {2507.13742v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {In the fast-moving world of AI, as organizations and researchers develop more
advanced models, they face challenges due to their sheer size and computational
demands. Deploying such models on edge devices or in resource-constrained
environments adds further challenges related to energy consumption, memory
usage and latency. To address these challenges, emerging trends are shaping the
future of efficient model optimization techniques. From this premise, by
employing supervised state-of-the-art transformer-based models, this research
introduces a systematic method for ontology alignment, grounded in cosine-based
semantic similarity between a biomedical layman vocabulary and the Unified
Medical Language System (UMLS) Metathesaurus. It leverages Microsoft Olive to
search for target optimizations among different Execution Providers (EPs) using
the ONNX Runtime backend, followed by an assembled process of dynamic
quantization employing Intel Neural Compressor and IPEX (Intel Extension for
PyTorch). Through our optimization process, we conduct extensive assessments on
the two tasks from the DEFT 2020 Evaluation Campaign, achieving a new
state-of-the-art in both. We retain performance metrics intact, while attaining
an average inference speed-up of 20x and reducing memory usage by approximately
70%.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.13742v1},
File          = {2507.13742v1.pdf}
}
@article{2507.13646v1,
Author        = {Nimisha Ghosh and Daniele Santoni and Debaleena Nawn and Eleonora Ottaviani and Giovanni Felici},
Title         = {A Comprehensive Review of Transformer-based language models for Protein
  Sequence Analysis and Design},
Eprint        = {2507.13646v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {The impact of Transformer-based language models has been unprecedented in
Natural Language Processing (NLP). The success of such models has also led to
their adoption in other fields including bioinformatics. Taking this into
account, this paper discusses recent advances in Transformer-based models for
protein sequence analysis and design. In this review, we have discussed and
analysed a significant number of works pertaining to such applications. These
applications encompass gene ontology, functional and structural protein
identification, generation of de novo proteins and binding of proteins. We
attempt to shed light on the strength and weaknesses of the discussed works to
provide a comprehensive insight to readers. Finally, we highlight shortcomings
in existing research and explore potential avenues for future developments. We
believe that this review will help researchers working in this field to have an
overall idea of the state of the art in this field, and to orient their future
studies.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.13646v1},
File          = {2507.13646v1.pdf}
}
@article{2507.13115v1,
Author        = {Jaya Caporusso and Matthew Purver and Senja Pollak},
Title         = {A Computational Framework to Identify Self-Aspects in Text},
Eprint        = {2507.13115v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This Ph.D. proposal introduces a plan to develop a computational framework to
identify Self-aspects in text. The Self is a multifaceted construct and it is
reflected in language. While it is described across disciplines like cognitive
science and phenomenology, it remains underexplored in natural language
processing (NLP). Many of the aspects of the Self align with psychological and
other well-researched phenomena (e.g., those related to mental health),
highlighting the need for systematic NLP-based analysis. In line with this, we
plan to introduce an ontology of Self-aspects and a gold-standard annotated
dataset. Using this foundation, we will develop and evaluate conventional
discriminative models, generative large language models, and embedding-based
retrieval approaches against four main criteria: interpretability, ground-truth
adherence, accuracy, and computational efficiency. Top-performing models will
be applied in case studies in mental health and empirical phenomenology.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.13115v1},
File          = {2507.13115v1.pdf}
}
@article{2507.12957v1,
Author        = {Miriam Meckel and Philipp Hacker and Lea Steinacker and Aurelija Lukoseviciene and Surjo R. Soekadar and Jacob Slosser and Gina-Maria Poehlmann},
Title         = {The Goldilocks zone of governing technology: Leveraging uncertainty for
  responsible quantum practices},
Eprint        = {2507.12957v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CY},
Abstract      = {Emerging technologies challenge conventional governance approaches,
especially when uncertainty is not a temporary obstacle but a foundational
feature as in quantum computing. This paper reframes uncertainty from a
governance liability to a generative force, using the paradigms of quantum
mechanics to propose adaptive, probabilistic frameworks for responsible
innovation. We identify three interdependent layers of uncertainty--physical,
technical, and societal--central to the evolution of quantum technologies. The
proposed Quantum Risk Simulator (QRS) serves as a conceptual example, an
imaginative blueprint rather than a prescriptive tool, meant to illustrate how
probabilistic reasoning could guide dynamic, uncertainty-based governance. By
foregrounding epistemic and ontological ambiguity, and drawing analogies from
cognitive neuroscience and predictive processing, we suggest a new model of
governance aligned with the probabilistic essence of quantum systems. This
model, we argue, is especially promising for the European Union as a third way
between laissez-faire innovation and state-led control, offering a flexible yet
responsible pathway for regulating quantum and other frontier technologies.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.12957v1},
File          = {2507.12957v1.pdf}
}
@article{2507.12286v1,
Author        = {Anouk Oudshoorn and Magdalena Ortiz and Mantas Simkus},
Title         = {SHACL Validation in the Presence of Ontologies: Semantics and Rewriting
  Techniques},
Eprint        = {2507.12286v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LO},
Abstract      = {SHACL and OWL are two prominent W3C standards for managing RDF data. These
languages share many features, but they have one fundamental difference: OWL,
designed for inferring facts from incomplete data, makes the open-world
assumption, whereas SHACL is a constraint language that treats the data as
complete and must be validated under the closed-world assumption. The
combination of both formalisms is very appealing and has been called for, but
their semantic gap is a major challenge, semantically and computationally. In
this paper, we advocate a semantics for SHACL validation in the presence of
ontologies based on core universal models. We provide a technique for
constructing these models for ontologies in the rich data-tractable description
logic Horn-ALCHIQ. Furthermore, we use a finite representation of this model to
develop a rewriting technique that reduces SHACL validation in the presence of
ontologies to standard validation. Finally, we study the complexity of SHACL
validation in the presence of ontologies, and show that even very simple
ontologies make the problem EXPTIME-complete, and PTIME-complete in data
complexity.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.12286v1},
File          = {2507.12286v1.pdf}
}
@article{2507.11770v1,
Author        = {Giang Nguyen and Mihai Pomarlan and Sascha Jongebloed and Nils Leusmann and Minh Nhat Vu and Michael Beetz},
Title         = {Generating Actionable Robot Knowledge Bases by Combining 3D Scene Graphs
  with Robot Ontologies},
Eprint        = {2507.11770v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {In robotics, the effective integration of environmental data into actionable
knowledge remains a significant challenge due to the variety and
incompatibility of data formats commonly used in scene descriptions, such as
MJCF, URDF, and SDF. This paper presents a novel approach that addresses these
challenges by developing a unified scene graph model that standardizes these
varied formats into the Universal Scene Description (USD) format. This
standardization facilitates the integration of these scene graphs with robot
ontologies through semantic reporting, enabling the translation of complex
environmental data into actionable knowledge essential for cognitive robotic
control. We evaluated our approach by converting procedural 3D environments
into USD format, which is then annotated semantically and translated into a
knowledge graph to effectively answer competency questions, demonstrating its
utility for real-time robotic decision-making. Additionally, we developed a
web-based visualization tool to support the semantic mapping process, providing
users with an intuitive interface to manage the 3D environment.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.11770v1},
File          = {2507.11770v1.pdf}
}
@article{2507.11568v1,
Author        = {Santos E. Moreta Reyes},
Title         = {La Ãltima Frontera de La FilosofÃ­a: Hacia una SÃ­ntesis de La
  Ãtica del Futuro a Largo Plazo, el Riesgo Existencial y la OntologÃ­a
  Posthumana},
Eprint        = {2507.11568v1},
DOI           = {10.48550/arXiv.2507.11568},
ArchivePrefix = {arXiv},
PrimaryClass  = {physics.soc-ph},
Abstract      = {Humanity's unprecedented technological capacity and concurrent existential
risks reveal a critical lacuna in the philosophical tradition: the absence of a
systematic framework for the long-term future. This article argues that
formulating such a framework is the central ethical imperative of our era. To
defend this thesis, it synthesizes the normative ethics of Hans Jonas and Derek
Parfit with the analytical framework of Nick Bostrom's work on existential risk
and longtermism. The analysis further addresses the ontological challenge posed
by posthumanism to the human 'subject' and explores the functional role of a
secular cosmic purpose in motivating long-term action. The paper's main
contribution is the articulation of a synthetic research agenda for a
prospective philosophy, one that integrates axiology, risk management, and
ontology to guide humanity through its perilous technological adolescence.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.11568v1},
File          = {2507.11568v1.pdf}
}
@article{2507.09617v1,
Author        = {Margherita Martorana and Francesca Urgese and Mark Adamik and Ilaria Tiddi},
Title         = {Bridging Bots: from Perception to Action via Multimodal-LMs and
  Knowledge Graphs},
Eprint        = {2507.09617v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Personal service robots are deployed to support daily living in domestic
environments, particularly for elderly and individuals requiring assistance.
These robots must perceive complex and dynamic surroundings, understand tasks,
and execute context-appropriate actions. However, current systems rely on
proprietary, hard-coded solutions tied to specific hardware and software,
resulting in siloed implementations that are difficult to adapt and scale
across platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to
enable interoperability across systems, through structured and standardized
representations of knowledge and reasoning. However, symbolic systems such as
KGs and ontologies struggle with raw and noisy sensory input. In contrast,
multimodal language models are well suited for interpreting input such as
images and natural language, but often lack transparency, consistency, and
knowledge grounding. In this work, we propose a neurosymbolic framework that
combines the perceptual strengths of multimodal language models with the
structured representations provided by KGs and ontologies, with the aim of
supporting interoperability in robotic applications. Our approach generates
ontology-compliant KGs that can inform robot behavior in a platform-independent
manner. We evaluated this framework by integrating robot perception data,
ontologies, and five multimodal models (three LLaMA and two GPT models), using
different modes of neural-symbolic interaction. We assess the consistency and
effectiveness of the generated KGs across multiple runs and configurations, and
perform statistical analyzes to evaluate performance. Results show that GPT-o1
and LLaMA 4 Maverick consistently outperform other models. However, our
findings also indicate that newer models do not guarantee better results,
highlighting the critical role of the integration strategy in generating
ontology-compliant KGs.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.09617v1},
File          = {2507.09617v1.pdf}
}
@article{2507.07906v1,
Author        = {Anant Gupta and Rajarshi Bhowmik and Geoffrey Gunow},
Title         = {Agentic Retrieval of Topics and Insights from Earnings Calls},
Eprint        = {2507.07906v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Tracking the strategic focus of companies through topics in their earnings
calls is a key task in financial analysis. However, as industries evolve,
traditional topic modeling techniques struggle to dynamically capture emerging
topics and their relationships. In this work, we propose an LLM-agent driven
approach to discover and retrieve emerging topics from quarterly earnings
calls. We propose an LLM-agent to extract topics from documents, structure them
into a hierarchical ontology, and establish relationships between new and
existing topics through a topic ontology. We demonstrate the use of extracted
topics to infer company-level insights and emerging trends over time. We
evaluate our approach by measuring ontology coherence, topic evolution
accuracy, and its ability to surface emerging financial trends.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.07906v1},
File          = {2507.07906v1.pdf}
}
@article{2507.07893v4,
Author        = {Mingda Zhang and Na Zhao and Jianglong Qing and Qing xu and Kaiwen Pan and Ting luo},
Title         = {An Integrated Framework of Prompt Engineering and Multidimensional
  Knowledge Graphs for Legal Dispute Analysis},
Eprint        = {2507.07893v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Legal dispute analysis is crucial for intelligent legal assistance systems.
However, current LLMs face significant challenges in understanding complex
legal concepts, maintaining reasoning consistency, and accurately citing legal
sources. This research presents a framework combining prompt engineering with
multidimensional knowledge graphs to improve LLMs' legal dispute analysis.
Specifically, the framework includes a three-stage hierarchical prompt
structure (task definition, knowledge background, reasoning guidance) along
with a three-layer knowledge graph (legal ontology, representation, instance
layers). Additionally, four supporting methods enable precise legal concept
retrieval: direct code matching, semantic vector similarity, ontology path
reasoning, and lexical segmentation. Through extensive testing, results show
major improvements: sensitivity increased by 11.1%-11.3%, specificity by
5.4%-6.0%, and citation accuracy by 29.5%-39.7%. As a result, the framework
provides better legal analysis and understanding of judicial logic, thus
offering a new technical method for intelligent legal assistance systems.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.07893v4},
File          = {2507.07893v4.pdf}
}
@article{2507.07748v1,
Author        = {Peizhang Shao and Linrui Xu and Jinxi Wang and Wei Zhou and Xingyu Wu},
Title         = {When Large Language Models Meet Law: Dual-Lens Taxonomy, Technical
  Advances, and Ethical Governance},
Eprint        = {2507.07748v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper establishes the first comprehensive review of Large Language
Models (LLMs) applied within the legal domain. It pioneers an innovative dual
lens taxonomy that integrates legal reasoning frameworks and professional
ontologies to systematically unify historical research and contemporary
breakthroughs. Transformer-based LLMs, which exhibit emergent capabilities such
as contextual reasoning and generative argumentation, surmount traditional
limitations by dynamically capturing legal semantics and unifying evidence
reasoning. Significant progress is documented in task generalization, reasoning
formalization, workflow integration, and addressing core challenges in text
processing, knowledge integration, and evaluation rigor via technical
innovations like sparse attention mechanisms and mixture-of-experts
architectures. However, widespread adoption of LLM introduces critical
challenges: hallucination, explainability deficits, jurisdictional adaptation
difficulties, and ethical asymmetry. This review proposes a novel taxonomy that
maps legal roles to NLP subtasks and computationally implements the Toulmin
argumentation framework, thus systematizing advances in reasoning, retrieval,
prediction, and dispute resolution. It identifies key frontiers including
low-resource systems, multimodal evidence integration, and dynamic rebuttal
handling. Ultimately, this work provides both a technical roadmap for
researchers and a conceptual framework for practitioners navigating the
algorithmic future, laying a robust foundation for the next era of legal
artificial intelligence. We have created a GitHub repository to index the
relevant papers: https://github.com/Kilimajaro/LLMs_Meet_Law.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.07748v1},
File          = {2507.07748v1.pdf}
}
@article{2507.07666v1,
Author        = {Yanzi Zhang and Felix Moorhoff and Sizhe Qiu and Wenjuan Dong and David Medina-Ortiz and Jing Zhao and Mehdi D. Davari},
Title         = {Machine Learning-Driven Enzyme Mining: Opportunities, Challenges, and
  Future Perspectives},
Eprint        = {2507.07666v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.BM},
Abstract      = {Enzyme mining is rapidly evolving as a data-driven strategy to identify
biocatalysts with tailored functions from the vast landscape of uncharacterized
proteins. The integration of machine learning into these workflows enables
high-throughput prediction of enzyme functions, including Enzyme Commission
numbers, Gene Ontology terms, substrate specificity, and key catalytic
properties such as kinetic parameters, optimal temperature, pH, solubility, and
thermophilicity. This review provides a systematic overview of state-of-the-art
machine learning models and highlights representative case studies that
demonstrate their effectiveness in accelerating enzyme discovery.
  Despite notable progress, current approaches remain limited by data scarcity,
model generalizability, and interpretability. We discuss emerging strategies to
overcome these challenges, including multi-task learning, integration of
multi-modal data, and explainable AI. Together, these developments establish
ML-guided enzyme mining as a scalable and predictive framework for uncovering
novel biocatalysts, with broad applications in biocatalysis, biotechnology, and
synthetic biology.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.07666v1},
File          = {2507.07666v1.pdf}
}
@article{2507.06208v2,
Author        = {Pablo Garcia-Cuadrillero and Fabio Revuelta and Jose Angel Capitan},
Title         = {Ontological differentiation as a measure of semantic accuracy},
Eprint        = {2507.06208v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cond-mat.dis-nn},
Abstract      = {Understanding semantic relationships within complex networks derived from
lexical resources is fundamental for network science and language modeling.
While network embedding methods capture contextual similarity, quantifying
semantic distance based directly on explicit definitional structure remains
challenging. Accurate measures of semantic similarity allow for navigation on
lexical networks based on maximizing semantic similarity in each navigation
jump (Semantic Navigation, SN). This work introduces Ontological
Differentiation (OD), a formal method for measuring divergence between concepts
by analyzing overlap during recursive definition expansion. The methodology is
applied to networks extracted from the Simple English Wiktionary, comparing OD
scores with other measures of semantic similarity proposed in the literature
(cosine similarity based on random-walk network exploration). We find weak
correlations between direct pairwise OD scores and cosine similarities across
~2 million word pairs, sampled from a pool representing over 50% of the entries
in the Wiktionary lexicon. This establishes OD as a largely independent,
definition-based semantic metric, whose orthogonality to cosine similarity
becomes more pronounced when low-semantic-content terms were removed from the
dataset. Additionally, we use cumulative OD scores to evaluate paths generated
by vector-based SN and structurally optimal Shortest Paths (SP) across
networks. We find SN paths consistently exhibit significantly lower cumulative
OD scores than shortest paths, suggesting that SN produces trajectories more
coherent with the dictionary's definitional structure, as measured by OD.
Ontological Differentiation thus provides a novel, definition-grounded tool for
analyzing semantic structure and validating navigation processes in lexical
networks.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.06208v2},
File          = {2507.06208v2.pdf}
}
@article{2507.06107v2,
Author        = {Junaid Ahmed Khan and Andrea Bartolini},
Title         = {A Unified Ontology for Scalable Knowledge Graph-Driven Operational Data
  Analytics in High-Performance Computing Systems},
Eprint        = {2507.06107v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DC},
Abstract      = {Modern high-performance computing (HPC) systems generate massive volumes of
heterogeneous telemetry data from millions of sensors monitoring compute,
memory, power, cooling, and storage subsystems. As HPC infrastructures scale to
support increasingly complex workloads-including generative AI-the need for
efficient, reliable, and interoperable telemetry analysis becomes critical.
Operational Data Analytics (ODA) has emerged to address these demands; however,
the reliance on schema-less storage solutions limits data accessibility and
semantic integration. Ontologies and knowledge graphs (KG) provide an effective
way to enable efficient and expressive data querying by capturing domain
semantics, but they face challenges such as significant storage overhead and
the limited applicability of existing ontologies, which are often tailored to
specific HPC systems only. In this paper, we present the first unified ontology
for ODA in HPC systems, designed to enable semantic interoperability across
heterogeneous data centers. Our ontology models telemetry data from the two
largest publicly available ODA datasets-M100 (Cineca, Italy) and F-DATA
(Fugaku, Japan)-within a single data model. The ontology is validated through
36 competency questions reflecting real-world stakeholder requirements, and we
introduce modeling optimizations that reduce knowledge graph (KG) storage
overhead by up to 38.84% compared to a previous approach, with an additional
26.82% reduction depending on the desired deployment configuration. This work
paves the way for scalable ODA KGs and supports not only analysis within
individual systems, but also cross-system analysis across heterogeneous HPC
systems.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.06107v2},
File          = {2507.06107v2.pdf}
}
@article{2507.05799v1,
Author        = {Amane Watahiki and Tomoki Doi and Taiga Shinozaki and Satoshi Nishida and Takuya Niikawa and Katsunori Miyahara and Hitomi Yanaka},
Title         = {Bridging Perception and Language: A Systematic Benchmark for LVLMs'
  Understanding of Amodal Completion Reports},
Eprint        = {2507.05799v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {One of the main objectives in developing large vision-language models (LVLMs)
is to engineer systems that can assist humans with multimodal tasks, including
interpreting descriptions of perceptual experiences. A central phenomenon in
this context is amodal completion, in which people perceive objects even when
parts of those objects are hidden. Although numerous studies have assessed
whether computer-vision algorithms can detect or reconstruct occluded regions,
the inferential abilities of LVLMs on texts related to amodal completion remain
unexplored. To address this gap, we constructed a benchmark grounded in Basic
Formal Ontology to achieve a systematic classification of amodal completion.
Our results indicate that while many LVLMs achieve human-comparable performance
overall, their accuracy diverges for certain types of objects being completed.
Notably, in certain categories, some LLaVA-NeXT variants and Claude 3.5 Sonnet
exhibit lower accuracy on original images compared to blank stimuli lacking
visual content. Intriguingly, this disparity emerges only under Japanese
prompting, suggesting a deficiency in Japanese-specific linguistic competence
among these models.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.05799v1},
File          = {2507.05799v1.pdf}
}
@article{2507.05767v1,
Author        = {Ngoc Luyen Le and Marie-HÃ©lÃ¨ne Abel and Bertrand Laforge},
Title         = {Vers un cadre ontologique pour la gestion des comp{Ã©}tences : {Ã }
  des fins de formation, de recrutement, de m{Ã©}tier, ou de recherches
  associ{Ã©}es},
Eprint        = {2507.05767v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {The rapid transformation of the labor market, driven by technological
advancements and the digital economy, requires continuous competence
development and constant adaptation. In this context, traditional competence
management systems lack interoperability, adaptability, and semantic
understanding, making it difficult to align individual competencies with labor
market needs and training programs. This paper proposes an ontology-based
framework for competence management, enabling a structured representation of
competencies, occupations, and training programs. By leveraging ontological
models and semantic reasoning, this framework aims to enhance the automation of
competence-to-job matching, the personalization of learning recommendations,
and career planning. This study discusses the design, implementation, and
potential applications of the framework, focusing on competence training
programs, job searching, and finding competent individuals.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.05767v1},
File          = {2507.05767v1.pdf}
}
@article{2507.04377v3,
Author        = {Xiao Zhang and Johan Bos},
Title         = {Multi-Modal Semantic Parsing for the Interpretation of Tombstone
  Inscriptions},
Eprint        = {2507.04377v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Tombstones are historically and culturally rich artifacts, encapsulating
individual lives, community memory, historical narratives and artistic
expression. Yet, many tombstones today face significant preservation
challenges, including physical erosion, vandalism, environmental degradation,
and political shifts. In this paper, we introduce a novel multi-modal framework
for tombstones digitization, aiming to improve the interpretation, organization
and retrieval of tombstone content. Our approach leverages vision-language
models (VLMs) to translate tombstone images into structured Tombstone Meaning
Representations (TMRs), capturing both image and text information. To further
enrich semantic parsing, we incorporate retrieval-augmented generation (RAG)
for integrate externally dependent elements such as toponyms, occupation codes,
and ontological concepts. Compared to traditional OCR-based pipelines, our
method improves parsing accuracy from an F1 score of 36.1 to 89.5. We
additionally evaluate the model's robustness across diverse linguistic and
cultural inscriptions, and simulate physical degradation through image fusion
to assess performance under noisy or damaged conditions. Our work represents
the first attempt to formalize tombstone understanding using large
vision-language models, presenting implications for heritage preservation.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.04377v3},
File          = {2507.04377v3.pdf}
}
@article{2507.03829v1,
Author        = {George Hannah and Jacopo de Berardinis and Terry R. Payne and Valentina Tamma and Andrew Mitchell and Ellen Piercy and Ewan Johnson and Andrew Ng and Harry Rostron and Boris Konev},
Title         = {RELRaE: LLM-Based Relationship Extraction, Labelling, Refinement, and
  Evaluation},
Eprint        = {2507.03829v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {A large volume of XML data is produced in experiments carried out by robots
in laboratories. In order to support the interoperability of data between labs,
there is a motivation to translate the XML data into a knowledge graph. A key
stage of this process is the enrichment of the XML schema to lay the foundation
of an ontology schema. To achieve this, we present the RELRaE framework, a
framework that employs large language models in different stages to extract and
accurately label the relationships implicitly present in the XML schema. We
investigate the capability of LLMs to accurately generate these labels and then
evaluate them. Our work demonstrates that LLMs can be effectively used to
support the generation of relationship labels in the context of lab automation,
and that they can play a valuable role within semi-automatic ontology
generation frameworks more generally.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.03829v1},
File          = {2507.03829v1.pdf}
}
@article{2507.03674v2,
Author        = {Tek Raj Chhetri and Yibei Chen and Puja Trivedi and Dorota Jarecka and Saif Haobsh and Patrick Ray and Lydia Ng and Satrajit S. Ghosh},
Title         = {STRUCTSENSE: A Task-Agnostic Agentic Framework for Structured
  Information Extraction with Human-In-The-Loop Evaluation and Benchmarking},
Eprint        = {2507.03674v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The ability to extract structured information from unstructured sources-such
as free-text documents and scientific literature-is critical for accelerating
scientific discovery and knowledge synthesis. Large Language Models (LLMs) have
demonstrated remarkable capabilities in various natural language processing
tasks, including structured information extraction. However, their
effectiveness often diminishes in specialized, domain-specific contexts that
require nuanced understanding and expert-level domain knowledge. In addition,
existing LLM-based approaches frequently exhibit poor transferability across
tasks and domains, limiting their scalability and adaptability. To address
these challenges, we introduce StructSense, a modular, task-agnostic,
open-source framework for structured information extraction built on LLMs.
StructSense is guided by domain-specific symbolic knowledge encoded in
ontologies, enabling it to navigate complex domain content more effectively. It
further incorporates agentic capabilities through self-evaluative judges that
form a feedback loop for iterative refinement, and includes human-in-the-loop
mechanisms to ensure quality and validation. We demonstrate that StructSense
can overcome both the limitations of domain sensitivity and the lack of
cross-task generalizability, as shown through its application to diverse
neuroscience information extraction tasks.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.03674v2},
File          = {2507.03674v2.pdf}
}
@article{2507.03329v1,
Author        = {Devendra Patel and Aaditya Jain and Jayant Verma and Divyansh Rajput and Sunil Mahala and Ketki Suresh Khapare and Jayateja Kalla},
Title         = {NDAI-NeuroMAP: A Neuroscience-Specific Embedding Model for
  Domain-Specific Retrieval},
Eprint        = {2507.03329v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We present NDAI-NeuroMAP, the first neuroscience-domain-specific dense vector
embedding model engineered for high-precision information retrieval tasks. Our
methodology encompasses the curation of an extensive domain-specific training
corpus comprising 500,000 carefully constructed triplets
(query-positive-negative configurations), augmented with 250,000
neuroscience-specific definitional entries and 250,000 structured
knowledge-graph triplets derived from authoritative neurological ontologies. We
employ a sophisticated fine-tuning approach utilizing the
FremyCompany/BioLORD-2023 foundation model, implementing a multi-objective
optimization framework combining contrastive learning with triplet-based metric
learning paradigms. Comprehensive evaluation on a held-out test dataset
comprising approximately 24,000 neuroscience-specific queries demonstrates
substantial performance improvements over state-of-the-art general-purpose and
biomedical embedding models. These empirical findings underscore the critical
importance of domain-specific embedding architectures for neuroscience-oriented
RAG systems and related clinical natural language processing applications.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.03329v1},
File          = {2507.03329v1.pdf}
}
@article{2507.02758v1,
Author        = {Jonathan St-Onge and Randall Harp and Giulio Burgio and Timothy M. Waring and Juniper Lovato and Laurent HÃ©bert-Dufresne},
Title         = {Defining and classifying models of groups: The social ontology of
  higher-order networks},
Eprint        = {2507.02758v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {physics.soc-ph},
Abstract      = {In complex systems research, the study of higher-order interactions has
exploded in recent years. Researchers have formalized various types of group
interactions, such as public goods games, biological contagion, and information
broadcasting, showing how higher-order networks can capture group effects more
directly than pairwise models. However, equating hyperedges-edges involving
more than two agents-with groups can be misleading, as it obscures the
polysemous nature of ``group interactions''. For instance, many models of
higher-order interactions focus on the internal state of the hyperedge,
specifying dynamical rules at the group level. These models often neglect how
interactions with external groups can influence behaviors and dynamics within
the group. Yet, anthropologists and philosophers remind us that external norms,
factors, and forces governing intergroup behavior are essential to defining
within-group dynamics. In this paper, we synthesize concepts from social
ontology relevant to the emerging physics of higher-order networks. We propose
a typology for classifying models of group interactions based on two
perspectives. The first focuses on individuals within groups engaging in
collective action, where shared agency serves as the binding force. The second
adopts a group-first approach, emphasizing institutional facts that extend
beyond the specific individuals involved. Building on these perspectives, we
introduce four dimensions to classify models of group interactions:
persistence, coupling, reducibility, and alignment. For the physics of
higher-order networks, we provide a hierarchy of nested mathematical models to
explore the complex properties of social groups. We highlight social
interactions not yet explored in the literature on higher-order networks and
propose future research avenues to foster collaboration between social ontology
and the physics of complex systems.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.02758v1},
File          = {2507.02758v1.pdf}
}
@article{2507.01410v2,
Author        = {Abeer Dyoub and Francesca A. Lisi},
Title         = {A Fuzzy Approach to the Specification, Verification and Validation of
  Risk-Based Ethical Decision Making Models},
Eprint        = {2507.01410v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The ontological and epistemic complexities inherent in the moral domain make
it challenging to establish clear standards for evaluating the performance of a
moral machine. In this paper, we present a formal method to describe Ethical
Decision Making models based on ethical risk assessment. Then, we show how
these models that are specified as fuzzy rules can be verified and validated
using fuzzy Petri nets. A case study from the medical field is considered to
illustrate the proposed approach.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.01410v2},
File          = {2507.01410v2.pdf}
}
@article{2507.02989v1,
Author        = {Reham Alharbi and Valentina Tamma and Terry R. Payne and Jacopo de Berardinis},
Title         = {A Comparative Study of Competency Question Elicitation Methods from
  Ontology Requirements},
Eprint        = {2507.02989v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Competency Questions (CQs) are pivotal in knowledge engineering, guiding the
design, validation, and testing of ontologies. A number of diverse formulation
approaches have been proposed in the literature, ranging from completely manual
to Large Language Model (LLM) driven ones. However, attempts to characterise
the outputs of these approaches and their systematic comparison are scarce.
This paper presents an empirical comparative evaluation of three distinct CQ
formulation approaches: manual formulation by ontology engineers, instantiation
of CQ patterns, and generation using state of the art LLMs. We generate CQs
using each approach from a set of requirements for cultural heritage, and
assess them across different dimensions: degree of acceptability, ambiguity,
relevance, readability and complexity. Our contribution is twofold: (i) the
first multi-annotator dataset of CQs generated from the same source using
different methods; and (ii) a systematic comparison of the characteristics of
the CQs resulting from each approach. Our study shows that different CQ
generation approaches have different characteristics and that LLMs can be used
as a way to initially elicit CQs, however these are sensitive to the model used
to generate CQs and they generally require a further refinement step before
they can be used to model requirements.},
Year          = {2025},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2507.02989v1},
File          = {2507.02989v1.pdf}
}
@article{2506.23841v1,
Author        = {Ãtalo Oliveira and Stefano M. Nicoletti and Gal Engelberg and Mattia Fumagalli and Dan Klein and Giancarlo Guizzardi},
Title         = {An ontological lens on attack trees: Toward adequacy and
  interoperability},
Eprint        = {2506.23841v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {Attack Trees (AT) are a popular formalism for security analysis. They are
meant to display an attacker's goal decomposed into attack steps needed to
achieve it and compute certain security metrics (e.g., attack cost,
probability, and damage). ATs offer three important services: (a) conceptual
modeling capabilities for representing security risk management scenarios, (b)
a qualitative assessment to find root causes and minimal conditions of
successful attacks, and (c) quantitative analyses via security metrics
computation under formal semantics, such as minimal time and cost among all
attacks. Still, the AT language presents limitations due to its lack of
ontological foundations, thus compromising associated services. Via an
ontological analysis grounded in the Common Ontology of Value and Risk (COVER)
-- a reference core ontology based on the Unified Foundational Ontology (UFO)
-- we investigate the ontological adequacy of AT and reveal four significant
shortcomings: (1) ambiguous syntactical terms that can be interpreted in
various ways; (2) ontological deficit concerning crucial domain-specific
concepts; (3) lacking modeling guidance to construct ATs decomposing a goal;
(4) lack of semantic interoperability, resulting in ad hoc stand-alone tools.
We also discuss existing incremental solutions and how our analysis paves the
way for overcoming those issues through a broader approach to risk management
modeling.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.23841v1},
File          = {2506.23841v1.pdf}
}
@article{2506.20851v1,
Author        = {Srikar Reddy Gadusu and Larry Callahan and Samir Lababidi and Arunasri Nishtala and Sophia Healey and Hande McGinty},
Title         = {Generating Reliable Adverse event Profiles for Health through Automated
  Integrated Data (GRAPH-AID): A Semi-Automated Ontology Building Approach},
Eprint        = {2506.20851v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {As data and knowledge expand rapidly, adopting systematic methodologies for
ontology generation has become crucial. With the daily increases in data
volumes and frequent content changes, the demand for databases to store and
retrieve information for the creation of knowledge graphs has become
increasingly urgent. The previously established Knowledge Acquisition and
Representation Methodology (KNARM) outlines a systematic approach to address
these challenges and create knowledge graphs. However, following this
methodology highlights the existing challenge of seamlessly integrating Neo4j
databases with the Web Ontology Language (OWL). Previous attempts to integrate
data from Neo4j into an ontology have been discussed, but these approaches
often require an understanding of description logics (DL) syntax, which may not
be familiar to many users. Thus, a more accessible method is necessary to
bridge this gap. This paper presents a user-friendly approach that utilizes
Python and its rdflib library to support ontology development. We showcase our
novel approach through a Neo4j database we created by integrating data from the
Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS)
database. Using this dataset, we developed a Python script that automatically
generates the required classes and their axioms, facilitating a smoother
integration process. This approach offers a practical solution to the
challenges of ontology generation in the context of rapidly growing adverse
drug event datasets, supporting improved drug safety monitoring and public
health decision-making.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.20851v1},
File          = {2506.20851v1.pdf}
}
@article{2506.19750v4,
Author        = {Takashi Nishibayashi and Seiji Kanazawa and Kumpei Yamada},
Title         = {Evaluating Rare Disease Diagnostic Performance in Symptom Checkers: A
  Synthetic Vignette Simulation Approach},
Eprint        = {2506.19750v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Symptom Checkers (SCs) provide medical information tailored to user symptoms.
A critical challenge in SC development is preventing unexpected performance
degradation for individual diseases, especially rare diseases, when updating
algorithms. This risk stems from the lack of practical pre-deployment
evaluation methods. For rare diseases, obtaining sufficient evaluation data
from user feedback is difficult. To evaluate the impact of algorithm updates on
the diagnostic performance for individual rare diseases before deployment, this
study proposes and validates a novel Synthetic Vignette Simulation Approach.
This approach aims to enable this essential evaluation efficiently and at a low
cost. To estimate the impact of algorithm updates, we generated synthetic
vignettes from disease-phenotype annotations in the Human Phenotype Ontology
(HPO), a publicly available knowledge base for rare diseases curated by
experts. Using these vignettes, we simulated SC interviews to predict changes
in diagnostic performance. The effectiveness of this approach was validated
retrospectively by comparing the predicted changes with actual performance
metrics using the R-squared ($R^2$) coefficient. Our experiment, covering eight
past algorithm updates for rare diseases, showed that the proposed method
accurately predicted performance changes for diseases with phenotype frequency
information in HPO (n=5). For these updates, we found a strong correlation for
both Recall@8 change ($R^2$ = 0.83,$p$ = 0.031) and Precision@8 change ($R^2$ =
0.78,$p$ = 0.047). Our proposed method enables the pre-deployment evaluation of
SC algorithm changes for individual rare diseases. This evaluation is based on
a publicly available medical knowledge database created by experts, ensuring
transparency and explainability for stakeholders. Additionally, SC developers
can efficiently improve diagnostic performance at a low cost.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.19750v4},
File          = {2506.19750v4.pdf}
}
@article{2506.16087v1,
Author        = {Tom Jeleniewski and Hamied Nabizada and Jonathan Reif and Felix Gehlhoff and Alexander Fay},
Title         = {Consistency Verification in Ontology-Based Process Models with Parameter
  Interdependencies},
Eprint        = {2506.16087v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The formalization of process knowledge using ontologies enables consistent
modeling of parameter interdependencies in manufacturing. These
interdependencies are typically represented as mathematical expressions that
define relations between process parameters, supporting tasks such as
calculation, validation, and simulation. To support cross-context application
and knowledge reuse, such expressions are often defined in a generic form and
applied across multiple process contexts. This highlights the necessity of a
consistent and semantically coherent model to ensure the correctness of data
retrieval and interpretation. Consequently, dedicated mechanisms are required
to address key challenges such as selecting context-relevant data, ensuring
unit compatibility between variables and data elements, and verifying the
completeness of input data required for evaluating mathematical expressions.
This paper presents a set of verification mechanisms for a previously developed
ontology-based process model that integrates standardized process semantics,
data element definitions, and formal mathematical constructs. The approach
includes (i) SPARQL-based filtering to retrieve process-relevant data, (ii) a
unit consistency check based on expected-unit annotations and semantic
classification, and (iii) a data completeness check to validate the
evaluability of interdependencies. The applicability of the approach is
demonstrated with a use case from Resin Transfer Molding (RTM), supporting the
development of machine-interpretable and verifiable engineering models.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.16087v1},
File          = {2506.16087v1.pdf}
}
@article{2506.14640v1,
Author        = {Ina K. Schieferdecker},
Title         = {Navigating the growing field of research on AI for software testing --
  the taxonomy for AI-augmented software testing and an ontology-driven
  literature survey},
Eprint        = {2506.14640v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {In industry, software testing is the primary method to verify and validate
the functionality, performance, security, usability, and so on, of
software-based systems. Test automation has gained increasing attention in
industry over the last decade, following decades of intense research into test
automation and model-based testing. However, designing, developing, maintaining
and evolving test automation is a considerable effort. Meanwhile, AI's
breakthroughs in many engineering fields are opening up new perspectives for
software testing, for both manual and automated testing. This paper reviews
recent research on AI augmentation in software test automation, from no
automation to full automation. It also discusses new forms of testing made
possible by AI. Based on this, the newly developed taxonomy, ai4st, is
presented and used to classify recent research and identify open research
questions.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.14640v1},
File          = {2506.14640v1.pdf}
}
@article{2506.14504v2,
Author        = {Emmanuel Deruty},
Title         = {Evolving music theory for emerging musical languages},
Eprint        = {2506.14504v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SD},
Abstract      = {This chapter reconsiders the concept of pitch in contemporary popular music
(CPM), particularly in electronic contexts where traditional assumptions may
fail. Drawing on phenomenological and inductive methods, it argues that pitch
is not an ontologically objective property but a perceptual construct shaped by
listeners and conditions. Analyses of quasi-harmonic tones reveal that a single
tone can convey multiple pitches, giving rise to tonal fission. The perception
of pitch may also be multistable, varying for the same listener over time. In
this framework, the tuning system may emerge from a tone's internal structure.
A parallel with the coastline paradox supports a model of pitch grounded in
perceptual variability, challenging inherited theoretical norms.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.14504v2},
File          = {2506.14504v2.pdf}
}
@article{2506.13467v1,
Author        = {JosÃ© A. Pardo and Alicia GÃ³mez-Pascual and JosÃ© T. Palma and Juan A. BotÃ­a},
Title         = {Enhancing Omics Cohort Discovery for Research on Neurodegeneration
  through Ontology-Augmented Embedding Models},
Eprint        = {2506.13467v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The growing volume of omics and clinical data generated for neurodegenerative
diseases (NDs) requires new approaches for their curation so they can be
ready-to-use in bioinformatics. NeuroEmbed is an approach for the engineering
of semantically accurate embedding spaces to represent cohorts and samples. The
NeuroEmbed method comprises four stages: (1) extraction of ND cohorts from
public repositories; (2) semi-automated normalization and augmentation of
metadata of cohorts and samples using biomedical ontologies and clustering on
the embedding space; (3) automated generation of a natural language
question-answering (QA) dataset for cohorts and samples based on randomized
combinations of standardized metadata dimensions and (4) fine-tuning of a
domain-specific embedder to optimize queries. We illustrate the approach using
the GEO repository and the PubMedBERT pretrained embedder. Applying NeuroEmbed,
we semantically indexed 2,801 repositories and 150,924 samples. Amongst many
biology-relevant categories, we normalized more than 1,700 heterogeneous tissue
labels from GEO into 326 unique ontology-aligned concepts and enriched
annotations with new ontology-aligned terms, leading to a fold increase in size
for the metadata terms between 2.7 and 20 fold. After fine-tuning PubMedBERT
with the QA training data augmented with the enlarged metadata, the model
increased its mean Retrieval Precision from 0.277 to 0.866 and its mean
Percentile Rank from 0.355 to 0.896. The NeuroEmbed methodology for the
creation of electronic catalogues of omics cohorts and samples will foster
automated bioinformatic pipelines construction. The NeuroEmbed catalogue of
cohorts and samples is available at https://github.com/JoseAdrian3/NeuroEmbed.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.13467v1},
File          = {2506.13467v1.pdf}
}
@article{2506.13252v1,
Author        = {Kaspar Rothenfusser and Bekk Blando},
Title         = {Vector Ontologies as an LLM world view extraction method},
Eprint        = {2506.13252v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large Language Models (LLMs) possess intricate internal representations of
the world, yet these latent structures are notoriously difficult to interpret
or repurpose beyond the original prediction task. Building on our earlier work
(Rothenfusser, 2025), which introduced the concept of vector ontologies as a
framework for translating high-dimensional neural representations into
interpretable geometric structures, this paper provides the first empirical
validation of that approach. A vector ontology defines a domain-specific vector
space spanned by ontologically meaningful dimensions, allowing geometric
analysis of concepts and relationships within a domain. We construct an
8-dimensional vector ontology of musical genres based on Spotify audio features
and test whether an LLM's internal world model of music can be consistently and
accurately projected into this space. Using GPT-4o-mini, we extract genre
representations through multiple natural language prompts and analyze the
consistency of these projections across linguistic variations and their
alignment with ground-truth data. Our results show (1) high spatial consistency
of genre projections across 47 query formulations, (2) strong alignment between
LLM-inferred genre locations and real-world audio feature distributions, and
(3) evidence of a direct relationship between prompt phrasing and spatial
shifts in the LLM's inferred vector ontology. These findings demonstrate that
LLMs internalize structured, repurposable knowledge and that vector ontologies
offer a promising method for extracting and analyzing this knowledge in a
transparent and verifiable way.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.13252v1},
File          = {2506.13252v1.pdf}
}
@article{2506.12466v1,
Author        = {Alexander Geiger and Immanuel Hacker and Ãmer Sen and Andreas Ulbig},
Title         = {Towards Safety and Security Testing of Cyberphysical Power Systems by
  Shape Validation},
Eprint        = {2506.12466v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {The increasing complexity of cyberphysical power systems leads to larger
attack surfaces to be exploited by malicious actors and a higher risk of faults
through misconfiguration. We propose to meet those risks with a declarative
approach to describe cyberphysical power systems and to automatically evaluate
security and safety controls. We leverage Semantic Web technologies as a
well-standardized framework, providing languages to specify ontologies, rules
and shape constraints. We model infrastructure through an ontology which
combines external ontologies, architecture and data models for sufficient
expressivity and interoperability with external systems. The ontology can
enrich itself through rules defined in SPARQL, allowing for the inference of
knowledge that is not explicitly stated. Through the evaluation of SHACL shape
constraints we can then validate the data and verify safety and security
constraints. We demonstrate this concept with two use cases and illustrate how
this solution can be developed further in a community-driven fashion.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.12466v1},
File          = {2506.12466v1.pdf}
}
@article{2506.12290v1,
Author        = {John Beverley and Andreas Tolk},
Title         = {Ontology Enabled Hybrid Modeling and Simulation},
Eprint        = {2506.12290v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We explore the role of ontologies in enhancing hybrid modeling and simulation
through improved semantic rigor, model reusability, and interoperability across
systems, disciplines, and tools. By distinguishing between methodological and
referential ontologies, we demonstrate how these complementary approaches
address interoperability challenges along three axes: Human-Human,
Human-Machine, and Machine-Machine. Techniques such as competency questions,
ontology design patterns, and layered strategies are highlighted for promoting
shared understanding and formal precision. Integrating ontologies with Semantic
Web Technologies, we showcase their dual role as descriptive domain
representations and prescriptive guides for simulation construction. Four
application cases - sea-level rise analysis, Industry 4.0 modeling, artificial
societies for policy support, and cyber threat evaluation - illustrate the
practical benefits of ontology-driven hybrid simulation workflows. We conclude
by discussing challenges and opportunities in ontology-based hybrid M&S,
including tool integration, semantic alignment, and support for explainable AI.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.12290v1},
File          = {2506.12290v1.pdf}
}
@article{2506.17276v1,
Author        = {Alexandre Le Nepvou},
Title         = {Modal Logic for Stratified Becoming: Actualization Beyond Possible
  Worlds},
Eprint        = {2506.17276v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LO},
Abstract      = {This article develops a novel framework for modal logic based on the idea of
stratified actualization, rather than the classical model of global possible
worlds. Traditional Kripke semantics treat modal operators as quantification
over fully determinate alternatives, neglecting the local, dynamic, and often
asymmetric nature of actualization processes. We propose a system Stratified
Actualization Logic (SAL) in which modalities are indexed by levels of
ontological stability, interpreted as admissibility regimes. Each modality
operates over a structured layer of possibility, grounded in the internal
coherence of transitions between layers. We formally define the syntax and
semantics of SAL, introduce its axioms, and prove soundness and completeness.
Applications are discussed in connection with temporal becoming, quantum
decoherence domains, and modal metaphysics. The result is a logic that captures
the ontological structure of actualization without recourse to abstract
possible worlds, offering a stratified alternative to standard modal realism.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.17276v1},
File          = {2506.17276v1.pdf}
}
@article{2506.10704v1,
Author        = {Arshad Beg and Diarmuid O'Donoghue and Rosemary Monahan},
Title         = {Formalising Software Requirements using Large Language Models},
Eprint        = {2506.10704v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {This paper is a brief introduction to our recently initiated project named
VERIFAI: Traceability and verification of natural language requirements. The
project addresses the challenges in the traceability and verification of formal
specifications through providing support for the automatic generation of the
formal specifications and the traceability of the requirements from the initial
software design stage through the systems implementation and verification.
Approaches explored in this project include Natural Language Processing, use of
ontologies to describe the software system domain, reuse of existing software
artefacts from similar systems (i.e. through similarity based reuse) and large
language models to identify and declare the specifications as well as use of
artificial intelligence to guide the process.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.10704v1},
File          = {2506.10704v1.pdf}
}
@article{2506.10678v1,
Author        = {Tom Westermann and Aljosha KÃ¶cher and Felix Gehlhoff},
Title         = {Automated Validation of Textual Constraints Against AutomationML via
  LLMs and SHACL},
Eprint        = {2506.10678v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {AutomationML (AML) enables standardized data exchange in engineering, yet
existing recommendations for proper AML modeling are typically formulated as
informal and textual constraints. These constraints cannot be validated
automatically within AML itself. This work-in-progress paper introduces a
pipeline to formalize and verify such constraints. First, AML models are mapped
to OWL ontologies via RML and SPARQL. In addition, a Large Language Model
translates textual rules into SHACL constraints, which are then validated
against the previously generated AML ontology. Finally, SHACL validation
results are automatically interpreted in natural language. The approach is
demonstrated on a sample AML recommendation. Results show that even complex
modeling rules can be semi-automatically checked -- without requiring users to
understand formal methods or ontology technologies.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.10678v1},
File          = {2506.10678v1.pdf}
}
@article{2506.11180v1,
Author        = {Luis Miguel Vieira da Silva and Aljosha KÃ¶cher and Felix Gehlhoff},
Title         = {Beyond Formal Semantics for Capabilities and Skills: Model Context
  Protocol in Manufacturing},
Eprint        = {2506.11180v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {Explicit modeling of capabilities and skills -- whether based on ontologies,
Asset Administration Shells, or other technologies -- requires considerable
manual effort and often results in representations that are not easily
accessible to Large Language Models (LLMs). In this work-in-progress paper, we
present an alternative approach based on the recently introduced Model Context
Protocol (MCP). MCP allows systems to expose functionality through a
standardized interface that is directly consumable by LLM-based agents. We
conduct a prototypical evaluation on a laboratory-scale manufacturing system,
where resource functions are made available via MCP. A general-purpose LLM is
then tasked with planning and executing a multi-step process, including
constraint handling and the invocation of resource functions via MCP. The
results indicate that such an approach can enable flexible industrial
automation without relying on explicit semantic models. This work lays the
basis for further exploration of external tool integration in LLM-driven
production systems.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.11180v1},
File          = {2506.11180v1.pdf}
}
@article{2506.09581v1,
Author        = {Miguel Ã. GonzÃ¡lez-Santamarta and Francisco J. RodrÃ­guez-Lera and David SobrÃ­n-Hidalgo and Ãngel Manuel Guerrero-Higueras and Vicente MatellÃn-Olivera},
Title         = {Integrating Quantized LLMs into Robotics Systems as Edge AI to Leverage
  their Natural Language Processing Capabilities},
Eprint        = {2506.09581v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Large Language Models (LLMs) have experienced great advancements in the last
year resulting in an increase of these models in several fields to face natural
language tasks. The integration of these models in robotics can also help to
improve several aspects such as human-robot interaction, navigation, planning
and decision-making. Therefore, this paper introduces llama\_ros, a tool
designed to integrate quantized Large Language Models (LLMs) into robotic
systems using ROS 2. Leveraging llama.cpp, a highly optimized runtime engine,
llama\_ros enables the efficient execution of quantized LLMs as edge artificial
intelligence (AI) in robotics systems with resource-constrained environments,
addressing the challenges of computational efficiency and memory limitations.
By deploying quantized LLMs, llama\_ros empowers robots to leverage the natural
language understanding and generation for enhanced decision-making and
interaction which can be paired with prompt engineering, knowledge graphs,
ontologies or other tools to improve the capabilities of autonomous robots.
Additionally, this paper provides insights into some use cases of using
llama\_ros for planning and explainability in robotics.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.09581v1},
File          = {2506.09581v1.pdf}
}
@article{2506.10037v2,
Author        = {Shawn Zheng Kai Tan and Aleix Puig-Barbe and Damien Goutte-Gattat and Caroline Eastwood and Brian Aevermann and Alida Avola and James P Balhoff and Ismail Ugur Bayindir and Jasmine Belfiore and Anita Reane Caron and David S Fischer and Nancy George and Benjamin M Gyori and Melissa A Haendel and Charles Tapley Hoyt and Huseyin Kir and Tiago Lubiana and Nicolas Matentzoglu and James A Overton and Beverly Peng and Bjoern Peters and Ellen M Quardokus and Patrick L Ray and Paola Roncaglia and Andrea D Rivera and Ray Stefancsik and Wei Kheng Teh and Sabrina Toro and Nicole Vasilevsky and Chuan Xu and Yun Zhang and Richard H Scheuermann and Chirstopher J Mungall and Alexander D Diehl and David Osumi-Sutherland},
Title         = {The Cell Ontology in the age of single-cell omics},
Eprint        = {2506.10037v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.OT},
Abstract      = {Single-cell omics technologies have transformed our understanding of cellular
diversity by enabling high-resolution profiling of individual cells. However,
the unprecedented scale and heterogeneity of these datasets demand robust
frameworks for data integration and annotation. The Cell Ontology (CL) has
emerged as a pivotal resource for achieving FAIR (Findable, Accessible,
Interoperable, and Reusable) data principles by providing standardized,
species-agnostic terms for canonical cell types - forming a core component of a
wide range of platforms and tools. In this paper, we describe the wide variety
of uses of CL in these platforms and tools and detail ongoing work to improve
and extend CL content including the addition of transcriptomically defined
types, working closely with major atlasing efforts including the Human Cell
Atlas and the Brain Initiative Cell Atlas Network to support their needs. We
cover the challenges and future plans for harmonising classical and
transcriptomic cell type definitions, integrating markers and using Large
Language Models (LLMs) to improve content and efficiency of CL workflows.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.10037v2},
File          = {2506.10037v2.pdf}
}
@article{2506.09133v1,
Author        = {Theodoros Yianni and Farid Shahandeh},
Title         = {Complexity of Contextuality},
Eprint        = {2506.09133v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {quant-ph},
Abstract      = {Generalized contextuality is a hallmark of nonclassical theories like quantum
mechanics. Yet, three fundamental computational problems concerning its
decidability and complexity remain open. First, determining the complexity of
deciding if a theory admits a noncontextual ontological model; Second,
determining the complexity of deciding if such a model is possible for a
specific dimension $k$; Third, efficiently computing the smallest such model
when it exists, given that finding the smallest ontological model is NP-hard.
We address the second problem by presenting an algorithm derived from a
geometric formulation and its reduction to the intermediate simplex problem in
computational geometry. We find that the complexity of deciding the existence
of a noncontextual ontological model of dimension $k$ is at least exponential
in the dimension of the theory and at most exponential in $k$. This, in turn,
implies that computing the smallest noncontextual ontological model is
inefficient in general. Finally, we demonstrate the fundamental difference
between finding the smallest noncontextual ontological model and the smallest
ontological model using an explicit example wherein the respective minimum
ontic sizes are five and four.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.09133v1},
File          = {2506.09133v1.pdf}
}
@article{2506.08897v4,
Author        = {Hiba Khey and Amine Lakhder and Salma Rouichi and Imane El Ghabi and Kamal Hejjaoui and Younes En-nahli and Fahd Kalloubi and Moez Amri},
Title         = {PlantDeBERTa: An Open Source Language Model for Plant Science},
Eprint        = {2506.08897v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The rapid advancement of transformer-based language models has catalyzed
breakthroughs in biomedical and clinical natural language processing; however,
plant science remains markedly underserved by such domain-adapted tools. In
this work, we present PlantDeBERTa, a high-performance, open-source language
model specifically tailored for extracting structured knowledge from plant
stress-response literature. Built upon the DeBERTa architecture-known for its
disentangled attention and robust contextual encoding-PlantDeBERTa is
fine-tuned on a meticulously curated corpus of expert-annotated abstracts, with
a primary focus on lentil (Lens culinaris) responses to diverse abiotic and
biotic stressors. Our methodology combines transformer-based modeling with
rule-enhanced linguistic post-processing and ontology-grounded entity
normalization, enabling PlantDeBERTa to capture biologically meaningful
relationships with precision and semantic fidelity. The underlying corpus is
annotated using a hierarchical schema aligned with the Crop Ontology,
encompassing molecular, physiological, biochemical, and agronomic dimensions of
plant adaptation. PlantDeBERTa exhibits strong generalization capabilities
across entity types and demonstrates the feasibility of robust domain
adaptation in low-resource scientific fields.By providing a scalable and
reproducible framework for high-resolution entity recognition, PlantDeBERTa
bridges a critical gap in agricultural NLP and paves the way for intelligent,
data-driven systems in plant genomics, phenomics, and agronomic knowledge
discovery. Our model is publicly released to promote transparency and
accelerate cross-disciplinary innovation in computational plant science.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.08897v4},
File          = {2506.08897v4.pdf}
}
@article{2506.08872v1,
Author        = {Nataliya Kosmyna and Eugene Hauptmann and Ye Tong Yuan and Jessica Situ and Xian-Hao Liao and Ashly Vivian Beresnitzky and Iris Braunstein and Pattie Maes},
Title         = {Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI
  Assistant for Essay Writing Task},
Eprint        = {2506.08872v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This study explores the neural and behavioral consequences of LLM-assisted
essay writing. Participants were divided into three groups: LLM, Search Engine,
and Brain-only (no tools). Each completed three sessions under the same
condition. In a fourth session, LLM users were reassigned to Brain-only group
(LLM-to-Brain), and Brain-only users were reassigned to LLM condition
(Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18
completing session 4. We used electroencephalography (EEG) to assess cognitive
load during essay writing, and analyzed essays using NLP, as well as scoring
essays with the help from human teachers and an AI judge. Across groups, NERs,
n-gram patterns, and topic ontology showed within-group homogeneity. EEG
revealed significant differences in brain connectivity: Brain-only participants
exhibited the strongest, most distributed networks; Search Engine users showed
moderate engagement; and LLM users displayed the weakest connectivity.
Cognitive activity scaled down in relation to external tool use. In session 4,
LLM-to-Brain participants showed reduced alpha and beta connectivity,
indicating under-engagement. Brain-to-LLM users exhibited higher memory recall
and activation of occipito-parietal and prefrontal areas, similar to Search
Engine users. Self-reported ownership of essays was the lowest in the LLM group
and the highest in the Brain-only group. LLM users also struggled to accurately
quote their own work. While LLMs offer immediate convenience, our findings
highlight potential cognitive costs. Over four months, LLM users consistently
underperformed at neural, linguistic, and behavioral levels. These results
raise concerns about the long-term educational implications of LLM reliance and
underscore the need for deeper inquiry into AI's role in learning.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.08872v1},
File          = {2506.08872v1.pdf}
}
@article{2506.08569v1,
Author        = {Erwan Plantec and Gautier Hamon and Mayalen Etcheverry and Bert Wang-Chak Chan and Pierre-Yves Oudeyer and ClÃ©ment Moulin-Frier},
Title         = {Flow-Lenia: Emergent evolutionary dynamics in mass conservative
  continuous cellular automata},
Eprint        = {2506.08569v1},
DOI           = {10.1162/artl_a_00471},
ArchivePrefix = {arXiv},
PrimaryClass  = {nlin.CG},
Abstract      = {Central to the artificial life endeavour is the creation of artificial
systems spontaneously generating properties found in the living world such as
autopoiesis, self-replication, evolution and open-endedness. While numerous
models and paradigms have been proposed, cellular automata (CA) have taken a
very important place in the field notably as they enable the study of
phenomenons like self-reproduction and autopoiesis. Continuous CA like Lenia
have been showed to produce life-like patterns reminiscent, on an aesthetic and
ontological point of view, of biological organisms we call creatures. We
propose in this paper Flow-Lenia, a mass conservative extension of Lenia. We
present experiments demonstrating its effectiveness in generating
spatially-localized patters (SLPs) with complex behaviors and show that the
update rule parameters can be optimized to generate complex creatures showing
behaviors of interest. Furthermore, we show that Flow-Lenia allows us to embed
the parameters of the model, defining the properties of the emerging patterns,
within its own dynamics thus allowing for multispecies simulations. By using
the evolutionary activity framework as well as other metrics, we shed light on
the emergent evolutionary dynamics taking place in this system.},
Year          = {2025},
Month         = {Jun},
Note          = {Artificial Life (2025) 31(2): 228-248},
Url           = {http://arxiv.org/abs/2506.08569v1},
File          = {2506.08569v1.pdf}
}
@article{2506.08422v2,
Author        = {Ikkei Itoku and David Theil and Evelyn Eichelsdoerfer Uehara and Sreyoshi Bhaduri and Junnosuke Kuroda and Toshi Yumoto and Alex Gil and Natalie Perez and Rajesh Cherukuri and Naumaan Nayyar},
Title         = {Transforming Expert Knowledge into Scalable Ontology via Large Language
  Models},
Eprint        = {2506.08422v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Having a unified, coherent taxonomy is essential for effective knowledge
representation in domain-specific applications as diverse terminologies need to
be mapped to underlying concepts. Traditional manual approaches to taxonomy
alignment rely on expert review of concept pairs, but this becomes
prohibitively expensive and time-consuming at scale, while subjective
interpretations often lead to expert disagreements. Existing automated methods
for taxonomy alignment have shown promise but face limitations in handling
nuanced semantic relationships and maintaining consistency across different
domains. These approaches often struggle with context-dependent concept
mappings and lack transparent reasoning processes. We propose a novel framework
that combines large language models (LLMs) with expert calibration and
iterative prompt optimization to automate taxonomy alignment. Our method
integrates expert-labeled examples, multi-stage prompt engineering, and human
validation to guide LLMs in generating both taxonomy linkages and supporting
rationales. In evaluating our framework on a domain-specific mapping task of
concept essentiality, we achieved an F1-score of 0.97, substantially exceeding
the human benchmark of 0.68. These results demonstrate the effectiveness of our
approach in scaling taxonomy alignment while maintaining high-quality mappings
and preserving expert oversight for ambiguous cases.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.08422v2},
File          = {2506.08422v2.pdf}
}
@article{2506.07853v3,
Author        = {Hudson de Martim},
Title         = {Modeling the Diachronic Evolution of Legal Norms: An LRMoo-Based,
  Component-Level, Event-Centric Approach to Legal Knowledge Graphs},
Eprint        = {2506.07853v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Effectively representing legal norms for automated processing is a critical
challenge, particularly in tracking the temporal evolution of their
hierarchical components. While foundational conceptual frameworks like IFLA
LRMoo provide a generic toolkit for bibliographic data, and encoding standards
like Akoma Ntoso offer a robust syntax for legal documents, a dedicated, formal
modeling pattern for granular, component-level versioning is still required.
This limitation hinders the deterministic point-intime reconstruction of legal
texts, a fundamental capability for reliable Legal Tech and AI applications.
This paper proposes a structured, temporal modeling pattern grounded in the
LRMoo ontology to address this need. Our approach models the evolution of a
legal norm as a diachronic chain of F2 Expressions. We introduce a key
distinction between a language-agnostic Temporal Version (TV)-a semantic
snapshot of the norm's structure-and its concrete monolingual realizations, the
Language Versions (LV). Both are modeled as F2 Expressions linked by the
canonical R76 is derivative of property. This paradigm is applied recursively
to the legal text's internal structure, representing it as a parallel hierarchy
of abstract Component Works (F1) and their versioned Component Expressions
(F2). Furthermore, we formalize the legislative amendment process using the F28
Expression Creation event, allowing changes to be traced from an amending act
to its precise effect on the amended norm. Using the Brazilian Federal
Constitution as a case study, we demonstrate how this event-centric
architecture enables the precise, deterministic retrieval and reconstruction of
any part of a legal text as it existed on a specific date. The model provides a
robust foundation for building verifiable knowledge graphs and advanced AI
tools, overcoming the limitations of current generative models.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.07853v3},
File          = {2506.07853v3.pdf}
}
@article{2506.07332v1,
Author        = {Bo Fu and Mingjie Bi and Shota Umeda and Takahiro Nakano and Youichi Nonaka and Quan Zhou and Takaharu Matsui and Dawn M. Tilbury and Kira Barton},
Title         = {Digital Twin-based Smart Manufacturing: Dynamic Line Reconfiguration for
  Disturbance Handling},
Eprint        = {2506.07332v1},
DOI           = {10.1109/TASE.2025.3563320},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.MA},
Abstract      = {The increasing complexity of modern manufacturing, coupled with demand
fluctuation, supply chain uncertainties, and product customization, underscores
the need for manufacturing systems that can flexibly update their
configurations and swiftly adapt to disturbances. However, current research
falls short in providing a holistic reconfigurable manufacturing framework that
seamlessly monitors system disturbances, optimizes alternative line
configurations based on machine capabilities, and automates simulation
evaluation for swift adaptations. This paper presents a dynamic manufacturing
line reconfiguration framework to handle disturbances that result in operation
time changes. The framework incorporates a system process digital twin for
monitoring disturbances and triggering reconfigurations, a capability-based
ontology model capturing available agent and resource options, a configuration
optimizer generating optimal line configurations, and a simulation generation
program initializing simulation setups and evaluating line configurations at
approximately 400x real-time speed. A case study of a battery production line
has been conducted to evaluate the proposed framework. In two implemented
disturbance scenarios, the framework successfully recovers system throughput
with limited resources, preventing the 26% and 63% throughput drops that would
have occurred without a reconfiguration plan. The reconfiguration optimizer
efficiently finds optimal solutions, taking an average of 0.03 seconds to find
a reconfiguration plan for a manufacturing line with 51 operations and 40
available agents across 8 agent types.},
Year          = {2025},
Month         = {Jun},
Note          = {IEEE Transactions on Automation Science and Engineering, vol. 22,
  pp. 14892-14905, 2025},
Url           = {http://arxiv.org/abs/2506.07332v1},
File          = {2506.07332v1.pdf}
}
@article{2506.13773v1,
Author        = {Milapji Singh Gill and Tom Jeleniewski and Felix Gehlhoff and Alexander Fay},
Title         = {Representing Time-Continuous Behavior of Cyber-Physical Systems in
  Knowledge Graphs},
Eprint        = {2506.13773v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Time-continuous dynamic models are essential for various Cyber-Physical
System (CPS) applications. To ensure effective usability in different lifecycle
phases, such behavioral information in the form of differential equations must
be contextualized and integrated with further CPS information. While knowledge
graphs provide a formal description and structuring mechanism for this task,
there is a lack of reusable ontological artifacts and methods to reduce manual
instantiation effort. Hence, this contribution introduces two artifacts:
Firstly, a modular semantic model based on standards is introduced to represent
differential equations directly within knowledge graphs and to enrich them
semantically. Secondly, a method for efficient knowledge graph generation is
presented. A validation of these artifacts was conducted in the domain of
aviation maintenance. Results show that differential equations of a complex
Electro-Hydraulic Servoactuator can be formally represented in a knowledge
graph and be contextualized with other lifecycle data, proving the artifacts'
practical applicability.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.13773v1},
File          = {2506.13773v1.pdf}
}
@article{2506.07042v3,
Author        = {Stergios Chatzikyriakidis},
Title         = {Reasoning with RAGged events: RAG-Enhanced Event Knowledge Base
  Construction and reasoning with proof-assistants},
Eprint        = {2506.07042v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Extracting structured computational representations of historical events from
narrative text remains computationally expensive when constructed manually.
While RDF/OWL reasoners enable graph-based reasoning, they are limited to
fragments of first-order logic, preventing deeper temporal and semantic
analysis. This paper addresses both challenges by developing automatic
historical event extraction models using multiple LLMs (GPT-4, Claude, Llama
3.2) with three enhancement strategies: pure base generation, knowledge graph
enhancement, and Retrieval-Augmented Generation (RAG). We conducted
comprehensive evaluations using historical texts from Thucydides. Our findings
reveal that enhancement strategies optimize different performance dimensions
rather than providing universal improvements. For coverage and historical
breadth, base generation achieves optimal performance with Claude and GPT-4
extracting comprehensive events. However, for precision, RAG enhancement
improves coordinate accuracy and metadata completeness. Model architecture
fundamentally determines enhancement sensitivity: larger models demonstrate
robust baseline performance with incremental RAG improvements, while Llama 3.2
shows extreme variance from competitive performance to complete failure. We
then developed an automated translation pipeline converting extracted RDF
representations into Coq proof assistant specifications, enabling higher-order
reasoning beyond RDF capabilities including multi-step causal verification,
temporal arithmetic with BC dates, and formal proofs about historical
causation. The Coq formalization validates that RAG-discovered event types
represent legitimate domain-specific semantic structures rather than
ontological violations.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.07042v3},
File          = {2506.07042v3.pdf}
}
@article{2506.07037v1,
Author        = {Zhongze Luo and Weixuan Wan and Qizhi Zheng and Yanhong Bai and Jingyun Sun and Jian Wang and Dan Wang},
Title         = {KG2QA: Knowledge Graph-enhanced Retrieval-Augmented Generation for
  Communication Standards Question Answering},
Eprint        = {2506.07037v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {There are many types of standards in the field of communication. The
traditional consulting model has a long cycle and relies on the knowledge and
experience of experts, making it difficult to meet the rapidly developing
technological demands. This paper combines the fine-tuning of large language
models with the construction of knowledge graphs to implement an intelligent
consultation and question-answering system for communication standards. The
experimental results show that after LoRA tuning on the constructed dataset of
6,587 questions and answers in the field of communication standards,
Qwen2.5-7B-Instruct demonstrates outstanding professional capabilities in the
field of communication standards on the test set. BLEU-4 rose from 18.8564 to
66.8993, and evaluation indicators such as ROUGE also increased significantly,
outperforming the fine-tuning effect of the comparison model
Llama-3-8B-Instruct. Based on the ontology framework containing 6 entity
attributes and 10 relation attributes, a knowledge graph of the communication
standard domain containing 13,906 entities and 13,524 relations was
constructed, showing a relatively good query accuracy rate. The intelligent
consultation and question-answering system enables the fine-tuned model on the
server side to access the locally constructed knowledge graph and conduct
graphical retrieval of key information first, which is conducive to improving
the question-answering effect. The evaluation using DeepSeek as the Judge on
the test set shows that our RAG framework enables the fine-tuned model to
improve the scores at all five angles, with an average score increase of 2.26%.
And combined with web services and API interfaces, it has achieved very good
results in terms of interaction experience and back-end access, and has very
good practical application value.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.07037v1},
File          = {2506.07037v1.pdf}
}
@article{2506.07035v1,
Author        = {Zixuan Jiang and Renjing Xu},
Title         = {AnnoDPO: Protein Functional Annotation Learning with Direct Preference
  Optimization},
Eprint        = {2506.07035v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.BM},
Abstract      = {Deciphering protein function remains a fundamental challenge in protein
representation learning. The task presents significant difficulties for protein
language models (PLMs) due to the sheer volume of functional annotation
categories and the highly imbalanced distribution of annotated instances across
biological ontologies. Inspired by the remarkable success of reinforcement
learning from human feedback (RLHF) in large language model (LLM) alignment, we
propose AnnoDPO, a novel multi-modal framework for protein function prediction
that leverages Direct Preference Optimization (DPO) to enhance annotation
learning. Our methodology addresses the dual challenges of annotation scarcity
and category imbalance through preference-aligned training objectives,
establishing a new paradigm for biological knowledge integration in protein
representation learning.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.07035v1},
File          = {2506.07035v1.pdf}
}
@article{2506.06977v1,
Author        = {Pengfei Hu and Xiaoxue Han and Fei Wang and Yue Ning},
Title         = {UdonCare: Hierarchy Pruning for Unseen Domain Discovery in Predictive
  Healthcare},
Eprint        = {2506.06977v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Domain generalization has become a critical challenge in clinical prediction,
where patient cohorts often exhibit shifting data distributions that degrade
model performance. Typical domain generalization approaches struggle in
real-world healthcare settings for two main reasons: (1) patient-specific
domain labels are typically unavailable, making domain discovery especially
difficult; (2) purely data-driven approaches overlook key clinical insights,
leading to a gap in medical knowledge integration. To address these problems,
we leverage hierarchical medical ontologies like the ICD-9-CM hierarchy to
group diseases into higher-level categories and discover more flexible latent
domains. In this paper, we introduce UdonCare, a hierarchy-guided framework
that iteratively prunes fine-grained domains, encodes these refined domains,
and applies a Siamese-type inference mechanism to separate domain-related
signals from patient-level features. Experimental results on clinical datasets
(MIMIC-III and MIMIC-IV) show that the proposed model achieves higher
performance compared to other domain generalization baselines when substantial
domain gaps presents, highlighting the untapped potential of medical knowledge
for enhancing domain generalization in practical healthcare applications.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.06977v1},
File          = {2506.06977v1.pdf}
}
@article{2506.06870v1,
Author        = {Bugra Kilictas and Faruk Alpay},
Title         = {Recursive Semantic Anchoring in ISO 639:2023: A Structural Extension to
  ISO/TC 37 Frameworks},
Eprint        = {2506.06870v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LO},
Abstract      = {ISO 639:2023 unifies the ISO language-code family and introduces contextual
metadata, but it lacks a machine-native mechanism for handling dialectal drift
and creole mixtures. We propose a formalisation of recursive semantic
anchoring, attaching to every language entity $\chi$ a family of fixed-point
operators $\phi_{n,m}$ that model bounded semantic drift via the relation
$\phi_{n,m}(\chi) = \chi \oplus \Delta(\chi)$, where $\Delta(\chi)$ is a drift
vector in a latent semantic manifold. The base anchor $\phi_{0,0}$ recovers the
canonical ISO 639:2023 identity, whereas $\phi_{99,9}$ marks the maximal drift
state that triggers a deterministic fallback. Using category theory, we treat
the operators $\phi_{n,m}$ as morphisms and drift vectors as arrows in a
category $\mathrm{DriftLang}$. A functor $\Phi: \mathrm{DriftLang} \to
\mathrm{AnchorLang}$ maps every drifted object to its unique anchor and proves
convergence. We provide an RDF/Turtle schema (\texttt{BaseLanguage},
\texttt{DriftedLanguage}, \texttt{ResolvedAnchor}) and worked examples -- e.g.,
$\phi_{8,4}$ (Standard Mandarin) versus $\phi_{8,7}$ (a colloquial variant),
and $\phi_{1,7}$ for Nigerian Pidgin anchored to English. Experiments with
transformer models show higher accuracy in language identification and
translation on noisy or code-switched input when the $\phi$-indices are used to
guide fallback routing. The framework is compatible with ISO/TC 37 and provides
an AI-tractable, drift-aware semantic layer for future standards.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.06870v1},
File          = {2506.06870v1.pdf}
}
@article{2506.12075v1,
Author        = {Nirmal Gelal and Chloe Snow and Ambyr Rios and Hande KÃ¼Ã§Ã¼k McGinty},
Title         = {T-TExTS (Teaching Text Expansion for Teacher Scaffolding): Enhancing
  Text Selection in High School Literature through Knowledge Graph-Based
  Recommendation},
Eprint        = {2506.12075v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {The implementation of transformational pedagogy in secondary education
classrooms requires a broad multiliteracy approach. Due to limited planning
time and resources, high school English Literature teachers often struggle to
curate diverse, thematically aligned literature text sets. This study addresses
the critical need for a tool that provides scaffolds for novice educators in
selecting literature texts that are diverse -- in terms of genre, theme,
subtheme, and author -- yet similar in context and pedagogical merits. We have
developed a recommendation system, Teaching Text Expansion for Teacher
Scaffolding (T-TExTS), that suggests high school English Literature books based
on pedagogical merits, genre, and thematic relevance using a knowledge graph.
We constructed a domain-specific ontology using the KNowledge Acquisition and
Representation Methodology (KNARM), transformed into a knowledge graph, which
was then embedded using DeepWalk, biased random walk, and a hybrid of both
approaches. The system was evaluated using link prediction and recommendation
performance metrics, including Area Under the Curve (AUC), Mean Reciprocal Rank
(MRR), Hits@K, and normalized Discounted Cumulative Gain (nDCG). DeepWalk
outperformed in most ranking metrics, with the highest AUC (0.9431), whereas
the hybrid model offered balanced performance. These findings demonstrate the
importance of semantic, ontology-driven approaches in recommendation systems
and suggest that T-TExTS can significantly ease the burden of English
Literature text selection for high school educators, promoting more informed
and inclusive curricular decisions. The source code for T-TExTS is available
at: https://github.com/koncordantlab/TTExTS},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.12075v1},
File          = {2506.12075v1.pdf}
}
@article{2506.09062v1,
Author        = {C. S. Unnikrishnan},
Title         = {Information versus Physicality: On the Nature of the Wavefunctions of
  Quantum Mechanics},
Eprint        = {2506.09062v1},
DOI           = {10.20935/AcadQuant7696},
ArchivePrefix = {arXiv},
PrimaryClass  = {quant-ph},
Abstract      = {The physical states of matter and fields are represented in the quantum
theory with complex valued wavefunctions, or more generally by quantum states
in an abstract linear vector space. Determining the physical nature of
wavefunctions remains an open problem that is at the very core of quantum
mechanics, About a decade ago, Pusey, Barrett and Rudolf (PBR) claimed to prove
an ontologically real status of wavefunctions by ruling out $\psi$-epistemic
models. The result was obtained by associating wavefunctions to hypothetical
distributions of notional physical states, and by examining whether some
physical states were associated with more than one wavefunction, a criterion
they chose for defining a wavefunction as `epistemic'. I show that the starting
assumption in the PBR argument, of associating a wavefunction with a
distribution of physical states, is flawed and contradictory to the linear
structure of quantum mechanics coupled with its quadratic Born's rule. Since
none of the axioms or calculations of observable statistical results in the
standard quantum theory depends on specifying the physical nature of a
$\psi$-function, the considerations in the PBR paper, involving a standard
process of the preparation and projective measurements of quantum states,
cannot address the ontological status of the wavefunctions in space and time.},
Year          = {2025},
Month         = {Jun},
Note          = {Academia Quantum 2, no. 2 (2025)},
Url           = {http://arxiv.org/abs/2506.09062v1},
File          = {2506.09062v1.pdf}
}
@article{2506.05128v1,
Author        = {Tanmay Parekh and Kartik Mehta and Ninareh Mehrabi and Kai-Wei Chang and Nanyun Peng},
Title         = {DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM
  Reasoning},
Eprint        = {2506.05128v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Zero-shot Event Detection (ED), the task of identifying event mentions in
natural language text without any training data, is critical for document
understanding in specialized domains. Understanding the complex event ontology,
extracting domain-specific triggers from the passage, and structuring them
appropriately overloads and limits the utility of Large Language Models (LLMs)
for zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent
reasoning framework that decouples the task of ED using Dreamer and Grounder.
Dreamer encourages divergent reasoning through open-ended event discovery,
which helps to boost event coverage. Conversely, Grounder introduces convergent
reasoning to align the free-form predictions with the task-specific
instructions using finite-state machine guided constrained decoding.
Additionally, an LLM-Judge verifies the final outputs to ensure high precision.
Through extensive experiments on six datasets across five domains and nine
LLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot,
transfer-learning, and reasoning baselines, achieving 4-7% average F1 gains
over the best baseline -- establishing DiCoRe as a strong zero-shot ED
framework.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.05128v1},
File          = {2506.05128v1.pdf}
}
@article{2506.04756v1,
Author        = {Loan Dao and Ngoc Quoc Ly},
Title         = {Ontology-based knowledge representation for bone disease diagnosis: a
  foundation for safe and sustainable medical artificial intelligence systems},
Eprint        = {2506.04756v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Medical artificial intelligence (AI) systems frequently lack systematic
domain expertise integration, potentially compromising diagnostic reliability.
This study presents an ontology-based framework for bone disease diagnosis,
developed in collaboration with Ho Chi Minh City Hospital for Traumatology and
Orthopedics. The framework introduces three theoretical contributions: (1) a
hierarchical neural network architecture guided by bone disease ontology for
segmentation-classification tasks, incorporating Visual Language Models (VLMs)
through prompts, (2) an ontology-enhanced Visual Question Answering (VQA)
system for clinical reasoning, and (3) a multimodal deep learning model that
integrates imaging, clinical, and laboratory data through ontological
relationships. The methodology maintains clinical interpretability through
systematic knowledge digitization, standardized medical terminology mapping,
and modular architecture design. The framework demonstrates potential for
extension beyond bone diseases through its standardized structure and reusable
components. While theoretical foundations are established, experimental
validation remains pending due to current dataset and computational resource
limitations. Future work will focus on expanding the clinical dataset and
conducting comprehensive system validation.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.04756v1},
File          = {2506.04756v1.pdf}
}
@article{2506.04512v1,
Author        = {Bohui Zhang and Yuan He and Lydia Pintscher and Albert MeroÃ±o PeÃ±uela and Elena Simperl},
Title         = {Schema Generation for Large Knowledge Graphs Using Large Language Models},
Eprint        = {2506.04512v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Schemas are vital for ensuring data quality in the Semantic Web and natural
language processing. Traditionally, their creation demands substantial
involvement from knowledge engineers and domain experts. Leveraging the
impressive capabilities of large language models (LLMs) in related tasks like
ontology engineering, we explore automatic schema generation using LLMs. To
bridge the resource gap, we introduce two datasets: YAGO Schema and Wikidata
EntitySchema, along with evaluation metrics. The LLM-based pipelines
effectively utilize local and global information from knowledge graphs (KGs) to
generate validating schemas in Shape Expressions (ShEx). Experiments
demonstrate LLMs' strong potential in producing high-quality ShEx schemas,
paving the way for scalable, automated schema generation for large KGs.
Furthermore, our benchmark introduces a new challenge for structured
generation, pushing the limits of LLMs on syntactically rich formalisms.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.04512v1},
File          = {2506.04512v1.pdf}
}
@article{2506.03301v1,
Author        = {Daham M. Mustafa and Abhishek Nadgeri and Diego Collarana and Benedikt T. Arnold and Christoph Quix and Christoph Lange and Stefan Decker},
Title         = {From Instructions to ODRL Usage Policies: An Ontology Guided Approach},
Eprint        = {2506.03301v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This study presents an approach that uses large language models such as GPT-4
to generate usage policies in the W3C Open Digital Rights Language ODRL
automatically from natural language instructions. Our approach uses the ODRL
ontology and its documentation as a central part of the prompt. Our research
hypothesis is that a curated version of existing ontology documentation will
better guide policy generation. We present various heuristics for adapting the
ODRL ontology and its documentation to guide an end-to-end KG construction
process. We evaluate our approach in the context of dataspaces, i.e.,
distributed infrastructures for trustworthy data exchange between multiple
participating organizations for the cultural domain. We created a benchmark
consisting of 12 use cases of varying complexity. Our evaluation shows
excellent results with up to 91.95% accuracy in the resulting knowledge graph.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.03301v1},
File          = {2506.03301v1.pdf}
}
@article{2506.03145v1,
Author        = {Pralaypati Ta and Sriram Venkatesaperumal and Keerthi Ram and Mohanasankar Sivaprakasam},
Title         = {Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and
  Semantic Understanding Capability of LLM},
Eprint        = {2506.03145v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Neuroscience research publications encompass a vast wealth of knowledge.
Accurately retrieving existing information and discovering new insights from
this extensive literature is essential for advancing the field. However, when
knowledge is dispersed across multiple sources, current state-of-the-art
retrieval methods often struggle to extract the necessary information. A
knowledge graph (KG) can integrate and link knowledge from multiple sources,
but existing methods for constructing KGs in neuroscience often rely on labeled
data and require domain expertise. Acquiring large-scale, labeled data for a
specialized area like neuroscience presents significant challenges. This work
proposes novel methods for constructing KG from unlabeled large-scale
neuroscience research corpus utilizing large language models (LLM),
neuroscience ontology, and text embeddings. We analyze the semantic relevance
of neuroscience text segments identified by LLM for building the knowledge
graph. We also introduce an entity-augmented information retrieval algorithm to
extract knowledge from the KG. Several experiments were conducted to evaluate
the proposed approaches, and the results demonstrate that our methods
significantly enhance knowledge discovery from the unlabeled neuroscience
research corpus. It achieves an F1 score of 0.84 for entity extraction, and the
knowledge obtained from the KG improves answers to over 54% of the questions.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.03145v1},
File          = {2506.03145v1.pdf}
}
@article{2506.02646v1,
Author        = {Sabah Al-Fedaghi},
Title         = {Textual-Based vs. Thinging Machines Conceptual Modeling},
Eprint        = {2506.02646v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {Software engineers typically interpret the domain description in natural
language and translate it into a conceptual model. Three approaches are used in
this domain modeling: textual languages, diagrammatic languages, and a mixed
based of text and diagrams. According to some researchers, relying on a
diagrammatic notation levies certain burdens for designing large models because
visual languages are intended to depict everything diagrammatically during a
development process but fail to do so for a lack of developer efficiency. It is
claimed that textual formats enable easier manipulation in editors and tools
and facilitate the integration of ontologies in software systems. In this
paper, we explore the problem of the relationship between textual format and
diagramming in conceptual modeling. The main focus is modeling based on the
so-called thinging machine (TM). Several examples are developed in detail to
contrast side-by-side targeted domains represented in textual description and
TM modeling. A TM model is defined as a thimac (thing/machine) with a time
feature that forms dynamic events over static thimacs utilizing five generic
actions: create, process, release, transfer, and receive. This provides a
conceptual foundation that can be simplified further by eliminating the actions
of release, transfer, and receive. A multilevel reduction in the TM diagram s
complexity can also be achieved by assuming diagrammatic notations represent
the actions of creation and processing. We envision that special tools will
help improve developer efficiency. The study s results of contrasting textual
and mix-based descriptions vs. TM modeling justify our claim that TM modeling
is a more appropriate methodology than other diagrammatic schemes (e.g., UML
classes) examined in this paper.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.02646v1},
File          = {2506.02646v1.pdf}
}
@article{2506.01617v1,
Author        = {Pablo Cayado and JoÃ£o Rosas and JoÃ£o Murta-Pina and Harold S. Ruiz},
Title         = {An Open and Collaborative Database of Properties of Materials for
  High-Temperature Superconducting-Based Devices},
Eprint        = {2506.01617v1},
DOI           = {10.1109/TASC.2025.3570458},
ArchivePrefix = {arXiv},
PrimaryClass  = {cond-mat.supr-con},
Abstract      = {The successful integration of high-temperature superconductors (HTS) into
modern technologies requires consistent, accessible, and comprehensive material
data, a need that is currently unmet due to the fragmented and incomplete
nature of existing resources. This paper introduces a new collaborative,
open-access database specifically designed to address this gap by providing
standardized data on HTS materials and crucial auxiliary components for HTS
applications. The database encompasses extensive data on structural, cryogenic,
electrical, magnetic, and superconducting materials, supporting diverse
requirements from HTS modelling to magnet design. Developed through
collaborative efforts and organized using an ontology-driven data model, this
platform is dynamically adaptable, ensuring that it can grow as new materials
and data emerge. Key features include user-driven contributions, peer-reviewed
data validation, and advanced filtering capabilities for efficient data
retrieval. This innovative database, to the knowledge of the authors, being the
largest publicly available for material properties of HTS technologies is
positioned as a valuable tool for the HTS community, promoting more efficient
research and development processes, accelerating the practical application of
HTS, and fostering a collaborative approach to knowledge sharing within the
field. The database is available at https://sc.hi-scale.grisenergia.pt/app.},
Year          = {2025},
Month         = {Jun},
Note          = {IEEE Transactions on Applied Superconductivity},
Url           = {http://arxiv.org/abs/2506.01617v1},
File          = {2506.01617v1.pdf}
}
@article{2506.01232v1,
Author        = {Mojtaba Nayyeri and Athish A Yogi and Nadeen Fathallah and Ratan Bahadur Thapa and Hans-Michael Tautenhahn and Anton Schnurpel and Steffen Staab},
Title         = {Retrieval-Augmented Generation of Ontologies from Relational Databases},
Eprint        = {2506.01232v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Transforming relational databases into knowledge graphs with enriched
ontologies enhances semantic interoperability and unlocks advanced graph-based
learning and reasoning over data. However, previous approaches either demand
significant manual effort to derive an ontology from a database schema or
produce only a basic ontology. We present RIGOR, Retrieval-augmented Iterative
Generation of RDB Ontologies, an LLM-driven approach that turns relational
schemas into rich OWL ontologies with minimal human effort. RIGOR combines
three sources via RAG, the database schema and its documentation, a repository
of domain ontologies, and a growing core ontology, to prompt a generative LLM
for producing successive, provenance-tagged delta ontology fragments. Each
fragment is refined by a judge-LLM before being merged into the core ontology,
and the process iterates table-by-table following foreign key constraints until
coverage is complete. Applied to real-world databases, our approach outputs
ontologies that score highly on standard quality dimensions such as accuracy,
completeness, conciseness, adaptability, clarity, and consistency, while
substantially reducing manual effort.},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.01232v1},
File          = {2506.01232v1.pdf}
}
@article{2506.00696v2,
Author        = {Megan Harris and Ehsanoddin Ghorbanichemazkati and Mohammad Mahdi Naderi and John C. Little and Amro M. Farid},
Title         = {Integrative, Scalable Modeling of Hydrological Systems with MBSE and
  HFGT},
Eprint        = {2506.00696v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {eess.SY},
Abstract      = {Worsening global challenges in the Anthropocene demand complex, adaptive
solutions grounded in a systems-level understanding of coupled social and
environmental dynamics. However, existing modeling approaches often fall short
due to disciplinary silos, limited scalability, and the absence of shared
ontological frameworks. Model-Based Systems Engineering (MBSE), when integrated
with Hetero-functional Graph Theory (HFGT), offers a powerful methodology for
modeling systems of systems while preserving subsystem heterogeneity and
enabling cross-disciplinary integration. This paper presents the first
application of the MBSE-HFGT methodology to environmental systems, using a
series of worked examples involving flow through lake and land segments. These
examples demonstrate how the approach enables consistent, scalable, and
integrative modeling of complex environmental processes.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2506.00696v2},
File          = {2506.00696v2.pdf}
}
@article{2506.00671v1,
Author        = {Yuelyu Ji and Hang Zhang and Shiven Verma and Hui Ji and Chun Li and Yushui Han and Yanshan Wang},
Title         = {DeepRAG: Integrating Hierarchical Reasoning and Process Supervision for
  Biomedical Multi-Hop QA},
Eprint        = {2506.00671v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We propose DeepRAG, a novel framework that integrates DeepSeek hierarchical
question decomposition capabilities with RAG Gym unified retrieval-augmented
generation optimization using process level supervision. Targeting the
challenging MedHopQA biomedical question answering task, DeepRAG systematically
decomposes complex queries into precise sub-queries and employs concept level
reward signals informed by the UMLS ontology to enhance biomedical accuracy.
Preliminary evaluations on the MedHopQA dataset indicate that DeepRAG
significantly outperforms baseline models, including standalone DeepSeek and
RAG Gym, achieving notable improvements in both Exact Match and concept level
accuracy.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2506.00671v1},
File          = {2506.00671v1.pdf}
}
@article{2506.00664v1,
Author        = {Yash Tiwari and Owais Ahmad Lone and Mayukha Pal},
Title         = {OntoRAG: Enhancing Question-Answering through Automated Ontology
  Derivation from Unstructured Knowledge Bases},
Eprint        = {2506.00664v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontologies are pivotal for structuring knowledge bases to enhance question
answering (QA) systems powered by Large Language Models (LLMs). However,
traditional ontology creation relies on manual efforts by domain experts, a
process that is time intensive, error prone, and impractical for large, dynamic
knowledge domains. This paper introduces OntoRAG, an automated pipeline
designed to derive ontologies from unstructured knowledge bases, with a focus
on electrical relay documents. OntoRAG integrates advanced techniques,
including web scraping, PDF parsing, hybrid chunking, information extraction,
knowledge graph construction, and ontology creation, to transform unstructured
data into a queryable ontology. By leveraging LLMs and graph based methods,
OntoRAG enhances global sensemaking capabilities, outperforming conventional
Retrieval Augmented Generation (RAG) and GraphRAG approaches in
comprehensiveness and diversity. Experimental results demonstrate OntoRAGs
effectiveness, achieving a comprehensiveness win rate of 85% against vector RAG
and 75% against GraphRAGs best configuration. This work addresses the critical
challenge of automating ontology creation, advancing the vision of the semantic
web.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2506.00664v1},
File          = {2506.00664v1.pdf}
}
@article{2506.00233v1,
Author        = {Aasish Kumar Sharma and Dimitar Kyosev and Julian Kunkel},
Title         = {Ethical AI: Towards Defining a Collective Evaluation Framework},
Eprint        = {2506.00233v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Artificial Intelligence (AI) is transforming sectors such as healthcare,
finance, and autonomous systems, offering powerful tools for innovation. Yet
its rapid integration raises urgent ethical concerns related to data ownership,
privacy, and systemic bias. Issues like opaque decision-making, misleading
outputs, and unfair treatment in high-stakes domains underscore the need for
transparent and accountable AI systems. This article addresses these challenges
by proposing a modular ethical assessment framework built on ontological blocks
of meaning-discrete, interpretable units that encode ethical principles such as
fairness, accountability, and ownership. By integrating these blocks with FAIR
(Findable, Accessible, Interoperable, Reusable) principles, the framework
supports scalable, transparent, and legally aligned ethical evaluations,
including compliance with the EU AI Act. Using a real-world use case in
AI-powered investor profiling, the paper demonstrates how the framework enables
dynamic, behavior-informed risk classification. The findings suggest that
ontological blocks offer a promising path toward explainable and auditable AI
ethics, though challenges remain in automation and probabilistic reasoning.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2506.00233v1},
File          = {2506.00233v1.pdf}
}
@article{2505.24554v2,
Author        = {Anna Sofia Lippolis and Minh Davide Ragagni and Paolo Ciancarini and Andrea Giovanni Nuzzolese and Valentina Presutti},
Title         = {Bench4KE: Benchmarking Automated Competency Question Generation},
Eprint        = {2505.24554v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The availability of Large Language Models (LLMs) presents a unique
opportunity to reinvigorate research on Knowledge Engineering (KE) automation,
a trend already evident in recent efforts developing LLM-based methods and
tools for the automatic generation of Competency Questions (CQs). However, the
evaluation of these tools lacks standardisation. This undermines the
methodological rigour and hinders the replication and comparison of results. To
address this gap, we introduce Bench4KE, an extensible API-based benchmarking
system for KE automation. Its first release focuses on evaluating tools that
generate CQs automatically. CQs are natural language questions used by ontology
engineers to define the functional requirements of an ontology. Bench4KE
provides a curated gold standard consisting of CQ datasets from four real-world
ontology projects. It uses a suite of similarity metrics to assess the quality
of the CQs generated. We present a comparative analysis of four recent CQ
generation systems, which are based on LLMs, establishing a baseline for future
research. Bench4KE is also designed to accommodate additional KE automation
tasks, such as SPARQL query generation, ontology testing and drafting. Code and
datasets are publicly available under the Apache 2.0 license.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.24554v2},
File          = {2505.24554v2.pdf}
}
@article{2505.24046v1,
Author        = {Amro M. Farid and Amirreza Hosseini and John C. Little},
Title         = {A Conceptual Introduction to Hetero-functional Graph Theory for
  Systems-of-Systems},
Eprint        = {2505.24046v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {eess.SY},
Abstract      = {A defining feature of twenty first century engineering challenges is their
inherent complexity, demanding the convergence of knowledge across diverse
disciplines. Establishing consistent methodological foundations for engineering
systems remains a challenge -- one that both systems engineering and network
science have sought to address. Model-based systems engineering (MBSE) has
recently emerged as a practical, interdisciplinary approach for developing
complex systems from concept through implementation. In contrast, network
science focuses on the quantitative analysis of networks present within
engineering systems. This paper introduces hetero-functional graph theory
(HFGT) as a conceptual bridge between these two fields, serving as a tutorial
for both communities. For systems engineers, HFGT preserves the heterogeneity
of conceptual and ontological constructs in MBSE, including system form,
function, and concept. For network scientists, it provides multiple graph-based
data structures enabling matrix-based quantitative analysis. The modeling
process begins with ontological foundations, defining an engineering system as
an abstraction and representing it with a model. Model fidelity is assessed
using four linguistic properties: soundness, completeness, lucidity, and
laconicity. A meta-architecture is introduced to manage the convergence
challenges between domain-specific reference architectures and case-specific
instantiations. Unlike other meta-architectures, HFGT is rooted in linguistic
structures, modeling resources as subjects, system processes as predicates, and
operands-such as matter, energy, organisms, information, and money-as objects.
These elements are integrated within a system meta-architecture expressed in
the Systems Modeling Language (SysML). The paper concludes by offering guidance
for further reading.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.24046v1},
File          = {2505.24046v1.pdf}
}
@article{2505.23695v1,
Author        = {Ran Zhang and Mohannad Elhamod},
Title         = {Data-to-Dashboard: Multi-Agent LLM Framework for Insightful
  Visualization in Enterprise Analytics},
Eprint        = {2505.23695v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The rapid advancement of LLMs has led to the creation of diverse agentic
systems in data analysis, utilizing LLMs' capabilities to improve insight
generation and visualization. In this paper, we present an agentic system that
automates the data-to-dashboard pipeline through modular LLM agents capable of
domain detection, concept extraction, multi-perspective analysis generation,
and iterative self-reflection. Unlike existing chart QA systems, our framework
simulates the analytical reasoning process of business analysts by retrieving
domain-relevant knowledge and adapting to diverse datasets without relying on
closed ontologies or question templates.
  We evaluate our system on three datasets across different domains.
Benchmarked against GPT-4o with a single-prompt baseline, our approach shows
improved insightfulness, domain relevance, and analytical depth, as measured by
tailored evaluation metrics and qualitative human assessment.
  This work contributes a novel modular pipeline to bridge the path from raw
data to visualization, and opens new opportunities for human-in-the-loop
validation by domain experts in business analytics. All code can be found here:
https://github.com/77luvC/D2D_Data2Dashboard},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.23695v1},
File          = {2505.23695v1.pdf}
}
@article{2505.21969v3,
Author        = {Tianjun Gu and Linfeng Li and Xuhong Wang and Chenghua Gong and Jingyu Gong and Zhizhong Zhang and Yuan Xie and Lizhuang Ma and Xin Tan},
Title         = {DORAEMON: Decentralized Ontology-aware Reliable Agent with Enhanced
  Memory Oriented Navigation},
Eprint        = {2505.21969v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Adaptive navigation in unfamiliar environments is crucial for household
service robots but remains challenging due to the need for both low-level path
planning and high-level scene understanding. While recent vision-language model
(VLM) based zero-shot approaches reduce dependence on prior maps and
scene-specific training data, they face significant limitations: spatiotemporal
discontinuity from discrete observations, unstructured memory representations,
and insufficient task understanding leading to navigation failures. We propose
DORAEMON (Decentralized Ontology-aware Reliable Agent with Enhanced Memory
Oriented Navigation), a novel cognitive-inspired framework consisting of
Ventral and Dorsal Streams that mimics human navigation capabilities. The
Dorsal Stream implements the Hierarchical Semantic-Spatial Fusion and Topology
Map to handle spatiotemporal discontinuities, while the Ventral Stream combines
RAG-VLM and Policy-VLM to improve decision-making. Our approach also develops
Nav-Ensurance to ensure navigation safety and efficiency. We evaluate DORAEMON
on the HM3D, MP3D, and GOAT datasets, where it achieves state-of-the-art
performance on both success rate (SR) and success weighted by path length (SPL)
metrics, significantly outperforming existing methods. We also introduce a new
evaluation metric (AORI) to assess navigation intelligence better.
Comprehensive experiments demonstrate DORAEMON's effectiveness in zero-shot
autonomous navigation without requiring prior map building or pre-training.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.21969v3},
File          = {2505.21969v3.pdf}
}
@article{2505.20949v1,
Author        = {Andrea Giovanni Nuzzolese},
Title         = {Streamlining Knowledge Graph Creation with PyRML},
Eprint        = {2505.20949v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Knowledge Graphs (KGs) are increasingly adopted as a foundational technology
for integrating heterogeneous data in domains such as climate science, cultural
heritage, and the life sciences. Declarative mapping languages like R2RML and
RML have played a central role in enabling scalable and reusable KG
construction, offering a transparent means of transforming structured and
semi-structured data into RDF. In this paper, we present PyRML, a lightweight,
Python-native library for building Knowledge Graphs through declarative
mappings. PyRML supports core RML constructs and provides a programmable
interface for authoring, executing, and testing mappings directly within Python
environments. It integrates with popular data and semantic web libraries (e.g.,
Pandas and RDFlib), enabling transparent and modular workflows. By lowering the
barrier to entry for KG creation and fostering reproducible, ontology-aligned
data integration, PyRML bridges the gap between declarative semantics and
practical KG engineering.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.20949v1},
File          = {2505.20949v1.pdf}
}
@article{2505.20020v1,
Author        = {Natallia Kokash and Lei Wang and Thomas H. Gillespie and Adam Belloum and Paola Grosso and Sara Quinney and Lang Li and Bernard de Bono},
Title         = {Ontology- and LLM-based Data Harmonization for Federated Learning in
  Healthcare},
Eprint        = {2505.20020v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {The rise of electronic health records (EHRs) has unlocked new opportunities
for medical research, but privacy regulations and data heterogeneity remain key
barriers to large-scale machine learning. Federated learning (FL) enables
collaborative modeling without sharing raw data, yet faces challenges in
harmonizing diverse clinical datasets. This paper presents a two-step data
alignment strategy integrating ontologies and large language models (LLMs) to
support secure, privacy-preserving FL in healthcare, demonstrating its
effectiveness in a real-world project involving semantic mapping of EHR data.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.20020v1},
File          = {2505.20020v1.pdf}
}
@article{2505.19971v1,
Author        = {Kilian Sennrich and Sina Ahmadi},
Title         = {Conversational Lexicography: Querying Lexicographic Data on Knowledge
  Graphs with SPARQL through Natural Language},
Eprint        = {2505.19971v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graphs offer an excellent solution for representing the
lexical-semantic structures of lexicographic data. However, working with the
SPARQL query language represents a considerable hurdle for many non-expert
users who could benefit from the advantages of this technology. This paper
addresses the challenge of creating natural language interfaces for
lexicographic data retrieval on knowledge graphs such as Wikidata. We develop a
multidimensional taxonomy capturing the complexity of Wikidata's lexicographic
data ontology module through four dimensions and create a template-based
dataset with over 1.2 million mappings from natural language utterances to
SPARQL queries. Our experiments with GPT-2 (124M), Phi-1.5 (1.3B), and
GPT-3.5-Turbo reveal significant differences in model capabilities. While all
models perform well on familiar patterns, only GPT-3.5-Turbo demonstrates
meaningful generalization capabilities, suggesting that model size and diverse
pre-training are crucial for adaptability in this domain. However, significant
challenges remain in achieving robust generalization, handling diverse
linguistic data, and developing scalable solutions that can accommodate the
full complexity of lexicographic knowledge representation.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.19971v1},
File          = {2505.19971v1.pdf}
}
@article{2505.19581v1,
Author        = {Ritesh K. Singh and Souradeep Sasmal and S. Nautiyal and A. K. Pan},
Title         = {Self-testing in a constrained prepare-measure scenario sans assuming
  quantum dimension},
Eprint        = {2505.19581v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {quant-ph},
Abstract      = {We present a device-independent (DI) self-testing protocol in a constrained
prepare-measure scenario, based on the $n-$bit parity-oblivious multiplexing
(POM) task. In this scenario, a parity-oblivious constraint is imposed on the
preparations, allowing us to define a classical bound derived from a
preparation noncontextual ontological model. We derive the optimal quantum
success probability in the POM task devoid of assuming the dimension of the
quantum system, an essential step towards DI self-testing, which has hitherto
not been demonstrated in prepare-measure scenario. We demonstrate that the
optimal quantum value exceeds preparation noncontextual bound and, as a result,
this establishes DI self-testing of the preparations and the measurement
devices. Furthermore, by explicitly constructing the required unitaries, we
show that the optimal preparations and measurements in an unknown but finite
dimensional Hilbert space, responsible for the observed input-output
correlations, can be mapped, via an unitary, onto a known finite-dimensional
quantum system. Our results thus pave the way for scalable, single system based
DI certification protocols in the prepare-measure scenario.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.19581v1},
File          = {2505.19581v1.pdf}
}
@article{2507.21065v2,
Author        = {Sabrina Patania and Luca Annese and Cansu Koyuturk and Azzurra Ruggeri and Dimitri Ognibene},
Title         = {AI Pedagogy: Dialogic Social Learning for Artificial Agents},
Eprint        = {2507.21065v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have demonstrated remarkable capabilities in
processing extensive offline datasets. However, they often face challenges in
acquiring and integrating complex, knowledge online. Traditional AI training
paradigms, predominantly based on supervised learning or reinforcement
learning, mirror a 'Piagetian' model of independent exploration. These
approaches typically rely on large datasets and sparse feedback signals,
limiting the models' ability to learn efficiently from interactions. Drawing
inspiration from Vygotsky's sociocultural theory, this study explores the
potential of socially mediated learning paradigms to address these limitations.
  We introduce a dynamic environment, termed the 'AI Social Gym', where an AI
learner agent engages in dyadic pedagogical dialogues with knowledgeable AI
teacher agents. These interactions emphasize external, structured dialogue as a
core mechanism for knowledge acquisition, contrasting with methods that depend
solely on internal inference or pattern recognition.
  Our investigation focuses on how different pedagogical strategies impact the
AI learning process in the context of ontology acquisition. Empirical results
indicate that such dialogic approaches-particularly those involving
mixed-direction interactions combining top-down explanations with
learner-initiated questioning-significantly enhance the LLM's ability to
acquire and apply new knowledge, outperforming both unidirectional
instructional methods and direct access to structured knowledge, formats
typically present in training datasets.
  These findings suggest that integrating pedagogical and psychological
insights into AI and robot training can substantially improve post-training
knowledge acquisition and response quality. This approach offers a
complementary pathway to existing strategies like prompt engineering},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2507.21065v2},
File          = {2507.21065v2.pdf}
}
@article{2505.18973v2,
Author        = {Sarang Patil and Ashish Parmanand Pandey and Ioannis Koutis and Mengjia Xu},
Title         = {Hierarchical Mamba Meets Hyperbolic Geometry: A New Paradigm for
  Structured Language Embeddings},
Eprint        = {2505.18973v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Selective state-space models have achieved great success in long-sequence
modeling. However, their capacity for language representation, especially in
complex hierarchical reasoning tasks, remains underexplored. Most large
language models rely on flat Euclidean embeddings, limiting their ability to
capture latent hierarchies. To address this limitation, we propose Hierarchical
Mamba (HiM), integrating efficient Mamba2 with exponential growth and curved
nature of hyperbolic geometry to learn hierarchy-aware language embeddings for
deeper linguistic understanding. Mamba2-processed sequences are projected to
the Poincare ball (via tangent-based mapping) or Lorentzian manifold (via
cosine and sine-based mapping) with "learnable" curvature, optimized with a
combined hyperbolic loss. Our HiM model facilitates the capture of relational
distances across varying hierarchical levels, enabling effective long-range
reasoning. This makes it well-suited for tasks like mixed-hop prediction and
multi-hop inference in hierarchical classification. We evaluated our HiM with
four linguistic and medical datasets for mixed-hop prediction and multi-hop
inference tasks. Experimental results demonstrated that: 1) Both HiM models
effectively capture hierarchical relationships for four ontological datasets,
surpassing Euclidean baselines. 2) HiM-Poincare captures fine-grained semantic
distinctions with higher h-norms, while HiM-Lorentz provides more stable,
compact, and hierarchy-preserving embeddings favoring robustness over detail.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.18973v2},
File          = {2505.18973v2.pdf}
}
@article{2505.18850v1,
Author        = {Mohamed Aly Bouke},
Title         = {The Theory of the Unique Latent Pattern: A Formal Epistemic Framework
  for Structural Singularity in Complex Systems},
Eprint        = {2505.18850v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper introduces the Theory of the Unique Latent Pattern (ULP), a formal
epistemic framework that redefines the origin of apparent complexity in dynamic
systems. Rather than attributing unpredictability to intrinsic randomness or
emergent nonlinearity, ULP asserts that every analyzable system is governed by
a structurally unique, deterministic generative mechanism, one that remains
hidden not due to ontological indeterminacy, but due to epistemic constraints.
The theory is formalized using a non-universal generative mapping \(
\mathcal{F}_S(P_S, t) \), where each system \( S \) possesses its own latent
structure \( P_S \), irreducible and non-replicable across systems. Observed
irregularities are modeled as projections of this generative map through
observer-limited interfaces, introducing epistemic noise \( \varepsilon_S(t) \)
as a measure of incomplete access. By shifting the locus of uncertainty from
the system to the observer, ULP reframes chaos as a context-relative failure of
representation. We contrast this position with foundational paradigms in chaos
theory, complexity science, and statistical learning. While they assume or
model shared randomness or collective emergence, ULP maintains that every
instance harbors a singular structural identity. Although conceptual, the
theory satisfies the criterion of falsifiability in the Popperian sense, it
invites empirical challenge by asserting that no two systems governed by
distinct latent mechanisms will remain indistinguishable under sufficient
resolution. This opens avenues for structurally individuated models in AI,
behavioral inference, and epistemic diagnostics.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.18850v1},
File          = {2505.18850v1.pdf}
}
@article{2505.18703v1,
Author        = {Gaurav Negi and Dhairya Dalal and Omnia Zayed and Paul Buitelaar},
Title         = {Towards Semantic Integration of Opinions: Unified Opinion Concepts
  Ontology and Extraction Task},
Eprint        = {2505.18703v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper introduces the Unified Opinion Concepts (UOC) ontology to
integrate opinions within their semantic context. The UOC ontology bridges the
gap between the semantic representation of opinion across different
formulations. It is a unified conceptualisation based on the facets of opinions
studied extensively in NLP and semantic structures described through symbolic
descriptions. We further propose the Unified Opinion Concept Extraction (UOCE)
task of extracting opinions from the text with enhanced expressivity.
Additionally, we provide a manually extended and re-annotated evaluation
dataset for this task and tailored evaluation metrics to assess the adherence
of extracted opinions to UOC semantics. Finally, we establish baseline
performance for the UOCE task using state-of-the-art generative models.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.18703v1},
File          = {2505.18703v1.pdf}
}
@article{2505.18553v1,
Author        = {John Oyekan and Christopher Turner and Michael Bax and Erich Graf},
Title         = {Applying Ontologies and Knowledge Augmented Large Language Models to
  Industrial Automation: A Decision-Making Guidance for Achieving Human-Robot
  Collaboration in Industry 5.0},
Eprint        = {2505.18553v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {The rapid advancement of Large Language Models (LLMs) has resulted in
interest in their potential applications within manufacturing systems,
particularly in the context of Industry 5.0. However, determining when to
implement LLMs versus other Natural Language Processing (NLP) techniques,
ontologies or knowledge graphs, remains an open question. This paper offers
decision-making guidance for selecting the most suitable technique in various
industrial contexts, emphasizing human-robot collaboration and resilience in
manufacturing. We examine the origins and unique strengths of LLMs, ontologies,
and knowledge graphs, assessing their effectiveness across different industrial
scenarios based on the number of domains or disciplines required to bring a
product from design to manufacture. Through this comparative framework, we
explore specific use cases where LLMs could enhance robotics for human-robot
collaboration, while underscoring the continued relevance of ontologies and
knowledge graphs in low-dependency or resource-constrained sectors.
Additionally, we address the practical challenges of deploying these
technologies, such as computational cost and interpretability, providing a
roadmap for manufacturers to navigate the evolving landscape of Language based
AI tools in Industry 5.0. Our findings offer a foundation for informed
decision-making, helping industry professionals optimize the use of Language
Based models for sustainable, resilient, and human-centric manufacturing. We
also propose a Large Knowledge Language Model architecture that offers the
potential for transparency and configuration based on complexity of task and
computing resources available.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.18553v1},
File          = {2505.18553v1.pdf}
}
@article{2505.18470v2,
Author        = {Christopher J. Mungall and Adnan Malik and Daniel R. Korn and Justin T. Reese and Noel M. O'Boyle and  Noel and Janna Hastings},
Title         = {Chemical classification program synthesis using generative artificial
  intelligence},
Eprint        = {2505.18470v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Accurately classifying chemical structures is essential for cheminformatics
and bioinformatics, including tasks such as identifying bioactive compounds of
interest, screening molecules for toxicity to humans, finding non-organic
compounds with desirable material properties, or organizing large chemical
libraries for drug discovery or environmental monitoring. However, manual
classification is labor-intensive and difficult to scale to large chemical
databases. Existing automated approaches either rely on manually constructed
classification rules, or are deep learning methods that lack explainability.
  This work presents an approach that uses generative artificial intelligence
to automatically write chemical classifier programs for classes in the Chemical
Entities of Biological Interest (ChEBI) database. These programs can be used
for efficient deterministic run-time classification of SMILES structures, with
natural language explanations. The programs themselves constitute an
explainable computable ontological model of chemical class nomenclature, which
we call the ChEBI Chemical Class Program Ontology (C3PO).
  We validated our approach against the ChEBI database, and compared our
results against deep learning models and a naive SMARTS pattern based
classifier. C3PO outperforms the naive classifier, but does not reach the
performance of state of the art deep learning methods. However, C3PO has a
number of strengths that complement deep learning methods, including
explainability and reduced data dependence. C3PO can be used alongside deep
learning classifiers to provide an explanation of the classification, where
both methods agree. The programs can be used as part of the ontology
development process, and iteratively refined by expert human curators.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.18470v2},
File          = {2505.18470v2.pdf}
}
@article{2506.18742v1,
Author        = {R. Lukyanenko and O. Pastor and V. C. Storey},
Title         = {Conceptual Modelling for Life Sciences Based on Systemist Foundations},
Eprint        = {2506.18742v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {All aspects of our society, including the life sciences, need a mechanism for
people working within them to represent the concepts they employ to carry out
their research. For the information systems being designed and developed to
support researchers and scientists in conducting their work, conceptual models
of the relevant domains are usually designed as both blueprints for a system
being developed and as a means of communication between the designer and
developer. Most conceptual modelling concepts are generic in the sense that
they are applied with the same understanding across many applications. Problems
in the life sciences, however, are especially complex and important, because
they deal with humans, their well-being, and their interactions with the
environment as well as other organisms. This work proposes a systemist
perspective for creating a conceptual model of a life scientist's problem. We
introduce the notion of a system and then show how it can be applied to the
development of an information system for handling genomic-related information.
We extend our discussion to show how the proposed systemist perspective can
support the modelling of precision medicine. This research recognizes
challenges in life sciences research of how to model problems to better
represent the connections between physical and digital worlds. We propose a new
notation that explicitly incorporates systemist thinking, as well as the
components of systems based on recent ontological foundations. The new notation
captures important semantics in the domain of life sciences. It may be used to
facilitate understanding, communication and problem-solving more broadly. We
also provide a precise, sound, ontologically supported characterization of the
term system, as a basic construct for conceptual modelling in life sciences.},
Year          = {2025},
Month         = {May},
Note          = {BMC Bioinformatics, 23(574), 1-27 (2023)},
Url           = {http://arxiv.org/abs/2506.18742v1},
File          = {2506.18742v1.pdf}
}
@article{2505.18222v1,
Author        = {Hessa Alawwad},
Title         = {A Domain Ontology for Modeling the Book of Purification in Islam},
Eprint        = {2505.18222v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {This paper aims to address a gap in major Islamic topics by developing an
ontology for the Book of Purification in Islam. Many authoritative Islamic
texts begin with the Book of Purification, as it is essential for performing
prayer (the second pillar of Islam after Shahadah, the profession of faith) and
other religious duties such as Umrah and Hajj.
  The ontology development strategy followed six key steps: (1) domain
identification, (2) knowledge acquisition, (3) conceptualization, (4)
classification, (5) integration and implementation, and (6) ontology
generation. This paper includes examples of the constructed tables and
classifications.
  The focus is on the design and analysis phases, as technical implementation
is beyond the scope of this study. However, an initial implementation is
provided to illustrate the steps of the proposed strategy.
  The developed ontology ensures reusability by formally defining and encoding
the key concepts, attributes, and relationships related to the Book of
Purification. This structured representation is intended to support knowledge
sharing and reuse.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.18222v1},
File          = {2505.18222v1.pdf}
}
@article{2505.17482v1,
Author        = {Chao Lei and Nir Lipovetzky and Krista A. Ehinger and Yanchuan Chang},
Title         = {From Reasoning to Generalization: Knowledge-Augmented LLMs for ARC
  Benchmark},
Eprint        = {2505.17482v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Recent reasoning-oriented LLMs have demonstrated strong performance on
challenging tasks such as mathematics and science examinations. However, core
cognitive faculties of human intelligence, such as abstract reasoning and
generalization, remain underexplored. To address this, we evaluate recent
reasoning-oriented LLMs on the Abstraction and Reasoning Corpus (ARC)
benchmark, which explicitly demands both faculties. We formulate ARC as a
program synthesis task and propose nine candidate solvers. Experimental results
show that repeated-sampling planning-aided code generation (RSPC) achieves the
highest test accuracy and demonstrates consistent generalization across most
LLMs. To further improve performance, we introduce an ARC solver, Knowledge
Augmentation for Abstract Reasoning (KAAR), which encodes core knowledge priors
within an ontology that classifies priors into three hierarchical levels based
on their dependencies. KAAR progressively expands LLM reasoning capacity by
gradually augmenting priors at each level, and invokes RSPC to generate
candidate solutions after each augmentation stage. This stage-wise reasoning
reduces interference from irrelevant priors and improves LLM performance.
Empirical results show that KAAR maintains strong generalization and
consistently outperforms non-augmented RSPC across all evaluated LLMs,
achieving around 5% absolute gains and up to 64.52% relative improvement.
Despite these achievements, ARC remains a challenging benchmark for
reasoning-oriented LLMs, highlighting future avenues of progress in LLMs.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.17482v1},
File          = {2505.17482v1.pdf}
}
@article{2505.17233v2,
Author        = {Andreas Patakis and Vassilis Lyberatos and Spyridon Kantarelis and Edmund Dervakos and Giorgos Stamou},
Title         = {Semantic-Aware Interpretable Multimodal Music Auto-Tagging},
Eprint        = {2505.17233v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Music auto-tagging is essential for organizing and discovering music in
extensive digital libraries. While foundation models achieve exceptional
performance in this domain, their outputs often lack interpretability, limiting
trust and usability for researchers and end-users alike. In this work, we
present an interpretable framework for music auto-tagging that leverages groups
of musically meaningful multimodal features, derived from signal processing,
deep learning, ontology engineering, and natural language processing. To
enhance interpretability, we cluster features semantically and employ an
expectation maximization algorithm, assigning distinct weights to each group
based on its contribution to the tagging process. Our method achieves
competitive tagging performance while offering a deeper understanding of the
decision-making process, paving the way for more transparent and user-centric
music tagging systems.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.17233v2},
File          = {2505.17233v2.pdf}
}
@article{2505.16097v1,
Author        = {Zifeng Wang and Qiao Jin and Jiacheng Lin and Junyi Gao and Jathurshan Pradeepkumar and Pengcheng Jiang and Benjamin Danek and Zhiyong Lu and Jimeng Sun},
Title         = {TrialPanorama: Database and Benchmark for Systematic Review and Design
  of Clinical Trials},
Eprint        = {2505.16097v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Developing artificial intelligence (AI) for vertical domains requires a solid
data foundation for both training and evaluation. In this work, we introduce
TrialPanorama, a large-scale, structured database comprising 1,657,476 clinical
trial records aggregated from 15 global sources. The database captures key
aspects of trial design and execution, including trial setups, interventions,
conditions, biomarkers, and outcomes, and links them to standard biomedical
ontologies such as DrugBank and MedDRA. This structured and ontology-grounded
design enables TrialPanorama to serve as a unified, extensible resource for a
wide range of clinical trial tasks, including trial planning, design, and
summarization. To demonstrate its utility, we derive a suite of benchmark tasks
directly from the TrialPanorama database. The benchmark spans eight tasks
across two categories: three for systematic review (study search, study
screening, and evidence summarization) and five for trial design (arm design,
eligibility criteria, endpoint selection, sample size estimation, and trial
completion assessment). The experiments using five state-of-the-art large
language models (LLMs) show that while general-purpose LLMs exhibit some
zero-shot capability, their performance is still inadequate for high-stakes
clinical trial workflows. We release TrialPanorama database and the benchmark
to facilitate further research on AI for clinical trials.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.16097v1},
File          = {2505.16097v1.pdf}
}
@article{2505.15970v1,
Author        = {Matthew Lyle Olson and Musashi Hinck and Neale Ratzlaff and Changbai Li and Phillip Howard and Vasudev Lal and Shao-Yen Tseng},
Title         = {Analyzing Hierarchical Structure in Vision Models with Sparse
  Autoencoders},
Eprint        = {2505.15970v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {The ImageNet hierarchy provides a structured taxonomy of object categories,
offering a valuable lens through which to analyze the representations learned
by deep vision models. In this work, we conduct a comprehensive analysis of how
vision models encode the ImageNet hierarchy, leveraging Sparse Autoencoders
(SAEs) to probe their internal representations. SAEs have been widely used as
an explanation tool for large language models (LLMs), where they enable the
discovery of semantically meaningful features. Here, we extend their use to
vision models to investigate whether learned representations align with the
ontological structure defined by the ImageNet taxonomy. Our results show that
SAEs uncover hierarchical relationships in model activations, revealing an
implicit encoding of taxonomic structure. We analyze the consistency of these
representations across different layers of the popular vision foundation model
DINOv2 and provide insights into how deep vision models internalize
hierarchical category information by increasing information in the class token
through each layer. Our study establishes a framework for systematic
hierarchical analysis of vision model representations and highlights the
potential of SAEs as a tool for probing semantic structure in deep networks.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.15970v1},
File          = {2505.15970v1.pdf}
}
@article{2506.11023v1,
Author        = {Tomas Bueno Momcilovic and Barbara Gallina and Ingmar Kessler and Dian Balta},
Title         = {OntoGSN: An Ontology for Dynamic Management of Assurance Cases},
Eprint        = {2506.11023v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Assurance cases (ACs) are a common artifact for building and maintaining
confidence in system properties such as safety or robustness. Constructing an
AC can be challenging, although existing tools provide support in static,
document-centric applications and methods for dynamic contexts (e.g.,
autonomous driving) are emerging. Unfortunately, managing ACs remains a
challenge, since maintaining the embedded knowledge in the face of changes
requires substantial effort, in the process deterring developers - or worse,
producing poorly managed cases that instill false confidence. To address this,
we present OntoGSN: an ontology and supporting middleware for managing ACs in
the Goal Structuring Notation (GSN) standard. OntoGSN offers a knowledge
representation and a queryable graph that can be automatically populated,
evaluated, and updated. Our contributions include: a 1:1 formalization of the
GSN Community Standard v3 in an OWL ontology with SWRL rules; a helper ontology
and parser for integration with a widely used AC tool; a repository and
documentation of design decisions for OntoGSN maintenance; a SPARQL query
library with automation patterns; and a prototypical interface. The ontology
strictly adheres to the standard's text and has been evaluated according to
FAIR principles, the OOPS framework, competency questions, and community
feedback. The development of other middleware elements is guided by the
community needs and subject to ongoing evaluations. To demonstrate the utility
of our contributions, we illustrate dynamic AC management in an example
involving assurance of adversarial robustness in large language models.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2506.11023v1},
File          = {2506.11023v1.pdf}
}
@article{2505.13182v9,
Author        = {Jianfeng Xu},
Title         = {Information Science Principles of Machine Learning: A Causal Chain
  Meta-Framework Based on Formalized Information Mapping},
Eprint        = {2505.13182v9},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LO},
Abstract      = {[Objective] This study addresses key challenges in machine learning, namely
the absence of a unified formal theoretical framework and the lack of
foundational theories for model interpretability and ethical safety. [Methods]
We first construct a formal information model, explicitly defining the
ontological states and carrier mappings of typical machine learning stages
using sets of well-formed formulas. By introducing learnable and processable
predicates, as well as learning and processing functions, we analyze the causal
chain logic and constraint laws governing machine learning processes. [Results]
We establish the Machine Learning Theory Meta-Framework (MLT-MF), on which we
further propose universal definitions for model interpretability and ethical
safety. We prove and validate three key theorems: the relationship between
model interpretability and information existence, ethical safety assurance, and
the upper bound estimation of total variation distance (TVD). [Limitations] The
current framework assumes ideal, noise-free information enabling mappings and
focuses primarily on model learning and processing logic in static scenarios.
It does not yet address information fusion and conflict resolution across
ontological spaces in multimodal or multi-agent systems. [Conclusions] This
work overcomes the limitations of fragmented research and provides a unified
theoretical foundation for systematically addressing critical issues in
contemporary machine learning.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.13182v9},
File          = {2505.13182v9.pdf}
}
@article{2505.12964v1,
Author        = {Shanshan Liu and Noriki Nishida and Rumana Ferdous Munne and Narumi Tokunaga and Yuki Yamagata and Kouji Kozaki and Yuji Matsumoto},
Title         = {MA-COIR: Leveraging Semantic Search Index and Generative Models for
  Ontology-Driven Biomedical Concept Recognition},
Eprint        = {2505.12964v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recognizing biomedical concepts in the text is vital for ontology refinement,
knowledge graph construction, and concept relationship discovery. However,
traditional concept recognition methods, relying on explicit mention
identification, often fail to capture complex concepts not explicitly stated in
the text. To overcome this limitation, we introduce MA-COIR, a framework that
reformulates concept recognition as an indexing-recognition task. By assigning
semantic search indexes (ssIDs) to concepts, MA-COIR resolves ambiguities in
ontology entries and enhances recognition efficiency. Using a pretrained
BART-based model fine-tuned on small datasets, our approach reduces
computational requirements to facilitate adoption by domain experts.
Furthermore, we incorporate large language models (LLMs)-generated queries and
synthetic data to improve recognition in low-resource settings. Experimental
results on three scenarios (CDR, HPO, and HOIP) highlight the effectiveness of
MA-COIR in recognizing both explicit and implicit concepts without the need for
mention-level annotations during inference, advancing ontology-driven concept
recognition in biomedical domain applications. Our code and constructed data
are available at https://github.com/sl-633/macoir-master.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.12964v1},
File          = {2505.12964v1.pdf}
}
@article{2505.11194v1,
Author        = {Xiao Fei and Michail Chatzianastasis and Sarah Almeida Carneiro and Hadi Abdine and Lawrence P. Petalidis and Michalis Vazirgiannis},
Title         = {Prot2Text-V2: Protein Function Prediction with Multimodal Contrastive
  Alignment},
Eprint        = {2505.11194v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CE},
Abstract      = {Predicting protein function from sequence is a central challenge in
computational biology. While existing methods rely heavily on structured
ontologies or similarity-based techniques, they often lack the flexibility to
express structure-free functional descriptions and novel biological functions.
In this work, we introduce Prot2Text-V2, a novel multimodal sequence-to-text
model that generates free-form natural language descriptions of protein
function directly from amino acid sequences. Our method combines a protein
language model as a sequence encoder (ESM-3B) and a decoder-only language model
(LLaMA-3.1-8B-Instruct) through a lightweight nonlinear modality projector. A
key innovation is our Hybrid Sequence-level Contrastive Alignment Learning
(H-SCALE), which improves cross-modal learning by matching mean- and std-pooled
protein embeddings with text representations via contrastive loss. After the
alignment phase, we apply instruction-based fine-tuning using LoRA on the
decoder to teach the model how to generate accurate protein function
descriptions conditioned on the protein sequence. We train Prot2Text-V2 on
about 250K curated entries from SwissProt and evaluate it under low-homology
conditions, where test sequences have low similarity with training samples.
Prot2Text-V2 consistently outperforms traditional and LLM-based baselines
across various metrics.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.11194v1},
File          = {2505.11194v1.pdf}
}
@article{2505.11185v1,
Author        = {Francesco Madeddu and Lucia Testa and Gianluca De Carlo and Michele Pieroni and Andrea Mastropietro and Aris Anagnostopoulos and Paolo Tieri and Sergio Barbarossa},
Title         = {VitaGraph: Building a Knowledge Graph for Biologically Relevant Learning
  Tasks},
Eprint        = {2505.11185v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {The intrinsic complexity of human biology presents ongoing challenges to
scientific understanding. Researchers collaborate across disciplines to expand
our knowledge of the biological interactions that define human life. AI
methodologies have emerged as powerful tools across scientific domains,
particularly in computational biology, where graph data structures effectively
model biological entities such as protein-protein interaction (PPI) networks
and gene functional networks. Those networks are used as datasets for paramount
network medicine tasks, such as gene-disease association prediction, drug
repurposing, and polypharmacy side effect studies. Reliable predictions from
machine learning models require high-quality foundational data. In this work,
we present a comprehensive multi-purpose biological knowledge graph constructed
by integrating and refining multiple publicly available datasets. Building upon
the Drug Repurposing Knowledge Graph (DRKG), we define a pipeline tasked with
a) cleaning inconsistencies and redundancies present in DRKG, b) coalescing
information from the main available public data sources, and c) enriching the
graph nodes with expressive feature vectors such as molecular fingerprints and
gene ontologies. Biologically and chemically relevant features improve the
capacity of machine learning models to generate accurate and well-structured
embedding spaces. The resulting resource represents a coherent and reliable
biological knowledge graph that serves as a state-of-the-art platform to
advance research in computational biology and precision medicine. Moreover, it
offers the opportunity to benchmark graph-based machine learning and network
medicine models on relevant tasks. We demonstrate the effectiveness of the
proposed dataset by benchmarking it against the task of drug repurposing, PPI
prediction, and side-effect prediction, modeled as link prediction problems.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.11185v1},
File          = {2505.11185v1.pdf}
}
@article{2505.11031v2,
Author        = {Xiao Zhang and Huiyuan Lai and Qianru Meng and Johan Bos},
Title         = {OntoURL: A Benchmark for Evaluating Large Language Models on Symbolic
  Ontological Understanding, Reasoning and Learning},
Eprint        = {2505.11031v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) have demonstrated remarkable capabilities across
a range of natural language processing tasks, yet their ability to process
structured symbolic knowledge remains underexplored. To address this gap, we
propose a taxonomy of LLMs' ontological capabilities and introduce OntoURL, the
first comprehensive benchmark designed to systematically evaluate LLMs'
proficiency in handling ontologies -- formal, symbolic representations of
domain knowledge through concepts, relationships, and instances. Based on the
proposed taxonomy, OntoURL systematically assesses three dimensions:
understanding, reasoning, and learning through 15 distinct tasks comprising
58,981 questions derived from 40 ontologies across 8 domains. Experiments with
20 open-source LLMs reveal significant performance differences across models,
tasks, and domains, with current LLMs showing proficiency in understanding
ontological knowledge but substantial weaknesses in reasoning and learning
tasks. These findings highlight fundamental limitations in LLMs' capability to
process symbolic knowledge and establish OntoURL as a critical benchmark for
advancing the integration of LLMs with formal knowledge representations.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.11031v2},
File          = {2505.11031v2.pdf}
}
@article{2505.10142v1,
Author        = {Tim Wittenborg and Ildar Baimuratov and Ludvig KnÃ¶Ã¶s FranzÃ©n and Ingo Staack and Ulrich RÃ¶mer and SÃ¶ren Auer},
Title         = {Knowledge-Based Aerospace Engineering -- A Systematic Literature Review},
Eprint        = {2505.10142v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CE},
Abstract      = {The aerospace industry operates at the frontier of technological innovation
while maintaining high standards regarding safety and reliability. In this
environment, with an enormous potential for re-use and adaptation of existing
solutions and methods, Knowledge-Based Engineering (KBE) has been applied for
decades. The objective of this study is to identify and examine
state-of-the-art knowledge management practices in the field of aerospace
engineering. Our contributions include: 1) A SWARM-SLR of over 1,000 articles
with qualitative analysis of 164 selected articles, supported by two aerospace
engineering domain expert surveys. 2) A knowledge graph of over 700
knowledge-based aerospace engineering processes, software, and data, formalized
in the interoperable Web Ontology Language (OWL) and mapped to Wikidata entries
where possible. The knowledge graph is represented on the Open Research
Knowledge Graph (ORKG), and an aerospace Wikibase, for reuse and continuation
of structuring aerospace engineering knowledge exchange. 3) Our resulting
intermediate and final artifacts of the knowledge synthesis, available as a
Zenodo dataset. This review sets a precedent for structured, semantic-based
approaches to managing aerospace engineering knowledge. By advancing these
principles, research, and industry can achieve more efficient design processes,
enhanced collaboration, and a stronger commitment to sustainable aviation.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.10142v1},
File          = {2505.10142v1.pdf}
}
@article{2505.10093v1,
Author        = {Hsuan-Lei Shao},
Title         = {From Text to Network: Constructing a Knowledge Graph of Taiwan-Based
  China Studies Using Generative AI},
Eprint        = {2505.10093v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Taiwanese China Studies (CS) has developed into a rich, interdisciplinary
research field shaped by the unique geopolitical position and long standing
academic engagement with Mainland China. This study responds to the growing
need to systematically revisit and reorganize decades of Taiwan based CS
scholarship by proposing an AI assisted approach that transforms unstructured
academic texts into structured, interactive knowledge representations. We apply
generative AI (GAI) techniques and large language models (LLMs) to extract and
standardize entity relation triples from 1,367 peer reviewed CS articles
published between 1996 and 2019. These triples are then visualized through a
lightweight D3.js based system, forming the foundation of a domain specific
knowledge graph and vector database for the field. This infrastructure allows
users to explore conceptual nodes and semantic relationships across the corpus,
revealing previously uncharted intellectual trajectories, thematic clusters,
and research gaps. By decomposing textual content into graph structured
knowledge units, our system enables a paradigm shift from linear text
consumption to network based knowledge navigation. In doing so, it enhances
scholarly access to CS literature while offering a scalable, data driven
alternative to traditional ontology construction. This work not only
demonstrates how generative AI can augment area studies and digital humanities
but also highlights its potential to support a reimagined scholarly
infrastructure for regional knowledge systems.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.10093v1},
File          = {2505.10093v1.pdf}
}
@article{2505.13494v1,
Author        = {Ying Zhao and Guanhua Chen and Jie Liu},
Title         = {Polymer Data Challenges in the AI Era: Bridging Gaps for Next-Generation
  Energy Materials},
Eprint        = {2505.13494v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cond-mat.soft},
Abstract      = {The pursuit of advanced polymers for energy technologies, spanning
photovoltaics, solid-state batteries, and hydrogen storage, is hindered by
fragmented data ecosystems that fail to capture the hierarchical complexity of
these materials. Polymer science lacks interoperable databases, forcing
reliance on disconnected literature and legacy records riddled with
unstructured formats and irreproducible testing protocols. This fragmentation
stifles machine learning (ML) applications and delays the discovery of
materials critical for global decarbonization. Three systemic barriers compound
the challenge. First, academic-industrial data silos restrict access to
proprietary industrial datasets, while academic publications often omit
critical synthesis details. Second, inconsistent testing methods undermine
cross-study comparability. Third, incomplete metadata in existing databases
limits their utility for training reliable ML models. Emerging solutions
address these gaps through technological and collaborative innovation. Natural
language processing (NLP) tools extract structured polymer data from decades of
literature, while high-throughput robotic platforms generate self-consistent
datasets via autonomous experimentation. Central to these advances is the
adoption of FAIR (Findable, Accessible, Interoperable, Reusable) principles,
adapted to polymer-specific ontologies, ensuring machine-readability and
reproducibility. Future breakthroughs hinge on cultural shifts toward open
science, accelerated by decentralized data markets and autonomous laboratories
that merge robotic experimentation with real-time ML validation. By addressing
data fragmentation through technological innovation, collaborative governance,
and ethical stewardship, the polymer community can transform bottlenecks into
accelerants.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.13494v1},
File          = {2505.13494v1.pdf}
}
@article{2505.09798v1,
Author        = {Bojan Ristov and Stefan Eftimov and Milena Trajanoska and Dimitar Trajanov},
Title         = {Ontology-Based Structuring and Analysis of North Macedonian Public
  Procurement Contracts},
Eprint        = {2505.09798v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Public procurement plays a critical role in government operations, ensuring
the efficient allocation of resources and fostering economic growth. However,
traditional procurement data is often stored in rigid, tabular formats,
limiting its analytical potential and hindering transparency. This research
presents a methodological framework for transforming structured procurement
data into a semantic knowledge graph, leveraging ontological modeling and
automated data transformation techniques. By integrating RDF and SPARQL-based
querying, the system enhances the accessibility and interpretability of
procurement records, enabling complex semantic queries and advanced analytics.
Furthermore, by incorporating machine learning-driven predictive modeling, the
system extends beyond conventional data analysis, offering insights into
procurement trends and risk assessment. This work contributes to the broader
field of public procurement intelligence by improving data transparency,
supporting evidence-based decision-making, and enabling in-depth analysis of
procurement activities in North Macedonia.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.09798v1},
File          = {2505.09798v1.pdf}
}
@article{2505.09339v1,
Author        = {Salwa Mostafa and Mohamed K. Abdel-Aziz and Mohammed S. Elbamby and Mehdi Bennis},
Title         = {RAG-Enabled Intent Reasoning for Application-Network Interaction},
Eprint        = {2505.09339v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.NI},
Abstract      = {Intent-based network (IBN) is a promising solution to automate network
operation and management. IBN aims to offer human-tailored network interaction,
allowing the network to communicate in a way that aligns with the network
users' language, rather than requiring the network users to understand the
technical language of the network/devices. Nowadays, different applications
interact with the network, each with its own specialized needs and domain
language. Creating semantic languages (i.e., ontology-based languages) and
associating them with each application to facilitate intent translation lacks
technical expertise and is neither practical nor scalable. To tackle the
aforementioned problem, we propose a context-aware AI framework that utilizes
machine reasoning (MR), retrieval augmented generation (RAG), and generative AI
technologies to interpret intents from different applications and generate
structured network intents. The proposed framework allows for
generalized/domain-specific intent expression and overcomes the drawbacks of
large language models (LLMs) and vanilla-RAG framework. The experimental
results show that our proposed intent-RAG framework outperforms the LLM and
vanilla-RAG framework in intent translation.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.09339v1},
File          = {2505.09339v1.pdf}
}
@article{2505.06885v1,
Author        = {Saravanan Krishnan and Amith Singhee and Keerthi Narayan Raghunath and Alex Mathai and Atul Kumar and David Wenk},
Title         = {Incremental Analysis of Legacy Applications Using Knowledge Graphs for
  Application Modernization},
Eprint        = {2505.06885v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {Industries such as banking, telecom, and airlines - o6en have large so6ware
systems that are several decades old. Many of these systems are written in old
programming languages such as COBOL, PL/1, Assembler, etc. In many cases, the
documentation is not updated, and those who developed/designed these systems
are no longer around. Understanding these systems for either modernization or
even regular maintenance has been a challenge. An extensive application may
have natural boundaries based on its code dependencies and architecture. There
are also other logical boundaries in an enterprise setting driven by business
functions, data domains, etc. Due to these complications, the system architects
generally plan their modernization across these logical boundaries in parts,
thereby adopting an incremental approach for the modernization journey of the
entire system. In this work, we present a so6ware system analysis tool that
allows a subject ma=er expert (SME) or system architect to analyze a large
so6ware system incrementally. We analyze the source code and other artifacts
(such as data schema) to create a knowledge graph using a customizable
ontology/schema. Entities and relations in our ontology can be defined for any
combination of programming languages and platforms. Using this knowledge graph,
the analyst can then define logical boundaries around dependent Entities (e.g.
Programs, Transactions, Database Tables etc.). Our tool then presents different
views showcasing the dependencies from the newly defined boundary to/from the
other logical groups of the system. This exercise is repeated interactively to
1) Identify the Entities and groupings of interest for a modernization task and
2) Understand how a change in one part of the system may affect the other
parts. To validate the efficacy of our tool, we provide an initial study of our
system on two client applications.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.06885v1},
File          = {2505.06885v1.pdf}
}
@article{2505.06716v1,
Author        = {Holger Bech Nielsen},
Title         = {Approximate Minimal SU(5), Several Fundamental Scales, Fluctuating
  Lattice},
Eprint        = {2505.06716v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {hep-ph},
Abstract      = {Having shortly reviewed our idea of the grand unified SU(5) being only exact
in a classical limit, in a truly existing lattice, an ontological lattice, we
go over to putting a series of different physical energy scales such the
approximate unification scale for the SU(5)(without any SUSY), the Planck
scale, and e.g. the scale of see-saw neutrino masses into a certain plot
showing the energy scales on a straight line. This straight line of this plot
supposed to result from such an ontological lattice, that fluctuates in link
size a and lattice density in a very strong way according to a log normal
distribution. The point is that different energy scales result from the link
size of the lattice to different powers, like the averages a to the n th ,
where the power n depends on the type of scale considered. Since the n th root
of the average of the n th power of the link size in the fluctuating lattice is
very strongly dependent on the power n, because of very huge fluctuations, the
different types of physical energy scales can get very different. With a Galton
i.e. log normal distribution of the link size the various energy scales have
their logarithms fall into a nice straight line versus the power of the link
size, on which they depend. Our model gives a surprisingly good number for the
small deviations of the experimental electron-and muon-anomalous magnetic
moment from the pure Standard Model value.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.06716v1},
File          = {2505.06716v1.pdf}
}
@article{2505.06287v1,
Author        = {Riccardo Sieve and Paul Kobialka and Laura Slaughter and Rudolf Schlatte and Einar Broch Johnsen and Silvia Lizeth Tapia Tarifa},
Title         = {BedreFlyt: Improving Patient Flows through Hospital Wards with Digital
  Twins},
Eprint        = {2505.06287v1},
DOI           = {10.4204/EPTCS.418.1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Digital twins are emerging as a valuable tool for short-term decision-making
as well as for long-term strategic planning across numerous domains, including
process industry, energy, space, transport, and healthcare. This paper reports
on our ongoing work on designing a digital twin to enhance resource planning,
e.g., for the in-patient ward needs in hospitals. By leveraging executable
formal models for system exploration, ontologies for knowledge representation
and an SMT solver for constraint satisfiability, our approach aims to explore
hypothetical "what-if" scenarios to improve strategic planning processes, as
well as to solve concrete, short-term decision-making tasks. Our proposed
solution uses the executable formal model to turn a stream of arriving
patients, that need to be hospitalized, into a stream of optimization problems,
e.g., capturing daily inpatient ward needs, that can be solved by SMT
techniques. The knowledge base, which formalizes domain knowledge, is used to
model the needed configuration in the digital twin, allowing the twin to
support both short-term decision-making and long-term strategic planning by
generating scenarios spanning average-case as well as worst-case resource
needs, depending on the expected treatment of patients, as well as ranging over
variations in available resources, e.g., bed distribution in different rooms.
We illustrate our digital twin architecture by considering the problem of bed
bay allocation in a hospital ward.},
Year          = {2025},
Month         = {May},
Note          = {EPTCS 418, 2025, pp. 1-15},
Url           = {http://arxiv.org/abs/2505.06287v1},
File          = {2505.06287v1.pdf}
}
@article{2505.04313v1,
Author        = {Stephen Richard Varey and Alessandro Di Stefano and The Anh Han},
Title         = {KERAIA: An Adaptive and Explainable Framework for Dynamic Knowledge
  Representation and Reasoning},
Eprint        = {2505.04313v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In this paper, we introduce KERAIA, a novel framework and software platform
for symbolic knowledge engineering designed to address the persistent
challenges of representing, reasoning with, and executing knowledge in dynamic,
complex, and context-sensitive environments. The central research question that
motivates this work is: How can unstructured, often tacit, human expertise be
effectively transformed into computationally tractable algorithms that AI
systems can efficiently utilise? KERAIA seeks to bridge this gap by building on
foundational concepts such as Minsky's frame-based reasoning and K-lines, while
introducing significant innovations. These include Clouds of Knowledge for
dynamic aggregation, Dynamic Relations (DRels) for context-sensitive
inheritance, explicit Lines of Thought (LoTs) for traceable reasoning, and
Cloud Elaboration for adaptive knowledge transformation. This approach moves
beyond the limitations of traditional, often static, knowledge representation
paradigms. KERAIA is designed with Explainable AI (XAI) as a core principle,
ensuring transparency and interpretability, particularly through the use of
LoTs. The paper details the framework's architecture, the KSYNTH representation
language, and the General Purpose Paradigm Builder (GPPB) to integrate diverse
inference methods within a unified structure. We validate KERAIA's versatility,
expressiveness, and practical applicability through detailed analysis of
multiple case studies spanning naval warfare simulation, industrial diagnostics
in water treatment plants, and strategic decision-making in the game of RISK.
Furthermore, we provide a comparative analysis against established knowledge
representation paradigms (including ontologies, rule-based systems, and
knowledge graphs) and discuss the implementation aspects and computational
considerations of the KERAIA platform.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.04313v1},
File          = {2505.04313v1.pdf}
}
@article{2505.02830v1,
Author        = {Qingqiu Li and Zihang Cui and Seongsu Bae and Jilan Xu and Runtian Yuan and Yuejie Zhang and Rui Feng and Quanli Shen and Xiaobo Zhang and Junjun He and Shujun Wang},
Title         = {AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal
  Model in Chest X-Ray Interpretation},
Eprint        = {2505.02830v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Chest X-rays (CXRs) are the most frequently performed imaging examinations in
clinical settings. Recent advancements in Large Multimodal Models (LMMs) have
enabled automated CXR interpretation, enhancing diagnostic accuracy and
efficiency. However, despite their strong visual understanding, current Medical
LMMs (MLMMs) still face two major challenges: (1) Insufficient region-level
understanding and interaction, and (2) Limited accuracy and interpretability
due to single-step reasoning. In this paper, we empower MLMMs with
anatomy-centric reasoning capabilities to enhance their interactivity and
explainability. Specifically, we first propose an Anatomical Ontology-Guided
Reasoning (AOR) framework, which centers on cross-modal region-level
information to facilitate multi-step reasoning. Next, under the guidance of
expert physicians, we develop AOR-Instruction, a large instruction dataset for
MLMMs training. Our experiments demonstrate AOR's superior performance in both
VQA and report generation tasks.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.02830v1},
File          = {2505.02830v1.pdf}
}
@article{2505.02405v1,
Author        = {Mario A. V. Saucedo and Vignesh Kottayam Viswanathan and Christoforos Kanellakis and George Nikolakopoulos},
Title         = {Estimating Commonsense Scene Composition on Belief Scene Graphs},
Eprint        = {2505.02405v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {This work establishes the concept of commonsense scene composition, with a
focus on extending Belief Scene Graphs by estimating the spatial distribution
of unseen objects. Specifically, the commonsense scene composition capability
refers to the understanding of the spatial relationships among related objects
in the scene, which in this article is modeled as a joint probability
distribution for all possible locations of the semantic object class. The
proposed framework includes two variants of a Correlation Information (CECI)
model for learning probability distributions: (i) a baseline approach based on
a Graph Convolutional Network, and (ii) a neuro-symbolic extension that
integrates a spatial ontology based on Large Language Models (LLMs).
Furthermore, this article provides a detailed description of the dataset
generation process for such tasks. Finally, the framework has been validated
through multiple runs on simulated data, as well as in a real-world indoor
environment, demonstrating its ability to spatially interpret scenes across
different room types.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.02405v1},
File          = {2505.02405v1.pdf}
}
@article{2505.01754v1,
Author        = {Orlando JÃ¤hde and Thorsten Weber and RÃ¼diger Buchkremer},
Title         = {Unraveling Media Perspectives: A Comprehensive Methodology Combining
  Large Language Models, Topic Modeling, Sentiment Analysis, and Ontology
  Learning to Analyse Media Bias},
Eprint        = {2505.01754v1},
DOI           = {10.1007/s42001-025-00372-0},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Biased news reporting poses a significant threat to informed decision-making
and the functioning of democracies. This study introduces a novel methodology
for scalable, minimally biased analysis of media bias in political news. The
proposed approach examines event selection, labeling, word choice, and
commission and omission biases across news sources by leveraging natural
language processing techniques, including hierarchical topic modeling,
sentiment analysis, and ontology learning with large language models. Through
three case studies related to current political events, we demonstrate the
methodology's effectiveness in identifying biases across news sources at
various levels of granularity. This work represents a significant step towards
scalable, minimally biased media bias analysis, laying the groundwork for tools
to help news consumers navigate an increasingly complex media landscape.},
Year          = {2025},
Month         = {May},
Note          = {J Comput Soc Sc 8, 41 (2025)},
Url           = {http://arxiv.org/abs/2505.01754v1},
File          = {2505.01754v1.pdf}
}
@article{2505.01696v1,
Author        = {Alireza Sadeghi and Farshid Hajati and Ahmadreza Argha and Nigel H Lovell and Min Yang and Hamid Alinejad-Rokny},
Title         = {Interpretable graph-based models on multimodal biomedical data
  integration: A technical review and benchmarking},
Eprint        = {2505.01696v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.GN},
Abstract      = {Integrating heterogeneous biomedical data including imaging, omics, and
clinical records supports accurate diagnosis and personalised care. Graph-based
models fuse such non-Euclidean data by capturing spatial and relational
structure, yet clinical uptake requires regulator-ready interpretability. We
present the first technical survey of interpretable graph based models for
multimodal biomedical data, covering 26 studies published between Jan 2019 and
Sep 2024. Most target disease classification, notably cancer and rely on static
graphs from simple similarity measures, while graph-native explainers are rare;
post-hoc methods adapted from non-graph domains such as gradient saliency, and
SHAP predominate. We group existing approaches into four interpretability
families, outline trends such as graph-in-graph hierarchies, knowledge-graph
edges, and dynamic topology learning, and perform a practical benchmark. Using
an Alzheimer disease cohort, we compare Sensitivity Analysis, Gradient
Saliency, SHAP and Graph Masking. SHAP and Sensitivity Analysis recover the
broadest set of known AD pathways and Gene-Ontology terms, whereas Gradient
Saliency and Graph Masking surface complementary metabolic and transport
signatures. Permutation tests show all four beat random gene sets, but with
distinct trade-offs: SHAP and Graph Masking offer deeper biology at higher
compute cost, while Gradient Saliency and Sensitivity Analysis are quicker
though coarser. We also provide a step-by-step flowchart covering graph
construction, explainer choice and resource budgeting to help researchers
balance transparency and performance. This review synthesises the state of
interpretable graph learning for multimodal medicine, benchmarks leading
techniques, and charts future directions, from advanced XAI tools to
under-studied diseases, serving as a concise reference for method developers
and translational scientists.},
Year          = {2025},
Month         = {May},
Url           = {http://arxiv.org/abs/2505.01696v1},
File          = {2505.01696v1.pdf}
}
@article{2505.01309v1,
Author        = {Anicet Lepetit Ondo and Laurence Capus and Mamadou Bousso},
Title         = {Enhancing SPARQL Query Rewriting for Complex Ontology Alignments},
Eprint        = {2505.01309v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {SPARQL query rewriting is a fundamental mechanism for uniformly querying
heterogeneous ontologies in the Linked Data Web. However, the complexity of
ontology alignments, particularly rich correspondences (c : c), makes this
process challenging. Existing approaches primarily focus on simple (s : s) and
partially complex ( s : c) alignments, thereby overlooking the challenges posed
by more expressive alignments. Moreover, the intricate syntax of SPARQL
presents a barrier for non-expert users seeking to fully exploit the knowledge
encapsulated in ontologies. This article proposes an innovative approach for
the automatic rewriting of SPARQL queries from a source ontology to a target
ontology, based on a user's need expressed in natural language. It leverages
the principles of equivalence transitivity as well as the advanced capabilities
of large language models such as GPT-4. By integrating these elements, this
approach stands out for its ability to efficiently handle complex alignments,
particularly (c : c) correspondences , by fully exploiting their
expressiveness. Additionally, it facilitates access to aligned ontologies for
users unfamiliar with SPARQL, providing a flexible solution for querying
heterogeneous data.},
Year          = {2025},
Month         = {May},
Note          = {International Journal of Web & Semantic Technology (IJWesT)
  Vol.16, No.2, April 2025},
Url           = {http://arxiv.org/abs/2505.01309v1},
File          = {2505.01309v1.pdf}
}
@article{2504.21694v1,
Author        = {Tom Westermann and Malte Ramonat and Johannes Hujer and Felix Gehlhoff and Alexander Fay},
Title         = {Automatic Mapping of AutomationML Files to Ontologies for Graph Queries
  and Validation},
Eprint        = {2504.21694v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {AutomationML has seen widespread adoption as an open data exchange format in
the automation domain. It is an open and vendor neutral standard based on the
extensible markup language XML. However, AutomationML extends XML with
additional semantics, that limit the applicability of common XML-tools for
applications like querying or data validation. This article provides
practitioners with 1) an up-to-date ontology of the concepts in the
AutomationML-standard, as well as 2) a declarative mapping to automatically
transform any AutomationML model into RDF triples. Together, these artifacts
allow practitioners an easy integration of AutomationML information into
industrial knowledge graphs. A study on examples from the automation domain
concludes that transforming AutomationML to OWL opens up new powerful ways for
querying and validation that are impossible without transformation.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.21694v1},
File          = {2504.21694v1.pdf}
}
@article{2504.21474v1,
Author        = {Hadi Bayrami Asl Tekanlou and Jafar Razmara and Mahsa Sanaei and Mostafa Rahgouy and Hamed Babaei Giglou},
Title         = {Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner
  for Subject Tagging},
Eprint        = {2504.21474v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper presents our system, Homa, for SemEval-2025 Task 5: Subject
Tagging, which focuses on automatically assigning subject labels to technical
records from TIBKAT using the Gemeinsame Normdatei (GND) taxonomy. We leverage
OntoAligner, a modular ontology alignment toolkit, to address this task by
integrating retrieval-augmented generation (RAG) techniques. Our approach
formulates the subject tagging problem as an alignment task, where records are
matched to GND categories based on semantic similarity. We evaluate
OntoAligner's adaptability for subject indexing and analyze its effectiveness
in handling multilingual records. Experimental results demonstrate the
strengths and limitations of this method, highlighting the potential of
alignment techniques for improving subject tagging in digital libraries.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.21474v1},
File          = {2504.21474v1.pdf}
}
@article{2505.00039v4,
Author        = {Hudson de Martim},
Title         = {An Ontology-Driven Graph RAG for Legal Norms: A Hierarchical, Temporal,
  and Deterministic Approach},
Eprint        = {2505.00039v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Retrieval-Augmented Generation (RAG) systems in the legal domain face a
critical challenge: standard, flat-text retrieval is blind to the hierarchical,
diachronic, and causal structure of law, leading to anachronistic and
unreliable answers. This paper introduces an ontology-driven Graph RAG
framework designed to overcome these limitations. We ground our knowledge graph
in a formal, LRMoo-inspired model that distinguishes abstract legal Works from
their versioned Expressions. We model temporal states as efficient aggregations
that reuse the versioned expressions (CTVs) of unchanged components, and we
reify legislative events as first-class Action nodes to make causality explicit
and queryable. This structured backbone enables a unified, planner-guided query
strategy that applies explicit policies to deterministically resolve complex
requests for (i) point-in-time retrieval, (ii) hierarchical impact analysis,
and (iii) auditable provenance reconstruction. Through a case study on the
Brazilian Constitution, we demonstrate how this approach provides a verifiable,
temporally-correct substrate for LLMs, enabling higher-order analytical
capabilities while drastically reducing the risk of factual errors. The result
is a practical framework for building more trustworthy and explainable legal AI
systems.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2505.00039v4},
File          = {2505.00039v4.pdf}
}
@article{2504.20816v1,
Author        = {Jan-Ãke Larsson},
Title         = {The Contextual Heisenberg Microscope},
Eprint        = {2504.20816v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {quant-ph},
Abstract      = {The Heisenberg microscope provides a powerful mental image of the measurement
process of quantum mechanics (QM), attempting to explain the uncertainty
relation through an uncontrollable back-action from the measurement device.
However, Heisenberg's proposed back-action uses features that are not present
in the QM description of the world, and according to Bohr not present in the
world. Therefore, Bohr argues, the mental image proposed by Heisenberg should
be avoided. Later developments by Bell and Kochen-Specker shows that a model
that contains the features used for the Heisenberg microscope is in principle
possible but must necessarily be nonlocal and contextual. In this paper we will
re-examine the measurement process within a restriction of QM known as
Stabilizer QM, that still exhibits for example Greenberger-Horne-Zeilinger
nonlocality and Peres-Mermin contextuality. The re-examination will use a
recent extension of stabilizer QM, the Contextual Ontological Model (COM),
where the system state gives a complete description of future measurement
outcomes reproducing the quantum predictions, including the mentioned
phenomena. We will see that the resulting contextual Heisenberg microscope
back-action can be completely described within COM, and that the associated
randomness originates in the initial state of the pointer system, exactly as in
the original description of the Heisenberg microscope. The presence of
contextuality, usually seen as prohibiting ontological models, suggests that
the contextual Heisenberg microscope picture can be enabled in general QM.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.20816v1},
File          = {2504.20816v1.pdf}
}
@article{2506.06284v2,
Author        = {John Beverley and Jim Logan and Barry Smith},
Title         = {Unreal Patterns},
Eprint        = {2506.06284v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper introduces a framework for representing information about entities
that do not exist or may never exist, such as those involving fictional
entities, blueprints, simulations, and future scenarios. Traditional approaches
that introduce "dummy instances" or rely on modal logic are criticized, and a
proposal is defended in which such cases are modeled using the intersections of
actual types rather than specific non existent tokens. The paper positions
itself within the Basic Formal Ontology and its realist commitments,
emphasizing the importance of practical, implementable solutions over purely
metaphysical or philosophical proposals, arguing that existing approaches to
non existent entities either overcommit to metaphysical assumptions or
introduce computational inefficiencies that hinder applications. By developing
a structured ontology driven approach to unreal patterns, the paper aims to
provide a useful and computationally viable means of handling references to
hypothetical or non existent entities.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2506.06284v2},
File          = {2506.06284v2.pdf}
}
@article{2504.19968v1,
Author        = {John Beverley and Regina Hurley},
Title         = {How Group Lives Go Well},
Eprint        = {2504.19968v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper explores the ontological space of group well being, proposing a
framework for representing collective welfare, group functions, and long term
contributions within an ontology engineering context. Traditional well being
theories focus on individual states, often relying on hedonistic, desire
satisfaction, or objective list models. Such approaches struggle to account for
cases where individual sacrifices contribute to broader social progress, a
critical challenge in modeling group flourishing. To address this, the paper
refines and extends the Counterfactual Account (CT) of well being, which
evaluates goodness of an event by comparing an individual's actual well being
with a hypothetical counterpart in a nearby possible world. While useful, this
framework is insufficient for group level ontologies, where well being depends
on functional persistence, institutional roles, and historical impact rather
than immediate individual outcomes. Drawing on Basic Formal Ontology (BFO), the
paper introduces a model in which group flourishing is evaluated in terms of
group functional, where members bear roles and exhibit persistence conditions
akin to biological systems or designed artifacts. This approach enables
semantic interoperability for modeling longitudinal social contributions,
allowing for structured reasoning about group welfare, social institutions, and
group flourishing over time.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.19968v1},
File          = {2504.19968v1.pdf}
}
@article{2506.05352v1,
Author        = {John Beverley and Regina Hurley},
Title         = {A Path to Loving},
Eprint        = {2506.05352v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This work lays the foundations for a rigorous ontological characterization of
love, addressing its philosophical complexity and scientific relevance, with
particular emphasis on psychology and sociology, as well as highlighting ways
in which such characterization enhances relevant AI based applications. The
position defended here is that love is best understood as a concatenation of
passive sensations (e.g., emotional arousal) and active evaluative judgments
(e.g., perceiving the beloved as valuable), in the interest of balancing the
involuntary aspects of love with its rational accountability. To provide a
structured foundation, the paper draws on Basic Formal Ontology (BFO) and other
applied ontological methods to differentiate various senses of love. This work
engages with objections to the understanding of love as concatenation,
particularly concerning the relationship between sensation and judgment. A
causal correlation model is defended, ensuring that the affective and cognitive
components are linked. By offering a precise and scalable ontological account,
this work lays the foundation for future interdisciplinary applications, making
love a subject of formal inquiry in ontology engineering, artificial
intelligence, and the sciences.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2506.05352v1},
File          = {2506.05352v1.pdf}
}
@article{2504.19667v3,
Author        = {Michael Banf and Johannes Kuhn},
Title         = {Tripartite-GraphRAG via Plugin Ontologies},
Eprint        = {2504.19667v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Large Language Models (LLMs) have shown remarkable capabilities across
various domains, yet they struggle with knowledge-intensive tasks in areas that
demand factual accuracy, e.g. industrial automation and healthcare. Key
limitations include their tendency to hallucinate, lack of source traceability
(provenance), and challenges in timely knowledge updates. Combining language
models with knowledge graphs (GraphRAG) offers promising avenues for overcoming
these deficits. However, a major challenge lies in creating such a knowledge
graph in the first place. Here, we propose a novel approach that combines LLMs
with a tripartite knowledge graph representation, which is constructed by
connecting complex, domain-specific objects via a curated ontology of
corresponding, domain-specific concepts to relevant sections within chunks of
text through a concept-anchored pre-analysis of source documents starting from
an initial lexical graph. Subsequently, we formulate LLM prompt creation as an
unsupervised node classification problem allowing for the optimization of
information density, coverage, and arrangement of LLM prompts at significantly
reduced lengths. An initial experimental evaluation of our approach on a
healthcare use case, involving multi-faceted analyses of patient anamneses
given a set of medical concepts as well as a series of clinical guideline
literature, indicates its potential to optimize information density, coverage,
and arrangement of LLM prompts while significantly reducing their lengths,
which, in turn, may lead to reduced costs as well as more consistent and
reliable LLM outputs.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.19667v3},
File          = {2504.19667v3.pdf}
}
@article{2504.21040v1,
Author        = {Chenyi Cai and Kosuke Kuriyama and Youlong Gu and Filip Biljecki and Pieter Herthogs},
Title         = {Can a Large Language Model Assess Urban Design Quality? Evaluating
  Walkability Metrics Across Expertise Levels},
Eprint        = {2504.21040v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Urban street environments are vital to supporting human activity in public
spaces. The emergence of big data, such as street view images (SVIs) combined
with multimodal large language models (MLLMs), is transforming how researchers
and practitioners investigate, measure, and evaluate semantic and visual
elements of urban environments. Considering the low threshold for creating
automated evaluative workflows using MLLMs, it is crucial to explore both the
risks and opportunities associated with these probabilistic models. In
particular, the extent to which the integration of expert knowledge can
influence the performance of MLLMs in evaluating the quality of urban design
has not been fully explored. This study sets out an initial exploration of how
integrating more formal and structured representations of expert urban design
knowledge into the input prompts of an MLLM (ChatGPT-4) can enhance the model's
capability and reliability in evaluating the walkability of built environments
using SVIs. We collect walkability metrics from the existing literature and
categorize them using relevant ontologies. We then select a subset of these
metrics, focusing on the subthemes of pedestrian safety and attractiveness, and
develop prompts for the MLLM accordingly. We analyze the MLLM's ability to
evaluate SVI walkability subthemes through prompts with varying levels of
clarity and specificity regarding evaluation criteria. Our experiments
demonstrate that MLLMs are capable of providing assessments and interpretations
based on general knowledge and can support the automation of multimodal
image-text evaluations. However, they generally provide more optimistic scores
and can make mistakes when interpreting the provided metrics, resulting in
incorrect evaluations. By integrating expert knowledge, the MLLM's evaluative
performance exhibits higher consistency and concentration.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.21040v1},
File          = {2504.21040v1.pdf}
}
@article{2504.19565v2,
Author        = {Meng Xiao and Xunxin Cai and Qingqing Long and Chengrui Wang and Yuanchun Zhou and Hengshu Zhu},
Title         = {m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation
  Framework for Biomedical Large Language Models Training},
Eprint        = {2504.19565v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Corpus distillation for biomedical large language models (LLMs) seeks to
address the pressing challenge of insufficient quantity and quality in
open-source annotated scientific corpora, which remains a bottleneck for
effective LLM training in biomedical research. This paper proposes a
knowledge-driven, agentic framework for scientific corpus distillation,
tailored explicitly for LLM training in the biomedical domain, addressing the
challenge posed by the complex hierarchy of biomedical knowledge. Central to
our approach is a collaborative multi-agent architecture, where specialized
agents, each guided by the Medical Subject Headings (MeSH) hierarchy, work in
concert to autonomously extract, synthesize, and self-evaluate high-quality
textual data from vast scientific literature. This agentic framework
collectively generates and refines domain-specific question-answer pairs,
ensuring comprehensive coverage and consistency with biomedical ontologies
while minimizing manual involvement. Extensive experimental results show that
language models trained on our multi-agent distilled datasets achieve notable
improvements in biomedical question-answering tasks, outperforming both strong
life sciences LLM baselines and advanced proprietary models. Notably, our
AI-Ready dataset enables Llama3-70B to surpass GPT-4 with MedPrompt and
Med-PaLM-2, despite their larger scale. Detailed ablation studies and case
analyses further validate the effectiveness and synergy of each agent within
the framework, highlighting the potential of multi-agent collaboration in
biomedical LLM training.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.19565v2},
File          = {2504.19565v2.pdf}
}
@article{2504.19023v1,
Author        = {Justin MÃ¼cke and Ansgar Scherp},
Title         = {GLaMoR: Consistency Checking of OWL Ontologies using Graph Language
  Models},
Eprint        = {2504.19023v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Semantic reasoning aims to infer new knowledge from existing knowledge, with
OWL ontologies serving as a standardized framework for organizing information.
A key challenge in semantic reasoning is verifying ontology consistency.
However, state-of-the-art reasoners are computationally expensive, and their
efficiency decreases as ontology sizes grow. While classical machine learning
models have been explored for consistency checking, they struggle to capture
complex relationships within ontologies. Large language models (LLMs) have
shown promising results for simple reasoning tasks but perform poorly on
structured reasoning. The recently introduced Graph Language Model (GLM) offers
a way to simultaneously process graph-structured data and text. This paper
proposes GLaMoR (Graph Language Model for Reasoning), a reasoning pipeline that
transforms OWL ontologies into graph-structured data and adapts the GLM
architecture for consistency checking. We evaluate GLaMoR on ontologies from
the NCBO BioPortal repository, converting them into triples suitable for model
input. Our results show that the GLM outperforms all baseline models, achieving
$95\%$ accuracy while being 20 times faster than classical reasoners.
  The Code is accessible under: https://github.com/JustinMuecke/GLaMoR},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.19023v1},
File          = {2504.19023v1.pdf}
}
@article{2504.18651v1,
Author        = {Filipi Miranda Soares and Antonio Mauro Saraiva and LuÃ­s Ferreira Pires and Luiz Olavo Bonino da Silva Santos and Dilvan de Abreu Moreira and Fernando Elias CorrÃªa and Kelly Rosa Braghetto and Debora Pignatari Drucker and Alexandre ClÃ¡udio Botazzo Delbem},
Title         = {Exploring a Large Language Model for Transforming Taxonomic Data into
  OWL: Lessons Learned and Implications for Ontology Development},
Eprint        = {2504.18651v1},
DOI           = {10.3724/2096-7004.di.2025.0020},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Managing scientific names in ontologies that represent species taxonomies is
challenging due to the ever-evolving nature of these taxonomies. Manually
maintaining these names becomes increasingly difficult when dealing with
thousands of scientific names. To address this issue, this paper investigates
the use of ChatGPT-4 to automate the development of the :Organism module in the
Agricultural Product Types Ontology (APTO) for species classification. Our
methodology involved leveraging ChatGPT-4 to extract data from the GBIF
Backbone API and generate OWL files for further integration in APTO. Two
alternative approaches were explored: (1) issuing a series of prompts for
ChatGPT-4 to execute tasks via the BrowserOP plugin and (2) directing ChatGPT-4
to design a Python algorithm to perform analogous tasks. Both approaches rely
on a prompting method where we provide instructions, context, input data, and
an output indicator. The first approach showed scalability limitations, while
the second approach used the Python algorithm to overcome these challenges, but
it struggled with typographical errors in data handling. This study highlights
the potential of Large language models like ChatGPT-4 to streamline the
management of species names in ontologies. Despite certain limitations, these
tools offer promising advancements in automating taxonomy-related tasks and
improving the efficiency of ontology development.},
Year          = {2025},
Month         = {Apr},
Note          = {2025},
Url           = {http://arxiv.org/abs/2504.18651v1},
File          = {2504.18651v1.pdf}
}
@article{2504.18422v1,
Author        = {Alan Khoja and Martin KÃ¶lbl and Stefan Leue and RÃ¼diger Wilhelmi},
Title         = {Automated Consistency Analysis for Legal Contracts},
Eprint        = {2504.18422v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LO},
Abstract      = {Business contracts, particularly sale and purchase agreements, often contain
a large number of clauses and are correspondingly long and complex. In
practice, it is therefore a great challenge to keep track of their legal
context and to identify and avoid inconsistencies in such contracts. Against
this background, we describe a method and tool called ContractCheck which
allows for the consistency analysis of legal contracts, in particular Share
Purchase Agreements (SPAs). In order to identify the concepts that are relevant
for an analysis we define an ontology for SPAs. The analysis is, then, based on
an encoding of the preconditions for the execution of the clauses of an SPA, as
well as on a set of proposed consistency constraints formalized using decidable
fragments of First-Order Logic (FOL). Based on the ontology for SPAs, textual
SPAs are first encoded in a structured natural language format that we refer to
as ``blocks''. ContractCheck interprets these blocks and constraints and
translates them into assertions formulated in FOL. It then invokes a
Satisfiability Modulo Theory (SMT) solver in order to check the executability
of a considered contract, either by providing a satisfying model, or by proving
the existence of conflicting clauses that prevent the contract from being
executed. We illustrate the application of ContractCheck to concrete SPAs,
including one example of an SPA of realistic size and complexity, and conclude
by suggesting directions for future research.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.18422v1},
File          = {2504.18422v1.pdf}
}
@article{2504.18380v1,
Author        = {Steven HÃ¤sler and Philipp Ackermann},
Title         = {Spatial Reasoner: A 3D Inference Pipeline for XR Applications},
Eprint        = {2504.18380v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {Modern extended reality XR systems provide rich analysis of image data and
fusion of sensor input and demand AR/VR applications that can reason about 3D
scenes in a semantic manner. We present a spatial reasoning framework that
bridges geometric facts with symbolic predicates and relations to handle key
tasks such as determining how 3D objects are arranged among each other ('on',
'behind', 'near', etc.). Its foundation relies on oriented 3D bounding box
representations, enhanced by a comprehensive set of spatial predicates, ranging
from topology and connectivity to directionality and orientation, expressed in
a formalism related to natural language. The derived predicates form a spatial
knowledge graph and, in combination with a pipeline-based inference model,
enable spatial queries and dynamic rule evaluation. Implementations for client-
and server-side processing demonstrate the framework's capability to
efficiently translate geometric data into actionable knowledge, ensuring
scalable and technology-independent spatial reasoning in complex 3D
environments. The Spatial Reasoner framework is fostering the creation of
spatial ontologies, and seamlessly integrates with and therefore enriches
machine learning, natural language processing, and rule systems in XR
applications.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.18380v1},
File          = {2504.18380v1.pdf}
}
@article{2504.17402v1,
Author        = {Anna Sofia Lippolis and Mohammad Javad Saeedizade and Robin Keskisarkka and Aldo Gangemi and Eva Blomqvist and Andrea Giovanni Nuzzolese},
Title         = {Assessing the Capability of Large Language Models for Domain-Specific
  Ontology Generation},
Eprint        = {2504.17402v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large Language Models (LLMs) have shown significant potential for ontology
engineering. However, it is still unclear to what extent they are applicable to
the task of domain-specific ontology generation. In this study, we explore the
application of LLMs for automated ontology generation and evaluate their
performance across different domains. Specifically, we investigate the
generalizability of two state-of-the-art LLMs, DeepSeek and o1-preview, both
equipped with reasoning capabilities, by generating ontologies from a set of
competency questions (CQs) and related user stories. Our experimental setup
comprises six distinct domains carried out in existing ontology engineering
projects and a total of 95 curated CQs designed to test the models' reasoning
for ontology engineering. Our findings show that with both LLMs, the
performance of the experiments is remarkably consistent across all domains,
indicating that these methods are capable of generalizing ontology generation
tasks irrespective of the domain. These results highlight the potential of
LLM-based approaches in achieving scalable and domain-agnostic ontology
construction and lay the groundwork for further research into enhancing
automated reasoning and knowledge representation techniques.},
Year          = {2025},
Month         = {Apr},
Note          = {Joint Proc. ESWC 2025 Workshops and Tutorials, CEUR-WS, Vol. 3977,
  2025},
Url           = {http://arxiv.org/abs/2504.17402v1},
File          = {2504.17402v1.pdf}
}
@article{2504.17828v1,
Author        = {Bozheng Li and Yongliang Wu and Yi Lu and Jiashuo Yu and Licheng Tang and Jiawang Cao and Wenqing Zhu and Yuyang Sun and Jay Wu and Wenbo Zhu},
Title         = {VEU-Bench: Towards Comprehensive Understanding of Video Editing},
Eprint        = {2504.17828v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Widely shared videos on the internet are often edited. Recently, although
Video Large Language Models (Vid-LLMs) have made great progress in general
video understanding tasks, their capabilities in video editing understanding
(VEU) tasks remain unexplored. To address this gap, in this paper, we introduce
VEU-Bench (Video Editing Understanding Benchmark), a comprehensive benchmark
that categorizes video editing components across various dimensions, from
intra-frame features like shot size to inter-shot attributes such as cut types
and transitions. Unlike previous video editing understanding benchmarks that
focus mainly on editing element classification, VEU-Bench encompasses 19
fine-grained tasks across three stages: recognition, reasoning, and judging. To
enhance the annotation of VEU automatically, we built an annotation pipeline
integrated with an ontology-based knowledge base. Through extensive experiments
with 11 state-of-the-art Vid-LLMs, our findings reveal that current Vid-LLMs
face significant challenges in VEU tasks, with some performing worse than
random choice. To alleviate this issue, we develop Oscars, a VEU expert model
fine-tuned on the curated VEU-Bench dataset. It outperforms existing
open-source Vid-LLMs on VEU-Bench by over 28.3% in accuracy and achieves
performance comparable to commercial models like GPT-4o. We also demonstrate
that incorporating VEU data significantly enhances the performance of Vid-LLMs
on general video understanding benchmarks, with an average improvement of 8.3%
across nine reasoning tasks.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.17828v1},
File          = {2504.17828v1.pdf}
}
@article{2505.05481v1,
Author        = {Ryan Williams},
Title         = {Structure & Quality: Conceptual and Formal Foundations for the Mind-Body
  Problem},
Eprint        = {2505.05481v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.NC},
Abstract      = {This paper explores the hard problem of consciousness from a different
perspective. Instead of drawing distinctions between the physical and the
mental, an exploration of a more foundational relationship is examined: the
relationship between structure and quality.
  Information-theoretic measures are developed to quantify the mutual
determinability between structure and quality, including a novel Q-S space for
analyzing fidelity between the two domains. This novel space naturally points
toward a five-fold categorization of possible relationships between structural
and qualitative properties, illustrating each through conceptual and formal
models.
  The ontological implications of each category are examined, shedding light on
debates around functionalism, emergentism, idealism, panpsychism, and neutral
monism.
  This new line of inquiry has established a framework for deriving theoretical
constraints on qualitative systems undergoing evolution that is explored in my
companion paper, Qualia & Natural Selection.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2505.05481v1},
File          = {2505.05481v1.pdf}
}
@article{2504.15929v2,
Author        = {Saban Ozturk and Melih B. Yilmaz and Muti Kara and M. Talat Yavuz and Aykut KoÃ§ and Tolga Ãukur},
Title         = {Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language
  Models},
Eprint        = {2504.15929v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Diagnostic imaging relies on interpreting both images and radiology reports,
but the growing data volumes place significant pressure on medical experts,
yielding increased errors and workflow backlogs. Medical vision-language models
(med-VLMs) have emerged as a powerful framework to efficiently process
multimodal imaging data, particularly in chest X-ray (CXR) evaluations, albeit
their performance hinges on how well image and text representations are
aligned. Existing alignment methods, predominantly based on contrastive
learning, prioritize separation between disease classes over segregation of
fine-grained pathology attributes like location, size or severity, leading to
suboptimal representations. Here, we propose MedTrim (Meta-entity-driven
Triplet mining), a novel method that enhances image-text alignment through
multimodal triplet learning synergistically guided by disease class as well as
adjectival and directional pathology descriptors. Unlike common alignment
methods that separate broad disease classes, MedTrim leverages structured
meta-entity information to preserve subtle but clinically significant
intra-class variations. For this purpose, we first introduce an ontology-based
entity recognition module that extracts pathology-specific meta-entities from
CXR reports, as annotations on pathology attributes are rare in public
datasets. For refined sample selection in triplet mining, we then introduce a
novel score function that captures an aggregate measure of inter-sample
similarity based on disease classes and adjectival/directional descriptors.
Lastly, we introduce a multimodal triplet alignment objective for explicit
within- and cross-modal alignment between samples sharing detailed pathology
characteristics. Our demonstrations indicate that MedTrim improves performance
in downstream retrieval and classification tasks compared to state-of-the-art
alignment methods.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.15929v2},
File          = {2504.15929v2.pdf}
}
@article{2504.14845v1,
Author        = {Qiushi Xiong and Zhipeng Xu and Zhenghao Liu and Mengjia Wang and Zulong Chen and Yue Sun and Yu Gu and Xiaohua Li and Ge Yu},
Title         = {Enhancing the Patent Matching Capability of Large Language Models via
  the Memory Graph},
Eprint        = {2504.14845v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Intellectual Property (IP) management involves strategically protecting and
utilizing intellectual assets to enhance organizational innovation,
competitiveness, and value creation. Patent matching is a crucial task in
intellectual property management, which facilitates the organization and
utilization of patents. Existing models often rely on the emergent capabilities
of Large Language Models (LLMs) and leverage them to identify related patents
directly. However, these methods usually depend on matching keywords and
overlook the hierarchical classification and categorical relationships of
patents. In this paper, we propose MemGraph, a method that augments the patent
matching capabilities of LLMs by incorporating a memory graph derived from
their parametric memory. Specifically, MemGraph prompts LLMs to traverse their
memory to identify relevant entities within patents, followed by attributing
these entities to corresponding ontologies. After traversing the memory graph,
we utilize extracted entities and ontologies to improve the capability of LLM
in comprehending the semantics of patents. Experimental results on the
PatentMatch dataset demonstrate the effectiveness of MemGraph, achieving a
17.68% performance improvement over baseline LLMs. The further analysis
highlights the generalization ability of MemGraph across various LLMs, both
in-domain and out-of-domain, and its capacity to enhance the internal reasoning
processes of LLMs during patent matching. All data and codes are available at
https://github.com/NEUIR/MemGraph.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.14845v1},
File          = {2504.14845v1.pdf}
}
@article{2504.16117v1,
Author        = {Sridevi Polavaram and Xin Zhou and Meenu Ravi and Mohammad Zarei and Anmol Srivastava},
Title         = {Context-Awareness and Interpretability of Rare Occurrences for Discovery
  and Formalization of Critical Failure Modes},
Eprint        = {2504.16117v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Vision systems are increasingly deployed in critical domains such as
surveillance, law enforcement, and transportation. However, their
vulnerabilities to rare or unforeseen scenarios pose significant safety risks.
To address these challenges, we introduce Context-Awareness and
Interpretability of Rare Occurrences (CAIRO), an ontology-based human-assistive
discovery framework for failure cases (or CP - Critical Phenomena) detection
and formalization. CAIRO by design incentivizes human-in-the-loop for testing
and evaluation of criticality that arises from misdetections, adversarial
attacks, and hallucinations in AI black-box models. Our robust analysis of
object detection model(s) failures in automated driving systems (ADS) showcases
scalable and interpretable ways of formalizing the observed gaps between camera
perception and real-world contexts, resulting in test cases stored as explicit
knowledge graphs (in OWL/XML format) amenable for sharing, downstream analysis,
logical reasoning, and accountability.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.16117v1},
File          = {2504.16117v1.pdf}
}
@article{2504.12977v1,
Author        = {Maksim Vishnevskiy},
Title         = {A Phenomenological Approach to Analyzing User Queries in IT Systems
  Using Heidegger's Fundamental Ontology},
Eprint        = {2504.12977v1},
DOI           = {10.5281/Zenodo.15241370},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {This paper presents a novel research analytical IT system grounded in Martin
Heidegger's Fundamental Ontology, distinguishing between beings (das Seiende)
and Being (das Sein). The system employs two modally distinct, descriptively
complete languages: a categorical language of beings for processing user inputs
and an existential language of Being for internal analysis. These languages are
bridged via a phenomenological reduction module, enabling the system to analyze
user queries (including questions, answers, and dialogues among IT
specialists), identify recursive and self-referential structures, and provide
actionable insights in categorical terms. Unlike contemporary systems limited
to categorical analysis, this approach leverages Heidegger's phenomenological
existential analysis to uncover deeper ontological patterns in query
processing, aiding in resolving logical traps in complex interactions, such as
metaphor usage in IT contexts. The path to full realization involves
formalizing the language of Being by a research team based on Heidegger's
Fundamental Ontology; given the existing completeness of the language of
beings, this reduces the system's computability to completeness, paving the way
for a universal query analysis tool. The paper presents the system's
architecture, operational principles, technical implementation, use
cases--including a case based on real IT specialist dialogues--comparative
evaluation with existing tools, and its advantages and limitations.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.12977v1},
File          = {2504.12977v1.pdf}
}
@article{2504.12915v2,
Author        = {Ebrahim Norouzi and Sven Hertling and Harald Sack},
Title         = {ConExion: Concept Extraction with Large Language Models},
Eprint        = {2504.12915v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this paper, an approach for concept extraction from documents using
pre-trained large language models (LLMs) is presented. Compared with
conventional methods that extract keyphrases summarizing the important
information discussed in a document, our approach tackles a more challenging
task of extracting all present concepts related to the specific domain, not
just the important ones. Through comprehensive evaluations of two widely used
benchmark datasets, we demonstrate that our method improves the F1 score
compared to state-of-the-art techniques. Additionally, we explore the potential
of using prompts within these models for unsupervised concept extraction. The
extracted concepts are intended to support domain coverage evaluation of
ontologies and facilitate ontology learning, highlighting the effectiveness of
LLMs in concept extraction tasks. Our source code and datasets are publicly
available at https://github.com/ISE-FIZKarlsruhe/concept_extraction.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.12915v2},
File          = {2504.12915v2.pdf}
}
@article{2504.08445v1,
Author        = {Catarina Canastra and CÃ¡tia Pesquita},
Title         = {A Systematic Evaluation of Knowledge Graph Embeddings for Gene-Disease
  Association Prediction},
Eprint        = {2504.08445v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Discovery gene-disease links is important in biology and medicine areas,
enabling disease identification and drug repurposing. Machine learning
approaches accelerate this process by leveraging biological knowledge
represented in ontologies and the structure of knowledge graphs. Still, many
existing works overlook ontologies explicitly representing diseases, missing
causal and semantic relationships between them. The gene-disease association
problem naturally frames itself as a link prediction task, where embedding
algorithms directly predict associations by exploring the structure and
properties of the knowledge graph. Some works frame it as a node-pair
classification task, combining embedding algorithms with traditional machine
learning algorithms. This strategy aligns with the logic of a machine learning
pipeline. However, the use of negative examples and the lack of validated
gene-disease associations to train embedding models may constrain its
effectiveness. This work introduces a novel framework for comparing the
performance of link prediction versus node-pair classification tasks, analyses
the performance of state of the art gene-disease association approaches, and
compares the different order-based formalizations of gene-disease association
prediction. It also evaluates the impact of the semantic richness through a
disease-specific ontology and additional links between ontologies. The
framework involves five steps: data splitting, knowledge graph integration,
embedding, modeling and prediction, and method evaluation. Results show that
enriching the semantic representation of diseases slightly improves
performance, while additional links generate a greater impact. Link prediction
methods better explore the semantic richness encoded in knowledge graphs.
Although node-pair classification methods identify all true positives, link
prediction methods outperform overall.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.08445v1},
File          = {2504.08445v1.pdf}
}
@article{2504.08373v1,
Author        = {Benedikt Kantz and Kevin Innerebner and Peter Waldert and Stefan Lengauer and Elisabeth Lex and Tobias Schreck},
Title         = {OnSET: Ontology and Semantic Exploration Toolkit},
Eprint        = {2504.08373v1},
DOI           = {10.1145/3726302.3730148},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Retrieval over knowledge graphs is usually performed using dedicated, complex
query languages like SPARQL. We propose a novel system, Ontology and Semantic
Exploration Toolkit (OnSET) that allows non-expert users to easily build
queries with visual user guidance provided by topic modelling and semantic
search throughout the application. OnSET allows users without any prior
information about the ontology or networked knowledge to start exploring topics
of interest over knowledge graphs, including the retrieval and detailed
exploration of prototypical sub-graphs and their instances. Existing systems
either focus on direct graph explorations or do not foster further exploration
of the result set. We, however, provide a node-based editor that can extend on
these missing properties of existing systems to support the search over big
ontologies with sub-graph instances. Furthermore, OnSET combines efficient and
open platforms to deploy the system on commodity hardware.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.08373v1},
File          = {2504.08373v1.pdf}
}
@article{2504.08329v3,
Author        = {Junmo Kim and Namkyeong Lee and Jiwon Kim and Kwangsoo Kim},
Title         = {MedRep: Medical Concept Representation for General Electronic Health
  Record Foundation Models},
Eprint        = {2504.08329v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Electronic health record (EHR) foundation models have been an area ripe for
exploration with their improved performance in various medical tasks. Despite
the rapid advances, there exists a fundamental limitation: Processing unseen
medical codes out of vocabulary. This problem limits the generalizability of
EHR foundation models and the integration of models trained with different
vocabularies. To alleviate this problem, we propose a set of novel medical
concept representations (MedRep) for EHR foundation models based on the
observational medical outcome partnership (OMOP) common data model (CDM). For
concept representation learning, we enrich the information of each concept with
a minimal definition through large language model (LLM) prompts and complement
the text-based representations through the graph ontology of OMOP vocabulary.
Our approach outperforms the vanilla EHR foundation model and the model with a
previously introduced medical code tokenizer in diverse prediction tasks. We
also demonstrate the generalizability of MedRep through external validation.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.08329v3},
File          = {2504.08329v3.pdf}
}
@article{2504.07640v1,
Author        = {Ruslan Idelfonso Magana Vsevolodovna and Marco Monti},
Title         = {Enhancing Large Language Models through Neuro-Symbolic Integration and
  Ontological Reasoning},
Eprint        = {2504.07640v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large Language Models (LLMs) demonstrate impressive capabilities in natural
language processing but suffer from inaccuracies and logical inconsistencies
known as hallucinations. This compromises their reliability, especially in
domains requiring factual accuracy. We propose a neuro-symbolic approach
integrating symbolic ontological reasoning and machine learning methods to
enhance the consistency and reliability of LLM outputs. Our workflow utilizes
OWL ontologies, a symbolic reasoner (e.g., HermiT) for consistency checking,
and a lightweight machine learning model (logistic regression) for mapping
natural language statements into logical forms compatible with the ontology.
When inconsistencies between LLM outputs and the ontology are detected, the
system generates explanatory feedback to guide the LLM towards a corrected,
logically coherent response in an iterative refinement loop. We present a
working Python prototype demonstrating this pipeline. Experimental results in a
defined domain suggest significant improvements in semantic coherence and
factual accuracy of LLM outputs, showcasing the potential of combining LLM
fluency with the rigor of formal semantics.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.07640v1},
File          = {2504.07640v1.pdf}
}
@article{2504.08006v1,
Author        = {Krzysztof Pancerz},
Title         = {A Python toolkit for dealing with Petri nets over ontological graphs},
Eprint        = {2504.08006v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We present theoretical rudiments of Petri nets over ontological graphs as
well as the designed and implemented Python toolkit for dealing with such nets.
In Petri nets over ontological graphs, the domain knowledge is enclosed in a
form of ontologies. In this way, some valuable knowledge (especially in terms
of semantic relations) can be added to model reasoning and control processes by
means of Petri nets. In the implemented approach, ontological graphs are
obtained from ontologies built in accordance with the OWL 2 Web Ontology
Language. The implemented tool enables the users to define the structure and
dynamics of Petri nets over ontological graphs.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.08006v1},
File          = {2504.08006v1.pdf}
}
@article{2504.06883v1,
Author        = {Hans-Thomas Elze},
Title         = {The Dirac Equation, Mass and Arithmetic by Permutations of Automaton
  States},
Eprint        = {2504.06883v1},
DOI           = {10.3390/e27040395},
ArchivePrefix = {arXiv},
PrimaryClass  = {quant-ph},
Abstract      = {The cornerstones of the Cellular Automaton Interpretation of Quantum
Mechanics are its underlying ontological states that evolve by permutations.
They do not create would-be quantum mechanical superposition states. We review
this with a classical automaton consisting of an Ising spin chain which is then
related to the Weyl equation in the continuum limit. Based on this and
generalizing, we construct a new ``Necklace of Necklaces'' automaton with a
torus-like topology that lends itself to represent the Dirac equation in 1 + 1
dimensions. Special attention has to be paid to its mass term, which
necessitates this enlarged structure and a particular scattering operator
contributing to the step-wise updates of the automaton. As discussed earlier,
such deterministic models of discrete spins or bits unavoidably become quantum
mechanical, when only slightly deformed.},
Year          = {2025},
Month         = {Apr},
Note          = {Entropy 2025, 27, 395},
Url           = {http://arxiv.org/abs/2504.06883v1},
File          = {2504.06883v1.pdf}
}
@article{2504.07994v2,
Author        = {Samah Alkhuzaey and Floriana Grasso and Terry R. Payne and Valentina Tamma},
Title         = {Evaluating the Fitness of Ontologies for the Task of Question Generation},
Eprint        = {2504.07994v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Ontology-based question generation is an important application of
semantic-aware systems that enables the creation of large question banks for
diverse learning environments. The effectiveness of these systems, both in
terms of the calibre and cognitive difficulty of the resulting questions,
depends heavily on the quality and modelling approach of the underlying
ontologies, making it crucial to assess their fitness for this task. To date,
there has been no comprehensive investigation into the specific ontology
aspects or characteristics that affect the question generation process.
Therefore, this paper proposes a set of requirements and task-specific metrics
for evaluating the fitness of ontologies for question generation tasks in
pedagogical settings. Using the ROMEO methodology (a structured framework used
for identifying task-specific metrics), a set of evaluation metrics have been
derived from an expert assessment of questions generated by a question
generation model. To validate the proposed metrics, we apply them to a set of
ontologies previously used in question generation to illustrate how the metric
scores align with and complement findings reported in earlier studies. The
analysis confirms that ontology characteristics significantly impact the
effectiveness of question generation, with different ontologies exhibiting
varying performance levels. This highlights the importance of assessing
ontology quality with respect to Automatic Question Generation (AQG) tasks.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.07994v2},
File          = {2504.07994v2.pdf}
}
@article{2504.03964v1,
Author        = {Simon A. Lee and Anthony Wu and Jeffrey N. Chiang},
Title         = {Clinical ModernBERT: An efficient and long context encoder for
  biomedical text},
Eprint        = {2504.03964v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We introduce Clinical ModernBERT, a transformer based encoder pretrained on
large scale biomedical literature, clinical notes, and medical ontologies,
incorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with
their textual descriptions. Building on ModernBERT the current state of the art
natural language text encoder featuring architectural upgrades such as rotary
positional embeddings (RoPE), Flash Attention, and extended context length up
to 8,192 tokens our model adapts these innovations specifically for biomedical
and clinical domains. Clinical ModernBERT excels at producing semantically rich
representations tailored for long context tasks. We validate this both by
analyzing its pretrained weights and through empirical evaluation on a
comprehensive suite of clinical NLP benchmarks.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.03964v1},
File          = {2504.03964v1.pdf}
}
@article{2504.03595v2,
Author        = {Fabio Lilliu and Amir Laadhar and Christian Thomsen and Diego Reforgiato Recupero and Torben Bach Pedersen},
Title         = {Extending the SAREF4ENER Ontology with Flexibility Based on FlexOffers},
Eprint        = {2504.03595v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {A key element to support the increased amounts of renewable energy in the
energy system is flexibility, i.e., the possibility of changing energy loads in
time and amount. Many flexibility models have been designed; however, exact
models fail to scale for long time horizons or many devices. Because of this,
the FlexOffer (FOs) model has been designed, to provide device-independent
approximations of flexibility with good accuracy, and much better scaling for
long time horizons and many devices. An important aspect of the real-life
implementation of energy flexibility is enabling flexible data exchange with
many types of smart energy appliances and market systems, e.g., in smart
buildings. For this, ontologies standardizing data formats are required.
However, the current industry standard ontology for integrating smart devices
for energy purposes, SAREF for Energy Flexibility (SAREF4ENER) only has limited
support for flexibility and thus cannot support important use cases. In this
paper we propose an extension of SAREF4ENER that integrates full support for
the complete FlexOffer model, including advanced use cases, while maintaining
backward compatibility. This novel ontology module can accurately describe
flexibility for advanced devices such as electric vehicles, batteries, and heat
pumps. It can also capture the inherent uncertainty associated with many
flexible load types.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.03595v2},
File          = {2504.03595v2.pdf}
}
@article{2504.03029v1,
Author        = {Nava Haghighi and Sunny Yu and James Landay and Daniela Rosner},
Title         = {Ontologies in Design: How Imagining a Tree Reveals Possibilites and
  Assumptions in Large Language Models},
Eprint        = {2504.03029v1},
DOI           = {10.1145/3706598.3713633},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {Amid the recent uptake of Generative AI, sociotechnical scholars and critics
have traced a multitude of resulting harms, with analyses largely focused on
values and axiology (e.g., bias). While value-based analyses are crucial, we
argue that ontologies -- concerning what we allow ourselves to think or talk
about -- is a vital but under-recognized dimension in analyzing these systems.
Proposing a need for a practice-based engagement with ontologies, we offer four
orientations for considering ontologies in design: pluralism, groundedness,
liveliness, and enactment. We share examples of potentialities that are opened
up through these orientations across the entire LLM development pipeline by
conducting two ontological analyses: examining the responses of four LLM-based
chatbots in a prompting exercise, and analyzing the architecture of an
LLM-based agent simulation. We conclude by sharing opportunities and
limitations of working with ontologies in the design and development of
sociotechnical systems.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.03029v1},
File          = {2504.03029v1.pdf}
}
@article{2504.02889v1,
Author        = {Takanori Ugai},
Title         = {Embedding Method for Knowledge Graph with Densely Defined Ontology},
Eprint        = {2504.02889v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SI},
Abstract      = {Knowledge graph embedding (KGE) is a technique that enhances knowledge graphs
by addressing incompleteness and improving knowledge retrieval. A limitation of
the existing KGE models is their underutilization of ontologies, specifically
the relationships between properties. This study proposes a KGE model, TransU,
designed for knowledge graphs with well-defined ontologies that incorporate
relationships between properties. The model treats properties as a subset of
entities, enabling a unified representation. We present experimental results
using a standard dataset and a practical dataset.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.02889v1},
File          = {2504.02889v1.pdf}
}
@article{2504.00752v1,
Author        = {Sameer Sadruddin and Jennifer D'Souza and Eleni Poupaki and Alex Watkins and Hamed Babaei Giglou and Anisa Rula and Bora Karasulu and SÃ¶ren Auer and Adrie Mackus and Erwin Kessels},
Title         = {LLMs4SchemaDiscovery: A Human-in-the-Loop Workflow for Scientific Schema
  Mining with Large Language Models},
Eprint        = {2504.00752v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Extracting structured information from unstructured text is crucial for
modeling real-world processes, but traditional schema mining relies on
semi-structured data, limiting scalability. This paper introduces schema-miner,
a novel tool that combines large language models with human feedback to
automate and refine schema extraction. Through an iterative workflow, it
organizes properties from text, incorporates expert input, and integrates
domain-specific ontologies for semantic depth. Applied to materials
science--specifically atomic layer deposition--schema-miner demonstrates that
expert-guided LLMs generate semantically rich schemas suitable for diverse
real-world applications.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.00752v1},
File          = {2504.00752v1.pdf}
}
@article{2504.00389v2,
Author        = {Chengshuai Zhao and Riccardo De Maria and Tharindu Kumarage and Kumar Satvik Chaudhary and Garima Agrawal and Yiwen Li and Jongchan Park and Yuli Deng and Ying-Chih Chen and Huan Liu},
Title         = {CyberBOT: Towards Reliable Cybersecurity Education via Ontology-Grounded
  Retrieval Augmented Generation},
Eprint        = {2504.00389v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Advancements in large language models (LLMs) have enabled the development of
intelligent educational tools that support inquiry-based learning across
technical domains. In cybersecurity education, where accuracy and safety are
paramount, systems must go beyond surface-level relevance to provide
information that is both trustworthy and domain-appropriate. To address this
challenge, we introduce CyberBOT, a question-answering chatbot that leverages a
retrieval-augmented generation (RAG) pipeline to incorporate contextual
information from course-specific materials and validate responses using a
domain-specific cybersecurity ontology. The ontology serves as a structured
reasoning layer that constrains and verifies LLM-generated answers, reducing
the risk of misleading or unsafe guidance. CyberBOT has been deployed in a
large graduate-level course at Arizona State University (ASU), where more than
one hundred students actively engage with the system through a dedicated
web-based platform. Computational evaluations in lab environments highlight the
potential capacity of CyberBOT, and a forthcoming field study will evaluate its
pedagogical impact. By integrating structured domain reasoning with modern
generative capabilities, CyberBOT illustrates a promising direction for
developing reliable and curriculum-aligned AI applications in specialized
educational contexts.},
Year          = {2025},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2504.00389v2},
File          = {2504.00389v2.pdf}
}
@article{2505.13450v1,
Author        = {Stanislav Semenov},
Title         = {Fractal Analysis on the Real Interval: A Constructive Approach via
  Fractal Countability},
Eprint        = {2505.13450v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LO},
Abstract      = {This paper develops a technical and practical reinterpretation of the real
interval [a,b] under the paradigm of fractal countability. Instead of assuming
the continuum as a completed uncountable totality, we model [a,b] as a layered
structure of constructively definable points, indexed by a hierarchy of formal
systems. We reformulate classical notions from real analysis -- continuity,
measure, differentiation, and integration -- in terms of stratified
definability levels S_n, thereby grounding the analytic apparatus in syntactic
accessibility rather than ontological postulation. The result is a framework
for fractal analysis, in which mathematical operations are relativized to
layers of expressibility, enabling new insights into approximation,
computability, and formal verification.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2505.13450v1},
File          = {2505.13450v1.pdf}
}
@article{2503.22939v3,
Author        = {Fadi Alharbi and Nishant Budhiraja and Aleksandar Vakanski and Boyu Zhang and Murtada K. Elbashir and Harshith Guduru and Mohanad Mohammed},
Title         = {Interpretable Graph Kolmogorov-Arnold Networks for Multi-Cancer
  Classification and Biomarker Identification using Multi-Omics Data},
Eprint        = {2503.22939v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {The integration of heterogeneous multi-omics datasets at a systems level
remains a central challenge for developing analytical and computational models
in precision cancer diagnostics. This paper introduces Multi-Omics Graph
Kolmogorov-Arnold Network (MOGKAN), a deep learning framework that utilizes
messenger-RNA, micro-RNA sequences, and DNA methylation samples together with
Protein-Protein Interaction (PPI) networks for cancer classification across 31
different cancer types. The proposed approach combines differential gene
expression with DESeq2, Linear Models for Microarray (LIMMA), and Least
Absolute Shrinkage and Selection Operator (LASSO) regression to reduce
multi-omics data dimensionality while preserving relevant biological features.
The model architecture is based on the Kolmogorov-Arnold theorem principle and
uses trainable univariate functions to enhance interpretability and feature
analysis. MOGKAN achieves classification accuracy of 96.28 percent and exhibits
low experimental variability in comparison to related deep learning-based
models. The biomarkers identified by MOGKAN were validated as cancer-related
markers through Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes
(KEGG) enrichment analysis. By integrating multi-omics data with graph-based
deep learning, our proposed approach demonstrates robust predictive performance
and interpretability with potential to enhance the translation of complex
multi-omics data into clinically actionable cancer diagnostics.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.22939v3},
File          = {2503.22939v3.pdf}
}
@article{2503.22006v1,
Author        = {Marc Brinner and Tarek Al Mustafa and Sina ZarrieÃ},
Title         = {Enhancing Domain-Specific Encoder Models with LLM-Generated Data: How to
  Leverage Ontologies, and How to Do Without Them},
Eprint        = {2503.22006v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We investigate the use of LLM-generated data for continual pretraining of
encoder models in specialized domains with limited training data, using the
scientific domain of invasion biology as a case study. To this end, we leverage
domain-specific ontologies by enriching them with LLM-generated data and
pretraining the encoder model as an ontology-informed embedding model for
concept definitions. To evaluate the effectiveness of this method, we compile a
benchmark specifically designed for assessing model performance in invasion
biology. After demonstrating substantial improvements over standard LLM
pretraining, we investigate the feasibility of applying the proposed approach
to domains without comprehensive ontologies by substituting ontological
concepts with concepts automatically extracted from a small corpus of
scientific abstracts and establishing relationships between concepts through
distributional statistics. Our results demonstrate that this automated approach
achieves comparable performance using only a small set of scientific abstracts,
resulting in a fully automated pipeline for enhancing domain-specific
understanding of small encoder models that is especially suited for application
in low-resource settings and achieves performance comparable to masked language
modeling pretraining on much larger datasets.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.22006v1},
File          = {2503.22006v1.pdf}
}
@article{2503.21902v1,
Author        = {Hamed Babaei Giglou and Jennifer D'Souza and Oliver Karras and SÃ¶ren Auer},
Title         = {OntoAligner: A Comprehensive Modular and Robust Python Toolkit for
  Ontology Alignment},
Eprint        = {2503.21902v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontology Alignment (OA) is fundamental for achieving semantic
interoperability across diverse knowledge systems. We present OntoAligner, a
comprehensive, modular, and robust Python toolkit for ontology alignment,
designed to address current limitations with existing tools faced by
practitioners. Existing tools are limited in scalability, modularity, and ease
of integration with recent AI advances. OntoAligner provides a flexible
architecture integrating existing lightweight OA techniques such as fuzzy
matching but goes beyond by supporting contemporary methods with
retrieval-augmented generation and large language models for OA. The framework
prioritizes extensibility, enabling researchers to integrate custom alignment
algorithms and datasets. This paper details the design principles,
architecture, and implementation of the OntoAligner, demonstrating its utility
through benchmarks on standard OA tasks. Our evaluation highlights
OntoAligner's ability to handle large-scale ontologies efficiently with few
lines of code while delivering high alignment quality. By making OntoAligner
open-source, we aim to provide a resource that fosters innovation and
collaboration within the OA community, empowering researchers and practitioners
with a toolkit for reproducible OA research and real-world applications.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.21902v1},
File          = {2503.21902v1.pdf}
}
@article{2503.21661v2,
Author        = {Paul Fabry and Adrien Barton and Jean-FranÃ§ois Ãthier},
Title         = {Rethinking meaning and ontologies from the perspective of ontological
  units},
Eprint        = {2503.21661v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.FL},
Abstract      = {Ontologies enable knowledge sharing and interdisciplinary collaboration by
providing standardized, structured vocabularies for diverse communities. While
logical axioms are a cornerstone of ontology design, natural language elements
such as annotations are equally critical for conveying intended meaning and
ensuring consistent term usage. This paper explores how meaning is represented
in ontologies and how it can be effectively represented and communicated,
addressing challenges such as indeterminacy of reference and meaning holism. To
this end, instead of following the conventional approach of beginning with
existing ontologies and working toward alignment or modularization, this
article proposes a reversal of perspective: taking the ontological term as the
starting point and introducing a new structure, named 'ontological unit',
characterized by: a term-centered design; enhanced characterization of both
formal and natural language statements; and an operationalizable definition of
communicated meaning based on general assertions. By formalizing the meaning of
ontological units, this work seeks to enhance the semantic robustness of terms,
improving their clarity and accessibility across domains. Furthermore, it may
offer a more effective foundation for ontology generation and significantly
improves support for key maintenance tasks such as reuse and versioning. This
article aims to establish the theoretical groundwork for the proposed approach
and to lay the foundations for future applications in applied ontologies.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.21661v2},
File          = {2503.21661v2.pdf}
}
@article{2503.21453v1,
Author        = {Ritesh Chandra and Sonali Agarwal and Shashi Shekhar Kumar and Navjot Singh},
Title         = {OCEP: An Ontology-Based Complex Event Processing Framework for
  Healthcare Decision Support in Big Data Analytics},
Eprint        = {2503.21453v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DC},
Abstract      = {The exponential expansion of real-time data streams across multiple domains
needs the development of effective event detection, correlation, and
decision-making systems. However, classic Complex Event Processing (CEP)
systems struggle with semantic heterogeneity, data interoperability, and
knowledge driven event reasoning in Big Data environments. To solve these
challenges, this research work presents an Ontology based Complex Event
Processing (OCEP) framework, which utilizes semantic reasoning and Big Data
Analytics to improve event driven decision support. The proposed OCEP
architecture utilizes ontologies to support reasoning to event streams. It
ensures compatibility with different data sources and lets you find the events
based on the context. The Resource Description Framework (RDF) organizes event
data, and SPARQL query enables rapid event reasoning and retrieval. The
approach is implemented within the Hadoop environment, which consists of Hadoop
Distributed File System (HDFS) for scalable storage and Apache Kafka for
real-time CEP based event execution. We perform a real-time healthcare analysis
and case study to validate the model, utilizing IoT sensor data for illness
monitoring and emergency responses. This OCEP framework successfully integrates
several event streams, leading to improved early disease detection and aiding
doctors in decision-making. The result shows that OCEP predicts event detection
with an accuracy of 85%. This research work utilizes an OCEP to solve the
problems with semantic interoperability and correlation of complex events in
Big Data analytics. The proposed architecture presents an intelligent, scalable
and knowledge driven event processing framework for healthcare based decision
support.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.21453v1},
File          = {2503.21453v1.pdf}
}
@article{2503.20737v2,
Author        = {Jeffery L Painter and FranÃ§ois Haguinet and Gregory E Powell and Andrew Bate},
Title         = {Ontology-based Semantic Similarity Measures for Clustering Medical
  Concepts in Drug Safety},
Eprint        = {2503.20737v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Semantic similarity measures (SSMs) are widely used in biomedical research
but remain underutilized in pharmacovigilance. This study evaluates six
ontology-based SSMs for clustering MedDRA Preferred Terms (PTs) in drug safety
data. Using the Unified Medical Language System (UMLS), we assess each method's
ability to group PTs around medically meaningful centroids. A high-throughput
framework was developed with a Java API and Python and R interfaces support
large-scale similarity computations. Results show that while path-based methods
perform moderately with F1 scores of 0.36 for WUPALMER and 0.28 for LCH,
intrinsic information content (IC)-based measures, especially INTRINSIC-LIN and
SOKAL, consistently yield better clustering accuracy (F1 score of 0.403).
Validated against expert review and standard MedDRA queries (SMQs), our
findings highlight the promise of IC-based SSMs in enhancing pharmacovigilance
workflows by improving early signal detection and reducing manual review.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.20737v2},
File          = {2503.20737v2.pdf}
}
@article{2503.20634v1,
Author        = {Valentina Anita Carriero and Mario Scrocca and Ilaria Baroni and Antonia Azzini and Irene Celino},
Title         = {Procedural Knowledge Ontology (PKO)},
Eprint        = {2503.20634v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Processes, workflows and guidelines are core to ensure the correct
functioning of industrial companies: for the successful operations of factory
lines, machinery or services, often industry operators rely on their past
experience and know-how. The effect is that this Procedural Knowledge (PK)
remains tacit and, as such, difficult to exploit efficiently and effectively.
This paper presents PKO, the Procedural Knowledge Ontology, which enables the
explicit modeling of procedures and their executions, by reusing and extending
existing ontologies. PKO is built on requirements collected from three
heterogeneous industrial use cases and can be exploited by any AI and
data-driven tools that rely on a shared and interoperable representation to
support the governance of PK throughout its life cycle. We describe its
structure and design methodology, and outline its relevance, quality, and
impact by discussing applications leveraging PKO for PK elicitation and
exploitation.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.20634v1},
File          = {2503.20634v1.pdf}
}
@article{2503.21826v1,
Author        = {Ludovic Tuncay and Etienne LabbÃ© and Thomas Pellegrini},
Title         = {Hierarchical Label Propagation: A Model-Size-Dependent Performance
  Booster for AudioSet Tagging},
Eprint        = {2503.21826v1},
DOI           = {10.1109/ICASSP49660.2025.10888798},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SD},
Abstract      = {AudioSet is one of the most used and largest datasets in audio tagging,
containing about 2 million audio samples that are manually labeled with 527
event categories organized into an ontology. However, the annotations contain
inconsistencies, particularly where categories that should be labeled as
positive according to the ontology are frequently mislabeled as negative. To
address this issue, we apply Hierarchical Label Propagation (HLP), which
propagates labels up the ontology hierarchy, resulting in a mean increase in
positive labels per audio clip from 1.98 to 2.39 and affecting 109 out of the
527 classes. Our results demonstrate that HLP provides performance benefits
across various model architectures, including convolutional neural networks
(PANN's CNN6 and ConvNeXT) and transformers (PaSST), with smaller models
showing more improvements. Finally, on FSD50K, another widely used dataset,
models trained on AudioSet with HLP consistently outperformed those trained
without HLP. Our source code will be made available on GitHub.},
Year          = {2025},
Month         = {Mar},
Note          = {ICASSP 2025 - 2025 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP), Apr 2025, Hyderabad, India. pp.1-5},
Url           = {http://arxiv.org/abs/2503.21826v1},
File          = {2503.21826v1.pdf}
}
@article{2503.21813v3,
Author        = {Zhangcheng Qiang and Kerry Taylor and Weiqing Wang and Jing Jiang},
Title         = {OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language
  Model Hallucinations in Ontology Matching},
Eprint        = {2503.21813v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Hallucinations are often inevitable in downstream tasks using large language
models (LLMs). To tackle the substantial challenge of addressing hallucinations
for LLM-based ontology matching (OM) systems, we introduce a new benchmark
dataset OAEI-LLM-T. The dataset evolves from seven TBox datasets in the
Ontology Alignment Evaluation Initiative (OAEI), capturing hallucinations of
ten different LLMs performing OM tasks. These OM-specific hallucinations are
organised into two primary categories and six sub-categories. We showcase the
usefulness of the dataset in constructing an LLM leaderboard for OM tasks and
for fine-tuning LLMs used in OM tasks.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.21813v3},
File          = {2503.21813v3.pdf}
}
@article{2503.21810v1,
Author        = {Zhenyu Wu and Jiaoyan Chen and Norman W. Paton},
Title         = {Taxonomy Inference for Tabular Data Using Large Language Models},
Eprint        = {2503.21810v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Taxonomy inference for tabular data is a critical task of schema inference,
aiming at discovering entity types (i.e., concepts) of the tables and building
their hierarchy. It can play an important role in data management, data
exploration, ontology learning, and many data-centric applications. Existing
schema inference systems focus more on XML, JSON or RDF data, and often rely on
lexical formats and structures of the data for calculating similarities, with
limited exploitation of the semantics of the text across a table. Motivated by
recent works on taxonomy completion and construction using Large Language
Models (LLMs), this paper presents two LLM-based methods for taxonomy inference
for tables: (i) EmTT which embeds columns by fine-tuning with contrastive
learning encoder-alone LLMs like BERT and utilises clustering for hierarchy
construction, and (ii) GeTT which generates table entity types and their
hierarchy by iterative prompting using a decoder-alone LLM like GPT-4.
Extensive evaluation on three real-world datasets with six metrics covering
different aspects of the output taxonomies has demonstrated that EmTT and GeTT
can both produce taxonomies with strong consistency relative to the Ground
Truth.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.21810v1},
File          = {2503.21810v1.pdf}
}
@article{2503.18502v1,
Author        = {AndrÃ©s GarcÃ­a-Silva and JosÃ© Manuel GÃ³mez-PÃ©rez},
Title         = {Autoregressive Language Models for Knowledge Base Population: A case
  study in the space mission domain},
Eprint        = {2503.18502v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge base population KBP plays a crucial role in populating and
maintaining knowledge bases up-to-date in organizations by leveraging domain
corpora. Motivated by the increasingly large context windows supported by large
language models, we propose to fine-tune an autoregressive language model for
end-toend KPB. Our case study involves the population of a space mission
knowledge graph. To fine-tune the model we generate a dataset for end-to-end
KBP tapping into existing domain resources. Our case study shows that
fine-tuned language models of limited size can achieve competitive and even
higher accuracy than larger models in the KBP task. Smaller models specialized
for KBP offer affordable deployment and lower-cost inference. Moreover, KBP
specialist models do not require the ontology to be included in the prompt,
allowing for more space in the context for additional input text or output
serialization.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.18502v1},
File          = {2503.18502v1.pdf}
}
@article{2503.16926v1,
Author        = {GÃ¡bor Hofer-SzabÃ³},
Title         = {Operational equivalence and causal structure},
Eprint        = {2503.16926v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {quant-ph},
Abstract      = {In operational quantum mechanics two measurements are called operationally
equivalent if they yield the same distribution of outcomes in every quantum
state and hence are represented by the same operator. In this paper, I will
show that the ontological models for quantum mechanics and, more generally, for
any operational theory sensitively depend on which measurement we choose from
the class of operationally equivalent measurements, or more precisely, which of
the chosen measurements can be performed simultaneously. To this goal, I will
take first three examples -- a classical theory, the EPR-Bell scenario and the
Popescu-Rochlich box; then realize each example by two operationally equivalent
but different operational theories -- one with a trivial and another with a
non-trivial compatibility structure; and finally show that the ontological
models for the different theories will be different with respect to their
causal structure, contextuality, and fine-tuning.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.16926v1},
File          = {2503.16926v1.pdf}
}
@article{2503.14957v3,
Author        = {Thanh-Son Nguyen and Hong Yang and Tzeh Yuan Neoh and Hao Zhang and Ee Yeo Keat and Basura Fernando},
Title         = {Neuro Symbolic Knowledge Reasoning for Procedural Video Question
  Answering},
Eprint        = {2503.14957v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {We introduce PKR-QA (Procedural Knowledge Reasoning Question Answering), a
new benchmark for question answering over procedural tasks that require
structured reasoning. PKR-QA is constructed semi-automatically using a
procedural knowledge graph (PKG), which encodes task-specific knowledge across
diverse domains. The PKG is built by curating and linking information from the
COIN instructional video dataset and the ontology, enriched with commonsense
knowledge from ConceptNet and structured outputs from Large Language Models
(LLMs), followed by manual verification. To generate question-answer pairs, we
design graph traversal templates where each template is applied systematically
over PKG. To enable interpretable reasoning, we propose a neurosymbolic
approach called Knowledge Module Learning (KML), which learns procedural
relations via neural modules and composes them for structured reasoning with
LLMs. Experiments demonstrate that this paradigm improves reasoning performance
on PKR-QA and enables step-by-step reasoning traces that facilitate
interpretability. Code and dataset will be released soon
https://github.com/LUNAProject22/KML.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.14957v3},
File          = {2503.14957v3.pdf}
}
@article{2503.14911v2,
Author        = {Siyuan Yan and Ming Hu and Yiwen Jiang and Xieji Li and Hao Fei and Philipp Tschandl and Harald Kittler and Zongyuan Ge},
Title         = {Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical
  Ontology Knowledge for Dermatology},
Eprint        = {2503.14911v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {The emergence of vision-language models has transformed medical AI, enabling
unprecedented advances in diagnostic capability and clinical applications.
However, progress in dermatology has lagged behind other medical domains due to
the lack of standard image-text pairs. Existing dermatological datasets are
limited in both scale and depth, offering only single-label annotations across
a narrow range of diseases instead of rich textual descriptions, and lacking
the crucial clinical context needed for real-world applications. To address
these limitations, we present Derm1M, the first large-scale vision-language
dataset for dermatology, comprising 1,029,761 image-text pairs. Built from
diverse educational resources and structured around a standard ontology
collaboratively developed by experts, Derm1M provides comprehensive coverage
for over 390 skin conditions across four hierarchical levels and 130 clinical
concepts with rich contextual information such as medical history, symptoms,
and skin tone. To demonstrate Derm1M potential in advancing both AI research
and clinical application, we pretrained a series of CLIP-like models,
collectively called DermLIP, on this dataset. The DermLIP family significantly
outperforms state-of-the-art foundation models on eight diverse datasets across
multiple tasks, including zero-shot skin disease classification, clinical and
artifacts concept identification, few-shot/full-shot learning, and cross-modal
retrieval. Our dataset and code will be publicly available at
https://github.com/SiyuanYan1/Derm1M upon acceptance.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.14911v2},
File          = {2503.14911v2.pdf}
}
@article{2503.15558v3,
Author        = { NVIDIA and  : and Alisson Azzolini and Junjie Bai and Hannah Brandon and Jiaxin Cao and Prithvijit Chattopadhyay and Huayu Chen and Jinju Chu and Yin Cui and Jenna Diamond and Yifan Ding and Liang Feng and Francesco Ferroni and Rama Govindaraju and Jinwei Gu and Siddharth Gururani and Imad El Hanafi and Zekun Hao and Jacob Huffman and Jingyi Jin and Brendan Johnson and Rizwan Khan and George Kurian and Elena Lantz and Nayeon Lee and Zhaoshuo Li and Xuan Li and Maosheng Liao and Tsung-Yi Lin and Yen-Chen Lin and Ming-Yu Liu and Xiangyu Lu and Alice Luo and Andrew Mathau and Yun Ni and Lindsey Pavao and Wei Ping and David W. Romero and Misha Smelyanskiy and Shuran Song and Lyne Tchapmi and Andrew Z. Wang and Boxin Wang and Haoxiang Wang and Fangyin Wei and Jiashu Xu and Yao Xu and Dinghao Yang and Xiaodong Yang and Zhuolin Yang and Jingxu Zhang and Xiaohui Zeng and Zhe Zhang},
Title         = {Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning},
Eprint        = {2503.15558v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Physical AI systems need to perceive, understand, and perform complex actions
in the physical world. In this paper, we present the Cosmos-Reason1 models that
can understand the physical world and generate appropriate embodied decisions
(e.g., next step action) in natural language through long chain-of-thought
reasoning processes. We begin by defining key capabilities for Physical AI
reasoning, with a focus on physical common sense and embodied reasoning. To
represent physical common sense, we use a hierarchical ontology that captures
fundamental knowledge about space, time, and physics. For embodied reasoning,
we rely on a two-dimensional ontology that generalizes across different
physical embodiments. Building on these capabilities, we develop two multimodal
large language models, Cosmos-Reason1-7B and Cosmos-Reason1-56B. We curate data
and train our models in two stages: Physical AI supervised fine-tuning (SFT)
and Physical AI reinforcement learning (RL). To evaluate our models, we build
comprehensive benchmarks for physical common sense and embodied reasoning
according to our ontologies. Evaluation results show that Physical AI SFT and
RL bring significant improvements. To facilitate the development of Physical
AI, we make our code and pre-trained models available under the NVIDIA Open
Model License at https://github.com/nvidia-cosmos/cosmos-reason1.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.15558v3},
File          = {2503.15558v3.pdf}
}
@article{2503.12286v1,
Author        = {Da Wu and Zhanliang Wang and Quan Nguyen and Kai Wang},
Title         = {Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances
  Rare Disease Diagnosis from Clinical Notes},
Eprint        = {2503.12286v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Background: Several studies show that large language models (LLMs) struggle
with phenotype-driven gene prioritization for rare diseases. These studies
typically use Human Phenotype Ontology (HPO) terms to prompt foundation models
like GPT and LLaMA to predict candidate genes. However, in real-world settings,
foundation models are not optimized for domain-specific tasks like clinical
diagnosis, yet inputs are unstructured clinical notes rather than standardized
terms. How LLMs can be instructed to predict candidate genes or disease
diagnosis from unstructured clinical notes remains a major challenge. Methods:
We introduce RAG-driven CoT and CoT-driven RAG, two methods that combine
Chain-of-Thought (CoT) and Retrieval Augmented Generation (RAG) to analyze
clinical notes. A five-question CoT protocol mimics expert reasoning, while RAG
retrieves data from sources like HPO and OMIM (Online Mendelian Inheritance in
Man). We evaluated these approaches on rare disease datasets, including 5,980
Phenopacket-derived notes, 255 literature-based narratives, and 220 in-house
clinical notes from Childrens Hospital of Philadelphia. Results: We found that
recent foundations models, including Llama 3.3-70B-Instruct and
DeepSeek-R1-Distill-Llama-70B, outperformed earlier versions such as Llama 2
and GPT-3.5. We also showed that RAG-driven CoT and CoT-driven RAG both
outperform foundation models in candidate gene prioritization from clinical
notes; in particular, both methods with DeepSeek backbone resulted in a top-10
gene accuracy of over 40% on Phenopacket-derived clinical notes. RAG-driven CoT
works better for high-quality notes, where early retrieval can anchor the
subsequent reasoning steps in domain-specific evidence, while CoT-driven RAG
has advantage when processing lengthy and noisy notes.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.12286v1},
File          = {2503.12286v1.pdf}
}
@article{2503.11724v1,
Author        = {Raoni Arroyo and Lauro de Matos Nunes Filho and Frederik Moreira dos Santos},
Title         = {QuestÃµes Ã  "InterpretaÃ§Ã£o da ConsciÃªncia Processual" da
  MecÃ¢nica QuÃ¢ntica},
Eprint        = {2503.11724v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {quant-ph},
Abstract      = {The processual consciousness interpretation (PCI), as developed in Arroyo
(2024, chap. 5) and Arroyo, Nunes Filho, and Moreira dos Santos (2024),
proposes a process ontology as a solution to the measurement problem. This
article presents the standard interpretation taken to its ultimate ontological
consequences; introduces the PCI; and offers reflections based on the questions
raised about the PCI during the XX ANPOF Meeting, in the session of the WG on
Philosophy of the Physical Sciences.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.11724v1},
File          = {2503.11724v1.pdf}
}
@article{2503.11723v1,
Author        = {Hyunmin Cheong and Adrian Butscher},
Title         = {Physics-based simulation ontology: an ontology to support modelling and
  reuse of data for physics-based simulation},
Eprint        = {2503.11723v1},
DOI           = {10.1080/09544828.2019.1644301},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The current work presents an ontology developed for physics-based simulation
in engineering design, called Physics-based Simulation Ontology (PSO). The
purpose of the ontology is to assist in modelling the physical phenomenon of
interest in a veridical manner, while capturing the necessary and reusable
information for physics-based simulation solvers. The development involved
extending an existing upper ontology, Basic Formal Ontology (BFO), to define
lower-level terms of PSO. PSO has two parts: PSO-Physics, which consists of
terms and relations used to model physical phenomena based on the perspective
of classical mechanics involving partial differential equations, and PSO-Sim,
which consists of terms used to represent the information artefacts that are
about the physical phenomena modelled with PSO-Physics. The former terms are
used to model the physical phenomenon of interest independent of
solver-specific interpretations, which can be reused across different solvers,
while the latter terms are used to instantiate solver-specific input data. A
case study involving two simulation solvers was conducted to demonstrate this
capability of PSO. Discussion around the benefits and limitations of using BFO
for the current work is also provided, which should be valuable for any future
work that extends an existing upper ontology to develop ontologies for
engineering applications.},
Year          = {2025},
Month         = {Mar},
Note          = {Journal of Engineering Design 2019},
Url           = {http://arxiv.org/abs/2503.11723v1},
File          = {2503.11723v1.pdf}
}
@article{2503.10094v1,
Author        = {Phoebe Koundouri and Conrad Landis and Georgios Feretzakis},
Title         = {Semantic Synergy: Unlocking Policy Insights and Learning Pathways
  Through Advanced Skill Mapping},
Eprint        = {2503.10094v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This research introduces a comprehensive system based on state-of-the-art
natural language processing, semantic embedding, and efficient search
techniques for retrieving similarities and thus generating actionable insights
from raw textual information. The system automatically extracts and aggregates
normalized competencies from multiple documents (such as policy files and
curricula vitae) and creates strong relationships between recognized
competencies, occupation profiles, and related learning courses. To validate
its performance, we conducted a multi-tier evaluation that included both
explicit and implicit skill references in synthetic and real-world documents.
The results showed near-human-level accuracy, with F1 scores exceeding 0.95 for
explicit skill detection and above 0.93 for implicit mentions. The system
thereby establishes a sound foundation for supporting in-depth collaboration
across the AE4RIA network. The methodology involves a multi-stage pipeline
based on extensive preprocessing and data cleaning, semantic embedding and
segmentation via SentenceTransformer, and skill extraction using a FAISS-based
search method. The extracted skills are associated with occupation frameworks
(as formulated in the ESCO ontology) and with learning paths offered through
the Sustainable Development Goals Academy. Moreover, interactive visualization
software, implemented with Dash and Plotly, presents graphs and tables for
real-time exploration and informed decision-making by those involved in
policymaking, training and learning supply, career transitions, and
recruitment. Overall, this system, backed by rigorous validation, offers
promising prospects for improved policymaking, human resource development, and
lifelong learning by providing structured and actionable insights from raw,
complex textual information.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.10094v1},
File          = {2503.10094v1.pdf}
}
@article{2503.08323v1,
Author        = {Morteza Rohanian and Tarun Mehra and Nicola Miglino and Farhad Nooralahzadeh and Michael Krauthammer and Andreas Wicki},
Title         = {Towards Scalable and Cross-Lingual Specialist Language Models for
  Oncology},
Eprint        = {2503.08323v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Clinical oncology generates vast, unstructured data that often contain
inconsistencies, missing information, and ambiguities, making it difficult to
extract reliable insights for data-driven decision-making. General-purpose
large language models (LLMs) struggle with these challenges due to their lack
of domain-specific reasoning, including specialized clinical terminology,
context-dependent interpretations, and multi-modal data integration. We address
these issues with an oncology-specialized, efficient, and adaptable NLP
framework that combines instruction tuning, retrieval-augmented generation
(RAG), and graph-based knowledge integration. Our lightweight models prove
effective at oncology-specific tasks, such as named entity recognition (e.g.,
identifying cancer diagnoses), entity linking (e.g., linking entities to
standardized ontologies), TNM staging, document classification (e.g., cancer
subtype classification from pathology reports), and treatment response
prediction. Our framework emphasizes adaptability and resource efficiency. We
include minimal German instructions, collected at the University Hospital
Zurich (USZ), to test whether small amounts of non-English language data can
effectively transfer knowledge across languages. This approach mirrors our
motivation for lightweight models, which balance strong performance with
reduced computational costs, making them suitable for resource-limited
healthcare settings. We validated our models on oncology datasets,
demonstrating strong results in named entity recognition, relation extraction,
and document classification.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.08323v1},
File          = {2503.08323v1.pdf}
}
@article{2503.07584v3,
Author        = {Audun Myers and Max Vargas and Sinan G. Aksoy and Cliff Joslyn and Benjamin Wilson and Lee Burke and Tom Grimes},
Title         = {Talking to GDELT Through Knowledge Graphs},
Eprint        = {2503.07584v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {In this work we study various Retrieval Augmented Regeneration (RAG)
approaches to gain an understanding of the strengths and weaknesses of each
approach in a question-answering analysis. To gain this understanding we use a
case-study subset of the Global Database of Events, Language, and Tone (GDELT)
dataset as well as a corpus of raw text scraped from the online news articles.
To retrieve information from the text corpus we implement a traditional vector
store RAG as well as state-of-the-art large language model (LLM) based
approaches for automatically constructing KGs and retrieving the relevant
subgraphs. In addition to these corpus approaches, we develop a novel
ontology-based framework for constructing knowledge graphs (KGs) from GDELT
directly which leverages the underlying schema of GDELT to create structured
representations of global events. For retrieving relevant information from the
ontology-based KGs we implement both direct graph queries and state-of-the-art
graph retrieval approaches. We compare the performance of each method in a
question-answering task. We find that while our ontology-based KGs are valuable
for question-answering, automated extraction of the relevant subgraphs is
challenging. Conversely, LLM-generated KGs, while capturing event summaries,
often lack consistency and interpretability. Our findings suggest benefits of a
synergistic approach between ontology and LLM-based KG construction, with
proposed avenues toward that end.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.07584v3},
File          = {2503.07584v3.pdf}
}
@article{2503.07304v1,
Author        = {Alejandro David Cayuela Tudela and Javier Pastor-Galindo and Pantaleone Nespoli and JosÃ© A. RuipÃ©rez-Valiente},
Title         = {The Influence Operation Ontology (IOO)},
Eprint        = {2503.07304v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SI},
Abstract      = {Ontologies provide a systematic framework for organizing and leveraging
knowledge, enabling smarter and more effective decision-making. In order to
advance in the capitalization and augmentation of intelligence related to
nowadays cyberoperations, the proposed Influence Operation Ontology (IOO)
establishes the main entities and relationships to model offensive tactics and
techniques by threat actors against the public audience through the information
environment. It aims to stimulate research and development in the field,
leading to innovative applications against influence operations, particularly
in the fields of intelligence, security, and defense.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.07304v1},
File          = {2503.07304v1.pdf}
}
@article{2503.07172v1,
Author        = {L. Thomas van Binsbergen and Marten C. Steketee and Milen G. Kebede and Heleen L. Janssen and Tom M. van Engers},
Title         = {Lawful and Accountable Personal Data Processing with GDPR-based Access
  and Usage Control in Distributed Systems},
Eprint        = {2503.07172v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Compliance with the GDPR privacy regulation places a significant burden on
organisations regarding the handling of personal data. The perceived efforts
and risks of complying with the GDPR further increase when data processing
activities span across organisational boundaries, as is the case in both
small-scale data sharing settings and in large-scale international data spaces.
  This paper addresses these concerns by proposing a case-generic method for
automated normative reasoning that establishes legal arguments for the
lawfulness of data processing activities. The arguments are established on the
basis of case-specific legal qualifications made by privacy experts, bringing
the human in the loop. The obtained expert system promotes transparency and
accountability, remains adaptable to extended or altered interpretations of the
GDPR, and integrates into novel or existing distributed data processing
systems.
  This result is achieved by defining a formal ontology and semantics for
automated normative reasoning based on an analysis of the purpose-limitation
principle of the GDPR. The ontology and semantics are implemented in eFLINT, a
domain-specific language for specifying and reasoning with norms. The XACML
architecture standard, applicable to both access and usage control, is
extended, demonstrating how GDPR-based normative reasoning can integrate into
(existing, distributed) systems for data processing. The resulting system is
designed and critically assessed in reference to requirements extracted from
the GPDR.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.07172v1},
File          = {2503.07172v1.pdf}
}
@article{2503.05884v1,
Author        = {Yujie Zhang and David Schmid and YÃ¬lÃ¨ YÄ«ng and Robert W. Spekkens},
Title         = {Reassessing the boundary between classical and nonclassical for
  individual quantum processes},
Eprint        = {2503.05884v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {quant-ph},
Abstract      = {There is a received wisdom about where to draw the boundary between classical
and nonclassical for various types of quantum processes. For instance, for
multipartite states, it is the divide between separable and entangled, for
channels, the divide between entanglement-breaking and not, for sets of
measurements, the divide between compatible and incompatible, and for
assemblages, the divide between steerable and unsteerable. However, no unified
justification of these placements of the classical-nonclassical divide has been
proposed. That is, although each might be motivated by some notion of what it
means to be classically explainable, it is not the same notion for all of them.
One well-motivated notion of classical explainability is the one based on
generalized noncontextuality: a set of circuits is classically explainable if
the statistics they generate can be realized by a generalized-noncontextual
ontological model. In this work, we show that this notion can be leveraged to
define a classical-nonclassical divide for individual quantum processes of
arbitrary type. A set of measurements is judged to be classical if and only if
a particular set of circuits -- the one obtained by contracting these
measurements with every possible quantum state -- is classically explainable in
the sense just articulated. We begin the task of characterizing where the
classical-nonclassical divide lies according to this proposal for a variety of
different types of processes. In particular, we show that all of the following
are judged to be nonclassical: every entangled state, every set of incompatible
measurements, every non-entanglement-breaking channel, every steerable
assemblage. However, it also judges certain subsets of the complementary
classes to be nonclassical, i.e., certain separable states, compatible sets of
measurements, entanglement-breaking channels, and unsteerable assemblages.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.05884v1},
File          = {2503.05884v1.pdf}
}
@article{2503.05388v1,
Author        = {Anna Sofia Lippolis and Mohammad Javad Saeedizade and Robin KeskisÃ¤rkkÃ¤ and Sara Zuppiroli and Miguel Ceriani and Aldo Gangemi and Eva Blomqvist and Andrea Giovanni Nuzzolese},
Title         = {Ontology Generation using Large Language Models},
Eprint        = {2503.05388v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The ontology engineering process is complex, time-consuming, and error-prone,
even for experienced ontology engineers. In this work, we investigate the
potential of Large Language Models (LLMs) to provide effective OWL ontology
drafts directly from ontological requirements described using user stories and
competency questions. Our main contribution is the presentation and evaluation
of two new prompting techniques for automated ontology development: Memoryless
CQbyCQ and Ontogenia. We also emphasize the importance of three structural
criteria for ontology assessment, alongside expert qualitative evaluation,
highlighting the need for a multi-dimensional evaluation in order to capture
the quality and usability of the generated ontologies. Our experiments,
conducted on a benchmark dataset of ten ontologies with 100 distinct CQs and 29
different user stories, compare the performance of three LLMs using the two
prompting techniques. The results demonstrate improvements over the current
state-of-the-art in LLM-supported ontology engineering. More specifically, the
model OpenAI o1-preview with Ontogenia produces ontologies of sufficient
quality to meet the requirements of ontology engineers, significantly
outperforming novice ontology engineers in modelling ability. However, we still
note some common mistakes and variability of result quality, which is important
to take into account when using LLMs for ontology authoring support. We discuss
these limitations and propose directions for future research.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.05388v1},
File          = {2503.05388v1.pdf}
}
@article{2503.04305v2,
Author        = {Dilek KÃ¼Ã§Ã¼k and Fazli Can},
Title         = {Computational Law: Datasets, Benchmarks, and Ontologies},
Eprint        = {2503.04305v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent developments in computer science and artificial intelligence have also
contributed to the legal domain, as revealed by the number and range of related
publications and applications. Machine and deep learning models require
considerable amount of domain-specific data for training and comparison
purposes, in order to attain high-performance in the legal domain.
Additionally, semantic resources such as ontologies are valuable for building
large-scale computational legal systems, in addition to ensuring
interoperability of such systems. Considering these aspects, we present an
up-to-date review of the literature on datasets, benchmarks, and ontologies
proposed for computational law. We believe that this comprehensive and recent
review will help researchers and practitioners when developing and testing
approaches and systems for computational law.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.04305v2},
File          = {2503.04305v2.pdf}
}
@article{2503.03904v1,
Author        = {Nikolaos Nakis and Chrysoula Kosma and Anastasia Brativnyk and Michail Chatzianastasis and Iakovos Evdaimon and Michalis Vazirgiannis},
Title         = {The Signed Two-Space Proximity Model for Learning Representations in
  Protein-Protein Interaction Networks},
Eprint        = {2503.03904v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Accurately predicting complex protein-protein interactions (PPIs) is crucial
for decoding biological processes, from cellular functioning to disease
mechanisms. However, experimental methods for determining PPIs are
computationally expensive. Thus, attention has been recently drawn to machine
learning approaches. Furthermore, insufficient effort has been made toward
analyzing signed PPI networks, which capture both activating (positive) and
inhibitory (negative) interactions. To accurately represent biological
relationships, we present the Signed Two-Space Proximity Model (S2-SPM) for
signed PPI networks, which explicitly incorporates both types of interactions,
reflecting the complex regulatory mechanisms within biological systems. This is
achieved by leveraging two independent latent spaces to differentiate between
positive and negative interactions while representing protein similarity
through proximity in these spaces. Our approach also enables the identification
of archetypes representing extreme protein profiles. S2-SPM's superior
performance in predicting the presence and sign of interactions in SPPI
networks is demonstrated in link prediction tasks against relevant baseline
methods. Additionally, the biological prevalence of the identified archetypes
is confirmed by an enrichment analysis of Gene Ontology (GO) terms, which
reveals that distinct biological tasks are associated with archetypal groups
formed by both interactions. This study is also validated regarding statistical
significance and sensitivity analysis, providing insights into the functional
roles of different interaction types. Finally, the robustness and consistency
of the extracted archetype structures are confirmed using the Bayesian
Normalized Mutual Information (BNMI) metric, proving the model's reliability in
capturing meaningful SPPI patterns.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.03904v1},
File          = {2503.03904v1.pdf}
}
@article{2503.00863v1,
Author        = {Muhammad Talha Sharif and Abdul Rehman},
Title         = {Systematic Literature Review on Clinical Trial Eligibility Matching},
Eprint        = {2503.00863v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Clinical trial eligibility matching is a critical yet often labor-intensive
and error-prone step in medical research, as it ensures that participants meet
precise criteria for safe and reliable study outcomes. Recent advances in
Natural Language Processing (NLP) have shown promise in automating and
improving this process by rapidly analyzing large volumes of unstructured
clinical text and structured electronic health record (EHR) data. In this
paper, we present a systematic overview of current NLP methodologies applied to
clinical trial eligibility screening, focusing on data sources, annotation
practices, machine learning approaches, and real-world implementation
challenges. A comprehensive literature search (spanning Google Scholar,
Mendeley, and PubMed from 2015 to 2024) yielded high-quality studies, each
demonstrating the potential of techniques such as rule-based systems, named
entity recognition, contextual embeddings, and ontology-based normalization to
enhance patient matching accuracy. While results indicate substantial
improvements in screening efficiency and precision, limitations persist
regarding data completeness, annotation consistency, and model scalability
across diverse clinical domains. The review highlights how explainable AI and
standardized ontologies can bolster clinician trust and broaden adoption.
Looking ahead, further research into advanced semantic and temporal
representations, expanded data integration, and rigorous prospective
evaluations is necessary to fully realize the transformative potential of NLP
in clinical trial recruitment.},
Year          = {2025},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2503.00863v1},
File          = {2503.00863v1.pdf}
}
@article{2502.21217v1,
Author        = {Jeff Beck and Maxwell J. D. Ramstead},
Title         = {Dynamic Markov Blanket Detection for Macroscopic Physics Discovery},
Eprint        = {2502.21217v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.NC},
Abstract      = {The free energy principle (FEP), along with the associated constructs of
Markov blankets and ontological potentials, have recently been presented as the
core components of a generalized modeling method capable of mathematically
describing arbitrary objects that persist in random dynamical systems; that is,
a mathematical theory of ``every'' ``thing''. Here, we leverage the FEP to
develop a mathematical physics approach to the identification of objects,
object types, and the macroscopic, object-type-specific rules that govern their
behavior. We take a generative modeling approach and use variational Bayesian
expectation maximization to develop a dynamic Markov blanket detection
algorithm that is capable of identifying and classifying macroscopic objects,
given partial observation of microscopic dynamics. This unsupervised algorithm
uses Bayesian attention to explicitly label observable microscopic elements
according to their current role in a given system, as either the internal or
boundary elements of a given macroscopic object; and it identifies macroscopic
physical laws that govern how the object interacts with its environment.
Because these labels are dynamic or evolve over time, the algorithm is capable
of identifying complex objects that travel through fixed media or exchange
matter with their environment. This approach leads directly to a flexible class
of structured, unsupervised algorithms that sensibly partition complex
many-particle or many-component systems into collections of interacting
macroscopic subsystems, namely, ``objects'' or ``things''. We derive a few
examples of this kind of macroscopic physics discovery algorithm and
demonstrate its utility with simple numerical experiments, in which the
algorithm correctly labels the components of Newton's cradle, a burning fuse,
the Lorenz attractor, and a simulated cell.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.21217v1},
File          = {2502.21217v1.pdf}
}
@article{2502.21214v1,
Author        = {Ariel Caticha},
Title         = {What is ontic and what is epistemic in the Quantum Mechanics of Spin?},
Eprint        = {2502.21214v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {quant-ph},
Abstract      = {Entropic Dynamics (ED) provides a framework that allows the reconstruction of
the quantum formalism by insisting on ontological and epistemic clarity and
adopting entropic methods and information geometry. Our present goal is to
extend the ED framework to account for spin. The result is a realist
{\psi}-epistemic model in which the ontology consists of a particle described
by a definite position plus a discrete variable that describes Pauli's peculiar
two-valuedness. The resulting dynamics of probabilities is, as might be
expected, described by the Pauli equation. What may be unexpected is that the
generators of transformations -- Hamiltonians and angular momenta including
spin, are all granted clear epistemic status. To the old question `what is
spinning?' ED provides a crisp answer: nothing is spinning.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.21214v1},
File          = {2502.21214v1.pdf}
}
@article{2502.19507v1,
Author        = {Nirmal Gelal and Aastha Gautam and Sanaz Saki Norouzi and Nico Giordano and Claudio Dias da Silva Jr and Jean Ribert Francois and Kelsey Andersen Onofre and Katherine Nelson and Stacy Hutchinson and Xiaomao Lin and Stephen Welch and Romulo Lollato and Pascal Hitzler and Hande KÃ¼Ã§Ã¼k McGinty},
Title         = {Building Knowledge Graphs Towards a Global Food Systems Datahub},
Eprint        = {2502.19507v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Sustainable agricultural production aligns with several sustainability goals
established by the United Nations (UN). However, there is a lack of studies
that comprehensively examine sustainable agricultural practices across various
products and production methods. Such research could provide valuable insights
into the diverse factors influencing the sustainability of specific crops and
produce while also identifying practices and conditions that are universally
applicable to all forms of agricultural production. While this research might
help us better understand sustainability, the community would still need a
consistent set of vocabularies. These consistent vocabularies, which represent
the underlying datasets, can then be stored in a global food systems datahub.
The standardized vocabularies might help encode important information for
further statistical analyses and AI/ML approaches in the datasets, resulting in
the research targeting sustainable agricultural production. A structured method
of representing information in sustainability, especially for wheat production,
is currently unavailable. In an attempt to address this gap, we are building a
set of ontologies and Knowledge Graphs (KGs) that encode knowledge associated
with sustainable wheat production using formal logic. The data for this set of
knowledge graphs are collected from public data sources, experimental results
collected at our experiments at Kansas State University, and a Sustainability
Workshop that we organized earlier in the year, which helped us collect input
from different stakeholders throughout the value chain of wheat. The modeling
of the ontology (i.e., the schema) for the Knowledge Graph has been in progress
with the help of our domain experts, following a modular structure using KNARM
methodology. In this paper, we will present our preliminary results and schemas
of our Knowledge Graph and ontologies.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.19507v1},
File          = {2502.19507v1.pdf}
}
@article{2502.19023v1,
Author        = {Anastasios Nentidis and Charilaos Akasiadis and Angelos Charalambidis and Alexander Artikis},
Title         = {Dealing with Inconsistency for Reasoning over Knowledge Graphs: A Survey},
Eprint        = {2502.19023v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In Knowledge Graphs (KGs), where the schema of the data is usually defined by
particular ontologies, reasoning is a necessity to perform a range of tasks,
such as retrieval of information, question answering, and the derivation of new
knowledge. However, information to populate KGs is often extracted (semi-)
automatically from natural language resources, or by integrating datasets that
follow different semantic schemas, resulting in KG inconsistency. This,
however, hinders the process of reasoning. In this survey, we focus on how to
perform reasoning on inconsistent KGs, by analyzing the state of the art
towards three complementary directions: a) the detection of the parts of the KG
that cause the inconsistency, b) the fixing of an inconsistent KG to render it
consistent, and c) the inconsistency-tolerant reasoning. We discuss existing
work from a range of relevant fields focusing on how, and in which cases they
are related to the above directions. We also highlight persisting challenges
and future directions.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.19023v1},
File          = {2502.19023v1.pdf}
}
@article{2502.18992v1,
Author        = {Hui Feng and Yuntzu Yin and Emiliano Reynares and Jay Nanavati},
Title         = {OntologyRAG: Better and Faster Biomedical Code Mapping with
  Retrieval-Augmented Generation (RAG) Leveraging Ontology Knowledge Graphs and
  Large Language Models},
Eprint        = {2502.18992v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Biomedical ontologies, which comprehensively define concepts and relations
for biomedical entities, are crucial for structuring and formalizing
domain-specific information representations. Biomedical code mapping identifies
similarity or equivalence between concepts from different ontologies. Obtaining
high-quality mapping usually relies on automatic generation of unrefined
mapping with ontology domain fine-tuned language models (LMs), followed by
manual selections or corrections by coding experts who have extensive domain
expertise and familiarity with ontology schemas. The LMs usually provide
unrefined code mapping suggestions as a list of candidates without reasoning or
supporting evidence, hence coding experts still need to verify each suggested
candidate against ontology sources to pick the best matches. This is also a
recurring task as ontology sources are updated regularly to incorporate new
research findings. Consequently, the need of regular LM retraining and manual
refinement make code mapping time-consuming and labour intensive. In this work,
we created OntologyRAG, an ontology-enhanced retrieval-augmented generation
(RAG) method that leverages the inductive biases from ontological knowledge
graphs for in-context-learning (ICL) in large language models (LLMs). Our
solution grounds LLMs to knowledge graphs with unrefined mappings between
ontologies and processes questions by generating an interpretable set of
results that include prediction rational with mapping proximity assessment. Our
solution doesn't require re-training LMs, as all ontology updates could be
reflected by updating the knowledge graphs with a standard process. Evaluation
results on a self-curated gold dataset show promises of using our method to
enable coding experts to achieve better and faster code mapping. The code is
available at https://github.com/iqvianlp/ontologyRAG.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.18992v1},
File          = {2502.18992v1.pdf}
}
@article{2502.17793v3,
Author        = {Hyeonjeong Ha and Xiaomeng Jin and Jeonghwan Kim and Jiateng Liu and Zhenhailong Wang and Khanh Duy Nguyen and Ansel Blume and Nanyun Peng and Kai-Wei Chang and Heng Ji},
Title         = {SYNTHIA: Novel Concept Design with Affordance Composition},
Eprint        = {2502.17793v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Text-to-image (T2I) models enable rapid concept design, making them widely
used in AI-driven design. While recent studies focus on generating semantic and
stylistic variations of given design concepts, functional coherence--the
integration of multiple affordances into a single coherent concept--remains
largely overlooked. In this paper, we introduce SYNTHIA, a framework for
generating novel, functionally coherent designs based on desired affordances.
Our approach leverages a hierarchical concept ontology that decomposes concepts
into parts and affordances, serving as a crucial building block for
functionally coherent design. We also develop a curriculum learning scheme
based on our ontology that contrastively fine-tunes T2I models to progressively
learn affordance composition while maintaining visual novelty. To elaborate, we
(i) gradually increase affordance distance, guiding models from basic
concept-affordance association to complex affordance compositions that
integrate parts of distinct affordances into a single, coherent form, and (ii)
enforce visual novelty by employing contrastive objectives to push learned
representations away from existing concepts. Experimental results show that
SYNTHIA outperforms state-of-the-art T2I models, demonstrating absolute gains
of 25.1% and 14.7% for novelty and functional coherence in human evaluation,
respectively.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.17793v3},
File          = {2502.17793v3.pdf}
}
@article{2502.16369v1,
Author        = {Holger Bech Nielsen},
Title         = {Fluctuating Lattice, Several Energy Scales},
Eprint        = {2502.16369v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {hep-ph},
Abstract      = {In part I: We find a series physical scales such as 1) Planck scale, 2)
Minimal approximate grand unification SU(5), 3) the mass scale of the see saw
model right handed or Majorana neutrinoes, some invented scale with many scalar
bosons, etc., and get the logarithms of these energy scales fitted by a
quantity q related to the dimensions of to thescales related dimensionalities
of coefficients in Lagrangian densities,or some generalization of this q to
something similar in the variouscases of the scales. The logarithm of the
energies behave as a straight line versus the dimension related number q. This
is being explained by an ontologically existing lattice, which fluctuates in
lattice canstant a from place to place in space time or more precisely, it
fluctuates quantum mechanically. In Part II: We find a fitting of the three
fine structure constantsin the Standard Model by means of a no-susy and
SU(5)-like - but only accurately SU(5) symmetric in the classical approximation
- by means of three other parameters for each of which, however, we have
speculative predictions: Quantum corrections due to the lattice whichare three
times as large as naive quantum corrections, because the lattice is supposed to
lie in layers, one layer for each fermion-family; criticallity of the unified
coupling, using the satnadrd model group S(U (2) x U (3)); the unification
scale is fitted inot the scale-system of part I},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.16369v1},
File          = {2502.16369v1.pdf}
}
@article{2502.15932v1,
Author        = {Rikhiya Ghosh and Hans-Martin von Stockhausen and Martin Schmitt and George Marica Vasile and Sanjeev Kumar Karn and Oladimeji Farri},
Title         = {CVE-LLM : Ontology-Assisted Automatic Vulnerability Evaluation Using
  Large Language Models},
Eprint        = {2502.15932v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The National Vulnerability Database (NVD) publishes over a thousand new
vulnerabilities monthly, with a projected 25 percent increase in 2024,
highlighting the crucial need for rapid vulnerability identification to
mitigate cybersecurity attacks and save costs and resources. In this work, we
propose using large language models (LLMs) to learn vulnerability evaluation
from historical assessments of medical device vulnerabilities in a single
manufacturer's portfolio. We highlight the effectiveness and challenges of
using LLMs for automatic vulnerability evaluation and introduce a method to
enrich historical data with cybersecurity ontologies, enabling the system to
understand new vulnerabilities without retraining the LLM. Our LLM system
integrates with the in-house application - Cybersecurity Management System
(CSMS) - to help Siemens Healthineers (SHS) product cybersecurity experts
efficiently assess the vulnerabilities in our products. Also, we present
guidelines for efficient integration of LLMs into the cybersecurity tool.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.15932v1},
File          = {2502.15932v1.pdf}
}
@article{2502.15615v1,
Author        = {Alisson Tezzin and BÃ¡rbara Amaral and Jonte R. Hance},
Title         = {Ontological models cannot adequately represent state update for
  sequential measurement of incompatible observables},
Eprint        = {2502.15615v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {quant-ph},
Abstract      = {Ontological models (as used in the generalized contextuality literature) play
a central role in current research on quantum foundations, serving as a
framework for defining classicality, constructing classical analogues of key
quantum phenomena, and even examining the ontology of quantum states. In this
work, we analyse the quantum state update rule in these models and argue that
incompatibility -- or, equivalently, order-dependent theoretical predictions of
sequential measurements -- is sufficient to stop them from being able to
adequately represent a scenario. This is because, as we argue, the quantum
state update rule requires ontological models to update their states according
to conditional probability, which in turn renders predictions of sequential
measurements order-independent. This implies that ontological models, even
contextual ones, must either act differently to what we would expect given the
quantum state update rule, or cannot model quantum behaviour. Building on this,
we argue that classical wave theory is equally incompatible with ontological
models, challenging the notion that generalized contextuality serves as a
signature of classicality.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.15615v1},
File          = {2502.15615v1.pdf}
}
@article{2502.14714v2,
Author        = {Ahmed Abdeen Hamed and Alessandro Crimi and Magdalena M. Misiak and Byung Suk Lee},
Title         = {From Knowledge Generation to Knowledge Verification: Examining the
  BioMedical Generative Capabilities of ChatGPT},
Eprint        = {2502.14714v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The generative capabilities of LLM models offer opportunities for
accelerating tasks but raise concerns about the authenticity of the knowledge
they produce. To address these concerns, we present a computational approach
that evaluates the factual accuracy of biomedical knowledge generated by an
LLM. Our approach consists of two processes: generating disease-centric
associations and verifying these associations using the semantic framework of
biomedical ontologies. Using ChatGPT as the selected LLM, we designed
prompt-engineering processes to establish linkages between diseases and related
drugs, symptoms, and genes, and assessed consistency across multiple ChatGPT
models (e.g., GPT-turbo, GPT-4, etc.). Experimental results demonstrate high
accuracy in identifying disease terms (88%-97%), drug names (90%-91%), and
genetic information (88%-98%). However, symptom term identification was notably
lower (49%-61%), due to the informal and verbose nature of symptom
descriptions, which hindered effective semantic matching with the formal
language of specialized ontologies. Verification of associations reveals
literature coverage rates of 89%-91% for disease-drug and disease-gene pairs,
while symptom-related associations exhibit lower coverage (49%-62%).},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.14714v2},
File          = {2502.14714v2.pdf}
}
@article{2502.13632v1,
Author        = {Or Raphael Bidusa and Shaul Markovitch},
Title         = {Concept Layers: Enhancing Interpretability and Intervenability via LLM
  Conceptualization},
Eprint        = {2502.13632v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {The opaque nature of Large Language Models (LLMs) has led to significant
research efforts aimed at enhancing their interpretability, primarily through
post-hoc methods. More recent in-hoc approaches, such as Concept Bottleneck
Models (CBMs), offer both interpretability and intervenability by incorporating
explicit concept representations. However, these methods suffer from key
limitations, including reliance on labeled concept datasets and significant
architectural modifications that challenges re-integration into existing system
pipelines. In this work, we introduce a new methodology for incorporating
interpretability and intervenability into an existing model by integrating
Concept Layers (CLs) into its architecture. Our approach projects the model's
internal vector representations into a conceptual, explainable vector space
before reconstructing and feeding them back into the model. Furthermore, we
eliminate the need for a human-selected concept set by algorithmically
searching an ontology for a set of concepts that can be either task-specific or
task-agnostic. We evaluate CLs across multiple tasks, demonstrating that they
maintain the original model's performance and agreement while enabling
meaningful interventions. Additionally, we present a proof of concept
showcasing an intervenability interface, allowing users to adjust model
behavior dynamically, such as mitigating biases during inference.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.13632v1},
File          = {2502.13632v1.pdf}
}
@article{2502.13619v1,
Author        = {Guilherme Sousa and Rinaldo Lima and Cassia Trojahn},
Title         = {Complex Ontology Matching with Large Language Model Embeddings},
Eprint        = {2502.13619v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Ontology, and more broadly, Knowledge Graph Matching is a challenging task in
which expressiveness has not been fully addressed. Despite the increasing use
of embeddings and language models for this task, approaches for generating
expressive correspondences still do not take full advantage of these models, in
particular, large language models (LLMs). This paper proposes to integrate LLMs
into an approach for generating expressive correspondences based on alignment
need and ABox-based relation discovery. The generation of correspondences is
performed by matching similar surroundings of instance sub-graphs. The
integration of LLMs results in different architectural modifications, including
label similarity, sub-graph matching, and entity matching. The performance word
embeddings, sentence embeddings, and LLM-based embeddings, was compared. The
results demonstrate that integrating LLMs surpasses all other models, enhancing
the baseline version of the approach with a 45\% increase in F-measure.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.13619v1},
File          = {2502.13619v1.pdf}
}
@article{2503.05733v1,
Author        = {Neda Bagherzadeh and Saeed Setayeshi and Samaneh Yazdani},
Title         = {Design an Ontology for Cognitive Business Strategy Based on Customer
  Satisfaction},
Eprint        = {2503.05733v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CY},
Abstract      = {Ontology is a general term used by researchers who want to share information
in a specific domain. One of the hallmarks of the greatest success of a
powerful manager of an organization is his ability to interpret unplanned and
unrelated events. Tools to solve this problem are vital to business growth.
Modern technology allows customers to be more informed and influential in their
roles as patrons and critics. This can make or break a business. Research shows
that businesses that employ a customer-first strategy and prioritize their
customers can generate more revenue. Even though there are many different
Ontologies offered to businesses, none of it is built from a cognitive
perspective. The objective of this study is to address the concept of strategic
business plans with a cognitive ontology approach as a basis for a new
management tool. This research proposes to design a cognitive ontology model
that links customer measurement with traditional business models, define
relationships between components and verify the accuracy of the added financial
value.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2503.05733v1},
File          = {2503.05733v1.pdf}
}
@article{2502.11842v3,
Author        = {Sina Soltani and Marco Erba and David Schmid and John H. Selby},
Title         = {Noncontextual ontological models of operational probabilistic theories},
Eprint        = {2502.11842v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {quant-ph},
Abstract      = {An experiment or theory is classically explainable if it can be reproduced by
some noncontextual ontological model. In this work, we adapt the notion of
ontological models and generalized noncontextuality so it applies to the
framework of operational probabilistic theories (OPTs). A defining feature of
quotiented OPTs, which sets them apart from the closely related framework of
generalized probabilistic theories (GPTs), is their explicit specification of
the structure of instruments, these being generalizations of $\textit{quantum
instruments}$ (including nondestructive measurements); in particular, one needs
to explicitly declare which collections of transformations constitute a valid
instrument. We are particularly interested in strongly causal OPTs, in which
the choice of a future instrument can be conditioned on a past measurement
outcome. This instrument structure might seem to permit the possibility of a
contextual kind of ontological representation, where the representation of a
given transformation depends on which instrument it is considered a part of.
However, we prove that this is not possible by showing that for strongly causal
quotiented OPTs the structure of instruments does $\textit{not}$ allow for such
a contextual ontological representation. It follows that ontological
representations of strongly causal quotiented OPTs are entirely determined by
their action on individual transformations, with no dependence on the structure
of instruments.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.11842v3},
File          = {2502.11842v3.pdf}
}
@article{2502.11491v2,
Author        = {Runxuan Liu and Bei Luo and Jiaqi Li and Baoxin Wang and Ming Liu and Dayong Wu and Shijin Wang and Bing Qin},
Title         = {Ontology-Guided Reverse Thinking Makes Large Language Models Stronger on
  Knowledge Graph Question Answering},
Eprint        = {2502.11491v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) have shown remarkable capabilities in natural
language processing. However, in knowledge graph question answering tasks
(KGQA), there remains the issue of answering questions that require multi-hop
reasoning. Existing methods rely on entity vector matching, but the purpose of
the question is abstract and difficult to match with specific entities. As a
result, it is difficult to establish reasoning paths to the purpose, which
leads to information loss and redundancy. To address this issue, inspired by
human reverse thinking, we propose Ontology-Guided Reverse Thinking (ORT), a
novel framework that constructs reasoning paths from purposes back to
conditions. ORT operates in three key phases: (1) using LLM to extract purpose
labels and condition labels, (2) constructing label reasoning paths based on
the KG ontology, and (3) using the label reasoning paths to guide knowledge
retrieval. Experiments on the WebQSP and CWQ datasets show that ORT achieves
state-of-the-art performance and significantly enhances the capability of LLMs
for KGQA.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.11491v2},
File          = {2502.11491v2.pdf}
}
@article{2502.10828v1,
Author        = {Amey P. Pasarkar and Adji Bousso Dieng},
Title         = {The Vendiscope: An Algorithmic Microscope For Data Collections},
Eprint        = {2502.10828v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {The evolution of microscopy, beginning with its invention in the late 16th
century, has continuously enhanced our ability to explore and understand the
microscopic world, enabling increasingly detailed observations of structures
and phenomena. In parallel, the rise of data-driven science has underscored the
need for sophisticated methods to explore and understand the composition of
complex data collections. This paper introduces the Vendiscope, the first
algorithmic microscope designed to extend traditional microscopy to
computational analysis. The Vendiscope leverages the Vendi scores -- a family
of differentiable diversity metrics rooted in ecology and quantum mechanics --
and assigns weights to data points based on their contribution to the overall
diversity of the collection. These weights enable high-resolution data analysis
at scale. We demonstrate this across biology, materials science, and machine
learning (ML). We analyzed the $250$ million protein sequences in the protein
universe, discovering that over $200$ million are near-duplicates and that
AlphaFold fails on proteins with Gene Ontology (GO) functions that contribute
most to diversity. Applying the Vendiscope to the Materials Project database
led to similar findings: more than $85\%$ of the crystals with formation energy
data are near-duplicates and ML models perform poorly on materials that enhance
diversity. Additionally, the Vendiscope can be used to study phenomena such as
memorization in generative models. We used the Vendiscope to identify memorized
training samples from $13$ different generative models and found that the
best-performing ones often memorize the training samples that contribute least
to diversity. Our findings demonstrate that the Vendiscope can serve as a
powerful tool for data-driven science.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.10828v1},
File          = {2502.10828v1.pdf}
}
@article{2502.10092v1,
Author        = {JaeHong Kim and Jaewon Shim},
Title         = {A novel approach to data generation in generative model},
Eprint        = {2502.10092v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Variational Autoencoders (VAEs) and other generative models are widely
employed in artificial intelligence to synthesize new data. However, current
approaches rely on Euclidean geometric assumptions and statistical
approximations that fail to capture the structured and emergent nature of data
generation. This paper introduces the Convergent Fusion Paradigm (CFP) theory,
a novel geometric framework that redefines data generation by integrating
dimensional expansion accompanied by qualitative transformation. By modifying
the latent space geometry to interact with emergent high-dimensional
structures, CFP theory addresses key challenges such as identifiability issues
and unintended artifacts like hallucinations in Large Language Models (LLMs).
CFP theory is based on two key conceptual hypotheses that redefine how
generative models structure relationships between data and algorithms. Through
the lens of CFP theory, we critically examine existing metric-learning
approaches. CFP theory advances this perspective by introducing time-reversed
metric embeddings and structural convergence mechanisms, leading to a novel
geometric approach that better accounts for data generation as a structured
epistemic process. Beyond its computational implications, CFP theory provides
philosophical insights into the ontological underpinnings of data generation.
By offering a systematic framework for high-dimensional learning dynamics, CFP
theory contributes to establishing a theoretical foundation for understanding
the data-relationship structures in AI. Finally, future research in CFP theory
will be led to its implications for fully realizing qualitative
transformations, introducing the potential of Hilbert space in generative
modeling.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.10092v1},
File          = {2502.10092v1.pdf}
}
@article{2502.09763v1,
Author        = {Ali Teymourian and Andrew M. Webb and Taha Gharaibeh and Arushi Ghildiyal and Ibrahim Baggili},
Title         = {SoK: Come Together -- Unifying Security, Information Theory, and
  Cognition for a Mixed Reality Deception Attack Ontology & Analysis Framework},
Eprint        = {2502.09763v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {We present a primary attack ontology and analysis framework for deception
attacks in Mixed Reality (MR). This is achieved through multidisciplinary
Systematization of Knowledge (SoK), integrating concepts from MR security,
information theory, and cognition. While MR grows in popularity, it presents
many cybersecurity challenges, particularly concerning deception attacks and
their effects on humans. In this paper, we use the Borden-Kopp model of
deception to develop a comprehensive ontology of MR deception attacks. Further,
we derive two models to assess impact of MR deception attacks on information
communication and decision-making. The first, an information-theoretic model,
mathematically formalizes the effects of attacks on information communication.
The second, a decision-making model, details the effects of attacks on
interlaced cognitive processes. Using our ontology and models, we establish the
MR Deception Analysis Framework (DAF) to assess the effects of MR deception
attacks on information channels, perception, and attention. Our SoK uncovers
five key findings for research and practice and identifies five research gaps
to guide future work.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.09763v1},
File          = {2502.09763v1.pdf}
}
@article{2502.09218v1,
Author        = {Flavio Bertini and Alessandro Dal PalÃ¹ and Federica Zaglio and Francesco Fabiano and Andrea Formisano},
Title         = {Data2Concept2Text: An Explainable Multilingual Framework for Data
  Analysis Narration},
Eprint        = {2502.09218v1},
DOI           = {10.4204/EPTCS.416.13},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LO},
Abstract      = {This paper presents a complete explainable system that interprets a set of
data, abstracts the underlying features and describes them in a natural
language of choice. The system relies on two crucial stages: (i) identifying
emerging properties from data and transforming them into abstract concepts, and
(ii) converting these concepts into natural language. Despite the impressive
natural language generation capabilities demonstrated by Large Language Models,
their statistical nature and the intricacy of their internal mechanism still
force us to employ these techniques as black boxes, forgoing trustworthiness.
Developing an explainable pipeline for data interpretation would allow
facilitating its use in safety-critical environments like processing medical
information and allowing non-experts and visually impaired people to access
narrated information. To this end, we believe that the fields of knowledge
representation and automated reasoning research could present a valid
alternative. Expanding on prior research that tackled the first stage (i), we
focus on the second stage, named Concept2Text. Being explainable, data
translation is easily modeled through logic-based rules, once again emphasizing
the role of declarative programming in achieving AI explainability. This paper
explores a Prolog/CLP-based rewriting system to interpret concepts-articulated
in terms of classes and relations, plus common knowledge-derived from a generic
ontology, generating natural language text. Its main features include
hierarchical tree rewritings, modular multilingual generation, support for
equivalent variants across semantic, grammar, and lexical levels, and a
transparent rule-based system. We outline the architecture and demonstrate its
flexibility through some examples capable of generating numerous diverse and
equivalent rewritings based on the input concept.},
Year          = {2025},
Month         = {Feb},
Note          = {EPTCS 416, 2025, pp. 139-152},
Url           = {http://arxiv.org/abs/2502.09218v1},
File          = {2502.09218v1.pdf}
}
@article{2502.09206v1,
Author        = {Haya Majid Qureshi and Wolfgang Faber},
Title         = {Efficient OWL2QL Meta-reasoning Using ASP-based Hybrid Knowledge Bases},
Eprint        = {2502.09206v1},
DOI           = {10.4204/EPTCS.416.17},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LO},
Abstract      = {Metamodeling refers to scenarios in ontologies in which classes and roles can
be members of classes or occur in roles. This is a desirable modelling feature
in several applications, but allowing it without restrictions is problematic
for several reasons, mainly because it causes undecidability. Therefore,
practical languages either forbid metamodeling explicitly or treat occurrences
of classes as instances to be semantically different from other occurrences,
thereby not allowing metamodeling semantically. Several extensions have been
proposed to provide metamodeling to some extent. Building on earlier work that
reduces metamodeling query answering to Datalog query answering, recently
reductions to query answering over hybrid knowledge bases were proposed with
the aim of using the Datalog transformation only where necessary. Preliminary
work showed that the approach works, but the hoped-for performance improvements
were not observed yet. In this work we expand on this body of work by improving
the theoretical basis of the reductions and by using alternative tools that
show competitive performance.},
Year          = {2025},
Month         = {Feb},
Note          = {EPTCS 416, 2025, pp. 188-200},
Url           = {http://arxiv.org/abs/2502.09206v1},
File          = {2502.09206v1.pdf}
}
@article{2502.08823v1,
Author        = {Sebastian Horvat},
Title         = {Notes on a future quantum event-ontology},
Eprint        = {2502.08823v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {physics.hist-ph},
Abstract      = {This essay is a two-step reflection on the question 'Which events (can be
said to) occur in quantum phenomena?' The first step regiments the ontological
category of "statistical phenomena" and studies the adequacy of "probabilistic
event models" as descriptions thereof. Guided by the conviction that quantum
phenomena are to be circumscribed within this same ontological category, the
second step highlights the peculiarities of probabilistic event models of some
non-relativistic quantum phenomena, and thereby of what appear to be some
plausible answers to our initial question. The reflection ends in an aporetic
state, as it is by now usual in encounters between ontology and the quantum.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.08823v1},
File          = {2502.08823v1.pdf}
}
@article{2502.08694v1,
Author        = {Florian Ahrens and Mihai Pomarlan and Daniel BeÃler and Michael Beetz and Manfred Herrmann},
Title         = {Neuronal Correlates of Semantic Event Classes during Presentation of
  Complex Naturalistic Stimuli: Anatomical Patterns, Context-Sensitivity, and
  Potential Impact on shared Human-Robot Ontologies},
Eprint        = {2502.08694v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.NC},
Abstract      = {The present study forms part of a research project that aims to develop
cognition-enabled robotic agents with environmental interaction capabilities
close to human proficiency. This approach is based on human-derived neuronal
data in combination with a shared ontology to enable robots to learn from human
experiences. To gain further insight into the relation between human neuronal
activity patterns and ontological classes, we introduced General Linear Model
(GLM) analyses on fMRI data of participants who were presented with complex
naturalistic video stimuli comparable to the robot tasks. We modeled four event
classes (pick, place, fetch and deliver) attached to different environmental
and object-related context and employed a Representational Similarity Analysis
(RSA) on associated brain activity patterns as a starting point for an
automatic hierarchical clustering. Based on the default values for the
Hemodynamic Response Function (HRF), the activity patterns were reliably
grouped according to their parent classes of object interaction and navigation.
Although fetch and deliver events were also distinguished by neuronal patterns,
pick and place events demonstrated higher ambiguity with respect to neuronal
activation patterns. Introducing a shorter HRF time-to-peak leads to a more
reliable grouping of all four semantic classes, despite contextual factors.
These data might give novel insights into the neuronal representation of
complex stimuli and may enable further research in ontology validation in
cognition-enabled robotics.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.08694v1},
File          = {2502.08694v1.pdf}
}
@article{2502.18484v1,
Author        = {Krishna Chaitanya Sunkara and Krishnaiah Narukulla},
Title         = {AI Enhanced Ontology Driven NLP for Intelligent Cloud Resource Query
  Processing Using Knowledge Graphs},
Eprint        = {2502.18484v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {The conventional resource search in cloud infrastructure relies on
keyword-based searches or GUIDs, which demand exact matches and significant
user effort to locate resources. These conventional search approaches often
fail to interpret the intent behind natural language queries, making resource
discovery inefficient and inaccessible to users. Though there exists some form
of NLP based search engines, they are limited and focused more on analyzing the
NLP query itself and extracting identifiers to find the resources. But they
fail to search resources based on their behavior or operations or their
capabilities or relationships or features or business relevance or the dynamic
changing state or the knowledge these resources have. The search criteria has
been changing with the inundation of AI based services which involved
discovering not just the requested resources and identifiers but seeking
insights. The real intent of a search has never been to just to list the
resources but with some actual context such as to understand causes of some
behavior in the system, compliance checks, capacity estimations, network
constraints, or troubleshooting or business insights. This paper proposes an
advanced Natural Language Processing (NLP) enhanced by ontology-based semantics
to enable intuitive, human-readable queries which allows users to actually
discover the intent-of-search itself. By constructing an ontology of cloud
resources, their interactions, and behaviors, the proposed framework enables
dynamic intent extraction and relevance ranking using Latent Semantic Indexing
(LSI) and AI models. It introduces an automated pipeline which integrates
ontology extraction by AI powered data crawlers, building a semantic knowledge
base for context aware resource discovery.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.18484v1},
File          = {2502.18484v1.pdf}
}
@article{2502.06874v2,
Author        = {Yanming Guo and Xiao Qian and Kevin Credit and Jin Ma},
Title         = {Group Reasoning Emission Estimation Networks},
Eprint        = {2502.06874v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Accurate greenhouse gas (GHG) emission reporting is critical for governments,
businesses, and investors. However, adoption remains limited particularly among
small and medium enterprises due to high implementation costs, fragmented
emission factor databases, and a lack of robust sector classification methods.
To address these challenges, we introduce Group Reasoning Emission Estimation
Networks (GREEN), an AI-driven carbon accounting framework that standardizes
enterprise-level emission estimation, constructs a large-scale benchmark
dataset, and leverages a novel reasoning approach with large language models
(LLMs). Specifically, we compile textual descriptions for 20,850 companies with
validated North American Industry Classification System (NAICS) labels and
align these with an economic model of carbon intensity factors. By reframing
sector classification as an information retrieval task, we fine-tune
Sentence-BERT models using a contrastive learning loss. To overcome the
limitations of single-stage models in handling thousands of hierarchical
categories, we propose a Group Reasoning method that ensembles LLM classifiers
based on the natural NAICS ontology, decomposing the task into multiple
sub-classification steps. We theoretically prove that this approach reduces
classification uncertainty and computational complexity. Experiments on 1,114
NAICS categories yield state-of-the-art performance (83.68% Top-1, 91.47%
Top-10 accuracy), and case studies on 20 companies report a mean absolute
percentage error (MAPE) of 45.88%. The project is available at:
https://huggingface.co/datasets/Yvnminc/ExioNAICS.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.06874v2},
File          = {2502.06874v2.pdf}
}
@article{2502.05478v1,
Author        = {Zhiqiang Liu and Chengtao Gan and Junjie Wang and Yichi Zhang and Zhongpu Bo and Mengshu Sun and Huajun Chen and Wen Zhang},
Title         = {OntoTune: Ontology-Driven Self-training for Aligning Large Language
  Models},
Eprint        = {2502.05478v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Existing domain-specific Large Language Models (LLMs) are typically developed
by fine-tuning general-purposed LLMs with large-scale domain-specific corpora.
However, training on large-scale corpora often fails to effectively organize
domain knowledge of LLMs, leading to fragmented understanding. Inspired by how
humans connect concepts and organize knowledge through mind maps, we aim to
emulate this approach by using ontology with hierarchical conceptual knowledge
to reorganize LLM's domain knowledge. From this perspective, we propose an
ontology-driven self-training framework called OntoTune, which aims to align
LLMs with ontology through in-context learning, enabling the generation of
responses guided by the ontology. We leverage in-context learning to identify
whether the LLM has acquired the specific concept's ontology knowledge, and
select the entries not yet mastered by LLM as the training set to further align
the LLM with ontology. Compared to existing domain LLMs based on newly
collected large-scale domain-specific corpora, our OntoTune, which relies on
the existing, long-term developed ontology and LLM itself, significantly
reduces data maintenance costs and offers improved generalization ability. We
conduct our study in the medical domain to evaluate the effectiveness of
OntoTune, utilizing a standardized medical ontology, SNOMED CT as our ontology
source. Experimental results demonstrate that OntoTune achieves
state-of-the-art performance in both in-ontology task hypernym discovery and
out-of-ontology task medical domain QA. Moreover, compared to the latest direct
ontology injection method TaxoLLaMA, our OntoTune better preserves original
knowledge of LLM. The code and data are available at
https://github.com/zjukg/OntoTune.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.05478v1},
File          = {2502.05478v1.pdf}
}
@article{2502.05110v1,
Author        = {Aisha Aijaz and Raghava Mutharaju and Manohar Kumar},
Title         = {ApplE: An Applied Ethics Ontology with Event Context},
Eprint        = {2502.05110v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CY},
Abstract      = {Applied ethics is ubiquitous in most domains, requiring much deliberation due
to its philosophical nature. Varying views often lead to conflicting courses of
action where ethical dilemmas become challenging to resolve. Although many
factors contribute to such a decision, the major driving forces can be
discretized and thus simplified to provide an indicative answer. Knowledge
representation and reasoning offer a way to explicitly translate abstract
ethical concepts into applicable principles within the context of an event. To
achieve this, we propose ApplE, an Applied Ethics ontology that captures
philosophical theory and event context to holistically describe the morality of
an action. The development process adheres to a modified version of the
Simplified Agile Methodology for Ontology Development (SAMOD) and utilizes
standard design and publication practices. Using ApplE, we model a use case
from the bioethics domain that demonstrates our ontology's social and
scientific value. Apart from the ontological reasoning and quality checks,
ApplE is also evaluated using the three-fold testing process of SAMOD. ApplE
follows FAIR principles and aims to be a viable resource for applied ethicists
and ontology engineers.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.05110v1},
File          = {2502.05110v1.pdf}
}
@article{2502.03992v1,
Author        = {Longquan Jiang and Junbo Huang and Cedric MÃ¶ller and Ricardo Usbeck},
Title         = {Ontology-Guided, Hybrid Prompt Learning for Generalization in Knowledge
  Graph Question Answering},
Eprint        = {2502.03992v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Most existing Knowledge Graph Question Answering (KGQA) approaches are
designed for a specific KG, such as Wikidata, DBpedia or Freebase. Due to the
heterogeneity of the underlying graph schema, topology and assertions, most
KGQA systems cannot be transferred to unseen Knowledge Graphs (KGs) without
resource-intensive training data. We present OntoSCPrompt, a novel Large
Language Model (LLM)-based KGQA approach with a two-stage architecture that
separates semantic parsing from KG-dependent interactions. OntoSCPrompt first
generates a SPARQL query structure (including SPARQL keywords such as SELECT,
ASK, WHERE and placeholders for missing tokens) and then fills them with
KG-specific information. To enhance the understanding of the underlying KG, we
present an ontology-guided, hybrid prompt learning strategy that integrates KG
ontology into the learning process of hybrid prompts (e.g., discrete and
continuous vectors). We also present several task-specific decoding strategies
to ensure the correctness and executability of generated SPARQL queries in both
stages. Experimental results demonstrate that OntoSCPrompt performs as well as
SOTA approaches without retraining on a number of KGQA datasets such as CWQ,
WebQSP and LC-QuAD 1.0 in a resource-efficient manner and can generalize well
to unseen domain-specific KGs like DBLP-QuAD and CoyPu KG Code:
\href{https://github.com/LongquanJiang/OntoSCPrompt}{https://github.com/LongquanJiang/OntoSCPrompt}},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.03992v1},
File          = {2502.03992v1.pdf}
}
@article{2502.04397v3,
Author        = {Xiaorui Su and Shvat Messica and Yepeng Huang and Ruth Johnson and Lukas Fesser and Shanghua Gao and Faryad Sahneh and Marinka Zitnik},
Title         = {Multimodal Medical Code Tokenizer},
Eprint        = {2502.04397v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Foundation models trained on patient electronic health records (EHRs) require
tokenizing medical data into sequences of discrete vocabulary items. Existing
tokenizers treat medical codes from EHRs as isolated textual tokens. However,
each medical code is defined by its textual description, its position in
ontological hierarchies, and its relationships to other codes, such as disease
co-occurrences and drug-treatment associations. Medical vocabularies contain
more than 600,000 codes with critical information for clinical reasoning. We
introduce MedTok, a multimodal medical code tokenizer that uses the text
descriptions and relational context of codes. MedTok processes text using a
language model encoder and encodes the relational structure with a graph
encoder. It then quantizes both modalities into a unified token space,
preserving modality-specific and cross-modality information. We integrate
MedTok into five EHR models and evaluate it on operational and clinical tasks
across in-patient and out-patient datasets, including outcome prediction,
diagnosis classification, drug recommendation, and risk stratification.
Swapping standard EHR tokenizers with MedTok improves AUPRC across all EHR
models, by 4.10% on MIMIC-III, 4.78% on MIMIC-IV, and 11.32% on EHRShot, with
the largest gains in drug recommendation. Beyond EHR modeling, we demonstrate
using MedTok tokenizer with medical QA systems. Our results demonstrate the
potential of MedTok as a unified tokenizer for medical codes, improving
tokenization for medical foundation models.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.04397v3},
File          = {2502.04397v3.pdf}
}
@article{2502.10420v1,
Author        = {Elija Perrier and Michael Timothy Bennett},
Title         = {Position: Stop Acting Like Language Model Agents Are Normal Agents},
Eprint        = {2502.10420v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Language Model Agents (LMAs) are increasingly treated as capable of
autonomously navigating interactions with humans and tools. Their design and
deployment tends to presume they are normal agents capable of sustaining
coherent goals, adapting across contexts and acting with a measure of
intentionality. These assumptions are critical to prospective use cases in
industrial, social and governmental settings. But LMAs are not normal agents.
They inherit the structural problems of the large language models (LLMs) around
which they are built: hallucinations, jailbreaking, misalignment and
unpredictability. In this Position paper we argue LMAs should not be treated as
normal agents, because doing so leads to problems that undermine their utility
and trustworthiness. We enumerate pathologies of agency intrinsic to LMAs.
Despite scaffolding such as external memory and tools, they remain
ontologically stateless, stochastic, semantically sensitive, and linguistically
intermediated. These pathologies destabilise the ontological properties of LMAs
including identifiability, continuity, persistence and and consistency,
problematising their claim to agency. In response, we argue LMA ontological
properties should be measured before, during and after deployment so that the
negative effects of pathologies can be mitigated.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.10420v1},
File          = {2502.10420v1.pdf}
}
@article{2501.18794v1,
Author        = {Matthew Neeley and Guantong Qi and Guanchu Wang and Ruixiang Tang and Dongxue Mao and Chaozhong Liu and Sasidhar Pasupuleti and Bo Yuan and Fan Xia and Pengfei Liu and Zhandong Liu and Xia Hu},
Title         = {Survey and Improvement Strategies for Gene Prioritization with Large
  Language Models},
Eprint        = {2501.18794v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.GN},
Abstract      = {Rare diseases are challenging to diagnose due to limited patient data and
genetic diversity. Despite advances in variant prioritization, many cases
remain undiagnosed. While large language models (LLMs) have performed well in
medical exams, their effectiveness in diagnosing rare genetic diseases has not
been assessed. To identify causal genes, we benchmarked various LLMs for gene
prioritization. Using multi-agent and Human Phenotype Ontology (HPO)
classification, we categorized patients based on phenotypes and solvability
levels. As gene set size increased, LLM performance deteriorated, so we used a
divide-and-conquer strategy to break the task into smaller subsets. At
baseline, GPT-4 outperformed other LLMs, achieving near 30% accuracy in ranking
causal genes correctly. The multi-agent and HPO approaches helped distinguish
confidently solved cases from challenging ones, highlighting the importance of
known gene-phenotype associations and phenotype specificity. We found that
cases with specific phenotypes or clear associations were more accurately
solved. However, we observed biases toward well-studied genes and input order
sensitivity, which hindered gene prioritization. Our divide-and-conquer
strategy improved accuracy by overcoming these biases. By utilizing HPO
classification, novel multi-agent techniques, and our LLM strategy, we improved
causal gene identification accuracy compared to our baseline evaluation. This
approach streamlines rare disease diagnosis, facilitates reanalysis of unsolved
cases, and accelerates gene discovery, supporting the development of targeted
diagnostics and therapies.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.18794v1},
File          = {2501.18794v1.pdf}
}
@article{2501.17348v2,
Author        = {Mert Ä°nan and Anthony Sicilia and Suvodip Dey and Vardhan Dongre and Tejas Srinivasan and Jesse Thomason and GÃ¶khan TÃ¼r and Dilek Hakkani-TÃ¼r and Malihe Alikhani},
Title         = {Better Slow than Sorry: Introducing Positive Friction for Reliable
  Dialogue Systems},
Eprint        = {2501.17348v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {While theories of discourse and cognitive science have long recognized the
value of unhurried pacing, recent dialogue research tends to minimize friction
in conversational systems. Yet, frictionless dialogue risks fostering
uncritical reliance on AI outputs, which can obscure implicit assumptions and
lead to unintended consequences. To meet this challenge, we propose integrating
positive friction into conversational AI, which promotes user reflection on
goals, critical thinking on system response, and subsequent re-conditioning of
AI systems. We hypothesize systems can improve goal alignment, modeling of user
mental states, and task success by deliberately slowing down conversations in
strategic moments to ask questions, reveal assumptions, or pause. We present an
ontology of positive friction and collect expert human annotations on
multi-domain and embodied goal-oriented corpora. Experiments on these corpora,
along with simulated interactions using state-of-the-art systems, suggest
incorporating friction not only fosters accountable decision-making, but also
enhances machine understanding of user beliefs and goals, and increases task
success rates.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.17348v2},
File          = {2501.17348v2.pdf}
}
@article{2501.14579v1,
Author        = {Alexander V. Belikov and Sacha Raoult},
Title         = {Knowledge Graphs Construction from Criminal Court Appeals: Insights from
  the French Cassation Court},
Eprint        = {2501.14579v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Despite growing interest, accurately and reliably representing unstructured
data, such as court decisions, in a structured form, remains a challenge.
Recent advancements in generative AI applied to language modeling enabled the
transformation of text into knowledge graphs, unlocking new opportunities for
analysis and modeling. This paper presents a framework for constructing
knowledge graphs from appeals to the French Cassation Court. The framework
includes a domain-specific ontology and a derived dataset, offering a
foundation for structured legal data representation and analysis.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.14579v1},
File          = {2501.14579v1.pdf}
}
@article{2501.13034v1,
Author        = {James McLaughlin and Josh Lagrimas and Haider Iqbal and Helen Parkinson and Henriette Harmse},
Title         = {OLS4: A new Ontology Lookup Service for a growing interdisciplinary
  knowledge ecosystem},
Eprint        = {2501.13034v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {The Ontology Lookup Service (OLS) is an open source search engine for
ontologies which is used extensively in the bioinformatics and chemistry
communities to annotate biological and biomedical data with ontology terms.
Recently there has been a significant increase in the size and complexity of
ontologies due to new scales of biological knowledge, such as spatial
transcriptomics, new ontology development methodologies, and curation on an
increased scale. Existing Web-based tools for ontology browsing such as
BioPortal and OntoBee do not support the full range of definitions used by
today's ontologies. In order to support the community going forward, we have
developed OLS4, implementing the complete OWL2 specification,
internationalization support for multiple languages, and a new user interface
with UX enhancements such as links out to external databases. OLS4 has replaced
OLS3 in production at EMBL-EBI and has a backwards compatible API supporting
users of OLS3 to transition.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.13034v1},
File          = {2501.13034v1.pdf}
}
@article{2501.12943v1,
Author        = {JoaquÃ­ Gayoso-Cabada and MarÃ­a Goicoechea-de-Jorge and Mercedes GÃ³mez-AlbarrÃ¡n and Amelia Sanz-Cabrerizo and Antonio Sarasa-Cabezuelo and JosÃ©-Luis Sierra},
Title         = {Ontology-Enhanced Educational Annotation Activities},
Eprint        = {2501.12943v1},
DOI           = {10.3390/su11164455},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Information and communications technology and technology-enhanced learning
have unquestionably transformed traditional teaching-learning processes and are
positioned as key factors to promote quality education, one of the basic
sustainable development goals of the 2030 agenda. Document annotation, which
was traditionally carried out with pencil and paper and currently benefits from
digital document annotation tools, is a representative example of this
transformation. Using document annotation tools, students can enrich the
documents with annotations that highlight the most relevant aspects of these
documents. As the conceptual complexity of the learning domain increases, the
annotation of the documents may require comprehensive domain knowledge and an
expert analysis capability that students usually lack. Consequently, a
proliferation of irrelevant, incorrect, and/or poorly decontextualized
annotations may appear, while other relevant aspects are completely ignored by
the students. The main hypothesis proposed by this paper is that the use of a
guiding annotation ontology in the annotation activities is a keystone aspect
to alleviate these shortcomings. Consequently, comprehension is improved,
exhaustive content analysis is promoted, and meta-reflective thinking is
developed. To test this hypothesis, we describe our own annotation tool,
\@note, which fully implements this ontology-enhanced annotation paradigm, and
we provide experimental evidence about how \@note can improve academic
performance via a pilot study concerning critical literary annotation.},
Year          = {2025},
Month         = {Jan},
Note          = {Sustainability, 2019},
Url           = {http://arxiv.org/abs/2501.12943v1},
File          = {2501.12943v1.pdf}
}
@article{2501.12603v2,
Author        = {Maciej Grzeszczuk and Kinga Skorupska and Grzegorz Marcin Wojcik},
Title         = {Bridging the Digital Divide: Approach to Documenting Early Computing
  Artifacts Using Established Standards for Cross-Collection Knowledge
  Integration Ontology},
Eprint        = {2501.12603v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {In this paper we address the challenges of documenting early digital
artifacts in collections built to offer historical context for future
generations. Through insights from active community members (N=20), we examine
current archival needs and obstacles. We assess the potential of the CIDOC
Conceptual Reference Model (CRM) for categorizing fragmented digital data.
Despite its complexity, CIDOC-CRM proves logical, human-readable, and
adaptable, enabling archivists to select minimal yet effective building blocks
set to empower community-led heritage projects.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.12603v2},
File          = {2501.12603v2.pdf}
}
@article{2501.12309v2,
Author        = {Eugenio Borzone and Leandro Di Persia and Matias Gerard},
Title         = {A Hybrid Supervised and Self-Supervised Graph Neural Network for
  Edge-Centric Applications},
Eprint        = {2501.12309v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {This paper presents a novel graph-based deep learning model for tasks
involving relations between two nodes (edge-centric tasks), where the focus
lies on predicting relationships and interactions between pairs of nodes rather
than node properties themselves. This model combines supervised and
self-supervised learning, taking into account for the loss function the
embeddings learned and patterns with and without ground truth. Additionally it
incorporates an attention mechanism that leverages both node and edge features.
The architecture, trained end-to-end, comprises two primary components:
embedding generation and prediction. First, a graph neural network (GNN)
transform raw node features into dense, low-dimensional embeddings,
incorporating edge attributes. Then, a feedforward neural model processes the
node embeddings to produce the final output. Experiments demonstrate that our
model matches or exceeds existing methods for protein-protein interactions
prediction and Gene Ontology (GO) terms prediction. The model also performs
effectively with one-hot encoding for node features, providing a solution for
the previously unsolved problem of predicting similarity between compounds with
unknown structures.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.12309v2},
File          = {2501.12309v2.pdf}
}
@article{2501.11441v2,
Author        = {Maria Taboada and Diego Martinez and Mohammed Arideh and Rosa Mosquera},
Title         = {Ontology Matching with Large Language Models and Prioritized Depth-First
  Search},
Eprint        = {2501.11441v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Ontology matching (OM) plays a key role in enabling data interoperability and
knowledge sharing, but it remains challenging due to the need for large
training datasets and limited vocabulary processing in machine learning
approaches. Recently, methods based on Large Language Model (LLMs) have shown
great promise in OM, particularly through the use of a retrieve-then-prompt
pipeline. In this approach, relevant target entities are first retrieved and
then used to prompt the LLM to predict the final matches. Despite their
potential, these systems still present limited performance and high
computational overhead. To address these issues, we introduce MILA, a novel
approach that embeds a retrieve-identify-prompt pipeline within a prioritized
depth-first search (PDFS) strategy. This approach efficiently identifies a
large number of semantic correspondences with high accuracy, limiting LLM
requests to only the most borderline cases. We evaluated MILA using the
biomedical challenge proposed in the 2023 and 2024 editions of the Ontology
Alignment Evaluation Initiative. Our method achieved the highest F-Measure in
four of the five unsupervised tasks, outperforming state-of-the-art OM systems
by up to 17%. It also performed better than or comparable to the leading
supervised OM systems. MILA further exhibited task-agnostic performance,
remaining stable across all tasks and settings, while significantly reducing
LLM requests. These findings highlight that high-performance LLM-based OM can
be achieved through a combination of programmed (PDFS), learned (embedding
vectors), and prompting-based heuristics, without the need of domain-specific
heuristics or fine-tuning.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.11441v2},
File          = {2501.11441v2.pdf}
}
@article{2501.11123v1,
Author        = {Juan CigarrÃ¡n-Recuero and JoaquÃ­n Gayoso-Cabada and Miguel RodrÃ­guez-Artacho and MarÃ­a-Dolores Romero-LÃ³pez and Antonio Sarasa-Cabezuelo and JosÃ©-Luis Sierra},
Title         = {Assessing Semantic Annotation Activities with Formal Concept Analysis},
Eprint        = {2501.11123v1},
DOI           = {10.1016/j.eswa.2014.02.036},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper describes an approach to assessing semantic annotation activities
based on formal concept analysis (FCA). In this approach, annotators use
taxonomical ontologies created by domain experts to annotate digital resources.
Then, using FCA, domain experts are provided with concept lattices that
graphically display how their ontologies were used during the semantic
annotation process. In consequence, they can advise annotators on how to better
use the ontologies, as well as how to refine them to better suit the needs of
the semantic annotators. To illustrate the approach, we describe its
implementation in @note, a Rich Internet Application (RIA) for the
collaborative annotation of digitized literary texts, we exemplify its use with
a case study, and we provide some evaluation results using the method.},
Year          = {2025},
Month         = {Jan},
Note          = {Expert Systems with Applications (2014)},
Url           = {http://arxiv.org/abs/2501.11123v1},
File          = {2501.11123v1.pdf}
}
@article{2501.10847v1,
Author        = {Zeinab Rajabi and Seyed Mohsen Rahnamafard},
Title         = {A Survey on Conceptual model of Enterprise ontology},
Eprint        = {2501.10847v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {Enterprise ontology serves as a foundational framework for semantically
comprehending the nature of organizations and the essential components that
uphold their integrity. The systematic and conceptual understanding of
organizations has garnered significant attention from researchers due to its
pivotal role in various domains, including business modeling, enterprise
architecture, business process management, context-aware systems, application
development, interoperability across diverse systems and platforms, knowledge
management, organizational learning and innovation, and conflict resolution
within organizations. Achieving a consensus on the concepts related to the
fundamental elements that constitute an organization is therefore critical.
This study aims to conduct a comprehensive analysis and comparison of existing
conceptual models of enterprises as documented in scholarly articles published
over the past decade. We discuss the strengths and weaknesses of each model and
introduce a robust framework for their evaluation. To facilitate this
evaluation, we propose several pertinent criteria derived from established
methodologies for assessing ontologies. Furthermore, we identify contemporary
challenges and issues that have been overlooked in prior studies, offering
insights and suggestions for future research directions in enterprise modeling.
This article ultimately presents a roadmap for enhancing the systematic
understanding of organizations through refined enterprise ontology frameworks.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.10847v1},
File          = {2501.10847v1.pdf}
}
@article{2501.10160v1,
Author        = {Subhashis Das and Debashis Naskar and Sara Rodriguez Gonzalez and Pamela Hussey},
Title         = {CSSDM Ontology to Enable Continuity of Care Data Interoperability},
Eprint        = {2501.10160v1},
DOI           = {10.1109/BIBM62325.2024.10822860},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The rapid advancement of digital technologies and recent global pandemic
scenarios have led to a growing focus on how these technologies can enhance
healthcare service delivery and workflow to address crises. Action plans that
consolidate existing digital transformation programs are being reviewed to
establish core infrastructure and foundations for sustainable healthcare
solutions. Reforming health and social care to personalize home care, for
example, can help avoid treatment in overcrowded acute hospital settings and
improve the experiences and outcomes for both healthcare professionals and
service users. In this information-intensive domain, addressing the
interoperability challenge through standards-based roadmaps is crucial for
enabling effective connections between health and social care services. This
approach facilitates safe and trustworthy data workflows between different
healthcare system providers. In this paper, we present a methodology for
extracting, transforming, and loading data through a semi-automated process
using a Common Semantic Standardized Data Model (CSSDM) to create personalized
healthcare knowledge graph (KG). The CSSDM is grounded in the formal ontology
of ISO 13940 ContSys and incorporates FHIR-based specifications to support
structural attributes for generating KGs. We propose that the CSSDM facilitates
data harmonization and linking, offering an alternative approach to
interoperability. This approach promotes a novel form of collaboration between
companies developing health information systems and cloud-enabled health
services. Consequently, it provides multiple stakeholders with access to
high-quality data and information sharing.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.10160v1},
File          = {2501.10160v1.pdf}
}
@article{2501.09744v1,
Author        = {Hajung Kim and Chanhwi Kim and Jiwoong Sohn and Tim Beck and Marek Rei and Sunkyu Kim and T Ian Simpson and Joram M Posma and Antoine Lain and Mujeen Sung and Jaewoo Kang},
Title         = {KU AIGEN ICL EDI@BC8 Track 3: Advancing Phenotype Named Entity
  Recognition and Normalization for Dysmorphology Physical Examination Reports},
Eprint        = {2501.09744v1},
DOI           = {10.5281/zenodo.10104803},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The objective of BioCreative8 Track 3 is to extract phenotypic key medical
findings embedded within EHR texts and subsequently normalize these findings to
their Human Phenotype Ontology (HPO) terms. However, the presence of diverse
surface forms in phenotypic findings makes it challenging to accurately
normalize them to the correct HPO terms. To address this challenge, we explored
various models for named entity recognition and implemented data augmentation
techniques such as synonym marginalization to enhance the normalization step.
Our pipeline resulted in an exact extraction and normalization F1 score 2.6\%
higher than the mean score of all submissions received in response to the
challenge. Furthermore, in terms of the normalization F1 score, our approach
surpassed the average performance by 1.9\%. These findings contribute to the
advancement of automated medical data extraction and normalization techniques,
showcasing potential pathways for future research and application in the
biomedical domain.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.09744v1},
File          = {2501.09744v1.pdf}
}
@article{2501.08540v1,
Author        = {Ning Pei Ding and Jingge Du and Zaiwen Feng},
Title         = {Knowledge prompt chaining for semantic modeling},
Eprint        = {2501.08540v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The task of building semantics for structured data such as CSV, JSON, and XML
files is highly relevant in the knowledge representation field. Even though we
have a vast of structured data on the internet, mapping them to domain
ontologies to build semantics for them is still very challenging as it requires
the construction model to understand and learn graph-structured knowledge.
Otherwise, the task will require human beings' effort and cost. In this paper,
we proposed a novel automatic semantic modeling framework: Knowledge Prompt
Chaining. It can serialize the graph-structured knowledge and inject it into
the LLMs properly in a Prompt Chaining architecture. Through this knowledge
injection and prompting chaining, the model in our framework can learn the
structure information and latent space of the graph and generate the semantic
labels and semantic graphs following the chains' insturction naturally. Based
on experimental results, our method achieves better performance than existing
leading techniques, despite using reduced structured input data.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.08540v1},
File          = {2501.08540v1.pdf}
}
@article{2501.07238v1,
Author        = {Blake Bullwinkel and Amanda Minnich and Shiven Chawla and Gary Lopez and Martin Pouliot and Whitney Maxwell and Joris de Gruyter and Katherine Pratt and Saphir Qi and Nina Chikanov and Roman Lutz and Raja Sekhar Rao Dheekonda and Bolor-Erdene Jagdagdorj and Eugenia Kim and Justin Song and Keegan Hines and Daniel Jones and Giorgio Severi and Richard Lundeen and Sam Vaughan and Victoria Westerhoff and Pete Bryan and Ram Shankar Siva Kumar and Yonatan Zunger and Chang Kawaguchi and Mark Russinovich},
Title         = {Lessons From Red Teaming 100 Generative AI Products},
Eprint        = {2501.07238v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In recent years, AI red teaming has emerged as a practice for probing the
safety and security of generative AI systems. Due to the nascency of the field,
there are many open questions about how red teaming operations should be
conducted. Based on our experience red teaming over 100 generative AI products
at Microsoft, we present our internal threat model ontology and eight main
lessons we have learned:
  1. Understand what the system can do and where it is applied
  2. You don't have to compute gradients to break an AI system
  3. AI red teaming is not safety benchmarking
  4. Automation can help cover more of the risk landscape
  5. The human element of AI red teaming is crucial
  6. Responsible AI harms are pervasive but difficult to measure
  7. LLMs amplify existing security risks and introduce new ones
  8. The work of securing AI systems will never be complete
  By sharing these insights alongside case studies from our operations, we
offer practical recommendations aimed at aligning red teaming efforts with real
world risks. We also highlight aspects of AI red teaming that we believe are
often misunderstood and discuss open questions for the field to consider.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.07238v1},
File          = {2501.07238v1.pdf}
}
@article{2501.05606v1,
Author        = {Zixuan Liang},
Title         = {Harmonizing Metadata of Language Resources for Enhanced Querying and
  Accessibility},
Eprint        = {2501.05606v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper addresses the harmonization of metadata from diverse repositories
of language resources (LRs). Leveraging linked data and RDF techniques, we
integrate data from multiple sources into a unified model based on DCAT and
META-SHARE OWL ontology. Our methodology supports text-based search, faceted
browsing, and advanced SPARQL queries through Linghub, a newly developed
portal. Real user queries from the Corpora Mailing List (CML) were evaluated to
assess Linghub capability to satisfy actual user needs. Results indicate that
while some limitations persist, many user requests can be successfully
addressed. The study highlights significant metadata issues and advocates for
adherence to open vocabularies and standards to enhance metadata harmonization.
This initial research underscores the importance of API-based access to LRs,
promoting machine usability and data subset extraction for specific purposes,
paving the way for more efficient and standardized LR utilization.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.05606v1},
File          = {2501.05606v1.pdf}
}
@article{2501.05486v1,
Author        = {Bart Gajderowicz and Mark S Fox and Yongchao Gao},
Title         = {Towards an Ontology of Traceable Impact Management in the Food Supply
  Chain},
Eprint        = {2501.05486v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {physics.soc-ph},
Abstract      = {The pursuit of quality improvements and accountability in the food supply
chains, especially how they relate to food-related outcomes, such as hunger,
has become increasingly vital, necessitating a comprehensive approach that
encompasses product quality and its impact on various stakeholders and their
communities. Such an approach offers numerous benefits in increasing product
quality and eliminating superfluous measurements while appraising and
alleviating the broader societal and environmental repercussions. A traceable
impact management model (TIMM) provides an impact structure and a reporting
mechanism that identifies each stakeholder's role in the total impact of food
production and consumption stages.
  The model aims to increase traceability's utility in understanding the impact
of changes on communities affected by food production and consumption, aligning
with current and future government requirements, and addressing the needs of
communities and consumers. This holistic approach is further supported by an
ontological model that forms the logical foundation and a unified terminology.
By proposing a holistic and integrated solution across multiple stakeholders,
the model emphasizes quality and the extensive impact of championing
accountability, sustainability, and responsible practices with global
traceability.
  With these combined efforts, the food supply chain moves toward a global
tracking and tracing process that not only ensures product quality but also
addresses its impact on a broader scale, fostering accountability,
sustainability, and responsible food production and consumption.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.05486v1},
File          = {2501.05486v1.pdf}
}
@article{2501.06239v1,
Author        = {Olga Sorokoletova and Emanuele Antonioni and Giordano ColÃ²},
Title         = {Towards a scalable AI-driven framework for data-independent Cyber Threat
  Intelligence Information Extraction},
Eprint        = {2501.06239v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {Cyber Threat Intelligence (CTI) is critical for mitigating threats to
organizations, governments, and institutions, yet the necessary data are often
dispersed across diverse formats. AI-driven solutions for CTI Information
Extraction (IE) typically depend on high-quality, annotated data, which are not
always available. This paper introduces 0-CTI, a scalable AI-based framework
designed for efficient CTI Information Extraction. Leveraging advanced Natural
Language Processing (NLP) techniques, particularly Transformer-based
architectures, the proposed system processes complete text sequences of CTI
reports to extract a cyber ontology of named entities and their relationships.
  Our contribution is the development of 0-CTI, the first modular framework for
CTI Information Extraction that supports both supervised and zero-shot
learning. Unlike existing state-of-the-art models that rely heavily on
annotated datasets, our system enables fully dataless operation through
zero-shot methods for both Entity and Relation Extraction, making it adaptable
to various data availability scenarios. Additionally, our supervised Entity
Extractor surpasses current state-of-the-art performance in cyber Entity
Extraction, highlighting the dual strength of the framework in both
low-resource and data-rich environments.
  By aligning the system's outputs with the Structured Threat Information
Expression (STIX) format, a standard for information exchange in the
cybersecurity domain, 0-CTI standardizes extracted knowledge, enhancing
communication and collaboration in cybersecurity operations.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.06239v1},
File          = {2501.06239v1.pdf}
}
@article{2504.11459v1,
Author        = {Peter Stockinger},
Title         = {From Conceptual Data Models to Multimodal Representation},
Eprint        = {2504.11459v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {1) Introduction and Conceptual Framework: This document explores the concept
of information design by dividing it into two major practices: defining the
meaning of a corpus of textual data and its visual or multimodal
representation. It draws on expertise in enriching textual corpora,
particularly audiovisual ones, and transforming them into multiple narrative
formats. The text highlights a crucial distinction between the semantic content
of a domain and the modalities of its graphic expression, illustrating this
approach with concepts rooted in structural semiotics and linguistics
traditions.
  2) Modeling and Conceptual Design: The article emphasizes the importance of
semantic modeling, often achieved through conceptual networks or graphs. These
tools enable the structuring of knowledge within a domain by accounting for
relationships between concepts, contexts of use, and specific objectives.
Stockinger also highlights the constraints and challenges involved in creating
dynamic and adaptable models, integrating elements such as thesauri or
interoperable ontologies to facilitate the analysis and publication of complex
corpora.
  3) Applications and Multimodal Visualization: The text concludes by examining
the practical application of these models in work environments like OKAPI,
developed to analyze, publish, and reuse audiovisual data. It also discusses
innovative approaches such as visual storytelling and document reengineering,
which involve transforming existing content into new resources tailored to
various contexts. These methods emphasize interoperability, flexibility, and
the intelligence of communication systems, paving the way for richer and more
collaborative use of digital data. The content of this document was presented
during the "Semiotics of Information Design" Day organized by Anne
Beyaert-Geslin of the University of Bordeaux Montaigne (MICA laboratory) on
June 21, 2018, in Bordeaux.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2504.11459v1},
File          = {2504.11459v1.pdf}
}
@article{2501.03067v1,
Author        = {Daniel Naro and Jaime Delgado and Silvia Llorente and Amanda Palomo},
Title         = {Design and implementation of tools to build an ontology of Security
  Requirements for Internet of Medical Things},
Eprint        = {2501.03067v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {When developing devices, architectures and services for the Internet of
Medical Things (IoMT) world, manufacturers or integrators must be aware of the
security requirements expressed by both laws and specifications. To provide
tools guiding through these requirements and to assure a third party of the
correct compliance, an ontology charting the relevant laws and specifications
(for the European context) is very useful. We here address the development of
this ontology. Due to the very high number and size of the considered
specification documents, we have put in place a methodology and tools to
simplify the transition from natural text to an ontology. The first step is a
manual highlighting of relevant concepts in the corpus, then a manual
translation to XML/XSD is operated. We have developed a tool allowing us to
convert this semi-structured data into an ontology. Because the different
specifications use similar but different wording, our approach favors the
creation of similar instances in the ontology. To improve the ontology
simplification through instance merging, we consider the use of LLMs. The
responses of the LLMs are compared against our manually defined correct
responses. The quality of the responses of the automated system does not prove
to be good enough to be trusted blindly, and should only be used as a starting
point for a manual correction.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.03067v1},
File          = {2501.03067v1.pdf}
}
@article{2501.03048v2,
Author        = {Qingyuan Zhao},
Title         = {On statistical and causal models associated with acyclic directed mixed
  graphs},
Eprint        = {2501.03048v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {math.ST},
Abstract      = {Causal models in statistics are often described using acyclic directed mixed
graphs (ADMGs), which contain directed and bidirected edges and no directed
cycles. This article surveys various interpretations of ADMGs, discusses their
relations in different sub-classes of ADMGs, and argues that one of them -- the
noise expansion (NE) model -- should be used as the default interpretation. Our
endorsement of the NE model is based on two observations. First, in a subclass
of ADMGs called unconfounded graphs (which retain most of the good properties
of directed acyclic graphs and bidirected graphs), the NE model is equivalent
to many other interpretations including the global Markov and nested Markov
models. Second, the NE model for an arbitrary ADMG is exactly the union of that
for all unconfounded expansions of that graph. This property is referred to as
completeness, as it shows that the model does not commit to any specific latent
variable explanation. In proving that the NE model is nested Markov, we also
develop an ADMG-based theory for causality. Finally, we compare the NE model
with the closely related but different interpretation of ADMGs as directed
acyclic graphs (DAGs) with latent variables that is commonly used in the
literature. We argue that the "latent DAG" interpretation is mathematically
unnecessary, makes obscure ontological assumptions, and discourages
practitioners from deliberating over important structural assumptions.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.03048v2},
File          = {2501.03048v2.pdf}
}
@article{2501.02685v1,
Author        = {Badis Ydri},
Title         = {Quantum Perspectivism vs Nietzschean Perspectivism},
Eprint        = {2501.02685v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {physics.hist-ph},
Abstract      = {This is a work of hard physical philosophy, where Quantum Perspectivism is
shown to function as both an interpretation of quantum mechanics and a physical
model for understanding Nietzsche's perspectivism. This framework combines
quantum logic, the principle of complementarity, and contextuality to examine
how perspectives construct reality. In this model, measurements correspond to
Perspectives and Meta-Perspectives, represented as Boolean subalgebras and
Hilbert sub-lattices within the Hilbert lattice, respectively. The Hilbert
lattice itself is reinterpreted as Jung's Unus Mundus, a unified ontological
reality. A metaphysical observation, made by a metaphysical observer, of a
given system (World) is identified with the set of all corresponding
meta-perspectives in the Hilbert lattice/Unus Mundus, the ocean of reality.
  Perspectives, likened to islands in this ocean, correspond to single
measurements of a system, capturing the logical structure of observed
properties. Meta-perspectives, analogous to continents, represent the synthesis
of multiple measurements, providing a broader yet inherently incomplete
understanding of the system. This structure emphasizes the complementarity and
contextual dependencies of measurements while exposing the limitations of
classical objectivity in the quantum domain. Advocating for a perspectival view
of both the world and truth, Quantum Perspectivism unites quantum mechanics and
Nietzschean philosophy into a cohesive framework for exploring the interplay
between consciousness, observation, and reality.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.02685v1},
File          = {2501.02685v1.pdf}
}
@article{2501.01827v1,
Author        = {Remi van Trijp and Katrien Beuls and Paul Van Eecke},
Title         = {The Proof is in the Almond Cookies},
Eprint        = {2501.01827v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper presents a case study on how to process cooking recipes (and more
generally, how-to instructions) in a way that makes it possible for a robot or
artificial cooking assistant to support human chefs in the kitchen. Such AI
assistants would be of great benefit to society, as they can help to sustain
the autonomy of aging adults or people with a physical impairment, or they may
reduce the stress in a professional kitchen. We propose a novel approach to
computational recipe understanding that mimics the human sense-making process,
which is narrative-based. Using an English recipe for almond crescent cookies
as illustration, we show how recipes can be modelled as rich narrative
structures by integrating various knowledge sources such as language
processing, ontologies, and mental simulation. We show how such narrative
structures can be used for (a) dealing with the challenges of recipe language,
such as zero anaphora, (b) optimizing a robot's planning process, (c) measuring
how well an AI system understands its current tasks, and (d) allowing recipe
annotations to become language-independent.},
Year          = {2025},
Month         = {Jan},
Note          = {In Steels, L. & Porzel, R. (eds). 2024. Narrative-based
  Understanding of Everyday Activities: A Cookbook. Venice: Venice
  International University. Pages 59-77},
Url           = {http://arxiv.org/abs/2501.01827v1},
File          = {2501.01827v1.pdf}
}
@article{2501.00644v1,
Author        = {Daniel B. Hier and Michael D. Carrithers and Thanh Son Do and Tayo Obafemi-Ajayi},
Title         = {Efficient Standardization of Clinical Notes using Large Language Models},
Eprint        = {2501.00644v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Clinician notes are a rich source of patient information but often contain
inconsistencies due to varied writing styles, colloquialisms, abbreviations,
medical jargon, grammatical errors, and non-standard formatting. These
inconsistencies hinder the extraction of meaningful data from electronic health
records (EHRs), posing challenges for quality improvement, population health,
precision medicine, decision support, and research.
  We present a large language model approach to standardizing a corpus of 1,618
clinical notes. Standardization corrected an average of $4.9 +/- 1.8$
grammatical errors, $3.3 +/- 5.2$ spelling errors, converted $3.1 +/- 3.0$
non-standard terms to standard terminology, and expanded $15.8 +/- 9.1$
abbreviations and acronyms per note. Additionally, notes were re-organized into
canonical sections with standardized headings. This process prepared notes for
key concept extraction, mapping to medical ontologies, and conversion to
interoperable data formats such as FHIR.
  Expert review of randomly sampled notes found no significant data loss after
standardization. This proof-of-concept study demonstrates that standardization
of clinical notes can improve their readability, consistency, and usability,
while also facilitating their conversion into interoperable data formats.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2501.00644v1},
File          = {2501.00644v1.pdf}
}
@article{2501.00276v1,
Author        = {Sabah Al-Fedaghi},
Title         = {Conceptual Modeling and Classification of Events},
Eprint        = {2501.00276v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {This paper is a sequel to an evolving research project on a diagrammatic
methodology called thinging machine (TM). Initially, it was proposed as a base
for conceptual modelling (e.g., conceptual UML) in areas such as requirement
engineering. Conceptual modelling involves a high-level representation of a
real-world system that integrates various components to refine it into a more
concrete (computer) executable form. The TM project has progressed into a more
comprehensive approach by applying it in several research areas and expanding
its theoretical and ontological foundation. Accordingly, the first part of the
paper involves enhancing some TM aspects related to structuring events in
existence, such as absent events. The second part of the paper focuses on how
to classify events and the kinds of relationships that can be recognized among
events. The notion of events has occupied a central role in modelling. It
influences computer science and such diverse disciplines as linguistics,
probability theory, artificial intelligence, physics, philosophy and history.
In TM, an event is defined as the so-called thimac (thing/machine) with a time
breath that infuses dynamism into the static description of the thimac called a
region. A region is a diagrammatic specification based on five generic actions:
create, process, release, transfer and receive. The results of this research
provide (a) an enrichment of conceptual modelling, especially concerning
varieties of existence, e.g., absent events of negative propositions, and (b) a
proposal that instead of semantic categorizations of events, it is possible to
develop a new type of classification based on graphs grounded on the TM model
diagrams.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2501.00276v1},
File          = {2501.00276v1.pdf}
}
@article{2412.20942v1,
Author        = {Xiaohan Feng and Xixin Wu and Helen Meng},
Title         = {Ontology-grounded Automatic Knowledge Graph Construction by LLM under
  Wikidata schema},
Eprint        = {2412.20942v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We propose an ontology-grounded approach to Knowledge Graph (KG) construction
using Large Language Models (LLMs) on a knowledge base. An ontology is authored
by generating Competency Questions (CQ) on knowledge base to discover knowledge
scope, extracting relations from CQs, and attempt to replace equivalent
relations by their counterpart in Wikidata. To ensure consistency and
interpretability in the resulting KG, we ground generation of KG with the
authored ontology based on extracted relations. Evaluation on benchmark
datasets demonstrates competitive performance in knowledge graph construction
task. Our work presents a promising direction for scalable KG construction
pipeline with minimal human intervention, that yields high quality and
human-interpretable KGs, which are interoperable with Wikidata semantics for
potential knowledge base expansion.},
Year          = {2024},
Month         = {Dec},
Note          = {CEUR Workshop Proceedings 3841 (2024) 117-135},
Url           = {http://arxiv.org/abs/2412.20942v1},
File          = {2412.20942v1.pdf}
}
@article{2412.20331v1,
Author        = {Moe Kayali and Fabian Wenz and Nesime Tatbul and ÃaÄatay Demiralp},
Title         = {Mind the Data Gap: Bridging LLMs to Enterprise Data Integration},
Eprint        = {2412.20331v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Leading large language models (LLMs) are trained on public data. However,
most of the world's data is dark data that is not publicly accessible, mainly
in the form of private organizational or enterprise data. We show that the
performance of methods based on LLMs seriously degrades when tested on
real-world enterprise datasets. Current benchmarks, based on public data,
overestimate the performance of LLMs. We release a new benchmark dataset, the
GOBY Benchmark, to advance discovery in enterprise data integration. Based on
our experience with this enterprise benchmark, we propose techniques to uplift
the performance of LLMs on enterprise data, including (1) hierarchical
annotation, (2) runtime class-learning, and (3) ontology synthesis. We show
that, once these techniques are deployed, the performance on enterprise data
becomes on par with that of public data. The Goby benchmark can be obtained at
https://goby-benchmark.github.io/.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.20331v1},
File          = {2412.20331v1.pdf}
}
@article{2412.20167v1,
Author        = {Roel Hulsman and Valentin Comte and Lorenzo Bertolini and Tobias Wiesenthal and Antonio Puertas Gallardo and Mario Ceresa},
Title         = {Conformal Risk Control for Pulmonary Nodule Detection},
Eprint        = {2412.20167v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Quantitative tools are increasingly appealing for decision support in
healthcare, driven by the growing capabilities of advanced AI systems. However,
understanding the predictive uncertainties surrounding a tool's output is
crucial for decision-makers to ensure reliable and transparent decisions. In
this paper, we present a case study on pulmonary nodule detection for lung
cancer screening, enhancing an advanced detection model with an uncertainty
quantification technique called conformal risk control (CRC). We demonstrate
that prediction sets with conformal guarantees are attractive measures of
predictive uncertainty in the safety-critical healthcare domain, allowing
end-users to achieve arbitrary validity by trading off false positives and
providing formal statistical guarantees on model performance. Among
ground-truth nodules annotated by at least three radiologists, our model
achieves a sensitivity that is competitive with that generally achieved by
individual radiologists, with a slight increase in false positives.
Furthermore, we illustrate the risks of using off-the-shelve prediction models
when faced with ontological uncertainty, such as when radiologists disagree on
what constitutes the ground truth on pulmonary nodules.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.20167v1},
File          = {2412.20167v1.pdf}
}
@article{2412.18706v1,
Author        = {Mohsen Nayebi Kerdabadi and Arya Hadizadeh Moghaddam and Bin Liu and Mei Liu and Zijun Yao},
Title         = {SurvAttack: Black-Box Attack On Survival Models through
  Ontology-Informed EHR Perturbation},
Eprint        = {2412.18706v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Survival analysis (SA) models have been widely studied in mining electronic
health records (EHRs), particularly in forecasting the risk of critical
conditions for prioritizing high-risk patients. However, their vulnerability to
adversarial attacks is much less explored in the literature. Developing
black-box perturbation algorithms and evaluating their impact on
state-of-the-art survival models brings two benefits to medical applications.
First, it can effectively evaluate the robustness of models in pre-deployment
testing. Also, exploring how subtle perturbations would result in significantly
different outcomes can provide counterfactual insights into the clinical
interpretation of model prediction. In this work, we introduce SurvAttack, a
novel black-box adversarial attack framework leveraging subtle clinically
compatible, and semantically consistent perturbations on longitudinal EHRs to
degrade survival models' predictive performance. We specifically develop a
greedy algorithm to manipulate medical codes with various adversarial actions
throughout a patient's medical history. Then, these adversarial actions are
prioritized using a composite scoring strategy based on multi-aspect
perturbation quality, including saliency, perturbation stealthiness, and
clinical meaningfulness. The proposed adversarial EHR perturbation algorithm is
then used in an efficient SA-specific strategy to attack a survival model when
estimating the temporal ranking of survival urgency for patients. To
demonstrate the significance of our work, we conduct extensive experiments,
including baseline comparisons, explainability analysis, and case studies. The
experimental results affirm our research's effectiveness in illustrating the
vulnerabilities of patient survival models, model interpretation, and
ultimately contributing to healthcare quality.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.18706v1},
File          = {2412.18706v1.pdf}
}
@article{2412.18485v1,
Author        = {Kalam Khadka},
Title         = {Persuasion and Phishing: Analysing the Interplay of Persuasion Tactics
  in Cyber Threats},
Eprint        = {2412.18485v1},
DOI           = {10.53735/cisse.v12i1.207},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {This study extends the research of Ferreira and Teles (2019), who synthesized
works by Cialdini (2007), Gragg (2003), and Stajano and Wilson (2011) to
propose a unique list of persuasion principles in social engineering. While
Ferreira and Teles focused on email subject lines, this research analyzed
entire email contents to identify principles of human persuasion in phishing
emails. This study also examined the goals and targets of phishing emails,
providing a novel contribution to the field. Applying these findings to the
ontological model by Mouton et al. (2014) reveals that when social engineers
use email for phishing, individuals are the primary targets. The goals are
typically unauthorized access, followed by financial gain and service
disruption, with Distraction as the most commonly used compliance principle.
This research highlights the importance of understanding human persuasion in
technology-mediated interactions to develop methods for detecting and
preventing phishing emails before they reach users. Despite previous
identification of luring elements in phishing emails, empirical findings have
been inconsistent. For example, Akbar (2014) found 'authority' and 'scarcity'
most common, while Ferreira et al. (2015) identified 'liking' and 'similarity.'
In this study, 'Distraction' was most frequently used, followed by 'Deception,'
'Integrity,' and 'Authority.' This paper offers additional insights into
phishing email tactics and suggests future solutions should leverage
socio-technical principles. Future work will apply this methodology to other
social engineering techniques beyond phishing emails, using the ontological
model to further inform the research community.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.18485v1},
File          = {2412.18485v1.pdf}
}
@article{2412.18419v1,
Author        = {Zihan Zhou and Ziyi Zeng and Wenhao Jiang and Yihui Zhu and Jiaxin Mao and Yonggui Yuan and Min Xia and Shubin Zhao and Mengyu Yao and Yunqian Chen},
Title         = {Research on the Proximity Relationships of Psychosomatic Disease
  Knowledge Graph Modules Extracted by Large Language Models},
Eprint        = {2412.18419v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {As social changes accelerate, the incidence of psychosomatic disorders has
significantly increased, becoming a major challenge in global health issues.
This necessitates an innovative knowledge system and analytical methods to aid
in diagnosis and treatment. Here, we establish the ontology model and entity
types, using the BERT model and LoRA-tuned LLM for named entity recognition,
constructing the knowledge graph with 9668 triples. Next, by analyzing the
network distances between disease, symptom, and drug modules, it was found that
closer network distances among diseases can predict greater similarities in
their clinical manifestations, treatment approaches, and psychological
mechanisms, and closer distances between symptoms indicate that they are more
likely to co-occur. Lastly, by comparing the proximity d and proximity z score,
it was shown that symptom-disease pairs in primary diagnostic relationships
have a stronger association and are of higher referential value than those in
diagnostic relationships. The research results revealed the potential
connections between diseases, co-occurring symptoms, and similarities in
treatment strategies, providing new perspectives for the diagnosis and
treatment of psychosomatic disorders and valuable information for future mental
health research and practice.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.18419v1},
File          = {2412.18419v1.pdf}
}
@article{2412.18042v1,
Author        = {Hangli Ge and Hirotsugu Seike and Noboru Koshizuka},
Title         = {Time-Probability Dependent Knowledge Extraction in IoT-enabled Smart
  Building},
Eprint        = {2412.18042v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Smart buildings incorporate various emerging Internet of Things (IoT)
applications for comprehensive management of energy efficiency, human comfort,
automation, and security. However, the development of a knowledge extraction
framework is fundamental. Currently, there is a lack of a unified and practical
framework for modeling heterogeneous sensor data within buildings. In this
paper, we propose a practical inference framework for extracting
status-to-event knowledge within smart building. Our proposal includes
IoT-based API integration, ontology model design, and time probability
dependent knowledge extraction methods. The Building Topology Ontology (BOT)
was leveraged to construct spatial relations among sensors and spaces within
the building. We utilized Apache Jena Fuseki's SPARQL server for storing and
querying the RDF triple data. Two types of knowledge could be extracted:
timestamp-based probability for abnormal event detection and time
interval-based probability for conjunction of multiple events. We conducted
experiments (over a 78-day period) in a real smart building environment. The
data of light and elevator states has been collected for evaluation. The
evaluation revealed several inferred events, such as room occupancy, elevator
trajectory tracking, and the conjunction of both events. The numerical values
of detected event counts and probability demonstrate the potential for
automatic control in the smart building.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.18042v1},
File          = {2412.18042v1.pdf}
}
@article{2412.17877v1,
Author        = {Fabio Le Piane and Matteo Baldoni and Mauro Gaspari and Francesco Mercuri},
Title         = {MAMBO: a lightweight ontology for multiscale materials and applications},
Eprint        = {2412.17877v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cond-mat.mtrl-sci},
Abstract      = {Advancements of both computational and experimental tools have recently led
to significant progress in the development of new advanced and functional
materials, paralleled by a quick growth of the overall amount of data and
information on materials. However, an effective unfolding of the potential of
advanced and data-intensive methodologies requires systematic and efficient
methods for the organization of knowledge in the context of materials research
and development. Semantic technologies can support the structured and formal
organization of knowledge, providing a platform for the integration and
interoperability of data. In this work, we introduce the Materials and
Molecules Basic Ontology (MAMBO), which aims at organizing knowledge in the
field of computational and experimental workflows on molecular materials and
related systems (nanomaterials, supramolecular systems, molecular aggregates,
etc.). Linking recent efforts on ontologies for materials sciences in
neighboring domains, MAMBO aims at filling gaps in current state-of-the-art
knowledge modelling approaches for materials development and design targeting
the intersection between the molecular scale and higher scale domains. With a
focus on operational processes, lightweight, and modularity, MAMBO enables
extensions to broader knowledge domains and integration of methodologies and
workflows related to both computational and experimental tools. MAMBO is
expected to advance the application of data-driven technologies to molecular
materials, including predictive machine learning frameworks for materials
design and discovery and automated platforms.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.17877v1},
File          = {2412.17877v1.pdf}
}
@article{2501.00031v1,
Author        = {Karthik S. Vedula and Annika Gupta and Akshay Swaminathan and Ivan Lopez and Suhana Bedi and Nigam H. Shah},
Title         = {Distilling Large Language Models for Efficient Clinical Information
  Extraction},
Eprint        = {2501.00031v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) excel at clinical information extraction but
their computational demands limit practical deployment. Knowledge
distillation--the process of transferring knowledge from larger to smaller
models--offers a potential solution. We evaluate the performance of distilled
BERT models, which are approximately 1,000 times smaller than modern LLMs, for
clinical named entity recognition (NER) tasks. We leveraged state-of-the-art
LLMs (Gemini and OpenAI models) and medical ontologies (RxNorm and SNOMED) as
teacher labelers for medication, disease, and symptom extraction. We applied
our approach to over 3,300 clinical notes spanning five publicly available
datasets, comparing distilled BERT models against both their teacher labelers
and BERT models fine-tuned on human labels. External validation was conducted
using clinical notes from the MedAlign dataset. For disease extraction, F1
scores were 0.82 (teacher model), 0.89 (BioBERT trained on human labels), and
0.84 (BioBERT-distilled). For medication, F1 scores were 0.84 (teacher model),
0.91 (BioBERT-human), and 0.87 (BioBERT-distilled). For symptoms: F1 score of
0.73 (teacher model) and 0.68 (BioBERT-distilled). Distilled BERT models had
faster inference (12x, 4x, 8x faster than GPT-4o, o1-mini, and Gemini Flash
respectively) and lower costs (85x, 101x, 2x cheaper than GPT-4o, o1-mini, and
Gemini Flash respectively). On the external validation dataset, the distilled
BERT model achieved F1 scores of 0.883 (medication), 0.726 (disease), and 0.699
(symptom). Distilled BERT models were up to 101x cheaper and 12x faster than
state-of-the-art LLMs while achieving similar performance on NER tasks.
Distillation offers a computationally efficient and scalable alternative to
large LLMs for clinical information extraction.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2501.00031v1},
File          = {2501.00031v1.pdf}
}
@article{2412.14387v1,
Author        = {Berkan ÃakÄ±r},
Title         = {Clinical Trials Ontology Engineering with Large Language Models},
Eprint        = {2412.14387v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Managing clinical trial information is currently a significant challenge for
the medical industry, as traditional methods are both time-consuming and
costly. This paper proposes a simple yet effective methodology to extract and
integrate clinical trial data in a cost-effective and time-efficient manner.
Allowing the medical industry to stay up-to-date with medical developments.
Comparing time, cost, and quality of the ontologies created by humans, GPT3.5,
GPT4, and Llama3 (8b & 70b). Findings suggest that large language models (LLM)
are a viable option to automate this process both from a cost and time
perspective. This study underscores significant implications for medical
research where real-time data integration from clinical trials could become the
norm.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.14387v1},
File          = {2412.14387v1.pdf}
}
@article{2412.14299v1,
Author        = {Mauro Nievas Offidani and Facundo Roffet and Claudio Augusto Delrieux and Maria Carolina Gonzalez Galtier and Marcos Zarate},
Title         = {The Multiplex Classification Framework: optimizing multi-label
  classifiers through problem transformation, ontology engineering, and model
  ensembling},
Eprint        = {2412.14299v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Classification is a fundamental task in machine learning. While conventional
methods-such as binary, multiclass, and multi-label classification-are
effective for simpler problems, they may not adequately address the
complexities of some real-world scenarios. This paper introduces the Multiplex
Classification Framework, a novel approach developed to tackle these and
similar challenges through the integration of problem transformation, ontology
engineering, and model ensembling. The framework offers several advantages,
including adaptability to any number of classes and logical constraints, an
innovative method for managing class imbalance, the elimination of confidence
threshold selection, and a modular structure. Two experiments were conducted to
compare the performance of conventional classification models with the
Multiplex approach. Our results demonstrate that the Multiplex approach can
improve classification performance significantly (up to 10% gain in overall F1
score), particularly in classification problems with a large number of classes
and pronounced class imbalances. However, it also has limitations, as it
requires a thorough understanding of the problem domain and some experience
with ontology engineering, and it involves training multiple models, which can
make the whole process more intricate. Overall, this methodology provides a
valuable tool for researchers and practitioners dealing with complex
classification problems in machine learning.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.14299v1},
File          = {2412.14299v1.pdf}
}
@article{2412.13964v2,
Author        = {Stefano M. Nicoletti and E. Moritz Hahn and Mattia Fumagalli and Giancarlo Guizzardi and MariÃ«lle Stoelinga},
Title         = {WATCHDOG: an ontology-aWare risk AssessmenT approaCH via object-oriented
  DisruptiOn Graphs},
Eprint        = {2412.13964v2},
DOI           = {10.1007/978-3-031-94571-7_18},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {When considering risky events or actions, we must not downplay the role of
involved objects: a charged battery in our phone averts the risk of being
stranded in the desert after a flat tyre, and a functional firewall mitigates
the risk of a hacker intruding the network. The Common Ontology of Value and
Risk (COVER) highlights how the role of objects and their relationships remains
pivotal to performing transparent, complete and accountable risk assessment. In
this paper, we operationalize some of the notions proposed by COVER -- such as
parthood between objects and participation of objects in events/actions -- by
presenting a new framework for risk assessment: WATCHDOG. WATCHDOG enriches the
expressivity of vetted formal models for risk -- i.e., fault trees and attack
trees -- by bridging the disciplines of ontology and formal methods into an
ontology-aware formal framework composed by a more expressive modelling
formalism, Object-Oriented Disruption Graphs (DOGs), logic (DOGLog) and an
intermediate query language (DOGLang). With these, WATCHDOG allows risk
assessors to pose questions about disruption propagation, disruption likelihood
and risk levels, keeping the fundamental role of objects at risk always in
sight.},
Year          = {2024},
Month         = {Dec},
Note          = {(2025). WATCHDOG: an ontology-aWare risk AssessmenT approaCH via
  object-oriented DisruptiOn Graphs. In: Krogstie, J., Rinderle-Ma, S., Kappel,
  G., Proper, H.A. (eds) Advanced Information Systems Engineering. CAiSE 2025},
Url           = {http://arxiv.org/abs/2412.13964v2},
File          = {2412.13964v2.pdf}
}
@article{2412.13799v1,
Author        = {Ramona KÃ¼hn and Jelena MitroviÄ and Michael Granitzer},
Title         = {Enhancing Rhetorical Figure Annotation: An Ontology-Based Web
  Application with RAG Integration},
Eprint        = {2412.13799v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Rhetorical figures play an important role in our communication. They are used
to convey subtle, implicit meaning, or to emphasize statements. We notice them
in hate speech, fake news, and propaganda. By improving the systems for
computational detection of rhetorical figures, we can also improve tasks such
as hate speech and fake news detection, sentiment analysis, opinion mining, or
argument mining. Unfortunately, there is a lack of annotated data, as well as
qualified annotators that would help us build large corpora to train machine
learning models for the detection of rhetorical figures. The situation is
particularly difficult in languages other than English, and for rhetorical
figures other than metaphor, sarcasm, and irony. To overcome this issue, we
develop a web application called "Find your Figure" that facilitates the
identification and annotation of German rhetorical figures. The application is
based on the German Rhetorical ontology GRhOOT which we have specially adapted
for this purpose. In addition, we improve the user experience with Retrieval
Augmented Generation (RAG). In this paper, we present the restructuring of the
ontology, the development of the web application, and the built-in RAG
pipeline. We also identify the optimal RAG settings for our application. Our
approach is one of the first to practically use rhetorical ontologies in
combination with RAG and shows promising results.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.13799v1},
File          = {2412.13799v1.pdf}
}
@article{2412.13688v1,
Author        = {C. Maria Keet and Zubeida Casmod Khan},
Title         = {Discerning and Characterising Types of Competency Questions for
  Ontologies},
Eprint        = {2412.13688v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Competency Questions (CQs) are widely used in ontology development by
guiding, among others, the scoping and validation stages. However, very limited
guidance exists for formulating CQs and assessing whether they are good CQs,
leading to issues such as ambiguity and unusable formulations. To solve this,
one requires insight into the nature of CQs for ontologies and their
constituent parts, as well as which ones are not. We aim to contribute to such
theoretical foundations in this paper, which is informed by analysing
questions, their uses, and the myriad of ontology development tasks. This
resulted in a first Model for Competency Questions, which comprises five main
types of CQs, each with a different purpose: Scoping (SCQ), Validating (VCQ),
Foundational (FCQ), Relationship (RCQ), and Metaproperty (MpCQ) questions. This
model enhances the clarity of CQs and therewith aims to improve on the
effectiveness of CQs in ontology development, thanks to their respective
identifiable distinct constituent elements. We illustrate and evaluate them
with a user story and demonstrate where which type can be used in ontology
development tasks. To foster use and research, we created an annotated
repository of 438 CQs, the Repository of Ontology Competency QuestionS (ROCQS),
incorporating an existing CQ dataset and new CQs and CQ templates, which
further demonstrate distinctions among types of CQs.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.13688v1},
File          = {2412.13688v1.pdf}
}
@article{2412.13140v1,
Author        = {Gordon Lim and Stefan Larson and Kevin Leach},
Title         = {Label Errors in the Tobacco3482 Dataset},
Eprint        = {2412.13140v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Tobacco3482 is a widely used document classification benchmark dataset.
However, our manual inspection of the entire dataset uncovers widespread
ontological issues, especially large amounts of annotation label problems in
the dataset. We establish data label guidelines and find that 11.7% of the
dataset is improperly annotated and should either have an unknown label or a
corrected label, and 16.7% of samples in the dataset have multiple valid
labels. We then analyze the mistakes of a top-performing model and find that
35% of the model's mistakes can be directly attributed to these label issues,
highlighting the inherent problems with using a noisily labeled dataset as a
benchmark. Supplementary material, including dataset annotations and code, is
available at https://github.com/gordon-lim/tobacco3482-mistakes/.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.13140v1},
File          = {2412.13140v1.pdf}
}
@article{2412.13019v1,
Author        = {Luigi Bellomarini and Livia Blasi and Markus Nissl and Emanuel Sallinger},
Title         = {The Temporal Vadalog System: Temporal Datalog-based Reasoning},
Eprint        = {2412.13019v1},
DOI           = {10.1017/S1471068425000018},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {In the wake of the recent resurgence of the Datalog language of databases,
together with its extensions for ontological reasoning settings, this work aims
to bridge the gap between the theoretical studies of DatalogMTL (Datalog
extended with metric temporal logic) and the development of production-ready
reasoning systems. In particular, we lay out the functional and architectural
desiderata of a modern reasoner and propose our system, Temporal Vadalog.
Leveraging the vast amount of experience from the database community, we go
beyond the typical chase-based implementations of reasoners, and propose a set
of novel techniques and a system that adopts a modern data pipeline
architecture. We discuss crucial architectural choices, such as how to
guarantee termination when infinitely many time intervals are possibly
generated, how to merge intervals, and how to sustain a limited memory
footprint. We discuss advanced features of the system, such as the support for
time series, and present an extensive experimental evaluation. This paper is a
substantially extended version of "The Temporal Vadalog System" as presented at
RuleML+RR '22. Under consideration in Theory and Practice of Logic Programming
(TPLP).},
Year          = {2024},
Month         = {Dec},
Note          = {Theory and Practice of Logic Programming 25 (2025) 168-196},
Url           = {http://arxiv.org/abs/2412.13019v1},
File          = {2412.13019v1.pdf}
}
@article{2412.12929v1,
Author        = {Quentin ManiÃ¨re and Marcin PrzybyÅko},
Title         = {Spectra of Cardinality Queries over Description Logic Knowledge Bases},
Eprint        = {2412.12929v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Recent works have explored the use of counting queries coupled with
Description Logic ontologies. The answer to such a query in a model of a
knowledge base is either an integer or $\infty$, and its spectrum is the set of
its answers over all models. While it is unclear how to compute and manipulate
such a set in general, we identify a class of counting queries whose spectra
can be effectively represented. Focusing on atomic counting queries, we
pinpoint the possible shapes of a spectrum over $\mathcal{ALCIF}$ ontologies:
they are essentially the subsets of $\mathbb{N} \cup \{ \infty \}$ closed under
addition. For most sublogics of $\mathcal{ALCIF}$, we show that possible
spectra enjoy simpler shapes, being $[ m, \infty ]$ or variations thereof. To
obtain our results, we refine constructions used for finite model reasoning and
notably rely on a cycle-reversion technique for the Horn fragment of
$\mathcal{ALCIF}$. We also study the data complexity of computing the proposed
effective representation and establish the
$\mathsf{FP}^{\mathsf{NP}[\log]}$-completeness of this task under several
settings.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.12929v1},
File          = {2412.12929v1.pdf}
}
@article{2412.12688v1,
Author        = {Yuwei Miao and Yuzhi Guo and Hehuan Ma and Jingquan Yan and Feng Jiang and Weizhi An and Jean Gao and Junzhou Huang},
Title         = {UniEntrezDB: Large-scale Gene Ontology Annotation Dataset and Evaluation
  Benchmarks with Unified Entrez Gene Identifiers},
Eprint        = {2412.12688v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Gene studies are crucial for fields such as protein structure prediction,
drug discovery, and cancer genomics, yet they face challenges in fully
utilizing the vast and diverse information available. Gene studies require
clean, factual datasets to ensure reliable results. Ontology graphs, neatly
organized domain terminology graphs, provide ideal sources for domain facts.
However, available gene ontology annotations are currently distributed across
various databases without unified identifiers for genes and gene products. To
address these challenges, we introduce Unified Entrez Gene Identifier Dataset
and Benchmarks (UniEntrezDB), the first systematic effort to unify large-scale
public Gene Ontology Annotations (GOA) from various databases using unique gene
identifiers. UniEntrezDB includes a pre-training dataset and four downstream
tasks designed to comprehensively evaluate gene embedding performance from
gene, protein, and cell levels, ultimately enhancing the reliability and
applicability of LLMs in gene research and other professional settings.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.12688v1},
File          = {2412.12688v1.pdf}
}
@article{2412.11483v1,
Author        = {Moming Duan and Rui Zhao and Linshan Jiang and Nigel Shadbolt and Bingsheng He},
Title         = {"They've Stolen My GPL-Licensed Model!": Toward Standardized and
  Transparent Model Licensing},
Eprint        = {2412.11483v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CY},
Abstract      = {As model parameter sizes reach the billion-level range and their training
consumes zettaFLOPs of computation, components reuse and collaborative
development are become increasingly prevalent in the Machine Learning (ML)
community. These components, including models, software, and datasets, may
originate from various sources and be published under different licenses, which
govern the use and distribution of licensed works and their derivatives.
However, commonly chosen licenses, such as GPL and Apache, are
software-specific and are not clearly defined or bounded in the context of
model publishing. Meanwhile, the reused components may also have free-content
licenses and model licenses, which pose a potential risk of license
noncompliance and rights infringement within the model production workflow. In
this paper, we propose addressing the above challenges along two lines: 1) For
license analysis, we have developed a new vocabulary for ML workflow management
and encoded license rules to enable ontological reasoning for analyzing rights
granting and compliance issues. 2) For standardized model publishing, we have
drafted a set of model licenses that provide flexible options to meet the
diverse needs of model publishing. Our analysis tool is built on Turtle
language and Notation3 reasoning engine, envisioned as a first step toward
Linked Open Model Production Data. We have also encoded our proposed model
licenses into rules and demonstrated the effects of GPL and other commonly used
licenses in model publishing, along with the flexibility advantages of our
licenses, through comparisons and experiments.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.11483v1},
File          = {2412.11483v1.pdf}
}
@article{2412.15256v1,
Author        = {Edward Kim and Manil Shrestha and Richard Foty and Tom DeLay and Vicki Seyfert-Margolis},
Title         = {Structured Extraction of Real World Medical Knowledge using LLMs for
  Summarization and Search},
Eprint        = {2412.15256v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Creation and curation of knowledge graphs can accelerate disease discovery
and analysis in real-world data. While disease ontologies aid in biological
data annotation, codified categories (SNOMED-CT, ICD10, CPT) may not capture
patient condition nuances or rare diseases. Multiple disease definitions across
data sources complicate ontology mapping and disease clustering. We propose
creating patient knowledge graphs using large language model extraction
techniques, allowing data extraction via natural language rather than rigid
ontological hierarchies. Our method maps to existing ontologies (MeSH,
SNOMED-CT, RxNORM, HPO) to ground extracted entities.
  Using a large ambulatory care EHR database with 33.6M patients, we
demonstrate our method through the patient search for Dravet syndrome, which
received ICD10 recognition in October 2020. We describe our construction of
patient-specific knowledge graphs and symptom-based patient searches. Using
confirmed Dravet syndrome ICD10 codes as ground truth, we employ LLM-based
entity extraction to characterize patients in grounded ontologies. We then
apply this method to identify Beta-propeller protein-associated
neurodegeneration (BPAN) patients, demonstrating real-world discovery where no
ground truth exists.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.15256v1},
File          = {2412.15256v1.pdf}
}
@article{2412.11043v1,
Author        = {Minhao Bai and Jinshuai Yang and Kaiyi Pang and Yongfeng Huang and Yue Gao},
Title         = {Semantic Steganography: A Framework for Robust and High-Capacity
  Information Hiding using Large Language Models},
Eprint        = {2412.11043v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {In the era of Large Language Models (LLMs), generative linguistic
steganography has become a prevalent technique for hiding information within
model-generated texts. However, traditional steganography methods struggle to
effectively align steganographic texts with original model-generated texts due
to the lower entropy of the predicted probability distribution of LLMs. This
results in a decrease in embedding capacity and poses challenges for decoding
stegos in real-world communication channels. To address these challenges, we
propose a semantic steganography framework based on LLMs, which construct a
semantic space and map secret messages onto this space using ontology-entity
trees. This framework offers robustness and reliability for transmission in
complex channels, as well as resistance to text rendering and word blocking.
Additionally, the stegos generated by our framework are indistinguishable from
the covers and achieve a higher embedding capacity compared to state-of-the-art
steganography methods, while producing higher quality stegos.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.11043v1},
File          = {2412.11043v1.pdf}
}
@article{2501.04008v2,
Author        = {Mayukh Bagchi},
Title         = {A Generative AI-driven Metadata Modelling Approach},
Eprint        = {2501.04008v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {Since decades, the modelling of metadata has been core to the functioning of
any academic library. Its importance has only enhanced with the increasing
pervasiveness of Generative Artificial Intelligence (AI)-driven information
activities and services which constitute a library's outreach. However, with
the rising importance of metadata, there arose several outstanding problems
with the process of designing a library metadata model impacting its
reusability, crosswalk and interoperability with other metadata models. This
paper posits that the above problems stem from an underlying thesis that there
should only be a few core metadata models which would be necessary and
sufficient for any information service using them, irrespective of the
heterogeneity of intra-domain or inter-domain settings. To that end, this paper
advances a contrary view of the above thesis and substantiates its argument in
three key steps. First, it introduces a novel way of thinking about a library
metadata model as an ontology-driven composition of five functionally
interlinked representation levels from perception to its intensional definition
via properties. Second, it introduces the representational manifoldness
implicit in each of the five levels which cumulatively contributes to a
conceptually entangled library metadata model. Finally, and most importantly,
it proposes a Generative AI-driven Human-Large Language Model (LLM)
collaboration based metadata modelling approach to disentangle the entanglement
inherent in each representation level leading to the generation of a
conceptually disentangled metadata model. Throughout the paper, the arguments
are exemplified by motivating scenarios and examples from representative
libraries handling cancer information.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2501.04008v2},
File          = {2501.04008v2.pdf}
}
@article{2412.09770v1,
Author        = {Jonghyuk Park and Alex Lascarides and Subramanian Ramamoorthy},
Title         = {Learning Visually Grounded Domain Ontologies via Embodied Conversation
  and Explanation},
Eprint        = {2412.09770v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In this paper, we offer a learning framework in which the agent's knowledge
gaps are overcome through corrective feedback from a teacher whenever the agent
explains its (incorrect) predictions. We test it in a low-resource visual
processing scenario, in which the agent must learn to recognize distinct types
of toy truck. The agent starts the learning process with no ontology about what
types of trucks exist nor which parts they have, and a deficient model for
recognizing those parts from visual input. The teacher's feedback to the
agent's explanations addresses its lack of relevant knowledge in the ontology
via a generic rule (e.g., "dump trucks have dumpers"), whereas an inaccurate
part recognition is corrected by a deictic statement (e.g., "this is not a
dumper"). The learner utilizes this feedback not only to improve its estimate
of the hypothesis space of possible domain ontologies and probability
distributions over them, but also to use those estimates to update its visual
interpretation of the scene. Our experiments demonstrate that teacher-learner
pairs utilizing explanations and corrections are more data-efficient than those
without such a faculty.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.09770v1},
File          = {2412.09770v1.pdf}
}
@article{2412.09587v2,
Author        = {Chester Palen-Michel and Maxwell Pickering and Maya Kruse and Jonne SÃ¤levÃ¤ and Constantine Lignos},
Title         = {OpenNER 1.0: Standardized Open-Access Named Entity Recognition Datasets
  in 50+ Languages},
Eprint        = {2412.09587v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We present OpenNER 1.0, a standardized collection of openly-available named
entity recognition (NER) datasets. OpenNER contains 36 NER corpora that span 52
languages, human-annotated in varying named entity ontologies. We correct
annotation format issues, standardize the original datasets into a uniform
representation with consistent entity type names across corpora, and provide
the collection in a structure that enables research in multilingual and
multi-ontology NER. We provide baseline results using three pretrained
multilingual language models and two large language models to compare the
performance of recent models and facilitate future research in NER. We find
that no single model is best in all languages and that significant work remains
to obtain high performance from LLMs on the NER task.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.09587v2},
File          = {2412.09587v2.pdf}
}
@article{2412.09223v1,
Author        = {Subhashis Das and Debashis Naskar and Sara Rodriguez Gonzalez},
Title         = {CSSDH: An Ontology for Social Determinants of Health to Operational
  Continuity of Care Data Interoperability},
Eprint        = {2412.09223v1},
DOI           = {10.1007/978-3-031-77731-8_10},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LO},
Abstract      = {The rise of digital platforms has led to an increasing reliance on
technology-driven, home-based healthcare solutions, enabling individuals to
monitor their health and share information with healthcare professionals as
needed. However, creating an efficient care plan management system requires
more than just analyzing hospital summaries and Electronic Health Records
(EHRs). Factors such as individual user needs and social determinants of
health, including living conditions and the flow of healthcare information
between different settings, must also be considered. Challenges in this complex
healthcare network involve schema diversity (in EHRs, personal health records,
etc.) and terminology diversity (e.g., ICD, SNOMED-CT) across ancillary
healthcare operations. Establishing interoperability among various systems and
applications is crucial, with the European Interoperability Framework (EIF)
emphasizing the need for patient-centric access and control of healthcare data.
In this paper, we propose an integrated ontological model, the Common Semantic
Data Model for Social Determinants of Health (CSSDH), by combining ISO/DIS
13940:2024 ContSys with WHO Social Determinants of Health. CSSDH aims to
achieve interoperability within the Continuity of Care Network.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.09223v1},
File          = {2412.09223v1.pdf}
}
@article{2412.15235v1,
Author        = {Kartik Sharma and Peeyush Kumar and Yunqing Li},
Title         = {OG-RAG: Ontology-Grounded Retrieval-Augmented Generation For Large
  Language Models},
Eprint        = {2412.15235v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper presents OG-RAG, an Ontology-Grounded Retrieval Augmented
Generation method designed to enhance LLM-generated responses by anchoring
retrieval processes in domain-specific ontologies. While LLMs are widely used
for tasks like question answering and search, they struggle to adapt to
specialized knowledge, such as industrial workflows or knowledge work, without
expensive fine-tuning or sub-optimal retrieval methods. Existing
retrieval-augmented models, such as RAG, offer improvements but fail to account
for structured domain knowledge, leading to suboptimal context generation.
Ontologies, which conceptually organize domain knowledge by defining entities
and their interrelationships, offer a structured representation to address this
gap. OG-RAG constructs a hypergraph representation of domain documents, where
each hyperedge encapsulates clusters of factual knowledge grounded using
domain-specific ontology. An optimization algorithm then retrieves the minimal
set of hyperedges that constructs a precise, conceptually grounded context for
the LLM. This method enables efficient retrieval while preserving the complex
relationships between entities. OG-RAG applies to domains where fact-based
reasoning is essential, particularly in tasks that require workflows or
decision-making steps to follow predefined rules and procedures. These include
industrial workflows in healthcare, legal, and agricultural sectors, as well as
knowledge-driven tasks such as news journalism, investigative research,
consulting and more. Our evaluations demonstrate that OG-RAG increases the
recall of accurate facts by 55% and improves response correctness by 40% across
four different LLMs. Additionally, OG-RAG enables 30% faster attribution of
responses to context and boosts fact-based reasoning accuracy by 27% compared
to baseline methods.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.15235v1},
File          = {2412.15235v1.pdf}
}
@article{2412.08742v1,
Author        = {Udari Madhushani Sehwag and Kassiani Papasotiriou and Jared Vann and Sumitra Ganesh},
Title         = {In-Context Learning with Topological Information for Knowledge Graph
  Completion},
Eprint        = {2412.08742v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graphs (KGs) are crucial for representing and reasoning over
structured information, supporting a wide range of applications such as
information retrieval, question answering, and decision-making. However, their
effectiveness is often hindered by incompleteness, limiting their potential for
real-world impact. While knowledge graph completion (KGC) has been extensively
studied in the literature, recent advances in generative AI models,
particularly large language models (LLMs), have introduced new opportunities
for innovation. In-context learning has recently emerged as a promising
approach for leveraging pretrained knowledge of LLMs across a range of natural
language processing tasks and has been widely adopted in both academia and
industry. However, how to utilize in-context learning for effective KGC remains
relatively underexplored. We develop a novel method that incorporates
topological information through in-context learning to enhance KGC performance.
By integrating ontological knowledge and graph structure into the context of
LLMs, our approach achieves strong performance in the transductive setting
i.e., nodes in the test graph dataset are present in the training graph
dataset. Furthermore, we apply our approach to KGC in the more challenging
inductive setting, i.e., nodes in the training graph dataset and test graph
dataset are disjoint, leveraging the ontology to infer useful information about
missing nodes which serve as contextual cues for the LLM during inference. Our
method demonstrates superior performance compared to baselines on the
ILPC-small and ILPC-large datasets.},
Year          = {2024},
Month         = {Dec},
Note          = {Proceedings of the ICML 2024 Workshop on Structured Probabilistic
  Inference & Generative Modeling},
Url           = {http://arxiv.org/abs/2412.08742v1},
File          = {2412.08742v1.pdf}
}
@article{2412.08739v1,
Author        = {Atalay Mert Ileri and Nalen Rangarajan and Jack Cannell and Hande McGinty},
Title         = {VEL: A Formally Verified Reasoner for OWL2 EL Profile},
Eprint        = {2412.08739v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LO},
Abstract      = {Over the past two decades, the Web Ontology Language (OWL) has been
instrumental in advancing the development of ontologies and knowledge graphs,
providing a structured framework that enhances the semantic integration of
data. However, the reliability of deductive reasoning within these systems
remains challenging, as evidenced by inconsistencies among popular reasoners in
recent competitions. This evidence underscores the limitations of current
testing-based methodologies, particularly in high-stakes domains such as
healthcare. To mitigate these issues, in this paper, we have developed VEL, a
formally verified EL++ reasoner equipped with machine-checkable correctness
proofs that ensure the validity of outputs across all possible inputs. This
formalization, based on the algorithm of Baader et al., has been transformed
into executable OCaml code using the Coq proof assistant's extraction
capabilities. Our formalization revealed several errors in the original
completeness proofs, which led to changes to the algorithm to ensure its
completeness. Our work demonstrates the necessity of mechanization of reasoning
algorithms to ensure their correctness at theoretical and implementation
levels.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.08739v1},
File          = {2412.08739v1.pdf}
}
@article{2412.08258v2,
Author        = {Tanay Aggarwal and Angelo Salatino and Francesco Osborne and Enrico Motta},
Title         = {Large Language Models for Scholarly Ontology Generation: An Extensive
  Analysis in the Engineering Field},
Eprint        = {2412.08258v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {Ontologies of research topics are crucial for structuring scientific
knowledge, enabling scientists to navigate vast amounts of research, and
forming the backbone of intelligent systems such as search engines and
recommendation systems. However, manual creation of these ontologies is
expensive, slow, and often results in outdated and overly general
representations. As a solution, researchers have been investigating ways to
automate or semi-automate the process of generating these ontologies. This
paper offers a comprehensive analysis of the ability of large language models
(LLMs) to identify semantic relationships between different research topics,
which is a critical step in the development of such ontologies. To this end, we
developed a gold standard based on the IEEE Thesaurus to evaluate the task of
identifying four types of relationships between pairs of topics: broader,
narrower, same-as, and other. Our study evaluates the performance of seventeen
LLMs, which differ in scale, accessibility (open vs. proprietary), and model
type (full vs. quantised), while also assessing four zero-shot reasoning
strategies. Several models have achieved outstanding results, including
Mixtral-8x7B, Dolphin-Mistral-7B, and Claude 3 Sonnet, with F1-scores of 0.847,
0.920, and 0.967, respectively. Furthermore, our findings demonstrate that
smaller, quantised models, when optimised through prompt engineering, can
deliver performance comparable to much larger proprietary models, while
requiring significantly fewer computational resources.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.08258v2},
File          = {2412.08258v2.pdf}
}
@article{2412.14191v1,
Author        = {Chengshuai Zhao and Garima Agrawal and Tharindu Kumarage and Zhen Tan and Yuli Deng and Ying-Chih Chen and Huan Liu},
Title         = {Ontology-Aware RAG for Improved Question-Answering in Cybersecurity
  Education},
Eprint        = {2412.14191v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CY},
Abstract      = {Integrating AI into education has the potential to transform the teaching of
science and technology courses, particularly in the field of cybersecurity.
AI-driven question-answering (QA) systems can actively manage uncertainty in
cybersecurity problem-solving, offering interactive, inquiry-based learning
experiences. Large language models (LLMs) have gained prominence in AI-driven
QA systems, offering advanced language understanding and user engagement.
However, they face challenges like hallucinations and limited domain-specific
knowledge, which reduce their reliability in educational settings. To address
these challenges, we propose CyberRAG, an ontology-aware retrieval-augmented
generation (RAG) approach for developing a reliable and safe QA system in
cybersecurity education. CyberRAG employs a two-step approach: first, it
augments the domain-specific knowledge by retrieving validated cybersecurity
documents from a knowledge base to enhance the relevance and accuracy of the
response. Second, it mitigates hallucinations and misuse by integrating a
knowledge graph ontology to validate the final answer. Experiments on publicly
available cybersecurity datasets show that CyberRAG delivers accurate, reliable
responses aligned with domain knowledge, demonstrating the potential of AI
tools to enhance education.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.14191v1},
File          = {2412.14191v1.pdf}
}
@article{2412.07743v1,
Author        = {Zijian Chen and John-Michael Gamble and Micaela Jantzi and John P. Hirdes and Jimmy Lin},
Title         = {Zero-Shot ATC Coding with Large Language Models for Clinical Assessments},
Eprint        = {2412.07743v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Manual assignment of Anatomical Therapeutic Chemical (ATC) codes to
prescription records is a significant bottleneck in healthcare research and
operations at Ontario Health and InterRAI Canada, requiring extensive expert
time and effort. To automate this process while maintaining data privacy, we
develop a practical approach using locally deployable large language models
(LLMs). Inspired by recent advances in automatic International Classification
of Diseases (ICD) coding, our method frames ATC coding as a hierarchical
information extraction task, guiding LLMs through the ATC ontology level by
level. We evaluate our approach using GPT-4o as an accuracy ceiling and focus
development on open-source Llama models suitable for privacy-sensitive
deployment. Testing across Health Canada drug product data, the RABBITS
benchmark, and real clinical notes from Ontario Health, our method achieves 78%
exact match accuracy with GPT-4o and 60% with Llama 3.1 70B. We investigate
knowledge grounding through drug definitions, finding modest improvements in
accuracy. Further, we show that fine-tuned Llama 3.1 8B matches zero-shot Llama
3.1 70B accuracy, suggesting that effective ATC coding is feasible with smaller
models. Our results demonstrate the feasibility of automatic ATC coding in
privacy-sensitive healthcare environments, providing a foundation for future
deployments.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.07743v1},
File          = {2412.07743v1.pdf}
}
@article{2412.06077v1,
Author        = {Martin Thomas Horsch and Fadi Al Machot and Jadran Vrabec},
Title         = {Scope of physics-based simulation artefacts},
Eprint        = {2412.06077v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {physics.comp-ph},
Abstract      = {Data and metadata documentation requirements for explainable-AI-ready (XAIR)
models and data in physics-based simulation technology are discussed by
analysing different perspectives from the literature on two core aspects:
First, the scope of the simulation; this category is taken to include subject
matter, the objective with which the simulation is conducted, and the object of
reference, i.e., the simulated physical system or process. Second, the
artefacts that need to be documented in order to make data and models XAIR, and
modelling and simulation workflows explainable; two CEN workshop agreements,
MODA and ModGra, are compared for this purpose. As a result, minimum
requirements for an ontologization of the scope of simulation artefacts are
formulated, and the object-objective abstractness diagram is proposed as a tool
for visualizing the landscape of use cases for physics-based simulation.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.06077v1},
File          = {2412.06077v1.pdf}
}
@article{2412.05868v1,
Author        = {Vijayalaxmi Sahadevan and Sushil Mario and Yash Jaiswal and Divyanshu Bajpai and Vishal Singh and Hiralal Aggarwal and Suhas Suresh and Manjunath Maigur},
Title         = {Automated Extraction and Creation of FBS Design Reasoning Knowledge
  Graphs from Structured Data in Product Catalogues Lacking Contextual
  Information},
Eprint        = {2412.05868v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Ontology-based knowledge graphs (KG) are desirable for effective knowledge
management and reuse in various decision making scenarios, including design.
Creating and populating extensive KG based on specific ontological models can
be highly labour and time-intensive unless automated processes are developed
for knowledge extraction and graph creation. Most research and development on
automated extraction and creation of KG is based on extensive unstructured data
sets that provide contextual information. However, some of the most useful
information about the products and services of a company has traditionally been
recorded as structured data. Such structured data sets rarely follow a standard
ontology, do not capture explicit mapping of relationships between the
entities, and provide no contextual information. Therefore, this research
reports a method and digital workflow developed to address this gap. The
developed method and workflow employ rule-based techniques to extract and
create a Function Behaviour-Structure (FBS) ontology-based KG from legacy
structured data, especially specification sheets and product catalogues. The
solution approach consists of two main components: a process for deriving
context and context-based classification rules for FBS ontology concepts and a
workflow for populating and retrieving the FBS ontology-based KG. KG and
Natural Language Processing (NLP) are used to automate knowledge extraction,
representation, and retrieval. The workflow's effectiveness is demonstrated via
pilot implementation in an industrial context. Insights gained from the pilot
study are reported regarding the challenges and opportunities, including
discussing the FBS ontology and concepts.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.05868v1},
File          = {2412.05868v1.pdf}
}
@article{2412.05776v1,
Author        = {Azwad Tamir and Jiann-Shiun Yuan},
Title         = {ProtGO: A Transformer based Fusion Model for accurately predicting Gene
  Ontology (GO) Terms from full scale Protein Sequences},
Eprint        = {2412.05776v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Recent developments in next generation sequencing technology have led to the
creation of extensive, open-source protein databases consisting of hundreds of
millions of sequences. To render these sequences applicable in biomedical
applications, they must be meticulously annotated by wet lab testing or
extracting them from existing literature. Over the last few years, researchers
have developed numerous automatic annotation systems, particularly deep
learning models based on machine learning and artificial intelligence, to
address this issue. In this work, we propose a transformer-based fusion model
capable of predicting Gene Ontology (GO) terms from full-scale protein
sequences, achieving state-of-the-art accuracy compared to other contemporary
machine learning annotation systems. The approach performs particularly well on
clustered split datasets, which comprise training and testing samples
originating from distinct distributions that are structurally diverse. This
demonstrates that the model is able to understand both short and long term
dependencies within the enzyme's structure and can precisely identify the
motifs associated with the various GO terms. Furthermore, the technique is
lightweight and less computationally expensive compared to the benchmark
methods, while at the same time not unaffected by sequence length, rendering it
appropriate for diverse applications with varying sequence lengths.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.05776v1},
File          = {2412.05776v1.pdf}
}
@article{2412.04846v1,
Author        = {Ye Sun and Lei Shi and Yongxin Tong},
Title         = {eXpath: Explaining Knowledge Graph Link Prediction with Ontological
  Closed Path Rules},
Eprint        = {2412.04846v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Link prediction (LP) is crucial for Knowledge Graphs (KG) completion but
commonly suffers from interpretability issues. While several methods have been
proposed to explain embedding-based LP models, they are generally limited to
local explanations on KG and are deficient in providing human interpretable
semantics. Based on real-world observations of the characteristics of KGs from
multiple domains, we propose to explain LP models in KG with path-based
explanations. An integrated framework, namely eXpath, is introduced which
incorporates the concept of relation path with ontological closed path rules to
enhance both the efficiency and effectiveness of LP interpretation. Notably,
the eXpath explanations can be fused with other single-link explanation
approaches to achieve a better overall solution. Extensive experiments across
benchmark datasets and LP models demonstrate that introducing eXpath can boost
the quality of resulting explanations by about 20% on two key metrics and
reduce the required explanation time by 61.4%, in comparison to the best
existing method. Case studies further highlight eXpath's ability to provide
more semantically meaningful explanations through path-based evidence.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.04846v1},
File          = {2412.04846v1.pdf}
}
@article{2412.04529v1,
Author        = {Alexander Chervov and Anton Vakhrushev and Sergei Fironov and Loredana Martignetti},
Title         = {ProtBoost: protein function prediction with Py-Boost and Graph Neural
  Networks -- CAFA5 top2 solution},
Eprint        = {2412.04529v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.QM},
Abstract      = {Predicting protein properties, functions and localizations are important
tasks in bioinformatics. Recent progress in machine learning offers an
opportunities for improving existing methods. We developed a new approach
called ProtBoost, which relies on the strength of pretrained protein language
models, the new Py-Boost gradient boosting method and Graph Neural Networks
(GCN). The ProtBoost method was ranked second best model in the recent Critical
Assessment of Functional Annotation (CAFA5) international challenge with more
than 1600 participants. Py-Boost is the first gradient boosting method capable
of predicting thousands of targets simultaneously, making it an ideal fit for
tasks like the CAFA challange. Our GCN-based approach performs stacking of many
individual models and boosts the performance significantly. Notably, it can be
applied to any task where targets are arranged in a hierarchical structure,
such as Gene Ontology. Additionally, we introduced new methods for leveraging
the graph structure of targets and present an analysis of protein language
models for protein function prediction task. ProtBoost is publicly available
at: https://github.com/btbpanda/CAFA5-protein-function-prediction-2nd-place.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.04529v1},
File          = {2412.04529v1.pdf}
}
@article{2412.03176v1,
Author        = {Leon-Paul Schaub Torre and Pelayo Quiros and Helena Garcia Mieres},
Title         = {Automatic detection of diseases in Spanish clinical notes combining
  medical language models and ontologies},
Eprint        = {2412.03176v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this paper we present a hybrid method for the automatic detection of
dermatological pathologies in medical reports. We use a large language model
combined with medical ontologies to predict, given a first appointment or
follow-up medical report, the pathology a person may suffer from. The results
show that teaching the model to learn the type, severity and location on the
body of a dermatological pathology, as well as in which order it has to learn
these three features, significantly increases its accuracy. The article
presents the demonstration of state-of-the-art results for classification of
medical texts with a precision of 0.84, micro and macro F1-score of 0.82 and
0.75, and makes both the method and the data set used available to the
community.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.03176v1},
File          = {2412.03176v1.pdf}
}
@article{2412.02035v1,
Author        = {Nadeen Fathallah and Steffen Staab and Alsayed Algergawy},
Title         = {LLMs4Life: Large Language Models for Ontology Learning in Life Sciences},
Eprint        = {2412.02035v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontology learning in complex domains, such as life sciences, poses
significant challenges for current Large Language Models (LLMs). Existing LLMs
struggle to generate ontologies with multiple hierarchical levels, rich
interconnections, and comprehensive class coverage due to constraints on the
number of tokens they can generate and inadequate domain adaptation. To address
these issues, we extend the NeOn-GPT pipeline for ontology learning using LLMs
with advanced prompt engineering techniques and ontology reuse to enhance the
generated ontologies' domain-specific reasoning and structural depth. Our work
evaluates the capabilities of LLMs in ontology learning in the context of
highly specialized and complex domains such as life science domains. To assess
the logical consistency, completeness, and scalability of the generated
ontologies, we use the AquaDiva ontology developed and used in the
collaborative research center AquaDiva as a case study. Our evaluation shows
the viability of LLMs for ontology learning in specialized domains, providing
solutions to longstanding limitations in model performance and scalability.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.02035v1},
File          = {2412.02035v1.pdf}
}
@article{2412.01331v1,
Author        = {Elizabeth Remfry and Rafael Henkin and Michael R Barnes and Aakanksha Naik},
Title         = {Exploring Long-Term Prediction of Type 2 Diabetes Microvascular
  Complications},
Eprint        = {2412.01331v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Electronic healthcare records (EHR) contain a huge wealth of data that can
support the prediction of clinical outcomes. EHR data is often stored and
analysed using clinical codes (ICD10, SNOMED), however these can differ across
registries and healthcare providers. Integrating data across systems involves
mapping between different clinical ontologies requiring domain expertise, and
at times resulting in data loss. To overcome this, code-agnostic models have
been proposed. We assess the effectiveness of a code-agnostic representation
approach on the task of long-term microvascular complication prediction for
individuals living with Type 2 Diabetes. Our method encodes individual EHRs as
text using fine-tuned, pretrained clinical language models. Leveraging
large-scale EHR data from the UK, we employ a multi-label approach to
simultaneously predict the risk of microvascular complications across 1-, 5-,
and 10-year windows. We demonstrate that a code-agnostic approach outperforms a
code-based model and illustrate that performance is better with longer
prediction windows but is biased to the first occurring complication. Overall,
we highlight that context length is vitally important for model performance.
This study highlights the possibility of including data from across different
clinical ontologies and is a starting point for generalisable clinical models.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.01331v1},
File          = {2412.01331v1.pdf}
}
@article{2412.00830v1,
Author        = {Eyad Algahtani},
Title         = {SPILDL: A Scalable and Parallel Inductive Learner in Description Logic},
Eprint        = {2412.00830v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We present SPILDL, a Scalable and Parallel Inductive Learner in Description
Logic (DL). SPILDL is based on the DL-Learner (the state of the art in DL-based
ILP learning). As a DL-based ILP learner, SPILDL targets the
$\mathcal{ALCQI}^{\mathcal{(D)}}$ DL language, and can learn DL hypotheses
expressed as disjunctions of conjunctions (using the $\sqcup$ operator).
Moreover, SPILDL's hypothesis language also incorporates the use of string
concrete roles (also known as string data properties in the Web Ontology
Language, OWL); As a result, this incorporation of powerful DL constructs,
enables SPILDL to learn powerful DL-based hypotheses for describing many
real-world complex concepts. SPILDL employs a hybrid parallel approach which
combines both shared-memory and distributed-memory approaches, to accelerates
ILP learning (for both hypothesis search and evaluation). According to
experimental results, SPILDL's parallel search improved performance by up to
$\sim$27.3 folds (best case). For hypothesis evaluation, SPILDL improved
evaluation performance through HT-HEDL (our multi-core CPU + multi-GPU
hypothesis evaluation engine), by up to 38 folds (best case). By combining both
parallel search and evaluation, SPILDL improved performance by up to $\sim$560
folds (best case). In terms of worst case scenario, SPILDL's parallel search
doesn't provide consistent speedups on all datasets, and is highly dependent on
the search space nature of the ILP dataset. For some datasets, increasing the
number of parallel search threads result in reduced performance, similar or
worse than baseline. Some ILP datasets benefit from parallel search, while
others don't (or the performance gains are negligible). In terms of parallel
evaluation, on small datasets, parallel evaluation provide similar or worse
performance than baseline.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.00830v1},
File          = {2412.00830v1.pdf}
}
@article{2412.00608v3,
Author        = {Mohammad Sadeq Abolhasani and Rong Pan},
Title         = {Leveraging LLM for Automated Ontology Extraction and Knowledge Graph
  Generation},
Eprint        = {2412.00608v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Extracting relevant and structured knowledge from large, complex technical
documents within the Reliability and Maintainability (RAM) domain is
labor-intensive and prone to errors. Our work addresses this challenge by
presenting OntoKGen, a genuine pipeline for ontology extraction and Knowledge
Graph (KG) generation. OntoKGen leverages Large Language Models (LLMs) through
an interactive user interface guided by our adaptive iterative Chain of Thought
(CoT) algorithm to ensure that the ontology extraction process and, thus, KG
generation align with user-specific requirements. Although KG generation
follows a clear, structured path based on the confirmed ontology, there is no
universally correct ontology as it is inherently based on the user's
preferences. OntoKGen recommends an ontology grounded in best practices,
minimizing user effort and providing valuable insights that may have been
overlooked, all while giving the user complete control over the final ontology.
Having generated the KG based on the confirmed ontology, OntoKGen enables
seamless integration into schemeless, non-relational databases like Neo4j. This
integration allows for flexible storage and retrieval of knowledge from
diverse, unstructured sources, facilitating advanced querying, analysis, and
decision-making. Moreover, the generated KG serves as a robust foundation for
future integration into Retrieval Augmented Generation (RAG) systems, offering
enhanced capabilities for developing domain-specific intelligent applications.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2412.00608v3},
File          = {2412.00608v3.pdf}
}
@article{2411.19918v1,
Author        = {Livio Robaldo and Gianluca Pozzato},
Title         = {Handling irresolvable conflicts in the Semantic Web: an RDF-based
  conflict-tolerant version of the Deontic Traditional Scheme},
Eprint        = {2411.19918v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper presents a new ontology that implements the well-known Deontic
Traditional Scheme in RDFs and SPARQL, fit to handle irresolvable conflicts,
i.e., situations in which two or more statements prescribe conflicting
obligations, prohibitions, or permissions, with none of them being "stronger"
than the other one(s). In our view, this paper marks a significant advancement
in standard theoretical research in formal Deontic Logic. Most contemporary
approaches in this field are confined to the propositional level, mainly focus
on the notion of obligation, and lack implementations. The proposed framework
is encoded in RDF, which is not only a first-order language but also the most
widely used knowledge representation language, as it forms the foundation of
the Semantic Web. Moreover, the proposed computational ontology formalizes all
deontic modalities defined in the Deontic Traditional Scheme, without
specifically focusing on obligations, and offers constructs to model and reason
with various types of irresolvable conflicts, violations, and the interaction
between deontic modalities and contextual constraints in a given state of
affairs. To the best of our knowledge, no existing approach in the literature
addresses all these aspects within a unified integrated framework. All examples
presented and discussed in this paper, together with Java code and clear
instructions to re-execute them locally, are available at
https://github.com/liviorobaldo/conflict-tolerantDeonticTraditionalScheme},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.19918v1},
File          = {2411.19918v1.pdf}
}
@article{2411.19113v1,
Author        = {Eduard Manziuk and Oleksander Barmak and Pavlo Radiuk and Vladislav Kuznetsov and Iurii Krak and Sergiy Yakovlev},
Title         = {Integration of Contextual Descriptors in Ontology Alignment for
  Enrichment of Semantic Correspondence},
Eprint        = {2411.19113v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper proposes a novel approach to semantic ontology alignment using
contextual descriptors. A formalization was developed that enables the
integration of essential and contextual descriptors to create a comprehensive
knowledge model. The hierarchical structure of the semantic approach and the
mathematical apparatus for analyzing potential conflicts between concepts,
particularly in the example of "Transparency" and "Privacy" in the context of
artificial intelligence, are demonstrated. Experimental studies showed a
significant improvement in ontology alignment metrics after the implementation
of contextual descriptors, especially in the areas of privacy, responsibility,
and freedom & autonomy. The application of contextual descriptors achieved an
average overall improvement of approximately 4.36%. The results indicate the
effectiveness of the proposed approach for more accurately reflecting the
complexity of knowledge and its contextual dependence.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.19113v1},
File          = {2411.19113v1.pdf}
}
@article{2411.18520v1,
Author        = {Yichen Wang and Jie Wang and Fulin Wang and Xiang Li and Hao Yin and Bhiksha Raj},
Title         = {Perturbation Ontology based Graph Attention Networks},
Eprint        = {2411.18520v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {In recent years, graph representation learning has undergone a paradigm
shift, driven by the emergence and proliferation of graph neural networks
(GNNs) and their heterogeneous counterparts. Heterogeneous GNNs have shown
remarkable success in extracting low-dimensional embeddings from complex graphs
that encompass diverse entity types and relationships. While meta-path-based
techniques have long been recognized for their ability to capture semantic
affinities among nodes, their dependence on manual specification poses a
significant limitation. In contrast, matrix-focused methods accelerate
processing by utilizing structural cues but often overlook contextual richness.
In this paper, we challenge the current paradigm by introducing ontology as a
fundamental semantic primitive within complex graphs. Our goal is to integrate
the strengths of both matrix-centric and meta-path-based approaches into a
unified framework. We propose perturbation Ontology-based Graph Attention
Networks (POGAT), a novel methodology that combines ontology subgraphs with an
advanced self-supervised learning paradigm to achieve a deep contextual
understanding. The core innovation of POGAT lies in our enhanced homogeneous
perturbing scheme designed to generate rigorous negative samples, encouraging
the model to explore minimal contextual features more thoroughly. Through
extensive empirical evaluations, we demonstrate that POGAT significantly
outperforms state-of-the-art baselines, achieving a groundbreaking improvement
of up to 10.78\% in F1-score for the critical task of link prediction and
12.01\% in Micro-F1 for the critical task of node classification.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.18520v1},
File          = {2411.18520v1.pdf}
}
@article{2412.03589v1,
Author        = {Valentina Anita Carriero and Antonia Azzini and Ilaria Baroni and Mario Scrocca and Irene Celino},
Title         = {Human Evaluation of Procedural Knowledge Graph Extraction from Text with
  Large Language Models},
Eprint        = {2412.03589v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Procedural Knowledge is the know-how expressed in the form of sequences of
steps needed to perform some tasks. Procedures are usually described by means
of natural language texts, such as recipes or maintenance manuals, possibly
spread across different documents and systems, and their interpretation and
subsequent execution is often left to the reader. Representing such procedures
in a Knowledge Graph (KG) can be the basis to build digital tools to support
those users who need to apply or execute them. In this paper, we leverage Large
Language Model (LLM) capabilities and propose a prompt engineering approach to
extract steps, actions, objects, equipment and temporal information from a
textual procedure, in order to populate a Procedural KG according to a
pre-defined ontology. We evaluate the KG extraction results by means of a user
study, in order to qualitatively and quantitatively assess the perceived
quality and usefulness of the LLM-extracted procedural knowledge. We show that
LLMs can produce outputs of acceptable quality and we assess the subjective
perception of AI by human evaluators.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2412.03589v1},
File          = {2412.03589v1.pdf}
}
@article{2411.16609v1,
Author        = {Ansgar Scherp and Thomas Franz and Carsten Saathoff and Steffen Staab},
Title         = {F -- A Model of Events based on the Foundational Ontology DOLCE+DnS
  Ultralite},
Eprint        = {2411.16609v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The lack of a formal model of events hinders interoperability in distributed
event-based systems. In this paper, we present a formal model of events, called
Event-Model-F. The model is based on the foundational ontology DOLCE+DnS
Ultralite (DUL) and provides comprehensive support to represent time and space,
objects and persons, as well as mereological, causal, and correlative
relationships between events. In addition, the Event-Model-F provides a
flexible means for event composition, modeling event causality and event
correlation, and representing different interpretations of the same event. The
Event-Model-F is developed following the pattern-oriented approach of DUL, is
modularized in different ontologies, and can be easily extended by domain
specific ontologies.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.16609v1},
File          = {2411.16609v1.pdf}
}
@article{2411.15672v1,
Author        = {Damodar Panigrahi and Shaswata Mitra and Subash Neupane and Sudip Mittal and Benjamin A. Blakely},
Title         = {IRSKG: Unified Intrusion Response System Knowledge Graph Ontology for
  Cyber Defense},
Eprint        = {2411.15672v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {Cyberattacks are becoming increasingly difficult to detect and prevent due to
their sophistication. In response, Autonomous Intelligent Cyber-defense Agents
(AICAs) are emerging as crucial solutions. One prominent AICA agent is the
Intrusion Response System (IRS), which is critical for mitigating threats after
detection. IRS uses several Tactics, Techniques, and Procedures (TTPs) to
mitigate attacks and restore the infrastructure to normal operations.
Continuous monitoring of the enterprise infrastructure is an essential TTP the
IRS uses. However, each system serves different purposes to meet operational
needs. Integrating these disparate sources for continuous monitoring increases
pre-processing complexity and limits automation, eventually prolonging critical
response time for attackers to exploit. We propose a unified IRS Knowledge
Graph ontology (IRSKG) that streamlines the onboarding of new enterprise
systems as a source for the AICAs. Our ontology can capture system monitoring
logs and supplemental data, such as a rules repository containing the
administrator-defined policies to dictate the IRS responses. Besides, our
ontology permits us to incorporate dynamic changes to adapt to the evolving
cyber-threat landscape. This robust yet concise design allows machine learning
models to train effectively and recover a compromised system to its desired
state autonomously with explainability.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.15672v1},
File          = {2411.15672v1.pdf}
}
@article{2411.15666v1,
Author        = {Gaya Mehenni and Amal Zouaq},
Title         = {Ontology-Constrained Generation of Domain-Specific Clinical Summaries},
Eprint        = {2411.15666v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) offer promising solutions for text
summarization. However, some domains require specific information to be
available in the summaries. Generating these domain-adapted summaries is still
an open challenge. Similarly, hallucinations in generated content is a major
drawback of current approaches, preventing their deployment. This study
proposes a novel approach that leverages ontologies to create domain-adapted
summaries both structured and unstructured. We employ an ontology-guided
constrained decoding process to reduce hallucinations while improving
relevance. When applied to the medical domain, our method shows potential in
summarizing Electronic Health Records (EHRs) across different specialties,
allowing doctors to focus on the most relevant information to their domain.
Evaluation on the MIMIC-III dataset demonstrates improvements in generating
domain-adapted summaries of clinical notes and hallucination reduction.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.15666v1},
File          = {2411.15666v1.pdf}
}
@article{2412.08649v1,
Author        = {SerbÃ¼lent Ãnsal and Sinem Ãzdemir and BÃ¼nyamin Kasap and M. ErÅan KalaycÄ± and Kemal Turhan and Tunca DoÄan and Aybar C. Acar},
Title         = {Multi-modal Representation Learning Enables Accurate Protein Function
  Prediction in Low-Data Setting},
Eprint        = {2412.08649v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.BM},
Abstract      = {In this study, we propose HOPER (HOlistic ProtEin Representation), a novel
multimodal learning framework designed to enhance protein function prediction
(PFP) in low-data settings. The challenge of predicting protein functions is
compounded by the limited availability of labeled data. Traditional machine
learning models already struggle in such cases, and while deep learning models
excel with abundant data, they also face difficulties when data is scarce.
HOPER addresses this issue by integrating three distinct modalities - protein
sequences, biomedical text, and protein-protein interaction (PPI) networks - to
create a comprehensive protein representation. The model utilizes autoencoders
to generate holistic embeddings, which are then employed for PFP tasks using
transfer learning. HOPER outperforms existing methods on a benchmark dataset
across all Gene Ontology categories, i.e., molecular function, biological
process, and cellular component. Additionally, we demonstrate its practical
utility by identifying new immune-escape proteins in lung adenocarcinoma,
offering insights into potential therapeutic targets. Our results highlight the
effectiveness of multimodal representation learning for overcoming data
limitations in biological research, potentially enabling more accurate and
scalable protein function prediction. HOPER source code and datasets are
available at https://github.com/kansil/HOPER},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2412.08649v1},
File          = {2412.08649v1.pdf}
}
@article{2411.14858v1,
Author        = {Alberto Bernardi and Luca Costabello},
Title         = {Domain and Range Aware Synthetic Negatives Generation for Knowledge
  Graph Embedding Models},
Eprint        = {2411.14858v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge Graph Embedding models, representing entities and edges in a
low-dimensional space, have been extremely successful at solving tasks related
to completing and exploring Knowledge Graphs (KGs). One of the key aspects of
training most of these models is teaching to discriminate between true
statements positives and false ones (negatives). However, the way in which
negatives can be defined is not trivial, as facts missing from the KG are not
necessarily false and a set of ground truth negatives is hardly ever given.
This makes synthetic negative generation a necessity. Different generation
strategies can heavily affect the quality of the embeddings, making it a
primary aspect to consider. We revamp a strategy that generates corruptions
during training respecting the domain and range of relations, we extend its
capabilities and we show our methods bring substantial improvement (+10% MRR)
for standard benchmark datasets and over +150% MRR for a larger ontology-backed
dataset.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.14858v1},
File          = {2411.14858v1.pdf}
}
@article{2411.14330v2,
Author        = {Thomas Gilray and Arash Sahebolamri and Yihao Sun and Sowmith Kunapaneni and Sidharth Kumar and Kristopher Micinski},
Title         = {Datalog with First-Class Facts},
Eprint        = {2411.14330v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Datalog is a popular logic programming language for deductive reasoning tasks
in a wide array of applications, including business analytics, program
analysis, and ontological reasoning. However, Datalog's restriction to flat
facts over atomic constants leads to challenges in working with tree-structured
data, such as derivation trees or abstract syntax trees. To ameliorate
Datalog's restrictions, popular extensions of Datalog support features such as
existential quantification in rule heads (Datalog$^\pm$, Datalog$^\exists$) or
algebraic data types (Souffl\'e). Unfortunately, these are imperfect solutions
for reasoning over structured and recursive data types, with general
existentials leading to complex implementations requiring unification, and ADTs
unable to trigger rule evaluation and failing to support efficient indexing.
  We present DL$^{\exists!}$, a Datalog with first-class facts, wherein every
fact is identified with a Skolem term unique to the fact. We show that this
restriction offers an attractive price point for Datalog-based reasoning over
tree-shaped data, demonstrating its application to databases, artificial
intelligence, and programming languages. We implemented DL$^{\exists!}$ as a
system \slog{}, which leverages the uniqueness restriction of DL$^{\exists!}$
to enable a communication-avoiding, massively-parallel implementation built on
MPI. We show that Slog outperforms leading systems (Nemo, Vlog, RDFox, and
Souffl\'e) on a variety of benchmarks, with the potential to scale to thousands
of threads.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.14330v2},
File          = {2411.14330v2.pdf}
}
@article{2411.11090v2,
Author        = {Jingyun Sun and Zhongze Luo},
Title         = {ForPKG: A Framework for Constructing Forestry Policy Knowledge Graph and
  Application Analysis},
Eprint        = {2411.11090v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {A policy knowledge graph can provide decision support for tasks such as
project compliance, policy analysis, and intelligent question answering, and
can also serve as an external knowledge base to assist the reasoning process of
related large language models. Although there have been many related works on
knowledge graphs, there is currently a lack of research on the construction
methods of policy knowledge graphs. This paper, focusing on the forestry field,
designs a complete policy knowledge graph construction framework, including:
firstly, proposing a fine-grained forestry policy domain ontology; then,
proposing an unsupervised policy information extraction method, and finally,
constructing a complete forestry policy knowledge graph. The experimental
results show that the proposed ontology has good expressiveness and
extensibility, and the policy information extraction method proposed in this
paper achieves better results than other unsupervised methods. Furthermore, by
analyzing the application of the knowledge graph in the
retrieval-augmented-generation task of the large language models, the practical
application value of the knowledge graph in the era of large language models is
confirmed. The knowledge graph resource will be released on an open-source
platform and can serve as the basic knowledge base for forestry policy-related
intelligent systems. It can also be used for academic research. In addition,
this study can provide reference and guidance for the construction of policy
knowledge graphs in other fields. Our data is provided on Github
https://github.com/luozhongze/ForPKG.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.11090v2},
File          = {2411.11090v2.pdf}
}
@article{2411.09601v1,
Author        = {Cogan Shimizu and Pascal Hitzler},
Title         = {Accelerating Knowledge Graph and Ontology Engineering with Large
  Language Models},
Eprint        = {2411.09601v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large Language Models bear the promise of significant acceleration of key
Knowledge Graph and Ontology Engineering tasks, including ontology modeling,
extension, modification, population, alignment, as well as entity
disambiguation. We lay out LLM-based Knowledge Graph and Ontology Engineering
as a new and coming area of research, and argue that modular approaches to
ontologies will be of central importance.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.09601v1},
File          = {2411.09601v1.pdf}
}
@article{2411.08696v1,
Author        = {Nandana Mihindukulasooriya and Sanju Tiwari and Daniil Dobriy and Finn Ãrup Nielsen and Tek Raj Chhetri and Axel Polleres},
Title         = {Scholarly Wikidata: Population and Exploration of Conference Data in
  Wikidata using LLMs},
Eprint        = {2411.08696v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {Several initiatives have been undertaken to conceptually model the domain of
scholarly data using ontologies and to create respective Knowledge Graphs. Yet,
the full potential seems unleashed, as automated means for automatic population
of said ontologies are lacking, and respective initiatives from the Semantic
Web community are not necessarily connected: we propose to make scholarly data
more sustainably accessible by leveraging Wikidata's infrastructure and
automating its population in a sustainable manner through LLMs by tapping into
unstructured sources like conference Web sites and proceedings texts as well as
already existing structured conference datasets. While an initial analysis
shows that Semantic Web conferences are only minimally represented in Wikidata,
we argue that our methodology can help to populate, evolve and maintain
scholarly data as a community within Wikidata. Our main contributions include
(a) an analysis of ontologies for representing scholarly data to identify gaps
and relevant entities/properties in Wikidata, (b) semi-automated extraction --
requiring (minimal) manual validation -- of conference metadata (e.g.,
acceptance rates, organizer roles, programme committee members, best paper
awards, keynotes, and sponsors) from websites and proceedings texts using LLMs.
Finally, we discuss (c) extensions to visualization tools in the Wikidata
context for data exploration of the generated scholarly data. Our study focuses
on data from 105 Semantic Web-related conferences and extends/adds more than
6000 entities in Wikidata. It is important to note that the method can be more
generally applicable beyond Semantic Web-related conferences for enhancing
Wikidata's utility as a comprehensive scholarly resource.
  Source Repository: https://github.com/scholarly-wikidata/
  DOI: https://doi.org/10.5281/zenodo.10989709
  License: Creative Commons CC0 (Data), MIT (Code)},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.08696v1},
File          = {2411.08696v1.pdf}
}
@article{2411.08469v2,
Author        = {Fadi Al Machot and Martin Thomas Horsch and Habib Ullah},
Title         = {Building Trustworthy AI: Transparent AI Systems via Large Language
  Models, Ontologies, and Logical Reasoning (TranspNet)},
Eprint        = {2411.08469v2},
DOI           = {10.1007/978-3-031-89274-5_3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Growing concerns over the lack of transparency in AI, particularly in
high-stakes fields like healthcare and finance, drive the need for explainable
and trustworthy systems. While Large Language Models (LLMs) perform
exceptionally well in generating accurate outputs, their "black box" nature
poses significant challenges to transparency and trust. To address this, the
paper proposes the TranspNet pipeline, which integrates symbolic AI with LLMs.
By leveraging domain expert knowledge, retrieval-augmented generation (RAG),
and formal reasoning frameworks like Answer Set Programming (ASP), TranspNet
enhances LLM outputs with structured reasoning and verification.This approach
strives to help AI systems deliver results that are as accurate, explainable,
and trustworthy as possible, aligning with regulatory expectations for
transparency and accountability. TranspNet provides a solution for developing
AI systems that are reliable and interpretable, making it suitable for
real-world applications where trust is critical.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.08469v2},
File          = {2411.08469v2.pdf}
}
@article{2411.08463v2,
Author        = {Fadi Al Machot and Martin Thomas Horsch and Habib Ullah},
Title         = {Symbolic-AI-Fusion Deep Learning (SAIF-DL): Encoding Knowledge into
  Training with Answer Set Programming Loss Penalties by a Novel Loss Function
  Approach},
Eprint        = {2411.08463v2},
DOI           = {10.1007/978-3-031-89274-5_4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper presents a hybrid methodology that enhances the training process
of deep learning (DL) models by embedding domain expert knowledge using
ontologies and answer set programming (ASP). By integrating these symbolic AI
methods, we encode domain-specific constraints, rules, and logical reasoning
directly into the model's learning process, thereby improving both performance
and trustworthiness. The proposed approach is flexible and applicable to both
regression and classification tasks, demonstrating generalizability across
various fields such as healthcare, autonomous systems, engineering, and battery
manufacturing applications. Unlike other state-of-the-art methods, the strength
of our approach lies in its scalability across different domains. The design
allows for the automation of the loss function by simply updating the ASP
rules, making the system highly scalable and user-friendly. This facilitates
seamless adaptation to new domains without significant redesign, offering a
practical solution for integrating expert knowledge into DL models in
industrial settings such as battery manufacturing.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.08463v2},
File          = {2411.08463v2.pdf}
}
@article{2411.06399v2,
Author        = {Jinbo Hu and Yin Cao and Ming Wu and Fang Kang and Feiran Yang and Wenwu Wang and Mark D. Plumbley and Jun Yang},
Title         = {PSELDNets: Pre-trained Neural Networks on a Large-scale Synthetic
  Dataset for Sound Event Localization and Detection},
Eprint        = {2411.06399v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {eess.AS},
Abstract      = {Sound event localization and detection (SELD) has seen substantial
advancements through learning-based methods. These systems, typically trained
from scratch on specific datasets, have shown considerable generalization
capabilities. Recently, deep neural networks trained on large-scale datasets
have achieved remarkable success in the sound event classification (SEC) field,
prompting an open question of whether these advances can be extended to the
development of SELD foundation models. In this paper, leveraging the power of
pre-trained SEC models, we propose pre-trained SELD networks (PSELDNets) on a
large-scale synthetic dataset. The synthetic dataset, generated by convolving
sound events with simulated spatial room impulse responses (SRIRs), contains
1,167 hours of audio clips with an ontology of 170 sound classes. These
PSELDNets are applied to various SELD scenarios. When we adapt PSELDNets to
specific scenarios, particularly in cases of low-resource data, we introduce a
data-efficient fine-tuning method, AdapterBit. PSELDNets are evaluated on
synthetic-test-set using collected SRIRs from the TAU Spatial Room Impulse
Response Database (TAU-SRIR DB) and achieve satisfactory performance. We also
carried out experiments to validate the transferability of PSELDNets to three
publicly available datasets and our own real-world recordings. The results
demonstrate that PSELDNets surpass state-of-the-art systems across all publicly
available datasets. Given the need for direction-of-arrival estimation, SELD
generally relies on sufficient multi-channel audio clips. However,
incorporating the AdapterBit, PSELDNets show more efficient adaptability to
various scenarios using minimal multi-channel or even just monophonic audio
clips, outperforming traditional fine-tuning approaches.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.06399v2},
File          = {2411.06399v2.pdf}
}
@article{2411.05521v2,
Author        = {Sithursan Sivasubramaniam and Cedric Osei-Akoto and Yi Zhang and Kurt Stockinger and Jonathan Fuerst},
Title         = {SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark},
Eprint        = {2411.05521v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Electronic health records (EHRs) are stored in various database systems with
different database models on heterogeneous storage architectures, such as
relational databases, document stores, or graph databases. These different
database models have a big impact on query complexity and performance. While
this has been a known fact in database research, its implications for the
growing number of Text-to-Query systems have surprisingly not been investigated
so far. In this paper, we present SM3-Text-to-Query, the first multi-model
medical Text-to-Query benchmark based on synthetic patient data from Synthea,
following the SNOMED-CT taxonomy -- a widely used knowledge graph ontology
covering medical terminology. SM3-Text-to-Query provides data representations
for relational databases (PostgreSQL), document stores (MongoDB), and graph
databases (Neo4j and GraphDB (RDF)), allowing the evaluation across four
popular query languages, namely SQL, MQL, Cypher, and SPARQL. We systematically
and manually develop 408 template questions, which we augment to construct a
benchmark of 10K diverse natural language question/query pairs for these four
query languages (40K pairs overall). On our dataset, we evaluate several common
in-context-learning (ICL) approaches for a set of representative closed and
open-source LLMs. Our evaluation sheds light on the trade-offs between database
models and query languages for different ICL strategies and LLMs. Last,
SM3-Text-to-Query is easily extendable to additional query languages or real,
standard-based patient databases.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.05521v2},
File          = {2411.05521v2.pdf}
}
@article{2411.05057v1,
Author        = {Monica Munnangi},
Title         = {A Brief History of Named Entity Recognition},
Eprint        = {2411.05057v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {A large amount of information in today's world is now stored in knowledge
bases. Named Entity Recognition (NER) is a process of extracting,
disambiguation, and linking an entity from raw text to insightful and
structured knowledge bases. More concretely, it is identifying and classifying
entities in the text that are crucial for Information Extraction, Semantic
Annotation, Question Answering, Ontology Population, and so on. The process of
NER has evolved in the last three decades since it first appeared in 1996. In
this survey, we study the evolution of techniques employed for NER and compare
the results, starting from supervised to the developing unsupervised learning
methods.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.05057v1},
File          = {2411.05057v1.pdf}
}
@article{2411.04794v3,
Author        = {Yuxin Zuo and Wenxuan Jiang and Wenxuan Liu and Zixuan Li and Long Bai and Hanbin Wang and Yutao Zeng and Xiaolong Jin and Jiafeng Guo and Xueqi Cheng},
Title         = {KnowCoder-X: Boosting Multilingual Information Extraction via Code},
Eprint        = {2411.04794v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Empirical evidence indicates that LLMs exhibit spontaneous cross-lingual
alignment. However, although LLMs show promising cross-lingual alignment in
Information Extraction (IE), a significant imbalance across languages persists,
highlighting an underlying deficiency. To address this, we propose KnowCoder-X,
a powerful code LLM with advanced cross-lingual and multilingual capabilities
for universal IE. Firstly, it standardizes the representation of multilingual
schemas using Python classes, ensuring a consistent ontology across different
languages. Then, IE across languages is formulated as a unified code generation
task. Secondly, we conduct IE cross-lingual alignment instruction tuning on the
translated instance prediction task to enhance the model's cross-lingual
transferability. During this phase, we also construct a high-quality and
diverse bilingual IE parallel dataset with 257k samples, called ParallelNER,
synthesized by our proposed robust three-stage pipeline, with manual annotation
to ensure quality. Although without training in 29 unseen languages,
KnowCoder-X surpasses ChatGPT by 30.17\% and SoTA by 20.03\%, thereby
demonstrating superior cross-lingual IE capabilities. Comprehensive evaluations
on 64 IE benchmarks in Chinese and English under various settings demonstrate
that KnowCoder-X significantly enhances cross-lingual IE transfer through
boosting the IE alignment. Our code and dataset are available at:
https://github.com/ICT-GoKnow/KnowCoder},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.04794v3},
File          = {2411.04794v3.pdf}
}
@article{2411.04585v2,
Author        = {Noam Dahan and Gabriel Stanovsky},
Title         = {The State and Fate of Summarization Datasets: A Survey},
Eprint        = {2411.04585v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Automatic summarization has consistently attracted attention due to its
versatility and wide application in various downstream tasks. Despite its
popularity, we find that annotation efforts have largely been disjointed, and
have lacked common terminology. Consequently, it is challenging to discover
existing resources or identify coherent research directions. To address this,
we survey a large body of work spanning 133 datasets in over 100 languages,
creating a novel ontology covering sample properties, collection methods and
distribution. With this ontology we make key observations, including the lack
in accessible high-quality datasets for low-resource languages, and the field's
over-reliance on the news domain and on automatically collected distant
supervision. Finally, we make available a web interface that allows users to
interact and explore our ontology and dataset collection, as well as a template
for a summarization data card, which can be used to streamline future research
into a more coherent body of work.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.04585v2},
File          = {2411.04585v2.pdf}
}
@article{2411.03962v8,
Author        = {Zhangcheng Qiang and Kerry Taylor and Weiqing Wang},
Title         = {How Does A Text Preprocessing Pipeline Affect Ontology Syntactic
  Matching?},
Eprint        = {2411.03962v8},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The classical text preprocessing pipeline, comprising Tokenisation,
Normalisation, Stop Words Removal, and Stemming/Lemmatisation, has been
implemented in many systems for syntactic ontology matching (OM). However, the
lack of standardisation in text preprocessing creates diversity in mapping
results. In this paper, we investigate the effect of the text preprocessing
pipeline on syntactic OM in 8 Ontology Alignment Evaluation Initiative (OAEI)
tracks with 49 distinct alignments. We find that Phase 1 text preprocessing
(Tokenisation and Normalisation) is more effective than Phase 2 text
preprocessing (Stop Words Removal and Stemming/Lemmatisation). We propose two
novel approaches to repair unwanted false mappings caused by Phase 2 text
preprocessing. One is an ad hoc logic-based repair approach that employs an
ontology-specific check to find common words that cause false mappings. These
words are stored in a reserved word set and applied before the text
preprocessing. By leveraging the power of large language models (LLMs), we also
propose a post hoc LLM-based repair approach. This approach utilises the strong
background knowledge provided by LLMs to repair non-existent and
counter-intuitive false mappings after the text preprocessing. It also
overcomes the tendency towards unstable true mappings by injecting the
classical text preprocessing pipeline via function calling. The experimental
results show that these two approaches can improve the matching correctness and
the overall matching performance.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.03962v8},
File          = {2411.03962v8.pdf}
}
@article{2411.03552v2,
Author        = {Holger Bech Nielsen},
Title         = {Remarkable Scale Relation, Approximate SU(5), Fluctuating Lattice},
Eprint        = {2411.03552v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {hep-ph},
Abstract      = {We discuss a series of 8 energy scales, some of which just speculated by
ourselves, and fit the logarithms of these energies as a straight line versus a
quantity related to the dimensionalities of action terms in a way to be defined
in the article. These terms in the action are related to the energy scales in
question. So e.g. the dimensionality of Einstein Hilbert action coefficient is
one related to the Planck scale. In fact we suppose in the cases described with
quantum field theory, that there is for each of our energy scales a pair of
associated terms in the Lagrangian density, one "kinetic" and one "mass- or
current" term. We use for our plotting of the energy scales the ratio of the
dimensionality of say the "non-kinetic" to the dimensionality of the "kinetic"
one The explanation for our phenomenological finding that the logarithm of the
energies depend as a straight line on the dimensionality defined integer $q$,
we give as an ontological - i.e. it really exists in nature in our model
-"fluctuating lattice" with a very broad distribution of say the link size
$a$.We take it Gaussian in the logarithm, $\ln(a)$. A fluctuating lattice is
very natural in a theory with general relativity, since it corresponds to
fluctuations in the gauge d.o.f. of general relativity. Intriguing are the
lowest ones of our energy scales, being by us not described by quantum field
theory as the other ones, but by actions for single particle or single string
respectivily, because the string scale fits well with hadronic strings, and the
particle scale is presumably the mass scale of Standard Model group monopoles,
a bound state of a couple of which might be the dimuon resonance (or
statistical fluctuation) found in LHC with mass 28 GeV.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.03552v2},
File          = {2411.03552v2.pdf}
}
@article{2411.02730v1,
Author        = {Zexu Li and Suraj P. Prabhu and Zachary T. Popp and Shubhi S. Jain and Vijetha Balakundi and Ting Fang Alvin Ang and Rhoda Au and Jinying Chen},
Title         = {A Natural Language Processing Approach to Support Biomedical Data
  Harmonization: Leveraging Large Language Models},
Eprint        = {2411.02730v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Biomedical research requires large, diverse samples to produce unbiased
results. Automated methods for matching variables across datasets can
accelerate this process. Research in this area has been limited, primarily
focusing on lexical matching and ontology based semantic matching. We aimed to
develop new methods, leveraging large language models (LLM) and ensemble
learning, to automate variable matching. Methods: We utilized data from two
GERAS cohort (European and Japan) studies to develop variable matching methods.
We first manually created a dataset by matching 352 EU variables with 1322
candidate JP variables, where matched variable pairs were positive and
unmatched pairs were negative instances. Using this dataset, we developed and
evaluated two types of natural language processing (NLP) methods, which matched
variables based on variable labels and definitions from data dictionaries: (1)
LLM-based and (2) fuzzy matching. We then developed an ensemble-learning
method, using the Random Forest model, to integrate individual NLP methods. RF
was trained and evaluated on 50 trials. Each trial had a random split (4:1) of
training and test sets, with the model's hyperparameters optimized through
cross-validation on the training set. For each EU variable, 1322 candidate JP
variables were ranked based on NLP-derived similarity scores or RF's
probability scores, denoting their likelihood to match the EU variable. Ranking
performance was measured by top-n hit ratio (HRn) and mean reciprocal rank
(MRR). Results:E5 performed best among individual methods, achieving 0.90 HR-30
and 0.70 MRR. RF performed better than E5 on all metrics over 50 trials (P less
than 0.001) and achieved an average HR 30 of 0.98 and MRR of 0.73. LLM-derived
features contributed most to RF's performance. One major cause of errors in
automatic variable matching was ambiguous variable definitions within data
dictionaries.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.02730v1},
File          = {2411.02730v1.pdf}
}
@article{2411.01612v1,
Author        = {Sanaz Saki Norouzi and Adrita Barua and Antrea Christou and Nikita Gautam and Andrew Eells and Pascal Hitzler and Cogan Shimizu},
Title         = {Ontology Population using LLMs},
Eprint        = {2411.01612v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge graphs (KGs) are increasingly utilized for data integration,
representation, and visualization. While KG population is critical, it is often
costly, especially when data must be extracted from unstructured text in
natural language, which presents challenges, such as ambiguity and complex
interpretations. Large Language Models (LLMs) offer promising capabilities for
such tasks, excelling in natural language understanding and content generation.
However, their tendency to ``hallucinate'' can produce inaccurate outputs.
Despite these limitations, LLMs offer rapid and scalable processing of natural
language data, and with prompt engineering and fine-tuning, they can
approximate human-level performance in extracting and structuring data for KGs.
This study investigates LLM effectiveness for the KG population, focusing on
the Enslaved.org Hub Ontology. In this paper, we report that compared to the
ground truth, LLM's can extract ~90% of triples, when provided a modular
ontology as guidance in the prompts.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.01612v1},
File          = {2411.01612v1.pdf}
}
@article{2411.01574v2,
Author        = {Olga Mashkova and Fernando Zhapa-Camacho and Robert Hoehndorf},
Title         = {DELE: Deductive $\mathcal{EL}^{++}$ Embeddings for Knowledge Base
  Completion},
Eprint        = {2411.01574v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontology embeddings map classes, relations, and individuals in ontologies
into $\mathbb{R}^n$, and within $\mathbb{R}^n$ similarity between entities can
be computed or new axioms inferred. For ontologies in the Description Logic
$\mathcal{EL}^{++}$, several optimization-based embedding methods have been
developed that explicitly generate models of an ontology. However, these
methods suffer from some limitations; they do not distinguish between
statements that are unprovable and provably false, and therefore they may use
entailed statements as negatives. Furthermore, they do not utilize the
deductive closure of an ontology to identify statements that are inferred but
not asserted. We evaluated a set of embedding methods for $\mathcal{EL}^{++}$
ontologies, incorporating several modifications that aim to make use of the
ontology deductive closure. In particular, we designed novel negative losses
that account both for the deductive closure and different types of negatives
and formulated evaluation methods for knowledge base completion. We demonstrate
that our embedding methods improve over the baseline ontology embedding in the
task of knowledge base or ontology completion.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.01574v2},
File          = {2411.01574v2.pdf}
}
@article{2411.01205v1,
Author        = {Jianyu Liu and Sheng Bi and Guilin Qi},
Title         = {PRIMO: Progressive Induction for Multi-hop Open Rule Generation},
Eprint        = {2411.01205v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Open rule refer to the implication from premise atoms to hypothesis atoms,
which captures various relations between instances in the real world. Injecting
open rule knowledge into the machine helps to improve the performance of
downstream tasks such as dialogue and relation extraction. Existing approaches
focus on single-hop open rule generation, ignoring multi-hop scenarios, leading
to logical inconsistencies between premise and hypothesis atoms, as well as
semantic duplication of generated rule atoms. To address these issues, we
propose a progressive multi-stage open rule generation method called PRIMO. We
introduce ontology information during the rule generation stage to reduce
ambiguity and improve rule accuracy. PRIMO constructs a multi-stage structure
consisting of generation, extraction, and ranking modules to fully leverage the
latent knowledge within the language model across multiple dimensions.
Furthermore, we employ reinforcement learning from human feedback to further
optimize model, enhancing the model's understanding of commonsense knowledge.
Experiments show that compared to baseline models, PRIMO significantly improves
rule quality and diversity while reducing the repetition rate of rule atoms.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.01205v1},
File          = {2411.01205v1.pdf}
}
@article{2411.00702v1,
Author        = {Armin Pournaki and Tom Willaert},
Title         = {A graph-based approach to extracting narrative signals from public
  discourse},
Eprint        = {2411.00702v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Narratives are key interpretative devices by which humans make sense of
political reality. As the significance of narratives for understanding current
societal issues such as polarization and misinformation becomes increasingly
evident, there is a growing demand for methods that support their empirical
analysis. To this end, we propose a graph-based formalism and machine-guided
method for extracting, representing, and analyzing selected narrative signals
from digital textual corpora, based on Abstract Meaning Representation (AMR).
The formalism and method introduced here specifically cater to the study of
political narratives that figure in texts from digital media such as archived
political speeches, social media posts, political manifestos and transcripts of
parliamentary debates. We conceptualize these political narratives as a type of
ontological narratives: stories by which actors position themselves as
political beings, and which are akin to political worldviews in which actors
present their normative vision of the world, or aspects thereof. We approach
the study of such political narratives as a problem of information retrieval:
starting from a textual corpus, we first extract a graph-like representation of
the meaning of each sentence in the corpus using AMR. Drawing on transferable
concepts from narratology, we then apply a set of heuristics to filter these
graphs for representations of 1) actors, 2) the events in which these actors
figure, and 3) traces of the perspectivization of these events. We approach
these references to actors, events, and instances of perspectivization as core
narrative signals that initiate a further analysis by alluding to larger
political narratives. By means of a case study of State of the European Union
addresses, we demonstrate how the formalism can be used to inductively surface
signals of political narratives from public discourse.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.00702v1},
File          = {2411.00702v1.pdf}
}
@article{2411.00477v1,
Author        = {Bubacarr Jobarteh and Madalina Mincu and Gavojdian Dinu and Suresh Neethirajan},
Title         = {Multi Modal Information Fusion of Acoustic and Linguistic Data for
  Decoding Dairy Cow Vocalizations in Animal Welfare Assessment},
Eprint        = {2411.00477v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SD},
Abstract      = {Understanding animal vocalizations through multi-source data fusion is
crucial for assessing emotional states and enhancing animal welfare in
precision livestock farming. This study aims to decode dairy cow contact calls
by employing multi-modal data fusion techniques, integrating transcription,
semantic analysis, contextual and emotional assessment, and acoustic feature
extraction. We utilized the Natural Language Processing model to transcribe
audio recordings of cow vocalizations into written form. By fusing multiple
acoustic features frequency, duration, and intensity with transcribed textual
data, we developed a comprehensive representation of cow vocalizations.
Utilizing data fusion within a custom-developed ontology, we categorized
vocalizations into high frequency calls associated with distress or arousal,
and low frequency calls linked to contentment or calmness. Analyzing the fused
multi dimensional data, we identified anxiety related features indicative of
emotional distress, including specific frequency measurements and sound
spectrum results. Assessing the sentiment and acoustic features of
vocalizations from 20 individual cows allowed us to determine differences in
calling patterns and emotional states. Employing advanced machine learning
algorithms, Random Forest, Support Vector Machine, and Recurrent Neural
Networks, we effectively processed and fused multi-source data to classify cow
vocalizations. These models were optimized to handle computational demands and
data quality challenges inherent in practical farm environments. Our findings
demonstrate the effectiveness of multi-source data fusion and intelligent
processing techniques in animal welfare monitoring. This study represents a
significant advancement in animal welfare assessment, highlighting the role of
innovative fusion technologies in understanding and improving the emotional
wellbeing of dairy cows.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.00477v1},
File          = {2411.00477v1.pdf}
}
@article{2410.23584v1,
Author        = {Andy Lo and Albert Q. Jiang and Wenda Li and Mateja Jamnik},
Title         = {End-to-End Ontology Learning with Large Language Models},
Eprint        = {2410.23584v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Ontologies are useful for automatic machine processing of domain knowledge as
they represent it in a structured format. Yet, constructing ontologies requires
substantial manual effort. To automate part of this process, large language
models (LLMs) have been applied to solve various subtasks of ontology learning.
However, this partial ontology learning does not capture the interactions
between subtasks. We address this gap by introducing OLLM, a general and
scalable method for building the taxonomic backbone of an ontology from
scratch. Rather than focusing on subtasks, like individual relations between
entities, we model entire subcomponents of the target ontology by finetuning an
LLM with a custom regulariser that reduces overfitting on high-frequency
concepts. We introduce a novel suite of metrics for evaluating the quality of
the generated ontology by measuring its semantic and structural similarity to
the ground truth. In contrast to standard metrics, our metrics use deep
learning techniques to define more robust distance measures between graphs.
Both our quantitative and qualitative results on Wikipedia show that OLLM
outperforms subtask composition methods, producing more semantically accurate
ontologies while maintaining structural integrity. We further demonstrate that
our model can be effectively adapted to new domains, like arXiv, needing only a
small number of training examples. Our source code and datasets are available
at https://github.com/andylolu2/ollm.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.23584v1},
File          = {2410.23584v1.pdf}
}
@article{2410.22996v1,
Author        = {Deperias Kerre and Anne Laurent and Kenneth Maussang and Dickson Owuor},
Title         = {Semantic Enrichment of the Quantum Cascade Laser Properties in Text- A
  Knowledge Graph Generation Approach},
Eprint        = {2410.22996v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {A well structured collection of the various Quantum Cascade Laser (QCL)
design and working properties data provides a platform to analyze and
understand the relationships between these properties. By analyzing these
relationships, we can gain insights into how different design features impact
laser performance properties such as the working temperature. Most of these QCL
properties are captured in scientific text. There is therefore need for
efficient methodologies that can be utilized to extract QCL properties from
text and generate a semantically enriched and interlinked platform where the
properties can be analyzed to uncover hidden relations. There is also the need
to maintain provenance and reference information on which these properties are
based. Semantic Web technologies such as Ontologies and Knowledge Graphs have
proven capability in providing interlinked data platforms for knowledge
representation in various domains. In this paper, we propose an approach for
generating a QCL properties Knowledge Graph (KG) from text for semantic
enrichment of the properties. The approach is based on the QCL ontology and a
Retrieval Augmented Generation (RAG) enabled information extraction pipeline
based on GPT 4-Turbo language model. The properties of interest include:
working temperature, laser design type, lasing frequency, laser optical power
and the heterostructure. The experimental results demonstrate the feasibility
and effectiveness of this approach for efficiently extracting QCL properties
from unstructured text and generating a QCL properties Knowledge Graph, which
has potential applications in semantic enrichment and analysis of QCL data.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.22996v1},
File          = {2410.22996v1.pdf}
}
@article{2410.22897v3,
Author        = {Haiyue Yuan and Ali Raza and Nikolay Matyunin and Jibesh Patra and Shujun Li},
Title         = {A Graph-Based Model for Vehicle-Centric Data Sharing Ecosystem},
Eprint        = {2410.22897v3},
DOI           = {10.1109/ITSC58415.2024.10919888},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SI},
Abstract      = {The development of technologies has prompted a paradigm shift in the
automotive industry, with an increasing focus on connected services and
autonomous driving capabilities. This transformation allows vehicles to collect
and share vast amounts of vehicle-specific and personal data. While these
technological advancements offer enhanced user experiences, they also raise
privacy concerns. To understand the ecosystem of data collection and sharing in
modern vehicles, we adopted the ontology 101 methodology to incorporate
information extracted from different sources, including analysis of privacy
policies using GPT-4, a small-scale systematic literature review, and an
existing ontology, to develop a high-level conceptual graph-based model, aiming
to get insights into how modern vehicles handle data exchange among different
parties. This serves as a foundational model with the flexibility and
scalability to further expand for modelling and analysing data sharing
practices across diverse contexts. Two realistic examples were developed to
demonstrate the usefulness and effectiveness of discovering insights into
privacy regarding vehicle-related data sharing. We also recommend several
future research directions, such as exploring advanced ontology languages for
reasoning tasks, supporting topological analysis for discovering data privacy
risks/concerns, and developing useful tools for comparative analysis, to
strengthen the understanding of the vehicle-centric data sharing ecosystem.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.22897v3},
File          = {2410.22897v3.pdf}
}
@article{2410.22767v1,
Author        = {Sejin Lee and Dongha Kim and Min Song},
Title         = {Beyond Ontology in Dialogue State Tracking for Goal-Oriented Chatbot},
Eprint        = {2410.22767v1},
DOI           = {10.1109/ICKG63256.2024.00030},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Goal-oriented chatbots are essential for automating user tasks, such as
booking flights or making restaurant reservations. A key component of these
systems is Dialogue State Tracking (DST), which interprets user intent and
maintains the dialogue state. However, existing DST methods often rely on fixed
ontologies and manually compiled slot values, limiting their adaptability to
open-domain dialogues. We propose a novel approach that leverages instruction
tuning and advanced prompt strategies to enhance DST performance, without
relying on any predefined ontologies. Our method enables Large Language Model
(LLM) to infer dialogue states through carefully designed prompts and includes
an anti-hallucination mechanism to ensure accurate tracking in diverse
conversation contexts. Additionally, we employ a Variational Graph Auto-Encoder
(VGAE) to model and predict subsequent user intent. Our approach achieved
state-of-the-art with a JGA of 42.57% outperforming existing ontology-less DST
models, and performed well in open-domain real-world conversations. This work
presents a significant advancement in creating more adaptive and accurate
goal-oriented chatbots.},
Year          = {2024},
Month         = {Oct},
Note          = {IEEE International Conference on Knowledge Graph (ICKG), 2024, pp.
  177-185},
Url           = {http://arxiv.org/abs/2410.22767v1},
File          = {2410.22767v1.pdf}
}
@article{2411.00046v1,
Author        = {Harry Caufield and Carlo Kroll and Shawn T O'Neil and Justin T Reese and Marcin P Joachimiak and Harshad Hegde and Nomi L Harris and Madan Krishnamurthy and James A McLaughlin and Damian Smedley and Melissa A Haendel and Peter N Robinson and Christopher J Mungall},
Title         = {CurateGPT: A flexible language-model assisted biocuration tool},
Eprint        = {2411.00046v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Effective data-driven biomedical discovery requires data curation: a
time-consuming process of finding, organizing, distilling, integrating,
interpreting, annotating, and validating diverse information into a structured
form suitable for databases and knowledge bases. Accurate and efficient
curation of these digital assets is critical to ensuring that they are FAIR,
trustworthy, and sustainable. Unfortunately, expert curators face significant
time and resource constraints. The rapid pace of new information being
published daily is exceeding their capacity for curation. Generative AI,
exemplified by instruction-tuned large language models (LLMs), has opened up
new possibilities for assisting human-driven curation. The design philosophy of
agents combines the emerging abilities of generative AI with more precise
methods. A curator's tasks can be aided by agents for performing reasoning,
searching ontologies, and integrating knowledge across external sources, all
efforts otherwise requiring extensive manual effort. Our LLM-driven annotation
tool, CurateGPT, melds the power of generative AI together with trusted
knowledge bases and literature sources. CurateGPT streamlines the curation
process, enhancing collaboration and efficiency in common workflows. Compared
to direct interaction with an LLM, CurateGPT's agents enable access to
information beyond that in the LLM's training data and they provide direct
links to the data supporting each claim. This helps curators, researchers, and
engineers scale up curation efforts to keep pace with the ever-increasing
volume of scientific data.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2411.00046v1},
File          = {2411.00046v1.pdf}
}
@article{2410.21060v2,
Author        = {Yutong Cheng and Osama Bajaber and Saimon Amanuel Tsegai and Dawn Song and Peng Gao},
Title         = {CTINexus: Automatic Cyber Threat Intelligence Knowledge Graph
  Construction Using Large Language Models},
Eprint        = {2410.21060v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {Textual descriptions in cyber threat intelligence (CTI) reports, such as
security articles and news, are rich sources of knowledge about cyber threats,
crucial for organizations to stay informed about the rapidly evolving threat
landscape. However, current CTI knowledge extraction methods lack flexibility
and generalizability, often resulting in inaccurate and incomplete knowledge
extraction. Syntax parsing relies on fixed rules and dictionaries, while model
fine-tuning requires large annotated datasets, making both paradigms
challenging to adapt to new threats and ontologies. To bridge the gap, we
propose CTINexus, a novel framework leveraging optimized in-context learning
(ICL) of large language models (LLMs) for data-efficient CTI knowledge
extraction and high-quality cybersecurity knowledge graph (CSKG) construction.
Unlike existing methods, CTINexus requires neither extensive data nor parameter
tuning and can adapt to various ontologies with minimal annotated examples.
This is achieved through: (1) a carefully designed automatic prompt
construction strategy with optimal demonstration retrieval for extracting a
wide range of cybersecurity entities and relations; (2) a hierarchical entity
alignment technique that canonicalizes the extracted knowledge and removes
redundancy; (3) an long-distance relation prediction technique to further
complete the CSKG with missing links. Our extensive evaluations using 150
real-world CTI reports collected from 10 platforms demonstrate that CTINexus
significantly outperforms existing methods in constructing accurate and
complete CSKG, highlighting its potential to transform CTI analysis with an
efficient and adaptable solution for the dynamic threat landscape.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.21060v2},
File          = {2410.21060v2.pdf}
}
@article{2410.20695v2,
Author        = {Gal Beeri and Benoit Chamot and Elena Latchem and Shruthi Venkatesh and Sarah Whalan and Van Zyl Kruger and David Martino},
Title         = {Combining Domain-Specific Models and LLMs for Automated Disease
  Phenotyping from Survey Data},
Eprint        = {2410.20695v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This exploratory pilot study investigated the potential of combining a
domain-specific model, BERN2, with large language models (LLMs) to enhance
automated disease phenotyping from research survey data. Motivated by the need
for efficient and accurate methods to harmonize the growing volume of survey
data with standardized disease ontologies, we employed BERN2, a biomedical
named entity recognition and normalization model, to extract disease
information from the ORIGINS birth cohort survey data. After rigorously
evaluating BERN2's performance against a manually curated ground truth dataset,
we integrated various LLMs using prompt engineering, Retrieval-Augmented
Generation (RAG), and Instructional Fine-Tuning (IFT) to refine the model's
outputs. BERN2 demonstrated high performance in extracting and normalizing
disease mentions, and the integration of LLMs, particularly with Few Shot
Inference and RAG orchestration, further improved accuracy. This approach,
especially when incorporating structured examples, logical reasoning prompts,
and detailed context, offers a promising avenue for developing tools to enable
efficient cohort profiling and data harmonization across large, heterogeneous
research datasets.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.20695v2},
File          = {2410.20695v2.pdf}
}
@article{2410.19955v2,
Author        = {Pengfei Hu and Chang Lu and Fei Wang and Yue Ning},
Title         = {Bridging Stepwise Lab-Informed Pretraining and Knowledge-Guided Learning
  for Diagnostic Reasoning},
Eprint        = {2410.19955v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Despite the growing use of Electronic Health Records (EHR) for AI-assisted
diagnosis prediction, most data-driven models struggle to incorporate
clinically meaningful medical knowledge. They often rely on limited ontologies,
lacking structured reasoning capabilities and comprehensive coverage. This
raises an important research question: Will medical knowledge improve
predictive models to support stepwise clinical reasoning as performed by human
doctors? To address this problem, we propose DuaLK, a dual-expertise framework
that combines two complementary sources of information. For external knowledge,
we construct a Diagnosis Knowledge Graph (KG) that encodes both hierarchical
and semantic relations enriched by large language models (LLM). To align with
patient data, we further introduce a lab-informed proxy task that guides the
model to follow a clinically consistent, stepwise reasoning process based on
lab test signals. Experimental results on two public EHR datasets demonstrate
that DuaLK consistently outperforms existing baselines across four clinical
prediction tasks. These findings highlight the potential of combining
structured medical knowledge with individual-level clinical signals to achieve
more accurate and interpretable diagnostic predictions. The source code is
publicly available on https://github.com/humphreyhuu/DuaLK.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.19955v2},
File          = {2410.19955v2.pdf}
}
@article{2410.18629v1,
Author        = {Sanjay Singh and Amaresh Chakrabarti},
Title         = {Supporting Assessment of Novelty of Design Problems Using Concept of
  Problem SAPPhIRE},
Eprint        = {2410.18629v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper proposes a framework for assessing the novelty of design problems
using the SAPPhIRE model of causality. The novelty of a problem is measured as
its minimum distance from the problems in a reference problem database. The
distance is calculated by comparing the current problem and each reference past
problem at the various levels of abstraction in the SAPPhIRE ontology. The
basis for comparison is textual similarity. To demonstrate the applicability of
the proposed framework, The current set of problems associated with an
artifact, as collected from its stakeholders, were compared with the past set
of problems, as collected from patents and other web sources, to assess the
novelty of the current set. This approach is aimed at providing a better
understanding of the degree of novelty of any given set of current problems by
comparing them to similar problems available from historical records. Since
manual assessment, the current mode of such assessments as reported in the
literature, is a tedious process, to reduce time complexity and to afford
better applicability for larger sets of problem statements, an automated
assessment is proposed and used in this paper.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.18629v1},
File          = {2410.18629v1.pdf}
}
@article{2410.18489v2,
Author        = {Ahmed R. Sadik and Sebastian Brulin and Markus Olhofer},
Title         = {LLM as a code generator in Agile Model Driven Development},
Eprint        = {2410.18489v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Leveraging Large Language Models (LLM) like GPT4 in the auto generation of
code represents a significant advancement, yet it is not without its
challenges. The ambiguity inherent in natural language descriptions of software
poses substantial obstacles to generating deployable, structured artifacts.
This research champions Model Driven Development (MDD) as a viable strategy to
overcome these challenges, proposing an Agile Model Driven Development (AMDD)
approach that employs GPT4 as a code generator. This approach enhances the
flexibility and scalability of the code auto generation process and offers
agility that allows seamless adaptation to changes in models or deployment
environments. We illustrate this by modeling a multi agent Unmanned Vehicle
Fleet (UVF) system using the Unified Modeling Language (UML), significantly
reducing model ambiguity by integrating the Object Constraint Language (OCL)
for code structure meta modeling, and the FIPA ontology language for
communication semantics meta modeling. Applying GPT4 auto generation
capabilities yields Java and Python code that is compatible with the JADE and
PADE frameworks, respectively. Our thorough evaluation of the auto generated
code verifies its alignment with expected behaviors and identifies enhancements
in agent interactions. Structurally, we assessed the complexity of code derived
from a model constrained solely by OCL meta models, against that influenced by
both OCL and FIPA ontology meta models. The results indicate that the ontology
constrained meta model produces inherently more complex code, yet its
cyclomatic complexity remains within manageable levels, suggesting that
additional meta model constraints can be incorporated without exceeding the
high risk threshold for complexity.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.18489v2},
File          = {2410.18489v2.pdf}
}
@article{2410.18393v1,
Author        = {Tanmay Parekh and Jeffrey Kwan and Jiarui Yu and Sparsh Johri and Hyosang Ahn and Sreya Muppalla and Kai-Wei Chang and Wei Wang and Nanyun Peng},
Title         = {SPEED++: A Multilingual Event Extraction Framework for Epidemic
  Prediction and Preparedness},
Eprint        = {2410.18393v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Social media is often the first place where communities discuss the latest
societal trends. Prior works have utilized this platform to extract
epidemic-related information (e.g. infections, preventive measures) to provide
early warnings for epidemic prediction. However, these works only focused on
English posts, while epidemics can occur anywhere in the world, and early
discussions are often in the local, non-English languages. In this work, we
introduce the first multilingual Event Extraction (EE) framework SPEED++ for
extracting epidemic event information for a wide range of diseases and
languages. To this end, we extend a previous epidemic ontology with 20 argument
roles; and curate our multilingual EE dataset SPEED++ comprising 5.1K tweets in
four languages for four diseases. Annotating data in every language is
infeasible; thus we develop zero-shot cross-lingual cross-disease models (i.e.,
training only on English COVID data) utilizing multilingual pre-training and
show their efficacy in extracting epidemic-related events for 65 diverse
languages across different diseases. Experiments demonstrate that our framework
can provide epidemic warnings for COVID-19 in its earliest stages in Dec 2019
(3 weeks before global discussions) from Chinese Weibo posts without any
training in Chinese. Furthermore, we exploit our framework's argument
extraction capabilities to aggregate community epidemic discussions like
symptoms and cure measures, aiding misinformation detection and public
attention monitoring. Overall, we lay a strong foundation for multilingual
epidemic preparedness.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.18393v1},
File          = {2410.18393v1.pdf}
}
@article{2410.17504v1,
Author        = {Shruthi Chari},
Title         = {An Ontology-Enabled Approach For User-Centered and Knowledge-Enabled
  Explanations of AI Systems},
Eprint        = {2410.17504v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Explainable Artificial Intelligence (AI) focuses on helping humans understand
the working of AI systems or their decisions and has been a cornerstone of AI
for decades. Recent research in explainability has focused on explaining the
workings of AI models or model explainability. There have also been several
position statements and review papers detailing the needs of end-users for
user-centered explainability but fewer implementations. Hence, this thesis
seeks to bridge some gaps between model and user-centered explainability. We
create an explanation ontology (EO) to represent literature-derived explanation
types via their supporting components. We implement a knowledge-augmented
question-answering (QA) pipeline to support contextual explanations in a
clinical setting. Finally, we are implementing a system to combine explanations
from different AI methods and data modalities. Within the EO, we can represent
fifteen different explanation types, and we have tested these representations
in six exemplar use cases. We find that knowledge augmentations improve the
performance of base large language models in the contextualized QA, and the
performance is variable across disease groups. In the same setting, clinicians
also indicated that they prefer to see actionability as one of the main foci in
explanations. In our explanations combination method, we plan to use similarity
metrics to determine the similarity of explanations in a chronic disease
detection setting. Overall, through this thesis, we design methods that can
support knowledge-enabled explanations across different use cases, accounting
for the methods in today's AI era that can generate the supporting components
of these explanations and domain knowledge sources that can enhance them.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.17504v1},
File          = {2410.17504v1.pdf}
}
@article{2410.17051v1,
Author        = {Shir Ashury-Tahan and Amir David Nissan Cohen and Nadav Cohen and Yoram Louzoun and Yoav Goldberg},
Title         = {Data-driven Coreference-based Ontology Building},
Eprint        = {2410.17051v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {While coreference resolution is traditionally used as a component in
individual document understanding, in this work we take a more global view and
explore what can we learn about a domain from the set of all document-level
coreference relations that are present in a large corpus. We derive coreference
chains from a corpus of 30 million biomedical abstracts and construct a graph
based on the string phrases within these chains, establishing connections
between phrases if they co-occur within the same coreference chain. We then use
the graph structure and the betweeness centrality measure to distinguish
between edges denoting hierarchy, identity and noise, assign directionality to
edges denoting hierarchy, and split nodes (strings) that correspond to multiple
distinct concepts. The result is a rich, data-driven ontology over concepts in
the biomedical domain, parts of which overlaps significantly with
human-authored ontologies. We release the coreference chains and resulting
ontology under a creative-commons license, along with the code.},
Year          = {2024},
Month         = {Oct},
Note          = {EMNLP 2024},
Url           = {http://arxiv.org/abs/2410.17051v1},
File          = {2410.17051v1.pdf}
}
@article{2411.02423v1,
Author        = {Biswanath Dutta and Debanjali Bain},
Title         = {Development of CODO: A Comprehensive Tool for COVID-19 Data
  Representation, Analysis, and Visualization},
Eprint        = {2411.02423v1},
DOI           = {10.17821/srels/2024/v61i5/171582},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {Artificial intelligence (AI) has become indispensable for managing and
processing the vast amounts of data generated during the COVID-19 pandemic.
Ontology, which formalizes knowledge within a domain using standardized
vocabularies and relationships, plays a crucial role in AI by enabling
automated reasoning, data integration, semantic interoperability, and
extracting meaningful insights from extensive datasets. The diversity of
COVID-19 datasets poses challenges in comprehending this information for both
human and machines. Existing COVID-19 ontologies are designed to address
specific aspects of the pandemic but lack comprehensive coverage across all
essential dimensions. To address this gap, CODO, an integrated ontological
model has been developed encompassing critical facets of COVID-19 information
such as aetiology, epidemiology, transmission, pathogenesis, diagnosis,
prevention, genomics, therapeutic safety, and more. This paper reviews CODO
since its inception in 2020, detailing its developments and highlighting CODO
as a tool for the aggregation, representation, analysis, and visualization of
diverse COVID-19 data. The major contribution of this paper is to provide a
summary of the development of CODO, and outline the overall development and
evaluation approach. By adhering to best practices and leveraging W3C
standards, CODO ensures data integration and semantic interoperability,
supporting effective navigation of COVID-19 complexities across various
domains.},
Year          = {2024},
Month         = {Oct},
Note          = {Journal of Information and Knowledge, Vol. 61, no. 5, pp. 245-253
  (2024)},
Url           = {http://arxiv.org/abs/2411.02423v1},
File          = {2411.02423v1.pdf}
}
@article{2410.16804v1,
Author        = {Haru Nakajima and Jun Miura},
Title         = {Combining Ontological Knowledge and Large Language Model for
  User-Friendly Service Robots},
Eprint        = {2410.16804v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Lifestyle support through robotics is an increasingly promising field, with
expectations for robots to take over or assist with chores like floor cleaning,
table setting and clearing, and fetching items. The growth of AI, particularly
foundation models, such as large language models (LLMs) and visual language
models (VLMs), is significantly shaping this sector. LLMs, by facilitating
natural interactions and providing vast general knowledge, are proving
invaluable for robotic tasks. This paper zeroes in on the benefits of LLMs for
"bring-me" tasks, where robots fetch specific items for users, often based on
vague instructions. Our previous efforts utilized an ontology extended to
handle environmental data to decipher such vagueness, but faced limitations
when unresolvable ambiguities required user intervention for clarity. Here, we
enhance our approach by integrating LLMs for providing additional commonsense
knowledge, pairing it with ontological data to mitigate the issue of
hallucinations and reduce the need for user queries, thus improving system
usability. We present a system that merges these knowledge bases and assess its
efficacy on "bring-me" tasks, aiming to provide a more seamless and efficient
robotic assistance experience.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.16804v1},
File          = {2410.16804v1.pdf}
}
@article{2410.16597v1,
Author        = {Prafulla Kumar Choubey and Xin Su and Man Luo and Xiangyu Peng and Caiming Xiong and Tiep Le and Shachar Rosenman and Vasudev Lal and Phil Mui and Ricky Ho and Phillip Howard and Chien-Sheng Wu},
Title         = {Distill-SynthKG: Distilling Knowledge Graph Synthesis Workflow for
  Improved Coverage and Efficiency},
Eprint        = {2410.16597v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graphs (KGs) generated by large language models (LLMs) are becoming
increasingly valuable for Retrieval-Augmented Generation (RAG) applications
that require knowledge-intensive reasoning. However, existing KG extraction
methods predominantly rely on prompt-based approaches, which are inefficient
for processing large-scale corpora. These approaches often suffer from
information loss, particularly with long documents, due to the lack of
specialized design for KG construction. Additionally, there is a gap in
evaluation datasets and methodologies for ontology-free KG construction. To
overcome these limitations, we propose SynthKG, a multi-step, document-level
ontology-free KG synthesis workflow based on LLMs. By fine-tuning a smaller LLM
on the synthesized document-KG pairs, we streamline the multi-step process into
a single-step KG generation approach called Distill-SynthKG, substantially
reducing the number of LLM inference calls. Furthermore, we re-purpose existing
question-answering datasets to establish KG evaluation datasets and introduce
new evaluation metrics. Using KGs produced by Distill-SynthKG, we also design a
novel graph-based retrieval framework for RAG. Experimental results demonstrate
that Distill-SynthKG not only surpasses all baseline models in KG quality --
including models up to eight times larger -- but also consistently excels in
retrieval and question-answering tasks. Our proposed graph retrieval framework
also outperforms all KG-retrieval methods across multiple benchmark datasets.
We release the SynthKG dataset and Distill-SynthKG model publicly to support
further research and development.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.16597v1},
File          = {2410.16597v1.pdf}
}
@article{2410.16016v1,
Author        = {Yinghui Wang and Yilong Ren and Zhiyong Cui and Haiyang Yu},
Title         = {Proactive security defense: cyber threat intelligence modeling for
  connected autonomous vehicles},
Eprint        = {2410.16016v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {Cybersecurity has become a crucial concern in the field of connected
autonomous vehicles. Cyber threat intelligence (CTI), as the collection of
cyber threat information, offers an ideal way for responding to emerging cyber
threats and realizing proactive security defense. However, instant analysis and
modeling of vehicle cybersecurity data is a fundamental challenge since its
complex and professional context. In this paper, we suggest an automotive CTI
modeling framework, Actim, to extract and analyse the interrelated
relationships among cyber threat elements. Specifically, we first design a
vehicle security-safety conceptual ontology model to depict various threat
entity classes and their relations. Then, we manually annotate the first
automobile CTI corpus by using real cybersecurity data, which comprises 908
threat intelligence texts, including 8195 entities and 4852 relationships. To
effectively extract cyber threat entities and their relations, we propose an
automotive CTI mining model based on cross-sentence context. Experiment results
show that the proposed BERT-DocHiatt-BiLSTM-LSTM model exceeds the performance
of existing methods. Finally, we define entity-relation matching rules and
create a CTI knowledge graph that structurally fuses various elements of cyber
threats. The Actim framework enables mining the intrinsic connections among
threat entities, providing valuable insight on the evolving cyber threat
landscape.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.16016v1},
File          = {2410.16016v1.pdf}
}
@article{2410.15238v1,
Author        = {Zachary Sheldon and Peeyush Kumar},
Title         = {Economic Anthropology in the Era of Generative Artificial Intelligence},
Eprint        = {2410.15238v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper explores the intersection of economic anthropology and generative
artificial intelligence (GenAI). It examines how large language models (LLMs)
can simulate human decision-making and the inductive biases present in AI
research. The study introduces two AI models: C.A.L.L.O.N. (Conventionally
Average Late Liberal ONtology) and M.A.U.S.S. (More Accurate Understanding of
Society and its Symbols). The former is trained on standard data, while the
latter is adapted with anthropological knowledge. The research highlights how
anthropological training can enhance LLMs' ability to recognize diverse
economic systems and concepts. The findings suggest that integrating economic
anthropology with AI can provide a more pluralistic understanding of economics
and improve the sustainability of non-market economic systems.},
Year          = {2024},
Month         = {Oct},
Note          = {Annual Joint Meeting of Society of Economic Anthropology and
  Society for the Anthropology of Work 2024},
Url           = {http://arxiv.org/abs/2410.15238v1},
File          = {2410.15238v1.pdf}
}
@article{2410.15019v1,
Author        = {Jinggui Liang and Yuxia Wu and Yuan Fang and Hao Fei and Lizi Liao},
Title         = {A Survey of Ontology Expansion for Conversational Understanding},
Eprint        = {2410.15019v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In the rapidly evolving field of conversational AI, Ontology Expansion
(OnExp) is crucial for enhancing the adaptability and robustness of
conversational agents. Traditional models rely on static, predefined
ontologies, limiting their ability to handle new and unforeseen user needs.
This survey paper provides a comprehensive review of the state-of-the-art
techniques in OnExp for conversational understanding. It categorizes the
existing literature into three main areas: (1) New Intent Discovery, (2) New
Slot-Value Discovery, and (3) Joint OnExp. By examining the methodologies,
benchmarks, and challenges associated with these areas, we highlight several
emerging frontiers in OnExp to improve agent performance in real-world
scenarios and discuss their corresponding challenges. This survey aspires to be
a foundational reference for researchers and practitioners, promoting further
exploration and innovation in this crucial domain.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.15019v1},
File          = {2410.15019v1.pdf}
}
@article{2410.14571v2,
Author        = {Hui Yang and Jiaoyan Chen and Uli Sattler},
Title         = {TransBox: EL++-closed Ontology Embedding},
Eprint        = {2410.14571v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {OWL (Web Ontology Language) ontologies, which are able to represent both
relational and type facts as standard knowledge graphs and complex domain
knowledge in Description Logic (DL) axioms, are widely adopted in domains such
as healthcare and bioinformatics. Inspired by the success of knowledge graph
embeddings, embedding OWL ontologies has gained significant attention in recent
years. Current methods primarily focus on learning embeddings for atomic
concepts and roles, enabling the evaluation based on normalized axioms through
specially designed score functions. However, they often neglect the embedding
of complex concepts, making it difficult to infer with more intricate axioms.
This limitation reduces their effectiveness in advanced reasoning tasks, such
as Ontology Learning and ontology-mediated Query Answering. In this paper, we
propose EL++-closed ontology embeddings which are able to represent any logical
expressions in DL via composition. Furthermore, we develop TransBox, an
effective EL++-closed ontology embedding method that can handle many-to-one,
one-to-many and many-to-many relations. Our extensive experiments demonstrate
that TransBox often achieves state-of-the-art performance across various
real-world datasets for predicting complex axioms.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.14571v2},
File          = {2410.14571v2.pdf}
}
@article{2410.14763v2,
Author        = {Hamed Fayyaz and Raphael Poulain and Rahmatollah Beheshti},
Title         = {Enabling Scalable Evaluation of Bias Patterns in Medical LLMs},
Eprint        = {2410.14763v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) have shown impressive potential in helping with
numerous medical challenges. Deploying LLMs in high-stakes applications such as
medicine, however, brings in many concerns. One major area of concern relates
to biased behaviors of LLMs in medical applications, leading to unfair
treatment of individuals. To pave the way for the responsible and impactful
deployment of Med LLMs, rigorous evaluation is a key prerequisite. Due to the
huge complexity and variability of different medical scenarios, existing work
in this domain has primarily relied on using manually crafted datasets for bias
evaluation. In this study, we present a new method to scale up such bias
evaluations by automatically generating test cases based on rigorous medical
evidence. We specifically target the challenges of a) domain-specificity of
bias characterization, b) hallucinating while generating the test cases, and c)
various dependencies between the health outcomes and sensitive attributes. To
that end, we offer new methods to address these challenges integrated with our
generative pipeline, using medical knowledge graphs, medical ontologies, and
customized general LLM evaluation frameworks in our method. Through a series of
extensive experiments, we show that the test cases generated by our proposed
method can effectively reveal bias patterns in Med LLMs at larger and more
flexible scales than human-crafted datasets. We publish a large bias evaluation
dataset using our pipeline, which is dedicated to a few medical case studies. A
live demo of our application for vignette generation is available at
https://vignette.streamlit.app. Our code is also available at
https://github.com/healthylaife/autofair.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.14763v2},
File          = {2410.14763v2.pdf}
}
@article{2410.14433v1,
Author        = {Rabea Khatun and Wahia Tasnim and Maksuda Akter and Md Manowarul Islam and Md. Ashraf Uddin and Md. Zulfiker Mahmud and Saurav Chandra Das},
Title         = {A Bioinformatic Approach Validated Utilizing Machine Learning Algorithms
  to Identify Relevant Biomarkers and Crucial Pathways in Gallbladder Cancer},
Eprint        = {2410.14433v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.GN},
Abstract      = {Gallbladder cancer (GBC) is the most frequent cause of disease among biliary
tract neoplasms. Identifying the molecular mechanisms and biomarkers linked to
GBC progression has been a significant challenge in scientific research. Few
recent studies have explored the roles of biomarkers in GBC. Our study aimed to
identify biomarkers in GBC using machine learning (ML) and bioinformatics
techniques. We compared GBC tumor samples with normal samples to identify
differentially expressed genes (DEGs) from two microarray datasets (GSE100363,
GSE139682) obtained from the NCBI GEO database. A total of 146 DEGs were found,
with 39 up-regulated and 107 down-regulated genes. Functional enrichment
analysis of these DEGs was performed using Gene Ontology (GO) terms and
REACTOME pathways through DAVID. The protein-protein interaction network was
constructed using the STRING database. To identify hub genes, we applied three
ranking algorithms: Degree, MNC, and Closeness Centrality. The intersection of
hub genes from these algorithms yielded 11 hub genes. Simultaneously, two
feature selection methods (Pearson correlation and recursive feature
elimination) were used to identify significant gene subsets. We then developed
ML models using SVM and RF on the GSE100363 dataset, with validation on
GSE139682, to determine the gene subset that best distinguishes GBC samples.
The hub genes outperformed the other gene subsets. Finally, NTRK2, COL14A1,
SCN4B, ATP1A2, SLC17A7, SLIT3, COL7A1, CLDN4, CLEC3B, ADCYAP1R1, and MFAP4 were
identified as crucial genes, with SLIT3, COL7A1, and CLDN4 being strongly
linked to GBC development and prediction.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.14433v1},
File          = {2410.14433v1.pdf}
}
@article{2410.14399v2,
Author        = {Magdalena Wysocka and Danilo Carvalho and Oskar Wysocki and Marco Valentino and Andre Freitas},
Title         = {SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic
  Reasoning},
Eprint        = {2410.14399v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Syllogistic reasoning is crucial for Natural Language Inference (NLI). This
capability is particularly significant in specialized domains such as
biomedicine, where it can support automatic evidence interpretation and
scientific discovery. This paper presents SylloBio-NLI, a novel framework that
leverages external ontologies to systematically instantiate diverse syllogistic
arguments for biomedical NLI. We employ SylloBio-NLI to evaluate Large Language
Models (LLMs) on identifying valid conclusions and extracting supporting
evidence across 28 syllogistic schemes instantiated with human genome pathways.
Extensive experiments reveal that biomedical syllogistic reasoning is
particularly challenging for zero-shot LLMs, which achieve an average accuracy
between 70% on generalized modus ponens and 23% on disjunctive syllogism. At
the same time, we found that few-shot prompting can boost the performance of
different LLMs, including Gemma (+14%) and LLama-3 (+43%). However, a deeper
analysis shows that both techniques exhibit high sensitivity to superficial
lexical variations, highlighting a dependency between reliability, models'
architecture, and pre-training regime. Overall, our results indicate that,
while in-context examples have the potential to elicit syllogistic reasoning in
LLMs, existing models are still far from achieving the robustness and
consistency required for safe biomedical NLI applications.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.14399v2},
File          = {2410.14399v2.pdf}
}
@article{2410.13948v1,
Author        = {Cogan Shimizu and Shirly Stephe and Adrita Barua and Ling Cai and Antrea Christou and Kitty Currier and Abhilekha Dalal and Colby K. Fisher and Pascal Hitzler and Krzysztof Janowicz and Wenwen Li and Zilong Liu and Mohammad Saeid Mahdavinejad and Gengchen Mai and Dean Rehberger and Mark Schildhauer and Meilin Shi and Sanaz Saki Norouzi and Yuanyuan Tian and Sizhe Wang and Zhangyu Wang and Joseph Zalewski and Lu Zhou and Rui Zhu},
Title         = {The KnowWhereGraph Ontology},
Eprint        = {2410.13948v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {KnowWhereGraph is one of the largest fully publicly available geospatial
knowledge graphs. It includes data from 30 layers on natural hazards (e.g.,
hurricanes, wildfires), climate variables (e.g., air temperature,
precipitation), soil properties, crop and land-cover types, demographics, and
human health, various place and region identifiers, among other themes. These
have been leveraged through the graph by a variety of applications to address
challenges in food security and agricultural supply chains; sustainability
related to soil conservation practices and farm labor; and delivery of
emergency humanitarian aid following a disaster. In this paper, we introduce
the ontology that acts as the schema for KnowWhereGraph. This broad overview
provides insight into the requirements and design specifications for the graph
and its schema, including the development methodology (modular ontology
modeling) and the resources utilized to implement, materialize, and deploy
KnowWhereGraph with its end-user interfaces and public query SPARQL endpoint.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.13948v1},
File          = {2410.13948v1.pdf}
}
@article{2410.13803v1,
Author        = {Gianluca Apriceno and Valentina Tamma and Tania Bailoni and Jacopo de Berardinis and Mauro Dragoni},
Title         = {A Pattern to Align Them All: Integrating Different Modalities to Define
  Multi-Modal Entities},
Eprint        = {2410.13803v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The ability to reason with and integrate different sensory inputs is the
foundation underpinning human intelligence and it is the reason for the growing
interest in modelling multi-modal information within Knowledge Graphs.
Multi-Modal Knowledge Graphs extend traditional Knowledge Graphs by associating
an entity with its possible modal representations, including text, images,
audio, and videos, all of which are used to convey the semantics of the entity.
Despite the increasing attention that Multi-Modal Knowledge Graphs have
received, there is a lack of consensus about the definitions and modelling of
modalities, whose definition is often determined by application domains. In
this paper, we propose a novel ontology design pattern that captures the
separation of concerns between an entity (and the information it conveys),
whose semantics can have different manifestations across different media, and
its realisation in terms of a physical information entity. By introducing this
abstract model, we aim to facilitate the harmonisation and integration of
different existing multi-modal ontologies which is crucial for many intelligent
applications across different domains spanning from medicine to digital
humanities.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.13803v1},
File          = {2410.13803v1.pdf}
}
@article{2410.13514v2,
Author        = {Efimia Panagiotaki and Georgi Pramatarov and Lars Kunze and Daniele De Martini},
Title         = {GraphSCENE: On-Demand Critical Scenario Generation for Autonomous
  Vehicles in Simulation},
Eprint        = {2410.13514v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Testing and validating Autonomous Vehicle (AV) performance in safety-critical
and diverse scenarios is crucial before real-world deployment. However,
manually creating such scenarios in simulation remains a significant and
time-consuming challenge. This work introduces a novel method that generates
dynamic temporal scene graphs corresponding to diverse traffic scenarios,
on-demand, tailored to user-defined preferences, such as AV actions, sets of
dynamic agents, and criticality levels. A temporal Graph Neural Network (GNN)
model learns to predict relationships between ego-vehicle, agents, and static
structures, guided by real-world spatiotemporal interaction patterns and
constrained by an ontology that restricts predictions to semantically valid
links. Our model consistently outperforms the baselines in accurately
generating links corresponding to the requested scenarios. We render the
predicted scenarios in simulation to further demonstrate their effectiveness as
testing environments for AV agents.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.13514v2},
File          = {2410.13514v2.pdf}
}
@article{2410.12631v1,
Author        = {Nicolas Lazzari and Stefano De Giorgis and Aldo Gangemi and Valentina Presutti},
Title         = {Explainable Moral Values: a neuro-symbolic approach to value
  classification},
Eprint        = {2410.12631v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This work explores the integration of ontology-based reasoning and Machine
Learning techniques for explainable value classification. By relying on an
ontological formalization of moral values as in the Moral Foundations Theory,
relying on the DnS Ontology Design Pattern, the \textit{sandra} neuro-symbolic
reasoner is used to infer values (fomalized as descriptions) that are
\emph{satisfied by} a certain sentence. Sentences, alongside their structured
representation, are automatically generated using an open-source Large Language
Model. The inferred descriptions are used to automatically detect the value
associated with a sentence. We show that only relying on the reasoner's
inference results in explainable classification comparable to other more
complex approaches. We show that combining the reasoner's inferences with
distributional semantics methods largely outperforms all the baselines,
including complex models based on neural network architectures. Finally, we
build a visualization tool to explore the potential of theory-based values
classification, which is publicly available at http://xmv.geomeaning.com/.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.12631v1},
File          = {2410.12631v1.pdf}
}
@article{2410.23303v1,
Author        = {Philipp Dechent and Elias Barbers and Simon Clark and Susanne Lehner and Brady Planden and Masaki Adachi and David A. Howey and Sabine Paarmann},
Title         = {Demonstrating Linked Battery Data To Accelerate Knowledge Flow in
  Battery Science},
Eprint        = {2410.23303v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Batteries are pivotal for transitioning to a climate-friendly future, leading
to a surge in battery research. Scopus (Elsevier) lists 14,388 papers that
mention "lithium-ion battery" in 2023 alone, making it infeasible for
individuals to keep up. This paper discusses strategies based on structured,
semantic, and linked data to manage this information overload. Structured data
follows a predefined, machine-readable format; semantic data includes metadata
for context; linked data references other semantic data, forming a web of
interconnected information. We use a battery-related ontology, BattINFO to
standardise terms and enable automated data extraction and analysis. Our
methodology integrates full-text search and machine-readable data, enhancing
data retrieval and battery testing. We aim to unify commercial cell information
and develop tools for the battery community such as manufacturer-independent
cycling procedure descriptions and external memory for Large Language Models.
Although only a first step, this approach significantly accelerates battery
research and digitalizes battery testing, inviting community participation for
continuous improvement. We provide the structured data and the tools to access
them as open source.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.23303v1},
File          = {2410.23303v1.pdf}
}
@article{2410.11141v1,
Author        = {Shriram M S and Sushmitha S and Gayathri K S and Shahina A},
Title         = {Can Structured Data Reduce Epistemic Uncertainty?},
Eprint        = {2410.11141v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In this work, we present a framework that utilizes ontology alignment to
improve the learning process of deep learning models. With this approach we
show that models fine-tuned using ontologies learn a downstream task at a
higher rate with better performance on a sequential classification task
compared to the native version of the model. Additionally, we extend our work
to showcase how subsumption mappings retrieved during the process of ontology
alignment can help enhance Retrieval-Augmented Generation in Large Language
Models. The results show that the responses obtained by using subsumption
mappings show an increase of 8.97% in contextual similarity and a 1% increase
in factual accuracy. We also use these scores to define our Hallucination Index
and show that this approach reduces hallucination in LLMs by 4.847%.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.11141v1},
File          = {2410.11141v1.pdf}
}
@article{2410.10899v2,
Author        = {Rongbin Li and Wenbo Chen and Jinbo Li and Hanwen Xing and Hua Xu and Zhao Li and W. Jim Zheng},
Title         = {GPTON: Generative Pre-trained Transformers enhanced with Ontology
  Narration for accurate annotation of biological data},
Eprint        = {2410.10899v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.QM},
Abstract      = {By leveraging GPT-4 for ontology narration, we developed GPTON to infuse
structured knowledge into LLMs through verbalized ontology terms, achieving
accurate text and ontology annotations for over 68% of gene sets in the top
five predictions. Manual evaluations confirm GPTON's robustness, highlighting
its potential to harness LLMs and structured knowledge to significantly advance
biomedical research beyond gene set annotation.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.10899v2},
File          = {2410.10899v2.pdf}
}
@article{2410.12860v1,
Author        = {Robert Porter and Adam Diehl and Benjamin Pastel and J. Henry Hinnefeld and Lawson Nerenberg and Pye Maung and Sebastien Kerbrat and Gillian Hanson and Troy Astorino and Stephen J. Tarsa},
Title         = {LLMD: A Large Language Model for Interpreting Longitudinal Medical
  Records},
Eprint        = {2410.12860v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We introduce LLMD, a large language model designed to analyze a patient's
medical history based on their medical records. Along with domain knowledge,
LLMD is trained on a large corpus of records collected over time and across
facilities, as well as tasks and labels that make nuanced connections among
them. This approach is critical to an accurate picture of patient health, and
has distinctive advantages over models trained on knowledge alone, unlabeled
records, structured EHR data, or records from a single health system.
  The recipe for LLMD continues pretraining a foundational model on both domain
knowledge and the contents of millions of records. These span an average of 10
years of care and as many as 140 care sites per patient. LLMD is then
instruction fine-tuned on structuring and abstraction tasks. The former jointly
identify and normalize document metadata, provenance information, clinical
named-entities, and ontology mappings, while the latter roll these into
higher-level representations, such a continuous era of time a patient was on a
medication. LLMD is deployed within a layered validation system that includes
continual random audits and review by experts, e.g. based on uncertainty,
disease-specific rules, or use-case.
  LLMD exhibits large gains over both more-powerful generalized models and
domain-specific models. On medical knowledge benchmarks, LLMD-8B achieves state
of the art accuracy on PubMedQA text responses, besting orders-of-magnitude
larger models. On production tasks, we show that LLMD significantly outperforms
all other models evaluated, and among alternatives, large general purpose LLMs
like GPT-4o are more accurate than models emphasizing medical knowledge. We
find strong evidence that accuracy on today's medical benchmarks is not the
most significant factor when analyzing real-world patient data, an insight with
implications for future medical LLMs.'},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.12860v1},
File          = {2410.12860v1.pdf}
}
@article{2410.09244v1,
Author        = {C. Civili and E. Sherkhonov and R. E. K. Stirewalt},
Title         = {Using off-the-shelf LLMs to query enterprise data by progressively
  revealing ontologies},
Eprint        = {2410.09244v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Ontologies are known to improve the accuracy of Large Language Models (LLMs)
when translating natural language queries into a formal query language like SQL
or SPARQL. There are two ways to leverage ontologies when working with LLMs.
One is to fine-tune the model, i.e., to enhance it with specific domain
knowledge. Another is the zero-shot prompting approach, where the ontology is
provided as part of the input question. Unfortunately, modern enterprises
typically have ontologies that are too large to fit in a prompt due to LLM's
token size limitations. We present a solution that incrementally reveals "just
enough" of an ontology that is needed to answer a given question.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.09244v1},
File          = {2410.09244v1.pdf}
}
@article{2410.07962v1,
Author        = {Tomas Bueno Momcilovic and Beat Buesser and Giulio Zizzo and Mark Purcell and Dian Balta},
Title         = {Towards Assurance of LLM Adversarial Robustness using Ontology-Driven
  Argumentation},
Eprint        = {2410.07962v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Despite the impressive adaptability of large language models (LLMs),
challenges remain in ensuring their security, transparency, and
interpretability. Given their susceptibility to adversarial attacks, LLMs need
to be defended with an evolving combination of adversarial training and
guardrails. However, managing the implicit and heterogeneous knowledge for
continuously assuring robustness is difficult. We introduce a novel approach
for assurance of the adversarial robustness of LLMs based on formal
argumentation. Using ontologies for formalization, we structure
state-of-the-art attacks and defenses, facilitating the creation of a
human-readable assurance case, and a machine-readable representation. We
demonstrate its application with examples in English language and code
translation tasks, and provide implications for theory and practice, by
targeting engineers, data scientists, users, and auditors.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.07962v1},
File          = {2410.07962v1.pdf}
}
@article{2410.07454v1,
Author        = {Suqi Liu and Tianxi Cai and Xiaoou Li},
Title         = {Representation-Enhanced Neural Knowledge Integration with Application to
  Large-Scale Medical Ontology Learning},
Eprint        = {2410.07454v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {stat.ME},
Abstract      = {A large-scale knowledge graph enhances reproducibility in biomedical data
discovery by providing a standardized, integrated framework that ensures
consistent interpretation across diverse datasets. It improves generalizability
by connecting data from various sources, enabling broader applicability of
findings across different populations and conditions. Generating reliable
knowledge graph, leveraging multi-source information from existing literature,
however, is challenging especially with a large number of node sizes and
heterogeneous relations. In this paper, we propose a general theoretically
guaranteed statistical framework, called RENKI, to enable simultaneous learning
of multiple relation types. RENKI generalizes various network models widely
used in statistics and computer science. The proposed framework incorporates
representation learning output into initial entity embedding of a neural
network that approximates the score function for the knowledge graph and
continuously trains the model to fit observed facts. We prove nonasymptotic
bounds for in-sample and out-of-sample weighted MSEs in relation to the
pseudo-dimension of the knowledge graph function class. Additionally, we
provide pseudo-dimensions for score functions based on multilayer neural
networks with ReLU activation function, in the scenarios when the embedding
parameters either fixed or trainable. Finally, we complement our theoretical
results with numerical studies and apply the method to learn a comprehensive
medical knowledge graph combining a pretrained language model representation
with knowledge graph links observed in several medical ontologies. The
experiments justify our theoretical findings and demonstrate the effect of
weighting in the presence of heterogeneous relations and the benefit of
incorporating representation learning in nonparametric models.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.07454v1},
File          = {2410.07454v1.pdf}
}
@article{2410.07358v1,
Author        = {Javier Lopez Zambrano and Juan A. Lara and Cristobal Romero},
Title         = {Improving the portability of predicting students performance models by
  using ontologies},
Eprint        = {2410.07358v1},
DOI           = {10.1007/s12528-021-09273-3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {One of the main current challenges in Educational Data Mining and Learning
Analytics is the portability or transferability of predictive models obtained
for a particular course so that they can be applied to other different courses.
To handle this challenge, one of the foremost problems is the models excessive
dependence on the low-level attributes used to train them, which reduces the
models portability. To solve this issue, the use of high level attributes with
more semantic meaning, such as ontologies, may be very useful. Along this line,
we propose the utilization of an ontology that uses a taxonomy of actions that
summarises students interactions with the Moodle learning management system. We
compare the results of this proposed approach against our previous results when
we used low-level raw attributes obtained directly from Moodle logs. The
results indicate that the use of the proposed ontology improves the portability
of the models in terms of predictive accuracy. The main contribution of this
paper is to show that the ontological models obtained in one source course can
be applied to other different target courses with similar usage levels without
losing prediction accuracy.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.07358v1},
File          = {2410.07358v1.pdf}
}
@article{2410.05252v1,
Author        = {Mourad Heddaya and Qingcheng Zeng and Chenhao Tan and Rob Voigt and Alexander Zentefis},
Title         = {Causal Micro-Narratives},
Eprint        = {2410.05252v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We present a novel approach to classify causal micro-narratives from text.
These narratives are sentence-level explanations of the cause(s) and/or
effect(s) of a target subject. The approach requires only a subject-specific
ontology of causes and effects, and we demonstrate it with an application to
inflation narratives. Using a human-annotated dataset spanning historical and
contemporary US news articles for training, we evaluate several large language
models (LLMs) on this multi-label classification task. The best-performing
model--a fine-tuned Llama 3.1 8B--achieves F1 scores of 0.87 on narrative
detection and 0.71 on narrative classification. Comprehensive error analysis
reveals challenges arising from linguistic ambiguity and highlights how model
errors often mirror human annotator disagreements. This research establishes a
framework for extracting causal micro-narratives from real-world data, with
wide-ranging applications to social science research.},
Year          = {2024},
Month         = {Oct},
Note          = {Proceedings of the The 6th Workshop on Narrative Understanding at
  EMNLP 2024, pages 67-84, Miami, Florida, USA. Association for Computational
  Linguistics},
Url           = {http://arxiv.org/abs/2410.05252v1},
File          = {2410.05252v1.pdf}
}
@article{2410.05306v1,
Author        = {Tomas Bueno Momcilovic and Beat Buesser and Giulio Zizzo and Mark Purcell and Dian Balta},
Title         = {Towards Assuring EU AI Act Compliance and Adversarial Robustness of LLMs},
Eprint        = {2410.05306v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {Large language models are prone to misuse and vulnerable to security threats,
raising significant safety and security concerns. The European Union's
Artificial Intelligence Act seeks to enforce AI robustness in certain contexts,
but faces implementation challenges due to the lack of standards, complexity of
LLMs and emerging security vulnerabilities. Our research introduces a framework
using ontologies, assurance cases, and factsheets to support engineers and
stakeholders in understanding and documenting AI system compliance and security
regarding adversarial robustness. This approach aims to ensure that LLMs adhere
to regulatory standards and are equipped to counter potential threats.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.05306v1},
File          = {2410.05306v1.pdf}
}
@article{2410.03235v2,
Author        = {Elias Crum and Antonio De Santis and Manon Ovide and Jiaxin Pan and Alessia Pisu and Nicolas Lazzari and Sebastian Rudolph},
Title         = {Enriching Ontologies with Disjointness Axioms using Large Language
  Models},
Eprint        = {2410.03235v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontologies often lack explicit disjointness declarations between classes,
despite their usefulness for sophisticated reasoning and consistency checking
in Knowledge Graphs. In this study, we explore the potential of Large Language
Models (LLMs) to enrich ontologies by identifying and asserting class
disjointness axioms. Our approach aims at leveraging the implicit knowledge
embedded in LLMs, using prompt engineering to elicit this knowledge for
classifying ontological disjointness. We validate our methodology on the
DBpedia ontology, focusing on open-source LLMs. Our findings suggest that LLMs,
when guided by effective prompt strategies, can reliably identify disjoint
class relationships, thus streamlining the process of ontology completion
without extensive manual input. For comprehensive disjointness enrichment, we
propose a process that takes logical relationships between disjointness and
subclass statements into account in order to maintain satisfiability and reduce
the number of calls to the LLM. This work provides a foundation for future
applications of LLMs in automated ontology enhancement and offers insights into
optimizing LLM performance through strategic prompt design. Our code is
publicly available on GitHub at https://github.com/n28div/llm-disjointness.},
Year          = {2024},
Month         = {Oct},
Note          = {Proc. of 2nd Workshop on Knowledge Base Construction from
  Pre-Trained Language Models (KBC-LM 2024) co-located with ISWC 2024,
  Baltimore, USA, November 12, 2024},
Url           = {http://arxiv.org/abs/2410.03235v2},
File          = {2410.03235v2.pdf}
}
@article{2410.02721v1,
Author        = {Ryan C. Barron and Ves Grantcharov and Selma Wanna and Maksim E. Eren and Manish Bhattarai and Nicholas Solovyev and George Tompkins and Charles Nicholas and Kim Ã. Rasmussen and Cynthia Matuszek and Boian S. Alexandrov},
Title         = {Domain-Specific Retrieval-Augmented Generation Using Vector Stores,
  Knowledge Graphs, and Tensor Factorization},
Eprint        = {2410.02721v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) are pre-trained on large-scale corpora and excel
in numerous general natural language processing (NLP) tasks, such as question
answering (QA). Despite their advanced language capabilities, when it comes to
domain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations,
knowledge cut-offs, and lack of knowledge attributions. Additionally, fine
tuning LLMs' intrinsic knowledge to highly specific domains is an expensive and
time consuming process. The retrieval-augmented generation (RAG) process has
recently emerged as a method capable of optimization of LLM responses, by
referencing them to a predetermined ontology. It was shown that using a
Knowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into
account relevant sub-graphs that preserve the information in a structured
manner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM
framework, that integrates RAG with KG and a vector store (VS) that store
factual domain specific information. Importantly, to avoid hallucinations in
the KG, we build these highly domain-specific KGs and VSs without the use of
LLMs, but via NLP, data mining, and nonnegative tensor factorization with
automatic model selection. Pairing our RAG with a domain-specific: (i) KG
(containing structured information), and (ii) VS (containing unstructured
information) enables the development of domain-specific chat-bots that
attribute the source of information, mitigate hallucinations, lessen the need
for fine-tuning, and excel in highly domain-specific question answering tasks.
We pair SMART-SLIC with chain-of-thought prompting agents. The framework is
designed to be generalizable to adapt to any specific or specialized domain. In
this paper, we demonstrate the question answering capabilities of our framework
on a corpus of scientific publications on malware analysis and anomaly
detection.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.02721v1},
File          = {2410.02721v1.pdf}
}
@article{2410.02356v1,
Author        = {Adam Caulton},
Title         = {Hume's dictum as a guide to ontology},
Eprint        = {2410.02356v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {physics.hist-ph},
Abstract      = {In this paper I aim to defend one version at least of Hume's dictum: roughly,
the idea that possibility is determined by ontology through something like
independent variation. My defence is broadly pragmatic, in the sense that
adherence to something like Hume's dictum delivers at least three benefits. The
first benefit is that, through Hume's dictum, a physical theory's ontology
delimits a range of possibilities, that I call \emph{kinematical
possibilities}, which serves as a sufficiently permissive notion of possibility
to sustain something like an intensional semantics for its claims, and a
sufficiently demanding notion of supervenience to sustain plausible claims of
inter-theoretic reduction and theoretical equivalence. The second benefit is
that Hume's dictum allows us to work backwards from a range of kinematical
possibilities to an ontology. This is especially useful when aiming to glean an
interpretation of a physical theory, since often we are more confident that we
have arrived at the right space of possibilities than that we have arrived at
the right ontology. The third benefit is that Hume's dictum -- at least the
version I aim to defend here -- may be applied to physical theories more or
less as we find them, and therefore we can practice something resembling
ontology without having to force our theories into some Procrustean bed, such
as a first-order language.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.02356v1},
File          = {2410.02356v1.pdf}
}
@article{2410.00616v1,
Author        = {LÃ©on-Paul Schaub Torre and Pelayo QuirÃ³s and Helena GarcÃ­a Mieres},
Title         = {DetecciÃ³n AutomÃ¡tica de PatologÃ­as en Notas ClÃ­nicas en
  EspaÃ±ol Combinando Modelos de Lenguaje y OntologÃ­as MÃ©dicos},
Eprint        = {2410.00616v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this paper we present a hybrid method for the automatic detection of
dermatological pathologies in medical reports. We use a large language model
combined with medical ontologies to predict, given a first appointment or
follow-up medical report, the pathology a person may suffer from. The results
show that teaching the model to learn the type, severity and location on the
body of a dermatological pathology as well as in which order it has to learn
these three features significantly increases its accuracy. The article presents
the demonstration of state-of-the-art results for classification of medical
texts with a precision of 0.84, micro and macro F1-score of 0.82 and 0.75, and
makes both the method and the dataset used available to the community.
  --
  En este art\'iculo presentamos un m\'etodo h\'ibrido para la detecci\'on
autom\'atica de patolog\'ias dermatol\'ogicas en informes m\'edicos. Usamos un
modelo de lenguaje amplio en espa\~nol combinado con ontolog\'ias m\'edicas
para predecir, dado un informe m\'edico de primera cita o de seguimiento, la
patolog\'ia del paciente. Los resultados muestran que el tipo, la gravedad y el
sitio en el cuerpo de una patolog\'ia dermatol\'ogica, as\'i como en qu\'e
orden tiene un modelo que aprender esas tres caracter\'isticas, aumentan su
precisi\'on. El art\'iculo presenta la demostraci\'on de resultados comparables
al estado del arte de clasificaci\'on de textos m\'edicos con una precisi\'on
de 0.84, micro y macro F1-score de 0.82 y 0.75, y deja a disposici\'on de la
comunidad tanto el m\'etodo como el conjunto de datos utilizado.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.00616v1},
File          = {2410.00616v1.pdf}
}
@article{2409.20302v4,
Author        = {Zhangcheng Qiang and Kerry Taylor and Weiqing Wang},
Title         = {OM4OV: Leveraging Ontology Matching for Ontology Versioning},
Eprint        = {2409.20302v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Due to the dynamic nature of the Semantic Web, version control is necessary
to capture time-varying information, particularly for widely used ontologies.
Despite the long-standing recognition of ontology versioning (OV) as a crucial
component for efficient ontology management, the growing size of ontologies and
accumulating errors caused by manual labour overwhelm current OV approaches. In
this paper, we propose a fresh approach to performing OV using existing
ontology matching (OM) techniques and systems. We introduce a unified OM4OV
pipeline. From an OM perspective, we reconstruct a new task formulation and
measurements for OV tasks. Building upon the prior alignment(s) from OM, we
propose a pipeline optimisation method called the cross-reference (CR)
mechanism to enhance overall OV performance. We experimentally validate the
OM4OV pipeline and the cross-reference mechanism in an OV testbed originating
from the Ontology Alignment Evaluation Initiative (OAEI) datasets. We also
discuss insights into OM used for OV tasks, where some apparent false mappings
detected by OV systems are not actually untrue.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.20302v4},
File          = {2409.20302v4.pdf}
}
@article{2409.20010v1,
Author        = {Frank Wawrzik and Matthias Plaue and Savan Vekariya and Christoph Grimm},
Title         = {Customized Information and Domain-centric Knowledge Graph Construction
  with Large Language Models},
Eprint        = {2409.20010v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In this paper we propose a novel approach based on knowledge graphs to
provide timely access to structured information, to enable actionable
technology intelligence, and improve cyber-physical systems planning. Our
framework encompasses a text mining process, which includes information
retrieval, keyphrase extraction, semantic network creation, and topic map
visualization. Following this data exploration process, we employ a selective
knowledge graph construction (KGC) approach supported by an electronics and
innovation ontology-backed pipeline for multi-objective decision-making with a
focus on cyber-physical systems. We apply our methodology to the domain of
automotive electrical systems to demonstrate the approach, which is scalable.
Our results demonstrate that our construction process outperforms GraphGPT as
well as our bi-LSTM and transformer REBEL with a pre-defined dataset by several
times in terms of class recognition, relationship construction and correct
"sublass of" categorization. Additionally, we outline reasoning applications
and provide a comparison with Wikidata to show the differences and advantages
of the approach.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.20010v1},
File          = {2409.20010v1.pdf}
}
@article{2409.19653v2,
Author        = {Paul Knowles and Bart Gajderowicz and Keith Dugas},
Title         = {Data-Centric Design: Introducing An Informatics Domain Model And Core
  Data Ontology For Computational Systems},
Eprint        = {2409.19653v2},
DOI           = {10.5121/csit.2024.141720},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DC},
Abstract      = {The Core Data Ontology (CDO) and the Informatics Domain Model represent a
transformative approach to computational systems, shifting from traditional
node-centric designs to a data-centric paradigm. This paper introduces a
framework where data is categorized into four modalities: objects, events,
concepts, and actions. This quadrimodal structure enhances data security,
semantic interoperability, and scalability across distributed data ecosystems.
The CDO offers a comprehensive ontology that supports AI development,
role-based access control, and multimodal data management. By focusing on the
intrinsic value of data, the Informatics Domain Model redefines system
architectures to prioritize data security, provenance, and auditability,
addressing vulnerabilities in current models. The paper outlines the
methodology for developing the CDO, explores its practical applications in
fields such as AI, robotics, and legal compliance, and discusses future
directions for scalable, decentralized, and interoperable data ecosystems.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.19653v2},
File          = {2409.19653v2.pdf}
}
@article{2409.18753v1,
Author        = {Jihen Amara and Birgitta KÃ¶nig-Ries and Sheeba Samuel},
Title         = {Enhancing Explainability in Multimodal Large Language Models Using
  Ontological Context},
Eprint        = {2409.18753v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Recently, there has been a growing interest in Multimodal Large Language
Models (MLLMs) due to their remarkable potential in various tasks integrating
different modalities, such as image and text, as well as applications such as
image captioning and visual question answering. However, such models still face
challenges in accurately captioning and interpreting specific visual concepts
and classes, particularly in domain-specific applications. We argue that
integrating domain knowledge in the form of an ontology can significantly
address these issues. In this work, as a proof of concept, we propose a new
framework that combines ontology with MLLMs to classify images of plant
diseases. Our method uses concepts about plant diseases from an existing
disease ontology to query MLLMs and extract relevant visual concepts from
images. Then, we use the reasoning capabilities of the ontology to classify the
disease according to the identified concepts. Ensuring that the model
accurately uses the concepts describing the disease is crucial in
domain-specific applications. By employing an ontology, we can assist in
verifying this alignment. Additionally, using the ontology's inference
capabilities increases transparency, explainability, and trust in the
decision-making process while serving as a judge by checking if the annotations
of the concepts by MLLMs are aligned with those in the ontology and displaying
the rationales behind their errors. Our framework offers a new direction for
synergizing ontologies and MLLMs, supported by an empirical study using
different well-known MLLMs.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.18753v1},
File          = {2409.18753v1.pdf}
}
@article{2409.17602v1,
Author        = {Andrea Cimmino and Juan Cano-Benito and RaÃºl GarcÃ­a-Castro},
Title         = {Open Digital Rights Enforcement Framework (ODRE): from descriptive to
  enforceable policies},
Eprint        = {2409.17602v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {From centralised platforms to decentralised ecosystems, like Data Spaces,
sharing data has become a paramount challenge. For this reason, the definition
of data usage policies has become crucial in these domains, highlighting the
necessity of effective policy enforcement mechanisms. The Open Digital Rights
Language (ODRL) is a W3C standard ontology designed to describe data usage
policies, however, it lacks built-in enforcement capabilities, limiting its
practical application. This paper introduces the Open Digital Rights
Enforcement (ODRE) framework, whose goal is to provide ODRL with enforcement
capabilities. The ODRE framework proposes a novel approach to express ODRL
policies that integrates the descriptive ontology terms of ODRL with other
languages that allow behaviour specification, such as dynamic data handling or
function evaluation. The framework includes an enforcement algorithm for ODRL
policies and two open-source implementations in Python and Java. The ODRE
framework is also designed to support future extensions of ODRL to specific
domain scenarios. In addition, current limitations of ODRE, ODRL, and current
challenges are reported. Finally, to demonstrate the enforcement capabilities
of the implementations, their performance, and their extensibility features,
several experiments have been carried out with positive results.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.17602v1},
File          = {2409.17602v1.pdf}
}
@article{2409.17109v1,
Author        = {Mert Keser and Gesina Schwalbe and Niki Amini-Naieni and Matthias Rottmann and Alois Knoll},
Title         = {Unveiling Ontological Commitment in Multi-Modal Foundation Models},
Eprint        = {2409.17109v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Ontological commitment, i.e., used concepts, relations, and assumptions, are
a corner stone of qualitative reasoning (QR) models. The state-of-the-art for
processing raw inputs, though, are deep neural networks (DNNs), nowadays often
based off from multimodal foundation models. These automatically learn rich
representations of concepts and respective reasoning. Unfortunately, the
learned qualitative knowledge is opaque, preventing easy inspection,
validation, or adaptation against available QR models. So far, it is possible
to associate pre-defined concepts with latent representations of DNNs, but
extractable relations are mostly limited to semantic similarity. As a next step
towards QR for validation and verification of DNNs: Concretely, we propose a
method that extracts the learned superclass hierarchy from a multimodal DNN for
a given set of leaf concepts. Under the hood we (1) obtain leaf concept
embeddings using the DNN's textual input modality; (2) apply hierarchical
clustering to them, using that DNNs encode semantic similarities via vector
distances; and (3) label the such-obtained parent concepts using search in
available ontologies from QR. An initial evaluation study shows that meaningful
ontological class hierarchies can be extracted from state-of-the-art foundation
models. Furthermore, we demonstrate how to validate and verify a DNN's learned
representations against given ontologies. Lastly, we discuss potential future
applications in the context of QR.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.17109v1},
File          = {2409.17109v1.pdf}
}
@article{2410.08215v1,
Author        = {SÃ©rgio Guerreiro and Jan Dietz},
Title         = {DEMO enhanced BPMN},
Eprint        = {2410.08215v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {This paper presents an integration between DEMO (Design and Engineering
Methodology for Organizations) and BPMN (Business Process Model and Notation).
While BPMN is widely used for its intuitive, flow-based representation of
business processes, it suffers from a lack of formal semantics, ambiguity, and
limitations in modeling multi-party collaborations. In contrast, DEMO offers a
theoretically robust, ontology-driven framework that focuses on abstracting the
essential structure of business processes. A novel approach combining the rigor
of DEMO's transaction patterns with the more practical, widely adopted BPMN
framework is proposed and demonstrated. This integration allows for the
benefits of DEMO's theoretical foundations to be utilized within BPMN diagrams,
providing a more comprehensive and precise understanding of business processes.
We argue that this combination enriches the modeling of business processes,
providing a more coherent and reliable tool for both practitioners and
researchers.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2410.08215v1},
File          = {2410.08215v1.pdf}
}
@article{2409.16036v1,
Author        = {Ryan Williams},
Title         = {Grounded Computation & Consciousness: A Framework for Exploring
  Consciousness in Machines & Other Organisms},
Eprint        = {2409.16036v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.NC},
Abstract      = {Computational modeling is a critical tool for understanding consciousness,
but is it enough on its own? This paper discusses the necessity for an
ontological basis of consciousness, and introduces a formal framework for
grounding computational descriptions into an ontological substrate. Utilizing
this technique, a method is demonstrated for estimating the difference in
qualitative experience between two systems. This framework has wide
applicability to computational theories of consciousness.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.16036v1},
File          = {2409.16036v1.pdf}
}
@article{2409.15881v1,
Author        = {Cezar Sas and Andrea Capiluppi},
Title         = {Automatic Bottom-Up Taxonomy Construction: A Software Application Domain
  Study},
Eprint        = {2409.15881v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {Previous research in software application domain classification has faced
challenges due to the lack of a proper taxonomy that explicitly models
relations between classes. As a result, current solutions are less effective
for real-world usage. This study aims to develop a comprehensive software
application domain taxonomy by integrating multiple datasources and leveraging
ensemble methods. The goal is to overcome the limitations of individual sources
and configurations by creating a more robust, accurate, and reproducible
taxonomy. This study employs a quantitative research design involving three
different datasources: an existing Computer Science Ontology (CSO), Wikidata,
and LLMs. The study utilises a combination of automated and human evaluations
to assess the quality of a taxonomy. The outcome measures include the number of
unlinked terms, self-loops, and overall connectivity of the taxonomy. The
results indicate that individual datasources have advantages and drawbacks: the
CSO datasource showed minimal variance across different configurations, but a
notable issue of missing technical terms and a high number of self-loops. The
Wikipedia datasource required significant filtering during construction to
improve metric performance. LLM-generated taxonomies demonstrated better
performance when using context-rich prompts. An ensemble approach showed the
most promise, successfully reducing the number of unlinked terms and
self-loops, thus creating a more connected and comprehensive taxonomy. The
study addresses the construction of a software application domain taxonomy
relying on pre-existing resources. Our results indicate that an ensemble
approach to taxonomy construction can effectively address the limitations of
individual datasources. Future work should focus on refining the ensemble
techniques and exploring additional datasources to enhance the taxonomy's
accuracy and completeness.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.15881v1},
File          = {2409.15881v1.pdf}
}
@article{2409.15861v3,
Author        = {Abdulfattah Safa and GÃ¶zde GÃ¼l Åahin},
Title         = {A Zero-Shot Open-Vocabulary Pipeline for Dialogue Understanding},
Eprint        = {2409.15861v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Dialogue State Tracking (DST) is crucial for understanding user needs and
executing appropriate system actions in task-oriented dialogues. Majority of
existing DST methods are designed to work within predefined ontologies and
assume the availability of gold domain labels, struggling with adapting to new
slots values. While Large Language Models (LLMs)-based systems show promising
zero-shot DST performance, they either require extensive computational
resources or they underperform existing fully-trained systems, limiting their
practicality. To address these limitations, we propose a zero-shot,
open-vocabulary system that integrates domain classification and DST in a
single pipeline. Our approach includes reformulating DST as a
question-answering task for less capable models and employing self-refining
prompts for more adaptable ones. Our system does not rely on fixed slot values
defined in the ontology allowing the system to adapt dynamically. We compare
our approach with existing SOTA, and show that it provides up to 20% better
Joint Goal Accuracy (JGA) over previous methods on datasets like Multi-WOZ 2.1,
with up to 90% fewer requests to the LLM API.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.15861v3},
File          = {2409.15861v3.pdf}
}
@article{2410.07154v1,
Author        = {Mikel EgaÃ±a Aranguren},
Title         = {The Transparent Relations Ontology (TRO): a vocabulary to describe
  conflicts of interest},
Eprint        = {2410.07154v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {The Transparent Relations Ontology (TRO) offers a vocabulary to publish data
about relations between powerful parties that should be more transparent, in
order to detect possible conflicts of interest. TRO is based on minimal
modelling, reusing common vocabularies to offer a simple yet useful resource to
publish interoperable data about pointers to relations that might result in
corruption cases. Additionally, best practices have been followed in order to
sustain a technically rigorous ontology development process. A usage example
with real data is mentioned, integrating information from Basque Government's
Open Data services and a news outlet. Building upon its foundational design,
future enhancements of TRO could significantly amplify its utility in
uncovering and scrutinizing opaque relationships that may lead to corruption.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2410.07154v1},
File          = {2410.07154v1.pdf}
}
@article{2409.14584v1,
Author        = {Ogen Schlachet Drukerman and Einat Minkov},
Title         = {The X Types -- Mapping the Semantics of the Twitter Sphere},
Eprint        = {2409.14584v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Social networks form a valuable source of world knowledge, where influential
entities correspond to popular accounts. Unlike factual knowledge bases (KBs),
which maintain a semantic ontology, structured semantic information is not
available on social media. In this work, we consider a social KB of roughly
200K popular Twitter accounts, which denotes entities of interest. We elicit
semantic information about those entities. In particular, we associate them
with a fine-grained set of 136 semantic types, e.g., determine whether a given
entity account belongs to a politician, or a musical artist. In the lack of
explicit type information in Twitter, we obtain semantic labels for a subset of
the accounts via alignment with the KBs of DBpedia and Wikidata. Given the
labeled dataset, we finetune a transformer-based text encoder to generate
semantic embeddings of the entities based on the contents of their accounts. We
then exploit this evidence alongside network-based embeddings to predict the
entities semantic types. In our experiments, we show high type prediction
performance on the labeled dataset. Consequently, we apply our type
classification model to all of the entity accounts in the social KB. Our
analysis of the results offers insights about the global semantics of the
Twitter sphere. We discuss downstream applications that should benefit from
semantic type information and the semantic embeddings of social entities
generated in this work. In particular, we demonstrate enhanced performance on
the key task of entity similarity assessment using this information.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.14584v1},
File          = {2409.14584v1.pdf}
}
@article{2409.14273v1,
Author        = {Anirudh S Chakravarthy and Meghana Reddy Ganesina and Peiyun Hu and Laura Leal-Taixe and Shu Kong and Deva Ramanan and Aljosa Osep},
Title         = {Lidar Panoptic Segmentation in an Open World},
Eprint        = {2409.14273v1},
DOI           = {10.1007/s11263-024-02166-9},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Addressing Lidar Panoptic Segmentation (LPS ) is crucial for safe deployment
of autonomous vehicles. LPS aims to recognize and segment lidar points w.r.t. a
pre-defined vocabulary of semantic classes, including thing classes of
countable objects (e.g., pedestrians and vehicles) and stuff classes of
amorphous regions (e.g., vegetation and road). Importantly, LPS requires
segmenting individual thing instances (e.g., every single vehicle). Current LPS
methods make an unrealistic assumption that the semantic class vocabulary is
fixed in the real open world, but in fact, class ontologies usually evolve over
time as robots encounter instances of novel classes that are considered to be
unknowns w.r.t. the pre-defined class vocabulary. To address this unrealistic
assumption, we study LPS in the Open World (LiPSOW): we train models on a
dataset with a pre-defined semantic class vocabulary and study their
generalization to a larger dataset where novel instances of thing and stuff
classes can appear. This experimental setting leads to interesting conclusions.
While prior art train class-specific instance segmentation methods and obtain
state-of-the-art results on known classes, methods based on class-agnostic
bottom-up grouping perform favorably on classes outside of the initial class
vocabulary (i.e., unknown classes). Unfortunately, these methods do not perform
on-par with fully data-driven methods on known classes. Our work suggests a
middle ground: we perform class-agnostic point clustering and over-segment the
input cloud in a hierarchical fashion, followed by binary point segment
classification, akin to Region Proposal Network [1]. We obtain the final point
cloud segmentation by computing a cut in the weighted hierarchical tree of
point segments, independently of semantic classification. Remarkably, this
unified approach leads to strong performance on both known and unknown classes.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.14273v1},
File          = {2409.14273v1.pdf}
}
@article{2409.14043v1,
Author        = {Pranav Gupta and Raunak Sharma and Rashmi Kumari and Sri Krishna Aditya and Shwetank Choudhary and Sumit Kumar and Kanchana M and Thilagavathy R},
Title         = {ECHO: Environmental Sound Classification with Hierarchical
  Ontology-guided Semi-Supervised Learning},
Eprint        = {2409.14043v1},
DOI           = {10.1109/CONECCT62155.2024.10677303},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SD},
Abstract      = {Environment Sound Classification has been a well-studied research problem in
the field of signal processing and up till now more focus has been laid on
fully supervised approaches. Over the last few years, focus has moved towards
semi-supervised methods which concentrate on the utilization of unlabeled data,
and self-supervised methods which learn the intermediate representation through
pretext task or contrastive learning. However, both approaches require a vast
amount of unlabelled data to improve performance. In this work, we propose a
novel framework called Environmental Sound Classification with Hierarchical
Ontology-guided semi-supervised Learning (ECHO) that utilizes label
ontology-based hierarchy to learn semantic representation by defining a novel
pretext task. In the pretext task, the model tries to predict coarse labels
defined by the Large Language Model (LLM) based on ground truth label ontology.
The trained model is further fine-tuned in a supervised way to predict the
actual task. Our proposed novel semi-supervised framework achieves an accuracy
improvement in the range of 1\% to 8\% over baseline systems across three
datasets namely UrbanSound8K, ESC-10, and ESC-50.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.14043v1},
File          = {2409.14043v1.pdf}
}
@article{2409.14038v5,
Author        = {Zhangcheng Qiang and Kerry Taylor and Weiqing Wang and Jing Jiang},
Title         = {OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model
  Hallucinations in Ontology Matching},
Eprint        = {2409.14038v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Hallucinations of large language models (LLMs) commonly occur in
domain-specific downstream tasks, with no exception in ontology matching (OM).
The prevalence of using LLMs for OM raises the need for benchmarks to better
understand LLM hallucinations. The OAEI-LLM dataset is an extended version of
the Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate
LLM-specific hallucinations in OM tasks. We outline the methodology used in
dataset construction and schema extension, and provide examples of potential
use cases.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.14038v5},
File          = {2409.14038v5.pdf}
}
@article{2409.13906v1,
Author        = {Harshad Hegde and Jennifer Vendetti and Damien Goutte-Gattat and J Harry Caufield and John B Graybeal and Nomi L Harris and Naouel Karam and Christian Kindermann and Nicolas Matentzoglu and James A Overton and Mark A Musen and Christopher J Mungall},
Title         = {A Change Language for Ontologies and Knowledge Graphs},
Eprint        = {2409.13906v1},
DOI           = {10.1093/database/baae133},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Ontologies and knowledge graphs (KGs) are general-purpose computable
representations of some domain, such as human anatomy, and are frequently a
crucial part of modern information systems. Most of these structures change
over time, incorporating new knowledge or information that was previously
missing. Managing these changes is a challenge, both in terms of communicating
changes to users, and providing mechanisms to make it easier for multiple
stakeholders to contribute.
  To fill that need, we have created KGCL, the Knowledge Graph Change Language,
a standard data model for describing changes to KGs and ontologies at a high
level, and an accompanying human-readable controlled natural language. This
language serves two purposes: a curator can use it to request desired changes,
and it can also be used to describe changes that have already happened,
corresponding to the concepts of "apply patch" and "diff" commonly used for
managing changes in text documents and computer programs. Another key feature
of KGCL is that descriptions are at a high enough level to be useful and
understood by a variety of stakeholders--for example, ontology edits can be
specified by commands like "add synonym 'arm' to 'forelimb'" or "move
'Parkinson disease' under 'neurodegenerative disease'".
  We have also built a suite of tools for managing ontology changes. These
include an automated agent that integrates with and monitors GitHub ontology
repositories and applies any requested changes, and a new component in the
BioPortal ontology resource that allows users to make change requests directly
from within the BioPortal user interface.
  Overall, the KGCL data model, its controlled natural language, and associated
tooling allow for easier management and processing of changes associated with
the development of ontologies and KGs.},
Year          = {2024},
Month         = {Sep},
Note          = {Database (Oxford). 2025 Jan 22;2025:baae133},
Url           = {http://arxiv.org/abs/2409.13906v1},
File          = {2409.13906v1.pdf}
}
@article{2409.13425v1,
Author        = {Sascha Meckler},
Title         = {Procedure Model for Building Knowledge Graphs for Industry Applications},
Eprint        = {2409.13425v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Enterprise knowledge graphs combine business data and organizational
knowledge by means of a semantic network of concepts, properties, individuals
and relationships. The graph-based integration of previously unconnected
information with domain knowledge provides new insights and enables intelligent
business applications. However, knowledge graph construction is a large
investment which requires a joint effort of domain and technical experts. This
paper presents a practical step-by-step procedure model for building an RDF
knowledge graph that interconnects heterogeneous data and expert knowledge for
an industry use case. The self-contained process adapts the "Cross Industry
Standard Process for Data Mining" and uses competency questions throughout the
entire development cycle. The procedure model starts with business and data
understanding, describes tasks for ontology modeling and the graph setup, and
ends with process steps for evaluation and deployment.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.13425v1},
File          = {2409.13425v1.pdf}
}
@article{2409.13038v1,
Author        = {JuliÃ¡n N. Acosta and Xiaoman Zhang and Siddhant Dogra and Hong-Yu Zhou and Seyedmehdi Payabvash and Guido J. Falcone and Eric K. Oermann and Pranav Rajpurkar},
Title         = {HeadCT-ONE: Enabling Granular and Controllable Automated Evaluation of
  Head CT Radiology Report Generation},
Eprint        = {2409.13038v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We present Head CT Ontology Normalized Evaluation (HeadCT-ONE), a metric for
evaluating head CT report generation through ontology-normalized entity and
relation extraction. HeadCT-ONE enhances current information extraction derived
metrics (such as RadGraph F1) by implementing entity normalization through
domain-specific ontologies, addressing radiological language variability.
HeadCT-ONE compares normalized entities and relations, allowing for
controllable weighting of different entity types or specific entities. Through
experiments on head CT reports from three health systems, we show that
HeadCT-ONE's normalization and weighting approach improves the capture of
semantically equivalent reports, better distinguishes between normal and
abnormal reports, and aligns with radiologists' assessment of clinically
significant errors, while offering flexibility to prioritize specific aspects
of report content. Our results demonstrate how HeadCT-ONE enables more
flexible, controllable, and granular automated evaluation of head CT reports.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.13038v1},
File          = {2409.13038v1.pdf}
}
@article{2409.15369v1,
Author        = {Bo Xiong},
Title         = {Geometric Relational Embeddings},
Eprint        = {2409.15369v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Relational representation learning transforms relational data into continuous
and low-dimensional vector representations. However, vector-based
representations fall short in capturing crucial properties of relational data
that are complex and symbolic. We propose geometric relational embeddings, a
paradigm of relational embeddings that respect the underlying symbolic
structures. Specifically, this dissertation introduces various geometric
relational embedding models capable of capturing: 1) complex structured
patterns like hierarchies and cycles in networks and knowledge graphs; 2)
logical structures in ontologies and logical constraints applicable for
constraining machine learning model outputs; and 3) high-order structures
between entities and relations. Our results obtained from benchmark and
real-world datasets demonstrate the efficacy of geometric relational embeddings
in adeptly capturing these discrete, symbolic, and structured properties
inherent in relational data.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.15369v1},
File          = {2409.15369v1.pdf}
}
@article{2409.11449v1,
Author        = {Yannis Vasilakis and Rachel Bittner and Johan Pauwels},
Title         = {Evaluation of pretrained language models on music understanding},
Eprint        = {2409.11449v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Music-text multimodal systems have enabled new approaches to Music
Information Research (MIR) applications such as audio-to-text and text-to-audio
retrieval, text-based song generation, and music captioning. Despite the
reported success, little effort has been put into evaluating the musical
knowledge of Large Language Models (LLM). In this paper, we demonstrate that
LLMs suffer from 1) prompt sensitivity, 2) inability to model negation (e.g.
'rock song without guitar'), and 3) sensitivity towards the presence of
specific words. We quantified these properties as a triplet-based accuracy,
evaluating the ability to model the relative similarity of labels in a
hierarchical ontology. We leveraged the Audioset ontology to generate triplets
consisting of an anchor, a positive (relevant) label, and a negative (less
relevant) label for the genre and instruments sub-tree. We evaluated the
triplet-based musical knowledge for six general-purpose Transformer-based
models. The triplets obtained through this methodology required filtering, as
some were difficult to judge and therefore relatively uninformative for
evaluation purposes. Despite the relatively high accuracy reported,
inconsistencies are evident in all six models, suggesting that off-the-shelf
LLMs need adaptation to music before use.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.11449v1},
File          = {2409.11449v1.pdf}
}
@article{2409.10146v1,
Author        = {Hamed Babaei Giglou and Jennifer D'Souza and SÃ¶ren Auer},
Title         = {LLMs4OL 2024 Overview: The 1st Large Language Models for Ontology
  Learning Challenge},
Eprint        = {2409.10146v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper outlines the LLMs4OL 2024, the first edition of the Large Language
Models for Ontology Learning Challenge. LLMs4OL is a community development
initiative collocated with the 23rd International Semantic Web Conference
(ISWC) to explore the potential of Large Language Models (LLMs) in Ontology
Learning (OL), a vital process for enhancing the web with structured knowledge
to improve interoperability. By leveraging LLMs, the challenge aims to advance
understanding and innovation in OL, aligning with the goals of the Semantic Web
to create a more intelligent and user-friendly web. In this paper, we give an
overview of the 2024 edition of the LLMs4OL challenge and summarize the
contributions.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.10146v1},
File          = {2409.10146v1.pdf}
}
@article{2409.08820v2,
Author        = {Xueli Pan and Jacco van Ossenbruggen and Victor de Boer and Zhisheng Huang},
Title         = {A RAG Approach for Generating Competency Questions in Ontology
  Engineering},
Eprint        = {2409.08820v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Competency question (CQ) formulation is central to several ontology
development and evaluation methodologies. Traditionally, the task of crafting
these competency questions heavily relies on the effort of domain experts and
knowledge engineers which is often time-consuming and labor-intensive. With the
emergence of Large Language Models (LLMs), there arises the possibility to
automate and enhance this process. Unlike other similar works which use
existing ontologies or knowledge graphs as input to LLMs, we present a
retrieval-augmented generation (RAG) approach that uses LLMs for the automatic
generation of CQs given a set of scientific papers considered to be a domain
knowledge base. We investigate its performance and specifically, we study the
impact of different number of papers to the RAG and different temperature
setting of the LLM. We conduct experiments using GPT-4 on two domain ontology
engineering tasks and compare results against ground-truth CQs constructed by
domain experts. Empirical assessments on the results, utilizing evaluation
metrics (precision and consistency), reveal that compared to zero-shot
prompting, adding relevant domain knowledge to the RAG improves the performance
of LLMs on generating CQs for concrete ontology engineering tasks.},
Year          = {2024},
Month         = {Sep},
Note          = {18th International Conference on Metadata and Semantics Research
  (MTSR2024)},
Url           = {http://arxiv.org/abs/2409.08820v2},
File          = {2409.08820v2.pdf}
}
@article{2409.13746v2,
Author        = {Thanh Son Do and Daniel B. Hier and Tayo Obafemi-Ajayi},
Title         = {Mapping Biomedical Ontology Terms to IDs: Effect of Domain Prevalence on
  Prediction Accuracy},
Eprint        = {2409.13746v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This study evaluates the ability of large language models (LLMs) to map
biomedical ontology terms to their corresponding ontology IDs across the Human
Phenotype Ontology (HPO), Gene Ontology (GO), and UniProtKB terminologies.
Using counts of ontology IDs in the PubMed Central (PMC) dataset as a surrogate
for their prevalence in the biomedical literature, we examined the relationship
between ontology ID prevalence and mapping accuracy. Results indicate that
ontology ID prevalence strongly predicts accurate mapping of HPO terms to HPO
IDs, GO terms to GO IDs, and protein names to UniProtKB accession numbers.
Higher prevalence of ontology IDs in the biomedical literature correlated with
higher mapping accuracy. Predictive models based on receiver operating
characteristic (ROC) curves confirmed this relationship.
  In contrast, this pattern did not apply to mapping protein names to Human
Genome Organisation's (HUGO) gene symbols. GPT-4 achieved a high baseline
performance (95%) in mapping protein names to HUGO gene symbols, with mapping
accuracy unaffected by prevalence. We propose that the high prevalence of HUGO
gene symbols in the literature has caused these symbols to become lexicalized,
enabling GPT-4 to map protein names to HUGO gene symbols with high accuracy.
These findings highlight the limitations of LLMs in mapping ontology terms to
low-prevalence ontology IDs and underscore the importance of incorporating
ontology ID prevalence into the training and evaluation of LLMs for biomedical
applications.},
Year          = {2024},
Month         = {Sep},
Note          = {2025 IEEE Conference on Artificial Intelligence (CAI)},
Url           = {http://arxiv.org/abs/2409.13746v2},
File          = {2409.13746v2.pdf}
}
@article{2409.07088v1,
Author        = {Daehee Kim and Deokhyung Kang and Sangwon Ryu and Gary Geunbae Lee},
Title         = {Ontology-Free General-Domain Knowledge Graph-to-Text Generation Dataset
  Synthesis using Large Language Model},
Eprint        = {2409.07088v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Graph-to-Text (G2T) generation involves verbalizing structured
knowledge graphs into natural language text. Recent advancements in Pretrained
Language Models (PLMs) have improved G2T performance, but their effectiveness
depends on datasets with precise graph-text alignment. However, the scarcity of
high-quality, general-domain G2T generation datasets restricts progress in the
general-domain G2T generation research. To address this issue, we introduce
Wikipedia Ontology-Free Graph-text dataset (WikiOFGraph), a new large-scale G2T
dataset generated using a novel method that leverages Large Language Model
(LLM) and Data-QuestEval. Our new dataset, which contains 5.85M general-domain
graph-text pairs, offers high graph-text consistency without relying on
external ontologies. Experimental results demonstrate that PLM fine-tuned on
WikiOFGraph outperforms those trained on other datasets across various
evaluation metrics. Our method proves to be a scalable and effective solution
for generating high-quality G2T data, significantly advancing the field of G2T
generation.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.07088v1},
File          = {2409.07088v1.pdf}
}
@article{2409.13744v2,
Author        = {Daniel B. Hier and Thanh Son Do and Tayo Obafemi-Ajayi},
Title         = {A Simplified Retriever to Improve Accuracy of Phenotype Normalizations
  by Large Language Models},
Eprint        = {2409.13744v2},
DOI           = {10.3389/fdgth.2025.1495040},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) have shown improved accuracy in phenotype term
normalization tasks when augmented with retrievers that suggest candidate
normalizations based on term definitions. In this work, we introduce a
simplified retriever that enhances LLM accuracy by searching the Human
Phenotype Ontology (HPO) for candidate matches using contextual word embeddings
from BioBERT without the need for explicit term definitions. Testing this
method on terms derived from the clinical synopses of Online Mendelian
Inheritance in Man (OMIM), we demonstrate that the normalization accuracy of a
state-of-the-art LLM increases from a baseline of 62.3% without augmentation to
90.3% with retriever augmentation. This approach is potentially generalizable
to other biomedical term normalization tasks and offers an efficient
alternative to more complex retrieval methods.},
Year          = {2024},
Month         = {Sep},
Note          = {Frontiers in Digital Health 7, (2025): 1495040. Accessed March 4,
  2025},
Url           = {http://arxiv.org/abs/2409.13744v2},
File          = {2409.13744v2.pdf}
}
@article{2409.06150v1,
Author        = {Naren Khatwani and James Geller},
Title         = {What makes a good concept anyway ?},
Eprint        = {2409.06150v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {A good medical ontology is expected to cover its domain completely and
correctly. On the other hand, large ontologies are hard to build, hard to
understand, and hard to maintain. Thus, adding new concepts (often multi-word
concepts) to an existing ontology must be done judiciously. Only "good"
concepts should be added; however, it is difficult to define what makes a
concept good. In this research, we propose a metric to measure the goodness of
a concept. We identified factors that appear to influence goodness judgments of
medical experts and combined them into a single metric. These factors include
concept name length (in words), concept occurrence frequency in the medical
literature, and syntactic categories of component words. As an added factor, we
used the simplicity of a term after mapping it into a specific foreign
language. We performed Bayesian optimization of factor weights to achieve
maximum agreement between the metric and three medical experts. The results
showed that our metric had a 50.67% overall agreement with the experts, as
measured by Krippendorff's alpha.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.06150v1},
File          = {2409.06150v1.pdf}
}
@article{2409.05556v1,
Author        = {Alireza Ghafarollahi and Markus J. Buehler},
Title         = {SciAgents: Automating scientific discovery through multi-agent
  intelligent graph reasoning},
Eprint        = {2409.05556v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {A key challenge in artificial intelligence is the creation of systems capable
of autonomously advancing scientific understanding by exploring novel domains,
identifying complex patterns, and uncovering previously unseen connections in
vast scientific data. In this work, we present SciAgents, an approach that
leverages three core concepts: (1) the use of large-scale ontological knowledge
graphs to organize and interconnect diverse scientific concepts, (2) a suite of
large language models (LLMs) and data retrieval tools, and (3) multi-agent
systems with in-situ learning capabilities. Applied to biologically inspired
materials, SciAgents reveals hidden interdisciplinary relationships that were
previously considered unrelated, achieving a scale, precision, and exploratory
power that surpasses traditional human-driven research methods. The framework
autonomously generates and refines research hypotheses, elucidating underlying
mechanisms, design principles, and unexpected material properties. By
integrating these capabilities in a modular fashion, the intelligent system
yields material discoveries, critique and improve existing hypotheses, retrieve
up-to-date data about existing research, and highlights their strengths and
limitations. Our case studies demonstrate scalable capabilities to combine
generative AI, ontological representations, and multi-agent modeling,
harnessing a `swarm of intelligence' similar to biological systems. This
provides new avenues for materials discovery and accelerates the development of
advanced materials by unlocking Nature's design principles.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.05556v1},
File          = {2409.05556v1.pdf}
}
@article{2409.04936v1,
Author        = {Amro M. Farid},
Title         = {A Hetero-functional Graph Resilience Analysis for Convergent
  Systems-of-Systems},
Eprint        = {2409.04936v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {eess.SY},
Abstract      = {Our modern life has grown to depend on many and nearly ubiquitous large
complex engineering systems. Many disciplines now seemingly ask the same
question: ``In the face of assumed disruption, to what degree will these
systems continue to perform and when will they be able to bounce back to normal
operation"? Furthermore, there is a growing recognition that the greatest
societal challenges of the Anthropocene era are intertwined, necessitating a
convergent systems-of-systems modeling and analysis framework based upon
reconciled ontologies, data, and theoretical methods. Consequently, this paper
develops a methodology for hetero-functional graph resilience analysis and
demonstrates it on a convergent system-of-systems. It uses the Systems Modeling
Language, model-based systems engineering and Hetero-Functional Graph Theory
(HFGT) to overcome the convergence research challenges when constructing models
and measures from multiple disciplines for systems resilience. The paper
includes both the ``survival" as well as ``recovery" components of resilience.
It also strikes a middle ground between two disparate approaches to resilience
measurement: structural measurement of formal graphs and detailed behavioral
simulation. This paper also generalizes a previous resilience measure based on
HFGT and benefits from recent theoretical and computational developments in
HFGT. To demonstrate the methodological developments, the resilience analysis
is conducted on a hypothetical energy-water nexus system of moderate size as a
type of system-of-systems.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.04936v1},
File          = {2409.04936v1.pdf}
}
@article{2409.04811v1,
Author        = {Sanne Vergouwen and Sebastian De Haro},
Title         = {Supersymmetry in the Seiberg-Witten Theory: A Window into Quantum Field
  Theory},
Eprint        = {2409.04811v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {physics.hist-ph},
Abstract      = {We take supersymmetry in the Seiberg-Witten theory as a case study of the
uses of (super)symmetry arguments in studying the ontology of four-dimensional
interacting quantum field theories. Together with a double expansion,
supersymmetry is a via media that helps to bridge the gap between the
ontologies of an exact quantum field theory and its semi-classical limit. We
discuss a class of states that exist at any value of the coupling, and whose
properties such as mass, electric and magnetic charges, and spin quantum
numbers can be precisely characterised at low energies. The low-energy theory
is best presented as a one-dimensional complex manifold, equipped with metric
and other structures: namely, the space of low-energy vacua, covered by three
open regions that are interpreted as macroscopic phases. We discuss two cases
of emergence: the emergence of the low-energy regime and the emergence between
models at low energies, thereby highlighting the significance of the topology
of the space of vacua for such cases of emergence.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.04811v1},
File          = {2409.04811v1.pdf}
}
@article{2409.02487v1,
Author        = {Bekir BaytaÅ and Ozan Ekin Derin},
Title         = {Category-theoretic formulation of relational materialism},
Eprint        = {2409.02487v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {physics.hist-ph},
Abstract      = {This brief brochure is intended to present a philosophical theory known as
relational materialism. We introduce the postulates and principles of the
theory, articulating its ontological and epistemological content using the
language of category theory. The identification of any existing entity is
primarily characterized by its relational, finite, and non-static nature.
Furthermore, we provide a categorical construction of particularities within
the relational materialist onto-epistemology. Our objective is to address and
transform a specific perspective prevalent in scientific communities into a
productive network of philosophical commitments.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.02487v1},
File          = {2409.02487v1.pdf}
}
@article{2409.09058v1,
Author        = {William Johnson and James Davis and Tara Kelly},
Title         = {Redefining Data-Centric Design: A New Approach with a Domain Model and
  Core Data Ontology for Computational Systems},
Eprint        = {2409.09058v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DC},
Abstract      = {This paper presents an innovative data-centric paradigm for designing
computational systems by introducing a new informatics domain model. The
proposed model moves away from the conventional node-centric framework and
focuses on data-centric categorization, using a multimodal approach that
incorporates objects, events, concepts, and actions. By drawing on
interdisciplinary research and establishing a foundational ontology based on
these core elements, the model promotes semantic consistency and secure data
handling across distributed ecosystems. We also explore the implementation of
this model as an OWL 2 ontology, discuss its potential applications, and
outline its scalability and future directions for research. This work aims to
serve as a foundational guide for system designers and data architects in
developing more secure, interoperable, and scalable data systems.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.09058v1},
File          = {2409.09058v1.pdf}
}
@article{2409.00830v1,
Author        = {Saransh Kumar Gupta and Lipika Dey and Partha Pratim Das and Ramesh Jain},
Title         = {Building FKG.in: a Knowledge Graph for Indian Food},
Eprint        = {2409.00830v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper presents an ontology design along with knowledge engineering, and
multilingual semantic reasoning techniques to build an automated system for
assimilating culinary information for Indian food in the form of a knowledge
graph. The main focus is on designing intelligent methods to derive ontology
designs and capture all-encompassing knowledge about food, recipes,
ingredients, cooking characteristics, and most importantly, nutrition, at
scale. We present our ongoing work in this workshop paper, describe in some
detail the relevant challenges in curating knowledge of Indian food, and
propose our high-level ontology design. We also present a novel workflow that
uses AI, LLM, and language technology to curate information from recipe blog
sites in the public domain to build knowledge graphs for Indian food. The
methods for knowledge curation proposed in this paper are generic and can be
replicated for any domain. The design is application-agnostic and can be used
for AI-driven smart analysis, building recommendation systems for Personalized
Digital Health, and complementing the knowledge graph for Indian food with
contextual information such as user information, food biochemistry, geographic
information, agricultural information, etc.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.00830v1},
File          = {2409.00830v1.pdf}
}
@article{2409.02122v1,
Author        = {Sumit Dalal and Sarika Jain and Mayank Dave},
Title         = {Deep Knowledge-Infusion For Explainable Depression Detection},
Eprint        = {2409.02122v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Discovering individuals depression on social media has become increasingly
important. Researchers employed ML/DL or lexicon-based methods for automated
depression detection. Lexicon based methods, explainable and easy to implement,
match words from user posts in a depression dictionary without considering
contexts. While the DL models can leverage contextual information, their
black-box nature limits their adoption in the domain. Though surrogate models
like LIME and SHAP can produce explanations for DL models, the explanations are
suitable for the developer and of limited use to the end user. We propose a
Knolwedge-infused Neural Network (KiNN) incorporating domain-specific knowledge
from DepressionFeature ontology (DFO) in a neural network to endow the model
with user-level explainability regarding concepts and processes the clinician
understands. Further, commonsense knowledge from the Commonsense Transformer
(COMET) trained on ATOMIC is also infused to consider the generic emotional
aspects of user posts in depression detection. The model is evaluated on three
expertly curated datasets related to depression. We observed the model to have
a statistically significant (p<0.1) boost in performance over the best
domain-specific model, MentalBERT, across CLEF e-Risk (25% MCC increase, 12% F1
increase). A similar trend is observed across the PRIMATE dataset, where the
proposed model performed better than MentalBERT (2.5% MCC increase, 19% F1
increase). The observations confirm the generated explanations to be
informative for MHPs compared to post hoc model explanations. Results
demonstrated that the user-level explainability of KiNN also surpasses the
performance of baseline models and can provide explanations where other
baselines fall short. Infusing the domain and commonsense knowledge in KiNN
enhances the ability of models like GPT-3.5 to generate application-relevant
explanations.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.02122v1},
File          = {2409.02122v1.pdf}
}
@article{2409.00610v1,
Author        = {Shania Mitra and Lei Huang and Manolis Kellis},
Title         = {ProteinRPN: Towards Accurate Protein Function Prediction with
  Graph-Based Region Proposals},
Eprint        = {2409.00610v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.QM},
Abstract      = {Protein function prediction is a crucial task in bioinformatics, with
significant implications for understanding biological processes and disease
mechanisms. While the relationship between sequence and function has been
extensively explored, translating protein structure to function continues to
present substantial challenges. Various models, particularly, CNN and
graph-based deep learning approaches that integrate structural and functional
data, have been proposed to address these challenges. However, these methods
often fall short in elucidating the functional significance of key residues
essential for protein functionality, as they predominantly adopt a
retrospective perspective, leading to suboptimal performance.
  Inspired by region proposal networks in computer vision, we introduce the
Protein Region Proposal Network (ProteinRPN) for accurate protein function
prediction. Specifically, the region proposal module component of ProteinRPN
identifies potential functional regions (anchors) which are refined through the
hierarchy-aware node drop pooling layer favoring nodes with defined secondary
structures and spatial proximity. The representations of the predicted
functional nodes are enriched using attention mechanisms and subsequently fed
into a Graph Multiset Transformer, which is trained with supervised contrastive
(SupCon) and InfoNCE losses on perturbed protein structures. Our model
demonstrates significant improvements in predicting Gene Ontology (GO) terms,
effectively localizing functional residues within protein structures. The
proposed framework provides a robust, scalable solution for protein function
annotation, advancing the understanding of protein structure-function
relationships in computational biology.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.00610v1},
File          = {2409.00610v1.pdf}
}
@article{2408.16422v1,
Author        = {Volodymyr A. Shekhovtsov and Bence Slajcho and Aron Sacherer and Johann Eder},
Title         = {CollectionLocator Level 1: Metadata-Based Search for Collections in
  Federated Biobanks},
Eprint        = {2408.16422v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Biobanks are indispensable resources for medical research collecting
biological material and associated data and making them available for research
projects and medical studies. For that, the biobank data has to meet certain
criteria which can be formulated as adherence to the FAIR (findable,
accessible, interoperable and reusable) principles.
  We developed a tool, CollectionLocator, which aims at increasing the FAIR
compliance of biobank data by supporting researchers in identifying which
biobank and which collection are likely to contain cases (material and data)
satisfying the requirements of a defined research project when the detailed
sample data is not available due to privacy restrictions. The CollectionLocator
is based on an ontology-based metadata model to address the enormous
heterogeneities and ensure the privacy of the donors of the biological samples
and the data. Furthermore, the CollectionLocator represents the data and
metadata quality of the collections such that the quality requirements of the
requester can be matched with the quality of the available data. The concept of
CollectionLocator is evaluated with a proof-of-concept implementation.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.16422v1},
File          = {2408.16422v1.pdf}
}
@article{2408.15294v2,
Author        = {Christos Theodoropoulos and Natasha Mulligan and Joao Bettencourt-Silva},
Title         = {Evaluating the Predictive Features of Person-Centric Knowledge Graph
  Embeddings: Unfolding Ablation Studies},
Eprint        = {2408.15294v2},
DOI           = {10.3233/SHTI240479},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Developing novel predictive models with complex biomedical information is
challenging due to various idiosyncrasies related to heterogeneity,
standardization or sparseness of the data. We previously introduced a
person-centric ontology to organize information about individual patients, and
a representation learning framework to extract person-centric knowledge graphs
(PKGs) and to train Graph Neural Networks (GNNs). In this paper, we propose a
systematic approach to examine the results of GNN models trained with both
structured and unstructured information from the MIMIC-III dataset. Through
ablation studies on different clinical, demographic, and social data, we show
the robustness of this approach in identifying predictive features in PKGs for
the task of readmission prediction.},
Year          = {2024},
Month         = {Aug},
Note          = {Studies in health technology and informatics vol. 316 (2024):
  575-579},
Url           = {http://arxiv.org/abs/2408.15294v2},
File          = {2408.15294v2.pdf}
}
@article{2408.14236v1,
Author        = {Hanna Abi Akl},
Title         = {DSTI at LLMs4OL 2024 Task A: Intrinsic versus extrinsic knowledge for
  type classification},
Eprint        = {2408.14236v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We introduce semantic towers, an extrinsic knowledge representation method,
and compare it to intrinsic knowledge in large language models for ontology
learning. Our experiments show a trade-off between performance and semantic
grounding for extrinsic knowledge compared to a fine-tuned model intrinsic
knowledge. We report our findings on the Large Language Models for Ontology
Learning (LLMs4OL) 2024 challenge.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.14236v1},
File          = {2408.14236v1.pdf}
}
@article{2408.13366v1,
Author        = {Ekaterina Trofimova and Emil Sataev and Abhijit Singh Jowhari},
Title         = {CodeRefine: A Pipeline for Enhancing LLM-Generated Code Implementations
  of Research Papers},
Eprint        = {2408.13366v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper presents CodeRefine, a novel framework for automatically
transforming research paper methodologies into functional code using Large
Language Models (LLMs). Our multi-step approach first extracts and summarizes
key text chunks from papers, analyzes their code relevance, and creates a
knowledge graph using a predefined ontology. Code is then generated from this
structured representation and enhanced through a proposed retrospective
retrieval-augmented generation approach. CodeRefine addresses the challenge of
bridging theoretical research and practical implementation, offering a more
accurate alternative to LLM zero-shot prompting. Evaluations on diverse
scientific papers demonstrate CodeRefine's ability to improve code
implementation from the paper, potentially accelerating the adoption of
cutting-edge algorithms in real-world applications.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.13366v1},
File          = {2408.13366v1.pdf}
}
@article{2408.12373v2,
Author        = {Xinyu Yuan and Zhihao Zhan and Zuobai Zhang and Manqi Zhou and Jianan Zhao and Boyu Han and Yue Li and Jian Tang},
Title         = {Cell-ontology guided transcriptome foundation model},
Eprint        = {2408.12373v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Transcriptome foundation models TFMs hold great promises of deciphering the
transcriptomic language that dictate diverse cell functions by self-supervised
learning on large-scale single-cell gene expression data, and ultimately
unraveling the complex mechanisms of human diseases. However, current TFMs
treat cells as independent samples and ignore the taxonomic relationships
between cell types, which are available in cell ontology graphs. We argue that
effectively leveraging this ontology information during the TFM pre-training
can improve learning biologically meaningful gene co-expression patterns while
preserving TFM as a general purpose foundation model for downstream zero-shot
and fine-tuning tasks. To this end, we present single cell, Cell-ontology
guided TFM scCello. We introduce cell-type coherence loss and ontology
alignment loss, which are minimized along with the masked gene expression
prediction loss during the pre-training. The novel loss component guide scCello
to learn the cell-type-specific representation and the structural relation
between cell types from the cell ontology graph, respectively. We pre-trained
scCello on 22 million cells from CellxGene database leveraging their cell-type
labels mapped to the cell ontology graph from Open Biological and Biomedical
Ontology Foundry. Our TFM demonstrates competitive generalization and
transferability performance over the existing TFMs on biologically important
tasks including identifying novel cell types of unseen cells, prediction of
cell-type-specific marker genes, and cancer drug responses.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.12373v2},
File          = {2408.12373v2.pdf}
}
@article{2408.11975v1,
Author        = {Camila DÃ­az and Jocelyn Dunstan and Lorena Etcheverry and Antonia Fonck and Alejandro Grez and Domingo Mery and Juan Reutter and Hugo Rojas},
Title         = {Automatic knowledge-graph creation from historical documents: The
  Chilean dictatorship as a case study},
Eprint        = {2408.11975v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CY},
Abstract      = {We present our results regarding the automatic construction of a knowledge
graph from historical documents related to the Chilean dictatorship period
(1973-1990). Our approach consists on using LLMs to automatically recognize
entities and relations between these entities, and also to perform resolution
between these sets of values. In order to prevent hallucination, the
interaction with the LLM is grounded in a simple ontology with 4 types of
entities and 7 types of relations. To evaluate our architecture, we use a gold
standard graph constructed using a small subset of the documents, and compare
this to the graph obtained from our approach when processing the same set of
documents. Results show that the automatic construction manages to recognize a
good portion of all the entities in the gold standard, and that those not
recognized are mostly explained by the level of granularity in which the
information is structured in the graph, and not because the automatic approach
misses an important entity in the graph. Looking forward, we expect this report
will encourage work on other similar projects focused on enhancing research in
humanities and social science, but we remark that better evaluation metrics are
needed in order to accurately fine-tune these types of architectures.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.11975v1},
File          = {2408.11975v1.pdf}
}
@article{2408.11359v1,
Author        = {Sakhinana Sagar Srinivas and Rajat Kumar Sarkar and Venkataramana Runkana},
Title         = {Hypergraph Learning based Recommender System for Anomaly Detection,
  Control and Optimization},
Eprint        = {2408.11359v1},
DOI           = {10.1109/BigData55660.2022.10020888},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Anomaly detection is fundamental yet, challenging problem with practical
applications in industry. The current approaches neglect the higher-order
dependencies within the networks of interconnected sensors in the
high-dimensional time series(multisensor data) for anomaly detection. To this
end, we present a self-adapting anomaly detection framework for joint learning
of (a) discrete hypergraph structure and (b) modeling the temporal trends and
spatial relations among the interdependent sensors using the hierarchical
encoder-decoder architecture to overcome the challenges. The hypergraph
representation learning-based framework exploits the relational inductive
biases in the hypergraph-structured data to learn the pointwise
single-step-ahead forecasts through the self-supervised autoregressive task and
predicts the anomalies based on the forecast error. Furthermore, our framework
incentivizes learning the anomaly-diagnosis ontology through a differentiable
approach. It derives the anomaly information propagation-based computational
hypergraphs for root cause analysis and provides recommendations through an
offline, optimal predictive control policy to remedy an anomaly. We conduct
extensive experiments to evaluate the proposed method on the benchmark datasets
for fair and rigorous comparison with the popular baselines. The proposed
method outperforms the baseline models and achieves SOTA performance. We report
the ablation studies to support the efficacy of the framework.},
Year          = {2024},
Month         = {Aug},
Note          = {IEEE International Conference on Big Data (2022) 1922-1929},
Url           = {http://arxiv.org/abs/2408.11359v1},
File          = {2408.11359v1.pdf}
}
@article{2409.00054v2,
Author        = {Yuting Hu and Dancheng Liu and Qingyun Wang and Charles Yu and Chenhui Xu and Qingxiao Zheng and Heng Ji and Jinjun Xiong},
Title         = {Automating Intervention Discovery from Scientific Literature: A
  Progressive Ontology Prompting and Dual-LLM Framework},
Eprint        = {2409.00054v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Identifying effective interventions from the scientific literature is
challenging due to the high volume of publications, specialized terminology,
and inconsistent reporting formats, making manual curation laborious and prone
to oversight. To address this challenge, this paper proposes a novel framework
leveraging large language models (LLMs), which integrates a progressive
ontology prompting (POP) algorithm with a dual-agent system, named LLM-Duo. On
the one hand, the POP algorithm conducts a prioritized breadth-first search
(BFS) across a predefined ontology, generating structured prompt templates and
action sequences to guide the automatic annotation process. On the other hand,
the LLM-Duo system features two specialized LLM agents, an explorer and an
evaluator, working collaboratively and adversarially to continuously refine
annotation quality. We showcase the real-world applicability of our framework
through a case study focused on speech-language intervention discovery.
Experimental results show that our approach surpasses advanced baselines,
achieving more accurate and comprehensive annotations through a fully automated
process. Our approach successfully identified 2,421 interventions from a corpus
of 64,177 research articles in the speech-language pathology domain,
culminating in the creation of a publicly accessible intervention knowledge
base with great potential to benefit the speech-language pathology community.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2409.00054v2},
File          = {2409.00054v2.pdf}
}
@article{2408.10689v2,
Author        = {Ievgeniia A. Tiukova and Daniel BrunnsÃ¥ker and Erik Y. BjurstrÃ¶m and Alexander H. Gower and Filip KronstrÃ¶m and Gabriel K. Reder and Ronald S. Reiserer and Konstantin Korovin and Larisa B. Soldatova and John P. Wikswo and Ross D. King},
Title         = {Genesis: Towards the Automation of Systems Biology Research},
Eprint        = {2408.10689v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The cutting edge of applying AI to science is the closed-loop automation of
scientific research: robot scientists. We have previously developed two robot
scientists: `Adam' (for yeast functional biology), and `Eve' (for early-stage
drug design)). We are now developing a next generation robot scientist Genesis.
With Genesis we aim to demonstrate that an area of science can be investigated
using robot scientists unambiguously faster, and at lower cost, than with human
scientists. Here we report progress on the Genesis project. Genesis is designed
to automatically improve system biology models with thousands of interacting
causal components. When complete Genesis will be able to initiate and execute
in parallel one thousand hypothesis-led closed-loop cycles of experiment
per-day. Here we describe the core Genesis hardware: the one thousand
computer-controlled $\mu$-bioreactors. For the integrated Mass Spectrometry
platform we have developed AutonoMS, a system to automatically run, process,
and analyse high-throughput experiments. We have also developed Genesis-DB, a
database system designed to enable software agents access to large quantities
of structured domain information. We have developed RIMBO (Revisions for
Improvements of Models in Biology Ontology) to describe the planned hundreds of
thousands of changes to the models. We have demonstrated the utility of this
infrastructure by developed two relational learning bioinformatic projects.
Finally, we describe LGEM+ a relational learning system for the automated
abductive improvement of genome-scale metabolic models.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.10689v2},
File          = {2408.10689v2.pdf}
}
@article{2408.10053v2,
Author        = {Haoran Li and Wei Fan and Yulin Chen and Jiayang Cheng and Tianshu Chu and Xuebing Zhou and Peizhao Hu and Yangqiu Song},
Title         = {Privacy Checklist: Privacy Violation Detection Grounding on Contextual
  Integrity Theory},
Eprint        = {2408.10053v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Privacy research has attracted wide attention as individuals worry that their
private data can be easily leaked during interactions with smart devices,
social platforms, and AI applications. Computer science researchers, on the
other hand, commonly study privacy issues through privacy attacks and defenses
on segmented fields. Privacy research is conducted on various sub-fields,
including Computer Vision (CV), Natural Language Processing (NLP), and Computer
Networks. Within each field, privacy has its own formulation. Though pioneering
works on attacks and defenses reveal sensitive privacy issues, they are
narrowly trapped and cannot fully cover people's actual privacy concerns.
Consequently, the research on general and human-centric privacy research
remains rather unexplored. In this paper, we formulate the privacy issue as a
reasoning problem rather than simple pattern matching. We ground on the
Contextual Integrity (CI) theory which posits that people's perceptions of
privacy are highly correlated with the corresponding social context. Based on
such an assumption, we develop the first comprehensive checklist that covers
social identities, private attributes, and existing privacy regulations. Unlike
prior works on CI that either cover limited expert annotated norms or model
incomplete social context, our proposed privacy checklist uses the whole Health
Insurance Portability and Accountability Act of 1996 (HIPAA) as an example, to
show that we can resort to large language models (LLMs) to completely cover the
HIPAA's regulations. Additionally, our checklist also gathers expert
annotations across multiple ontologies to determine private information
including but not limited to personally identifiable information (PII). We use
our preliminary results on the HIPAA to shed light on future context-centric
privacy research to cover more privacy regulations, social norms and standards.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.10053v2},
File          = {2408.10053v2.pdf}
}
@article{2408.10003v2,
Author        = {BjÃ¶rn Schembera and Frank WÃ¼bbeling and Hendrik Kleikamp and Burkhard Schmidt and Aurela Shehu and Marco Reidelbach and Christine Biedinger and Jochen Fiedler and Thomas Koprucki and Dorothea Iglezakis and Dominik GÃ¶ddeke},
Title         = {Towards a Knowledge Graph for Models and Algorithms in Applied
  Mathematics},
Eprint        = {2408.10003v2},
DOI           = {10.1007/978-3-031-81974-2_8},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Mathematical models and algorithms are an essential part of mathematical
research data, as they are epistemically grounding numerical data. In order to
represent models and algorithms as well as their relationship semantically to
make this research data FAIR, two previously distinct ontologies were merged
and extended, becoming a living knowledge graph. The link between the two
ontologies is established by introducing computational tasks, as they occur in
modeling, corresponding to algorithmic tasks. Moreover, controlled vocabularies
are incorporated and a new class, distinguishing base quantities from specific
use case quantities, was introduced. Also, both models and algorithms can now
be enriched with metadata. Subject-specific metadata is particularly relevant
here, such as the symmetry of a matrix or the linearity of a mathematical
model. This is the only way to express specific workflows with concrete models
and algorithms, as the feasible solution algorithm can only be determined if
the mathematical properties of a model are known. We demonstrate this using two
examples from different application areas of applied mathematics. In addition,
we have already integrated over 250 research assets from applied mathematics
into our knowledge graph.},
Year          = {2024},
Month         = {Aug},
Note          = {Sfakakis, M., Garoufallou, E., Damigos, M., Salaba, A.,
  Papatheodorou, C. (eds) Metadata and Semantic Research. MTSR 2024.
  Communications in Computer and Information Science, vol 2331. Springer, Cham},
Url           = {http://arxiv.org/abs/2408.10003v2},
File          = {2408.10003v2.pdf}
}
@article{2408.09626v1,
Author        = {Riley Kinahan and Spencer Killen and Kevin Wan and Jia-Huai You},
Title         = {On the Foundations of Conflict-Driven Solving for Hybrid MKNF Knowledge
  Bases},
Eprint        = {2408.09626v1},
DOI           = {10.1017/S1471068424000255},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Hybrid MKNF Knowledge Bases (HMKNF-KBs) constitute a formalism for tightly
integrated reasoning over closed-world rules and open-world ontologies. This
approach allows for accurate modeling of real-world systems, which often rely
on both categorical and normative reasoning. Conflict-driven solving is the
leading approach for computationally hard problems, such as satisfiability
(SAT) and answer set programming (ASP), in which MKNF is rooted. This paper
investigates the theoretical underpinnings required for a conflict-driven
solver of HMKNF-KBs. The approach defines a set of completion and loop
formulas, whose satisfaction characterizes MKNF models. This forms the basis
for a set of nogoods, which in turn can be used as the backbone for a
conflict-driven solver.},
Year          = {2024},
Month         = {Aug},
Note          = {Theory and Practice of Logic Programming 24 (2024) 901-920},
Url           = {http://arxiv.org/abs/2408.09626v1},
File          = {2408.09626v1.pdf}
}
@article{2408.08698v1,
Author        = {Genet Asefa Gesese and JÃ¶rg Waitelonis and Zongxiong Chen and Sonja Schimmler and Harald Sack},
Title         = {NFDI4DSO: Towards a BFO Compliant Ontology for Data Science},
Eprint        = {2408.08698v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The NFDI4DataScience (NFDI4DS) project aims to enhance the accessibility and
interoperability of research data within Data Science (DS) and Artificial
Intelligence (AI) by connecting digital artifacts and ensuring they adhere to
FAIR (Findable, Accessible, Interoperable, and Reusable) principles. To this
end, this poster introduces the NFDI4DS Ontology, which describes resources in
DS and AI and models the structure of the NFDI4DS consortium. Built upon the
NFDICore ontology and mapped to the Basic Formal Ontology (BFO), this ontology
serves as the foundation for the NFDI4DS knowledge graph currently under
development.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.08698v1},
File          = {2408.08698v1.pdf}
}
@article{2408.14480v1,
Author        = {Darius Has and Adrian Groza and Mihai Pomarlan},
Title         = {Handling abort commands for household kitchen robots},
Eprint        = {2408.14480v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {We propose a solution for handling abort commands given to robots. The
solution is exemplified with a running scenario with household kitchen robots.
The robot uses planning to find sequences of actions that must be performed in
order to gracefully cancel a previously received command. The Planning Domain
Definition Language (PDDL) is used to write a domain to model kitchen
activities and behaviours, and this domain is enriched with knowledge from
online ontologies and knowledge graphs, like DBPedia. We discuss the results
obtained in different scenarios.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.14480v1},
File          = {2408.14480v1.pdf}
}
@article{2408.08048v2,
Author        = {Jonathan Reif and Tom Jeleniewski and Aljosha KÃ¶cher and Tim Frerich and Felix Gehlhoff and Alexander Fay},
Title         = {Semantic Capability Model for the Simulation of Manufacturing Processes},
Eprint        = {2408.08048v2},
DOI           = {10.5220/0012945300003838},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {Simulations offer opportunities in the examination of manufacturing
processes. They represent various aspects of the production process and the
associated production systems. However, often a single simulation does not
suffice to provide a comprehensive understanding of specific process settings.
Instead, a combination of different simulations is necessary when the outputs
of one simulation serve as the input parameters for another, resulting in a
sequence of simulations. Manual planning of simulation sequences is a demanding
task that requires careful evaluation of factors like time, cost, and result
quality to choose the best simulation scenario for a given inquiry. In this
paper, an information model is introduced, which represents simulations, their
capabilities to generate certain knowledge, and their respective quality
criteria. The information model is designed to provide the foundation for
automatically generating simulation sequences. The model is implemented as an
extendable and adaptable ontology. It utilizes Ontology Design Patterns based
on established industrial standards to enhance interoperability and
reusability. To demonstrate the practicality of this information model, an
application example is provided. This example serves to illustrate the model's
capacity in a real-world context, thereby validating its utility and potential
for future applications.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.08048v2},
File          = {2408.08048v2.pdf}
}
@article{2408.07544v1,
Author        = {Tobias John and Patrick Koopmann},
Title         = {Planning with OWL-DL Ontologies (Extended Version)},
Eprint        = {2408.07544v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We introduce ontology-mediated planning, in which planning problems are
combined with an ontology. Our formalism differs from existing ones in that we
focus on a strong separation of the formalisms for describing planning problems
and ontologies, which are only losely coupled by an interface. Moreover, we
present a black-box algorithm that supports the full expressive power of OWL
DL. This goes beyond what existing approaches combining automated planning with
ontologies can do, which only support limited description logics such as
DL-Lite and description logics that are Horn. Our main algorithm relies on
rewritings of the ontology-mediated planning specifications into PDDL, so that
existing planning systems can be used to solve them. The algorithm relies on
justifications, which allows for a generic approach that is independent of the
expressivity of the ontology language. However, dedicated optimizations for
computing justifications need to be implemented to enable an efficient
rewriting procedure. We evaluated our implementation on benchmark sets from
several domains. The evaluation shows that our procedure works in practice and
that tailoring the reasoning procedure has significant impact on the
performance.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.07544v1},
File          = {2408.07544v1.pdf}
}
@article{2408.15256v3,
Author        = {Yihang Zhao and Bohui Zhang and Xi Hu and Shuyin Ouyang and Jongmo Kim and Nitisha Jain and Jacopo de Berardinis and Albert MeroÃ±o-PeÃ±uela and Elena Simperl},
Title         = {Improving Ontology Requirements Engineering with OntoChat and
  Participatory Prompting},
Eprint        = {2408.15256v3},
DOI           = {10.1609/aaaiss.v4i1.31799},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {Past ontology requirements engineering (ORE) has primarily relied on manual
methods, such as interviews and collaborative forums, to gather user
requirements from domain experts, especially in large projects. Current
OntoChat offers a framework for ORE that utilises large language models (LLMs)
to streamline the process through four key functions: user story creation,
competency question (CQ) extraction, CQ filtration and analysis, and ontology
testing support. In OntoChat, users are expected to prompt the chatbot to
generate user stories. However, preliminary evaluations revealed that they
struggle to do this effectively. To address this issue, we experimented with a
research method called participatory prompting, which involves
researcher-mediated interactions to help users without deep knowledge of LLMs
use the chatbot more effectively. This participatory prompting user study
produces pre-defined prompt templates based on user queries, focusing on
creating and refining personas, goals, scenarios, sample data, and data
resources for user stories. These refined user stories will subsequently be
converted into CQs.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.15256v3},
File          = {2408.15256v3.pdf}
}
@article{2408.04023v1,
Author        = {Wrick Talukdar and Anjanava Biswas},
Title         = {Improving Large Language Model (LLM) fidelity through context-aware
  grounding: A systematic approach to reliability and veracity},
Eprint        = {2408.04023v1},
DOI           = {10.30574/wjaets.2023.10.2.0317},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {As Large Language Models (LLMs) become increasingly sophisticated and
ubiquitous in natural language processing (NLP) applications, ensuring their
robustness, trustworthiness, and alignment with human values has become a
critical challenge. This paper presents a novel framework for contextual
grounding in textual models, with a particular emphasis on the Context
Representation stage. Our approach aims to enhance the reliability and ethical
alignment of these models through a comprehensive, context-aware methodology.
By explicitly capturing and representing relevant situational, cultural, and
ethical contexts in a machine-readable format, we lay the foundation for
anchoring a model's behavior within these contexts. Our approach leverages
techniques from knowledge representation and reasoning, such as ontologies,
semantic web technologies, and logic-based formalisms. We evaluate our
framework on real-world textual datasets, demonstrating its effectiveness in
improving model performance, fairness, and alignment with human expectations,
while maintaining high accuracy. Furthermore, we discuss the other key
components of the framework, including context-aware encoding, context-aware
learning, interpretability and explainability, and continuous monitoring and
adaptation. This research contributes to the growing body of work on
responsible AI, offering a practical approach to developing more reliable,
trustworthy, and ethically-aligned language models. Our findings have
significant implications for the deployment of LLMs in sensitive domains such
as healthcare, legal systems, and social services, where contextual
understanding is paramount.},
Year          = {2024},
Month         = {Aug},
Note          = {World Journal of Advanced Engineering Technology and Sciences,
  2023, 10(2), 283-296},
Url           = {http://arxiv.org/abs/2408.04023v1},
File          = {2408.04023v1.pdf}
}
@article{2408.04673v1,
Author        = {Tingyan Ma and Wei Liu and Bin Lu and Xiaoying Gan and Yunqiang Zhu and Luoyi Fu and Chenghu Zhou},
Title         = {AutoFAIR : Automatic Data FAIRification via Machine Reading},
Eprint        = {2408.04673v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The explosive growth of data fuels data-driven research, facilitating
progress across diverse domains. The FAIR principles emerge as a guiding
standard, aiming to enhance the findability, accessibility, interoperability,
and reusability of data. However, current efforts primarily focus on manual
data FAIRification, which can only handle targeted data and lack efficiency. To
address this issue, we propose AutoFAIR, an architecture designed to enhance
data FAIRness automately. Firstly, We align each data and metadata operation
with specific FAIR indicators to guide machine-executable actions. Then, We
utilize Web Reader to automatically extract metadata based on language models,
even in the absence of structured data webpage schemas. Subsequently, FAIR
Alignment is employed to make metadata comply with FAIR principles by ontology
guidance and semantic matching. Finally, by applying AutoFAIR to various data,
especially in the field of mountain hazards, we observe significant
improvements in findability, accessibility, interoperability, and reusability
of data. The FAIRness scores before and after applying AutoFAIR indicate
enhanced data value.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.04673v1},
File          = {2408.04673v1.pdf}
}
@article{2408.04661v1,
Author        = {Ali Riza Durmaz and Akhil Thomas and Lokesh Mishra and Rachana Niranjan Murthy and Thomas Straub},
Title         = {MaterioMiner -- An ontology-based text mining dataset for extraction of
  process-structure-property entities},
Eprint        = {2408.04661v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {While large language models learn sound statistical representations of the
language and information therein, ontologies are symbolic knowledge
representations that can complement the former ideally. Research at this
critical intersection relies on datasets that intertwine ontologies and text
corpora to enable training and comprehensive benchmarking of neurosymbolic
models. We present the MaterioMiner dataset and the linked materials mechanics
ontology where ontological concepts from the mechanics of materials domain are
associated with textual entities within the literature corpus. Another
distinctive feature of the dataset is its eminently fine-granular annotation.
Specifically, 179 distinct classes are manually annotated by three raters
within four publications, amounting to a total of 2191 entities that were
annotated and curated. Conceptual work is presented for the symbolic
representation of causal composition-process-microstructure-property
relationships. We explore the annotation consistency between the three raters
and perform fine-tuning of pre-trained models to showcase the feasibility of
named-entity recognition model training. Reusing the dataset can foster
training and benchmarking of materials language models, automated ontology
construction, and knowledge graph generation from textual data.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.04661v1},
File          = {2408.04661v1.pdf}
}
@article{2408.02361v2,
Author        = {Renato Vukovic and David Arps and Carel van Niekerk and Benjamin Matthias Ruppik and Hsien-Chin Lin and Michael Heck and Milica GaÅ¡iÄ},
Title         = {Dialogue Ontology Relation Extraction via Constrained Chain-of-Thought
  Decoding},
Eprint        = {2408.02361v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {State-of-the-art task-oriented dialogue systems typically rely on
task-specific ontologies for fulfilling user queries. The majority of
task-oriented dialogue data, such as customer service recordings, comes without
ontology and annotation. Such ontologies are normally built manually, limiting
the application of specialised systems. Dialogue ontology construction is an
approach for automating that process and typically consists of two steps: term
extraction and relation extraction. In this work, we focus on relation
extraction in a transfer learning set-up. To improve the generalisation, we
propose an extension to the decoding mechanism of large language models. We
adapt Chain-of-Thought (CoT) decoding, recently developed for reasoning
problems, to generative relation extraction. Here, we generate multiple
branches in the decoding space and select the relations based on a confidence
threshold. By constraining the decoding to ontology terms and relations, we aim
to decrease the risk of hallucination. We conduct extensive experimentation on
two widely used datasets and find improvements in performance on target
ontology for source fine-tuned and one-shot prompted large language models.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.02361v2},
File          = {2408.02361v2.pdf}
}
@article{2408.02181v2,
Author        = {Renjith Prasad and Chathurangi Shyalika and Ramtin Zand and Fadi El Kalach and Revathy Venkataramanan and Ramy Harik and Amit Sheth},
Title         = {AssemAI: Interpretable Image-Based Anomaly Detection for Manufacturing
  Pipelines},
Eprint        = {2408.02181v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Anomaly detection in manufacturing pipelines remains a critical challenge,
intensified by the complexity and variability of industrial environments. This
paper introduces AssemAI, an interpretable image-based anomaly detection system
tailored for smart manufacturing pipelines. Utilizing a curated image dataset
from an industry-focused rocket assembly pipeline, we address the challenge of
imbalanced image data and demonstrate the importance of image-based methods in
anomaly detection. Our primary contributions include deriving an image dataset,
fine-tuning an object detection model YOLO-FF, and implementing a custom
anomaly detection model for assembly pipelines. The proposed approach leverages
domain knowledge in data preparation, model development and reasoning. We
implement several anomaly detection models on the derived image dataset,
including a Convolutional Neural Network, Vision Transformer (ViT), and
pre-trained versions of these models. Additionally, we incorporate
explainability techniques at both user and model levels, utilizing ontology for
user-level explanations and SCORE-CAM for in-depth feature and model analysis.
Finally, the best-performing anomaly detection model and YOLO-FF are deployed
in a real-time setting. Our results include ablation studies on the baselines
and a comprehensive evaluation of the proposed system. This work highlights the
broader impact of advanced image-based anomaly detection in enhancing the
reliability and efficiency of smart manufacturing processes. The image dataset,
codes to reproduce the results and additional experiments are available at
https://github.com/renjithk4/AssemAI.},
Year          = {2024},
Month         = {Aug},
Note          = {Predictive Models in Engineering Applications special session
  (MLPMEA) at International Conference on Machine Learning and Applications
  (ICMLA) 2024},
Url           = {http://arxiv.org/abs/2408.02181v2},
File          = {2408.02181v2.pdf}
}
@article{2408.02023v2,
Author        = {Yee Ching Tok and Davis Yang Zheng and Sudipta Chattopadhyay},
Title         = {A Smart City Infrastructure Ontology for Threats, Cybercrime, and
  Digital Forensic Investigation},
Eprint        = {2408.02023v2},
DOI           = {10.1016/j.fsidi.2025.301883},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {Cybercrime and the market for cyber-related compromises are becoming
attractive revenue sources for state-sponsored actors, cybercriminals and
technical individuals affected by financial hardships. Due to burgeoning
cybercrime on new technological frontiers, efforts have been made to assist
digital forensic investigators (DFI) and law enforcement agencies (LEA) in
their investigative efforts.
  Forensic tool innovations and ontology developments, such as the Unified
Cyber Ontology (UCO) and Cyber-investigation Analysis Standard Expression
(CASE), have been proposed to assist DFI and LEA. Although these tools and
ontologies are useful, they lack extensive information sharing and tool
interoperability features, and the ontologies lack the latest Smart City
Infrastructure (SCI) context that was proposed.
  To mitigate the weaknesses in both solutions and to ensure a safer
cyber-physical environment for all, we propose the Smart City Ontological
Paradigm Expression (SCOPE), an expansion profile of the UCO and CASE ontology
that implements SCI threat models, SCI digital forensic evidence, attack
techniques, patterns and classifications from MITRE.
  We showcase how SCOPE could present complex data such as SCI-specific
threats, cybercrime, investigation data and incident handling workflows via an
incident scenario modelled after publicly reported real-world incidents
attributed to Advanced Persistent Threat (APT) groups. We also make SCOPE
available to the community so that threats, digital evidence and cybercrime in
emerging trends such as SCI can be identified, represented, and shared
collaboratively.},
Year          = {2024},
Month         = {Aug},
Note          = {Forensic Science International: Digital Investigation, Volume 52,
  2025},
Url           = {http://arxiv.org/abs/2408.02023v2},
File          = {2408.02023v2.pdf}
}
@article{2408.01787v1,
Author        = {Giacomo De Colle},
Title         = {Towards an ontology of state actors in cyberspace},
Eprint        = {2408.01787v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {To improve cyber threat analysis practices in cybersecurity, I present a plan
to build a formal ontological representation of state actors in cyberspace and
of cyber operations. I argue that modelling these phenomena via ontologies
allows for coherent integration of data coming from diverse sources, automated
reasoning over such data, as well as intelligence extraction and reuse from and
of them. Existing ontological tools in cybersecurity can be ameliorated by
connecting them to neighboring domains such as law, regulations, governmental
institutions, and documents. In this paper, I propose metrics to evaluate
currently existing ontological tools to create formal representations in the
cybersecurity domain, and I provide a plan to develop and extend them when they
are lacking.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.01787v1},
File          = {2408.01787v1.pdf}
}
@article{2408.01748v1,
Author        = {Hiroki Sakaji and Jason Bennett and Risa Murono and Kiyoshi Izumi and Hiroyuki Sakai},
Title         = {Discovery of Rare Causal Knowledge from Financial Statement Summaries},
Eprint        = {2408.01748v1},
DOI           = {10.1109/SSCI.2017.8285265},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {What would happen if temperatures were subdued and result in a cool summer?
One can easily imagine that air conditioner, ice cream or beer sales would be
suppressed as a result of this. Less obvious is that agricultural shipments
might be delayed, or that sound proofing material sales might decrease. The
ability to extract such causal knowledge is important, but it is also important
to distinguish between cause-effect pairs that are known and those that are
likely to be unknown, or rare. Therefore, in this paper, we propose a method
for extracting rare causal knowledge from Japanese financial statement
summaries produced by companies. Our method consists of three steps. First, it
extracts sentences that include causal knowledge from the summaries using a
machine learning method based on an extended language ontology. Second, it
obtains causal knowledge from the extracted sentences using syntactic patterns.
Finally, it extracts the rarest causal knowledge from the knowledge it has
obtained.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.01748v1},
File          = {2408.01748v1.pdf}
}
@article{2408.01700v1,
Author        = {Antonio De Santis and Marco Balduini and Federico De Santis and Andrea Proia and Arsenio Leo and Marco Brambilla and Emanuele Della Valle},
Title         = {Integrating Large Language Models and Knowledge Graphs for Extraction
  and Validation of Textual Test Data},
Eprint        = {2408.01700v1},
DOI           = {10.1007/978-3-031-77847-6_17},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Aerospace manufacturing companies, such as Thales Alenia Space, design,
develop, integrate, verify, and validate products characterized by high
complexity and low volume. They carefully document all phases for each product
but analyses across products are challenging due to the heterogeneity and
unstructured nature of the data in documents. In this paper, we propose a
hybrid methodology that leverages Knowledge Graphs (KGs) in conjunction with
Large Language Models (LLMs) to extract and validate data contained in these
documents. We consider a case study focused on test data related to electronic
boards for satellites. To do so, we extend the Semantic Sensor Network
ontology. We store the metadata of the reports in a KG, while the actual test
results are stored in parquet accessible via a Virtual Knowledge Graph. The
validation process is managed using an LLM-based approach. We also conduct a
benchmarking study to evaluate the performance of state-of-the-art LLMs in
executing this task. Finally, we analyze the costs and benefits of automating
preexisting processes of manual data extraction and validation for subsequent
cross-report analyses.},
Year          = {2024},
Month         = {Aug},
Note          = {ISWC 2024},
Url           = {http://arxiv.org/abs/2408.01700v1},
File          = {2408.01700v1.pdf}
}
@article{2408.01214v2,
Author        = {Daniel B. Hier and S. Ilyas Munzir and Anne Stahlfeld and Tayo Obafemi-Ajayi and Michael D. Carrithers},
Title         = {High-Throughput Phenotyping of Clinical Text Using Large Language Models},
Eprint        = {2408.01214v2},
DOI           = {10.1109/BHI62660.2024.10913712},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {High-throughput phenotyping automates the mapping of patient signs to
standardized ontology concepts and is essential for precision medicine. This
study evaluates the automation of phenotyping of clinical summaries from the
Online Mendelian Inheritance in Man (OMIM) database using large language
models. Due to their rich phenotype data, these summaries can be surrogates for
physician notes. We conduct a performance comparison of GPT-4 and
GPT-3.5-Turbo. Our results indicate that GPT-4 surpasses GPT-3.5-Turbo in
identifying, categorizing, and normalizing signs, achieving concordance with
manual annotators comparable to inter-rater agreement. Despite some limitations
in sign normalization, the extensive pre-training of GPT-4 results in high
performance and generalizability across several phenotyping tasks while
obviating the need for manually annotated training data. Large language models
are expected to be the dominant method for automating high-throughput
phenotyping of clinical text.},
Year          = {2024},
Month         = {Aug},
Note          = {2024 IEEE-EMBS International Conference on Biomedical and Health
  Informatics, Houston TX, USA pp. 1-8},
Url           = {http://arxiv.org/abs/2408.01214v2},
File          = {2408.01214v2.pdf}
}
@article{2408.00914v1,
Author        = {Steven Fincke and Adrien Bibal and Elizabeth Boschee},
Title         = {Granting GPT-4 License and Opportunity: Enhancing Accuracy and
  Confidence Estimation for Few-Shot Event Detection},
Eprint        = {2408.00914v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large Language Models (LLMs) such as GPT-4 have shown enough promise in the
few-shot learning context to suggest use in the generation of "silver" data and
refinement of new ontologies through iterative application and review. Such
workflows become more effective with reliable confidence estimation.
Unfortunately, confidence estimation is a documented weakness of models such as
GPT-4, and established methods to compensate require significant additional
complexity and computation. The present effort explores methods for effective
confidence estimation with GPT-4 with few-shot learning for event detection in
the BETTER ontology as a vehicle. The key innovation is expanding the prompt
and task presented to GPT-4 to provide License to speculate when unsure and
Opportunity to quantify and explain its uncertainty (L&O). This approach
improves accuracy and provides usable confidence measures (0.759 AUC) with no
additional machinery.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.00914v1},
File          = {2408.00914v1.pdf}
}
@article{2408.00444v1,
Author        = {Mathieu d'Aquin and Emmanuel Nauer},
Title         = {Ontological Relations from Word Embeddings},
Eprint        = {2408.00444v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {It has been reliably shown that the similarity of word embeddings obtained
from popular neural models such as BERT approximates effectively a form of
semantic similarity of the meaning of those words. It is therefore natural to
wonder if those embeddings contain enough information to be able to connect
those meanings through ontological relationships such as the one of
subsumption. If so, large knowledge models could be built that are capable of
semantically relating terms based on the information encapsulated in word
embeddings produced by pre-trained models, with implications not only for
ontologies (ontology matching, ontology evolution, etc.) but also on the
ability to integrate ontological knowledge in neural models. In this paper, we
test how embeddings produced by several pre-trained models can be used to
predict relations existing between classes and properties of popular
upper-level and general ontologies. We show that even a simple feed-forward
architecture on top of those embeddings can achieve promising accuracies, with
varying generalisation abilities depending on the input data. To achieve that,
we produce a dataset that can be used to further enhance those models, opening
new possibilities for applications integrating knowledge from web ontologies.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.00444v1},
File          = {2408.00444v1.pdf}
}
@article{2407.21708v1,
Author        = {Stefan Langer and Fabian Neuhaus and Andreas NÃ¼rnberger},
Title         = {CEAR: Automatic construction of a knowledge graph of chemical entities
  and roles from scientific literature},
Eprint        = {2407.21708v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontologies are formal representations of knowledge in specific domains that
provide a structured framework for organizing and understanding complex
information. Creating ontologies, however, is a complex and time-consuming
endeavor. ChEBI is a well-known ontology in the field of chemistry, which
provides a comprehensive resource for defining chemical entities and their
properties. However, it covers only a small fraction of the rapidly growing
knowledge in chemistry and does not provide references to the scientific
literature. To address this, we propose a methodology that involves augmenting
existing annotated text corpora with knowledge from Chebi and fine-tuning a
large language model (LLM) to recognize chemical entities and their roles in
scientific text. Our experiments demonstrate the effectiveness of our approach.
By combining ontological knowledge and the language understanding capabilities
of LLMs, we achieve high precision and recall rates in identifying both the
chemical entities and roles in scientific literature. Furthermore, we extract
them from a set of 8,000 ChemRxiv articles, and apply a second LLM to create a
knowledge graph (KG) of chemical entities and roles (CEAR), which provides
complementary information to ChEBI, and can help to extend it.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.21708v1},
File          = {2407.21708v1.pdf}
}
@article{2407.21276v3,
Author        = {Rubing Chen and Xulu Zhang and Jiaxin Wu and Wenqi Fan and Xiao-Yong Wei and Qing Li},
Title         = {Knowledge Pyramid Construction for Multi-Level Retrieval-Augmented
  Generation},
Eprint        = {2407.21276v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper addresses the need for improved precision in existing
knowledge-enhanced question-answering frameworks, specifically
Retrieval-Augmented Generation (RAG) methods that primarily focus on enhancing
recall. We propose a multi-layer knowledge pyramid approach within the RAG
framework to achieve a better balance between precision and recall. The
knowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs),
and chunk-based raw text. We employ cross-layer augmentation techniques for
comprehensive knowledge coverage and dynamic updates of the Ontology schema and
instances. To ensure compactness, we utilize cross-layer filtering methods for
knowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall
model for retrieval, starting from the top of the pyramid and progressing down
until a confident answer is obtained. We introduce two benchmarks for
domain-specific knowledge retrieval, one in the academic domain and the other
in the financial domain. The effectiveness of the methods has been validated
through comprehensive experiments by outperforming 19 SOTA methods. An
encouraging observation is that the proposed method has augmented the GPT-4,
providing 395% F1 gain by improving its performance from 0.1636 to 0.8109.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.21276v3},
File          = {2407.21276v3.pdf}
}
@article{2407.20782v1,
Author        = {Diego Figueira and S. Krishna and Om Swostik Mishra and Anantha Padmanabha},
Title         = {Boundedness for Unions of Conjunctive Regular Path Queries over Simple
  Regular Expressions},
Eprint        = {2407.20782v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {The problem of checking whether a recursive query can be rewritten as query
without recursion is a fundamental reasoning task, known as the boundedness
problem. Here we study the boundedness problem for Unions of Conjunctive
Regular Path Queries (UCRPQs), a navigational query language extensively used
in ontology and graph database querying. The boundedness problem for UCRPQs is
ExpSpace-complete. Here we focus our analysis on UCRPQs using simple regular
expressions, which are of high practical relevance and enjoy a lower reasoning
complexity. We show that the complexity for the boundedness problem for this
UCRPQs fragment is $\Pi^P_2$-complete, and that an equivalent bounded query can
be produced in polynomial time whenever possible. When the query turns out to
be unbounded, we also study the task of finding an equivalent maximally bounded
query, which we show to be feasible in $\Pi^P_2$. As a side result of
independent interest stemming from our developments, we study a notion of
succinct finite automata and prove that its membership problem is in NP.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.20782v1},
File          = {2407.20782v1.pdf}
}
@article{2407.20457v1,
Author        = {Emily Adlam},
Title         = {Quantum Field Theory and the Limits of Reductionism},
Eprint        = {2407.20457v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {physics.hist-ph},
Abstract      = {I suggest that the current situation in quantum field theory (QFT) provides
some reason to question the universal validity of ontological reductionism. I
argue that the renormalization group flow is reversible except at fixed points,
which makes the relation between large and small distance scales quite
symmetric in QFT, opening up at least the technical possibility of a
non-reductionist approach to QFT. I suggest that some conceptual problems
encountered within QFT may potentially be mitigated by moving to an alternative
picture in which it is no longer the case that the large supervenes on the
small. Finally, I explore some specific models in which a form of
non-reductionism might be implemented, and consider the prospects for future
development of these models.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.20457v1},
File          = {2407.20457v1.pdf}
}
@article{2407.20007v2,
Author        = {Lars Vogt and Kheir Eddine Farfar and Pallavi Karanth and Marcel Konrad and Allard Oelen and Manuel Prinz and Philip Stroemert},
Title         = {Rosetta Statements: Simplifying FAIR Knowledge Graph Construction with a
  User-Centered Approach},
Eprint        = {2407.20007v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Machines need data and metadata to be machine-actionable and FAIR (findable,
accessible, interoperable, reusable) to manage increasing data volumes.
Knowledge graphs and ontologies are key to this, but their use is hampered by
high access barriers due to required prior knowledge in semantics and data
modelling. The Rosetta Statement approach proposes modeling English natural
language statements instead of a mind-independent reality. We propose a
metamodel for creating semantic schema patterns for simple statement types. The
approach supports versioning of statements and provides a detailed editing
history. Each Rosetta Statement pattern has a dynamic label for displaying
statements as natural language sentences. Implemented in the Open Research
Knowledge Graph (ORKG) as a use case, this approach allows domain experts to
define data schema patterns without needing semantic knowledge. Future plans
include combining Rosetta Statements with semantic units to organize ORKG into
meaningful subgraphs, improving usability. A search interface for querying
statements without needing SPARQL or Cypher knowledge is also planned, along
with tools for data entry and display using Large Language Models. The Rosetta
Statement metamodel supports a three-step knowledge graph construction
procedure. Domain experts can model semantic content without support from
ontology engineers by using Wikidata, lowering entry barriers and increasing
cognitive interoperability. The second level involves mapping Wikidata terms to
established ontologies, and the third step developing semantic graph patterns
for reasoning, requiring collaboration with ontology engineers.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.20007v2},
File          = {2407.20007v2.pdf}
}
@article{2407.19998v1,
Author        = {Huu Tan Mai and Cuong Xuan Chu and Heiko Paulheim},
Title         = {Do LLMs Really Adapt to Domains? An Ontology Learning Perspective},
Eprint        = {2407.19998v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have demonstrated unprecedented prowess across
various natural language processing tasks in various application domains.
Recent studies show that LLMs can be leveraged to perform lexical semantic
tasks, such as Knowledge Base Completion (KBC) or Ontology Learning (OL).
However, it has not effectively been verified whether their success is due to
their ability to reason over unstructured or semi-structured data, or their
effective learning of linguistic patterns and senses alone. This unresolved
question is particularly crucial when dealing with domain-specific data, where
the lexical senses and their meaning can completely differ from what a LLM has
learned during its training stage. This paper investigates the following
question: Do LLMs really adapt to domains and remain consistent in the
extraction of structured knowledge, or do they only learn lexical senses
instead of reasoning? To answer this question and, we devise a controlled
experiment setup that uses WordNet to synthesize parallel corpora, with English
and gibberish terms. We examine the differences in the outputs of LLMs for each
corpus in two OL tasks: relation extraction and taxonomy discovery. Empirical
results show that, while adapting to the gibberish corpora, off-the-shelf LLMs
do not consistently reason over semantic relationships between concepts, and
instead leverage senses and their frame. However, fine-tuning improves the
performance of LLMs on lexical semantic tasks even when the domain-specific
terms are arbitrary and unseen during pre-training, hinting at the
applicability of pre-trained LLMs for OL.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.19998v1},
File          = {2407.19998v1.pdf}
}
@article{2407.20284v1,
Author        = {Shyam Dongre and Ritesh Chandra and Sonali Agarwal},
Title         = {MLtoGAI: Semantic Web based with Machine Learning for Enhanced Disease
  Prediction and Personalized Recommendations using Generative AI},
Eprint        = {2407.20284v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In modern healthcare, addressing the complexities of accurate disease
prediction and personalized recommendations is both crucial and challenging.
This research introduces MLtoGAI, which integrates Semantic Web technology with
Machine Learning (ML) to enhance disease prediction and offer user-friendly
explanations through ChatGPT. The system comprises three key components: a
reusable disease ontology that incorporates detailed knowledge about various
diseases, a diagnostic classification model that uses patient symptoms to
detect specific diseases accurately, and the integration of Semantic Web Rule
Language (SWRL) with ontology and ChatGPT to generate clear, personalized
health advice. This approach significantly improves prediction accuracy and
ensures results that are easy to understand, addressing the complexity of
diseases and diverse symptoms. The MLtoGAI system demonstrates substantial
advancements in accuracy and user satisfaction, contributing to developing more
intelligent and accessible healthcare solutions. This innovative approach
combines the strengths of ML algorithms with the ability to provide
transparent, human-understandable explanations through ChatGPT, achieving
significant improvements in prediction accuracy and user comprehension. By
leveraging semantic technology and explainable AI, the system enhances the
accuracy of disease prediction and ensures that the recommendations are
relevant and easily understood by individual patients. Our research highlights
the potential of integrating advanced technologies to overcome existing
challenges in medical diagnostics, paving the way for future developments in
intelligent healthcare systems. Additionally, the system is validated using 200
synthetic patient data records, ensuring robust performance and reliability.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.20284v1},
File          = {2407.20284v1.pdf}
}
@article{2407.18153v4,
Author        = {Gerard t Hooft},
Title         = {The Hidden Ontological Variable in Quantum Harmonic Oscillators},
Eprint        = {2407.18153v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {quant-ph},
Abstract      = {The standard quantum mechanical harmonic oscillator has an exact, dual
relationship with a completely classical system: a classical particle running
along a circle. Duality here means that there is a one-to-one relation between
all observables in one model, and the observables of the other model. Thus the
duality we find, appears to be in conflict with the usual assertion that
classical theories can never reproduce quantum effects as observed in many
quantum models. We suggest that there must be more of such relationships, but
we study only this one as a prototype. It reveals how classical "hidden
variables" may work. The classical states can form the basis of Hilbert space
that can be adopted in describing the quantum model. Wave functions in the
quantum system generate probability distributions in the classical one. One
finds that, where the classical system always obeys the rule "probability in =
probability out", the same probabilities are quantum probabilities in the
quantum system. It is shown how the quantum x and p operators in a quantum
oscillator can be given a classical meaning. It is explained how an apparent
clash with quantum logic can be explained away.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.18153v4},
File          = {2407.18153v4.pdf}
}
@article{2407.18058v1,
Author        = {Yannis Vasilakis and Rachel Bittner and Johan Pauwels},
Title         = {I can listen but cannot read: An evaluation of two-tower multimodal
  systems for instrument recognition},
Eprint        = {2407.18058v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SD},
Abstract      = {Music two-tower multimodal systems integrate audio and text modalities into a
joint audio-text space, enabling direct comparison between songs and their
corresponding labels. These systems enable new approaches for classification
and retrieval, leveraging both modalities. Despite the promising results they
have shown for zero-shot classification and retrieval tasks, closer inspection
of the embeddings is needed. This paper evaluates the inherent zero-shot
properties of joint audio-text spaces for the case-study of instrument
recognition. We present an evaluation and analysis of two-tower systems for
zero-shot instrument recognition and a detailed analysis of the properties of
the pre-joint and joint embeddings spaces. Our findings suggest that audio
encoders alone demonstrate good quality, while challenges remain within the
text encoder or joint space projection. Specifically, two-tower systems exhibit
sensitivity towards specific words, favoring generic prompts over musically
informed ones. Despite the large size of textual encoders, they do not yet
leverage additional textual context or infer instruments accurately from their
descriptions. Lastly, a novel approach for quantifying the semantic
meaningfulness of the textual space leveraging an instrument ontology is
proposed. This method reveals deficiencies in the systems' understanding of
instruments and provides evidence of the need for fine-tuning text encoders on
musical data.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.18058v1},
File          = {2407.18058v1.pdf}
}
@article{2408.01455v1,
Author        = {Tyler Fischella and Erin van Liemt and  Qiuyi and  Zhang},
Title         = {Ontology of Belief Diversity: A Community-Based Epistemological Approach},
Eprint        = {2408.01455v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CY},
Abstract      = {AI applications across classification, fairness, and human interaction often
implicitly require ontologies of social concepts. Constructing these well,
especially when there are many relevant categories, is a controversial task but
is crucial for achieving meaningful inclusivity. Here, we focus on developing a
pragmatic ontology of belief systems, which is a complex and often
controversial space. By iterating on our community-based design until mutual
agreement is reached, we found that epistemological methods were best for
categorizing the fundamental ways beliefs differ, maximally respecting our
principles of inclusivity and brevity. We demonstrate our methodology's utility
and interpretability via user studies in term annotation and sentiment analysis
experiments for belief fairness in language models.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2408.01455v1},
File          = {2408.01455v1.pdf}
}
@article{2407.17657v2,
Author        = {Carter Benson and Alec Sculley and Austin Liebers and John Beverley},
Title         = {My Ontologist: Evaluating BFO-Based AI for Definition Support},
Eprint        = {2407.17657v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Generative artificial intelligence (AI), exemplified by the release of
GPT-3.5 in 2022, has significantly advanced the potential applications of large
language models (LLMs), including in the realms of ontology development and
knowledge graph creation. Ontologies, which are structured frameworks for
organizing information, and knowledge graphs, which combine ontologies with
actual data, are essential for enabling interoperability and automated
reasoning. However, current research has largely overlooked the generation of
ontologies extending from established upper-level frameworks like the Basic
Formal Ontology (BFO), risking the creation of non-integrable ontology silos.
This study explores the extent to which LLMs, particularly GPT-4, can support
ontologists trained in BFO. Through iterative development of a specialized GPT
model named "My Ontologist," we aimed to generate BFO-conformant ontologies.
Initial versions faced challenges in maintaining definition conventions and
leveraging foundational texts effectively. My Ontologist 3.0 showed promise by
adhering to structured rules and modular ontology suites, yet the release of
GPT-4o disrupted this progress by altering the model's behavior. Our findings
underscore the importance of aligning LLM-generated ontologies with top-level
standards and highlight the complexities of integrating evolving AI
capabilities in ontology engineering.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.17657v2},
File          = {2407.17657v2.pdf}
}
@article{2407.17169v1,
Author        = {Luisa Vollmer and Sophie Fellenz and Fabian Jirasek and Heike Leitte and Hans Hasse},
Title         = {KnowTD-An Actionable Knowledge Representation System for Thermodynamics},
Eprint        = {2407.17169v1},
DOI           = {10.1021/acs.jcim.4c00647},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CE},
Abstract      = {We demonstrate that thermodynamic knowledge acquired by humans can be
transferred to computers so that the machine can use it to solve thermodynamic
problems and produce explainable solutions with a guarantee of correctness. The
actionable knowledge representation system that we have created for this
purpose is called KnowTD. It is based on an ontology of thermodynamics that
represents knowledge of thermodynamic theory, material properties, and
thermodynamic problems. The ontology is coupled with a reasoner that sets up
the problem to be solved based on user input, extracts the correct, pertinent
equations from the ontology, solves the resulting mathematical problem, and
returns the solution to the user, together with an explanation of how it was
obtained. KnowTD is presently limited to simple thermodynamic problems, similar
to those discussed in an introductory course in Engineering Thermodynamics.
This covers the basic theory and working principles of thermodynamics. KnowTD
is designed in a modular way and is easily extendable.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.17169v1},
File          = {2407.17169v1.pdf}
}
@article{2408.00800v2,
Author        = {Jonathan Reif and Tom Jeleniewski and Milapji Singh Gill and Felix Gehlhoff and Alexander Fay},
Title         = {Chatbot-Based Ontology Interaction Using Large Language Models and
  Domain-Specific Standards},
Eprint        = {2408.00800v2},
DOI           = {10.1109/ETFA61755.2024.10711065},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {The following contribution introduces a concept that employs Large Language
Models (LLMs) and a chatbot interface to enhance SPARQL query generation for
ontologies, thereby facilitating intuitive access to formalized knowledge.
Utilizing natural language inputs, the system converts user inquiries into
accurate SPARQL queries that strictly query the factual content of the
ontology, effectively preventing misinformation or fabrication by the LLM. To
enhance the quality and precision of outcomes, additional textual information
from established domain-specific standards is integrated into the ontology for
precise descriptions of its concepts and relationships. An experimental study
assesses the accuracy of generated SPARQL queries, revealing significant
benefits of using LLMs for querying ontologies and highlighting areas for
future research.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2408.00800v2},
File          = {2408.00800v2.pdf}
}
@article{2408.03339v1,
Author        = {Johannes Zimmermann and Dariusz Wiktorek and Thomas Meusburger and Miquel Monge-Dalmau and Antonio Fabregat and Alexander Jarasch and GÃ¼nter Schmidt and Jorge S. Reis-Filho and T. Ian Simpson},
Title         = {The Ontoverse: Democratising Access to Knowledge Graph-based Data
  Through a Cartographic Interface},
Eprint        = {2408.03339v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {As the number of scientific publications and preprints is growing
exponentially, several attempts have been made to navigate this complex and
increasingly detailed landscape. These have almost exclusively taken
unsupervised approaches that fail to incorporate domain knowledge and lack the
structural organisation required for intuitive interactive human exploration
and discovery. Especially in highly interdisciplinary fields, a deep
understanding of the connectedness of research works across topics is essential
for generating insights. We have developed a unique approach to data navigation
that leans on geographical visualisation and uses hierarchically structured
domain knowledge to enable end-users to explore knowledge spaces grounded in
their desired domains of interest. This can take advantage of existing
ontologies, proprietary intelligence schemata, or be directly derived from the
underlying data through hierarchical topic modelling. Our approach uses natural
language processing techniques to extract named entities from the underlying
data and normalise them against relevant domain references and navigational
structures. The knowledge is integrated by first calculating similarities
between entities based on their shared extracted feature space and then by
alignment to the navigational structures. The result is a knowledge graph that
allows for full text and semantic graph query and structured topic driven
navigation. This allows end-users to identify entities relevant to their needs
and access extensive graph analytics. The user interface facilitates graphical
interaction with the underlying knowledge graph and mimics a cartographic map
to maximise ease of use and widen adoption. We demonstrate an exemplar project
using our generalisable and scalable infrastructure for an academic biomedical
literature corpus that is grounded against hundreds of different named domain
entities.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2408.03339v1},
File          = {2408.03339v1.pdf}
}
@article{2407.14384v1,
Author        = {Piotr Ostropolski-Nalewaja and Sebastian Rudolph},
Title         = {The Sticky Path to Expressive Querying: Decidability of Navigational
  Queries under Existential Rules},
Eprint        = {2407.14384v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Extensive research in the field of ontology-based query answering has led to
the identification of numerous fragments of existential rules (also known as
tuple-generating dependencies) that exhibit decidable answering of atomic and
conjunctive queries. Motivated by the increased theoretical and practical
interest in navigational queries, this paper considers the question for which
of these fragments decidability of querying extends to regular path queries
(RPQs). In fact, decidability of RPQs has recently been shown to generally hold
for the comprehensive family of all fragments that come with the guarantee of
universal models being reasonably well-shaped (that is, being of finite
cliquewidth). Yet, for the second major family of fragments, known as finite
unification sets (short: fus), which are based on first-order-rewritability,
corresponding results have been largely elusive so far. We complete the picture
by showing that RPQ answering over arbitrary fus rulesets is undecidable. On
the positive side, we establish that the problem is decidable for the prominent
fus subclass of sticky rulesets, with the caveat that a very mild extension of
the RPQ formalism turns the problem undecidable again.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.14384v1},
File          = {2407.14384v1.pdf}
}
@article{2407.14234v1,
Author        = {Lu Chen},
Title         = {Symmetries as Isomorphisms},
Eprint        = {2407.14234v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {physics.hist-ph},
Abstract      = {Symmetries and isomorphisms play similar conceptual roles when we consider
how models represent physical situations, but they are formally distinct, as
two models related by symmetries are not typically isomorphic. I offer a
rigorous categorical strategy that formulate symmetries as isomorphisms between
models and apply it to classical electromagnetism, and evaluate its
philosophical significance in relation to the recent debate between
`sophistication' and `reduction'. In addition to traditional spacetime models,
I also consider algebraic models, in which case we can use the method of
natural operators to address the problem of ontological nonperspicuity faced by
the categorical strategy. Finally, I briefly expound on the significance of
symmetries as isomorphisms in the framework of Univalent Foundations, in which
isomorphic structures are formally identified.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.14234v1},
File          = {2407.14234v1.pdf}
}
@article{2407.14023v1,
Author        = {Aakash Sorathiya and Gouri Ginde},
Title         = {Towards Extracting Ethical Concerns-related Software Requirements from
  App Reviews},
Eprint        = {2407.14023v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {As mobile applications become increasingly integral to our daily lives,
concerns about ethics have grown drastically. Users share their experiences,
report bugs, and request new features in application reviews, often
highlighting safety, privacy, and accountability concerns. Approaches using
machine learning techniques have been used in the past to identify these
ethical concerns. However, understanding the underlying reasons behind them and
extracting requirements that could address these concerns is crucial for safer
software solution development. Thus, we propose a novel approach that leverages
a knowledge graph (KG) model to extract software requirements from app reviews,
capturing contextual data related to ethical concerns. Our framework consists
of three main components: developing an ontology with relevant entities and
relations, extracting key entities from app reviews, and creating connections
between them. This study analyzes app reviews of the Uber mobile application (a
popular taxi/ride app) and presents the preliminary results from the proposed
solution. Initial results show that KG can effectively capture contextual data
related to software ethical concerns, the underlying reasons behind these
concerns, and the corresponding potential requirements.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.14023v1},
File          = {2407.14023v1.pdf}
}
@article{2407.13416v1,
Author        = {Hana PokojnÃ¡ and Tobias Isenberg and Stefan Bruckner and Barbora KozlÃ­kovÃ¡ and Laura Garrison},
Title         = {The Language of Infographics: Toward Understanding Conceptual Metaphor
  Use in Scientific Storytelling},
Eprint        = {2407.13416v1},
DOI           = {10.1109/TVCG.2024.3456327},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {We apply an approach from cognitive linguistics by mapping Conceptual
Metaphor Theory (CMT) to the visualization domain to address patterns of visual
conceptual metaphors that are often used in science infographics. Metaphors
play an essential part in visual communication and are frequently employed to
explain complex concepts. However, their use is often based on intuition,
rather than following a formal process. At present, we lack tools and language
for understanding and describing metaphor use in visualization to the extent
where taxonomy and grammar could guide the creation of visual components, e.g.,
infographics. Our classification of the visual conceptual mappings within
scientific representations is based on the breakdown of visual components in
existing scientific infographics. We demonstrate the development of this
mapping through a detailed analysis of data collected from four domains
(biomedicine, climate, space, and anthropology) that represent a diverse range
of visual conceptual metaphors used in the visual communication of science.
This work allows us to identify patterns of visual conceptual metaphor use
within the domains, resolve ambiguities about why specific conceptual metaphors
are used, and develop a better overall understanding of visual metaphor use in
scientific infographics. Our analysis shows that ontological and orientational
conceptual metaphors are the most widely applied to translate complex
scientific concepts. To support our findings we developed a visual exploratory
tool based on the collected database that places the individual infographics on
a spatio-temporal scale and illustrates the breakdown of visual conceptual
metaphors.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.13416v1},
File          = {2407.13416v1.pdf}
}
@article{2407.13329v3,
Author        = {Lorenzo Paolini and Sahar Vahdati and Angelo Di Iorio and Robert Wardenga and Ivan Heibi and Silvio Peroni},
Title         = {CiteFusion: An Ensemble Framework for Citation Intent Classification
  Harnessing Dual-Model Binary Couples and SHAP Analyses},
Eprint        = {2407.13329v3},
DOI           = {10.5281/zenodo.15011985},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Understanding the motivations underlying scholarly citations is essential to
evaluate research impact and pro-mote transparent scholarly communication. This
study introduces CiteFusion, an ensemble framework designed to address the
multi-class Citation Intent Classification task on two benchmark datasets:
SciCite and ACL-ARC. The framework employs a one-vs-all decomposition of the
multi-class task into class-specific binary sub-tasks, leveraging complementary
pairs of SciBERT and XLNet models, independently tuned, for each citation
intent. The outputs of these base models are aggregated through a feedforward
neural network meta-classifier to reconstruct the original classification task.
To enhance interpretability, SHAP (SHapley Additive exPlanations) is employed
to analyze token-level contributions, and interactions among base models,
providing transparency into the classification dynamics of CiteFusion, and
insights about the kind of misclassifications of the ensem-ble. In addition,
this work investigates the semantic role of structural context by incorporating
section titles, as framing devices, into input sentences, assessing their
positive impact on classification accuracy. CiteFusion ul-timately demonstrates
robust performance in imbalanced and data-scarce scenarios: experimental
results show that CiteFusion achieves state-of-the-art performance, with
Macro-F1 scores of 89.60% on SciCite, and 76.24% on ACL-ARC. Furthermore, to
ensure interoperability and reusability, citation intents from both datasets
sche-mas are mapped to Citation Typing Ontology (CiTO) object properties,
highlighting some overlaps. Finally, we describe and release a web-based
application that classifies citation intents leveraging the CiteFusion models
developed on SciCite.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.13329v3},
File          = {2407.13329v3.pdf}
}
@article{2407.13799v1,
Author        = {Marcel van Kessel},
Title         = {Ontological States in Non-Interacting Quantum Field Theories},
Eprint        = {2407.13799v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {quant-ph},
Abstract      = {This is a paper in the field of ontological deterministic theories behind
Quantum Field Theories, like for example the cellular automaton theories
proposed by 't Hooft. In these theories one has ontological states in which the
state of reality is exactly known and no uncertainties are present. Also these
states evolve in time deterministically. A first step in finding the
ontological deterministic theory behind the Standard Model is to find in
Quantum Field Theory the states that behave as ontological states. We present
the ontological states for all non-interacting (3+1-dimensional) Quantum Field
Theories occurring in the Standard Model. We summarize the ontological states
for free scalar bosons and for free masless Dirac fermions, which are known
from the literature. We construct the ontological states for vector bosons, in
analogy to the scalar boson case. With this we have a set of ontological states
for all particles that are known to occur in reality and in the Standard Model.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.13799v1},
File          = {2407.13799v1.pdf}
}
@article{2407.10735v2,
Author        = {Xabier E. Barandiaran and Lola S. Almendros},
Title         = {Transforming Agency. On the mode of existence of Large Language Models},
Eprint        = {2407.10735v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper investigates the ontological characterization of Large Language
Models (LLMs) like ChatGPT. Between inflationary and deflationary accounts, we
pay special attention to their status as agents. This requires explaining in
detail the architecture, processing, and training procedures that enable LLMs
to display their capacities, and the extensions used to turn LLMs into
agent-like systems. After a systematic analysis we conclude that a LLM fails to
meet necessary and sufficient conditions for autonomous agency in the light of
embodied theories of mind: the individuality condition (it is not the product
of its own activity, it is not even directly affected by it), the normativity
condition (it does not generate its own norms or goals), and, partially the
interactional asymmetry condition (it is not the origin and sustained source of
its interaction with the environment). If not agents, then ... what are LLMs?
We argue that ChatGPT should be characterized as an interlocutor or linguistic
automaton, a library-that-talks, devoid of (autonomous) agency, but capable to
engage performatively on non-purposeful yet purpose-structured and
purpose-bounded tasks. When interacting with humans, a "ghostly" component of
the human-machine interaction makes it possible to enact genuine conversational
experiences with LLMs. Despite their lack of sensorimotor and biological
embodiment, LLMs textual embodiment (the training corpus) and resource-hungry
computational embodiment, significantly transform existing forms of human
agency. Beyond assisted and extended agency, the LLM-human coupling can produce
midtended forms of agency, closer to the production of intentional agency than
to the extended instrumentality of any previous technologies.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.10735v2},
File          = {2407.10735v2.pdf}
}
@article{2407.09212v4,
Author        = {Yunjie He and Daniel Hernandez and Mojtaba Nayyeri and Bo Xiong and Yuqicheng Zhu and Evgeny Kharlamov and Steffen Staab},
Title         = {Generating $SROI^-$ Ontologies via Knowledge Graph Query Embedding
  Learning},
Eprint        = {2407.09212v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Query embedding approaches answer complex logical queries over incomplete
knowledge graphs (KGs) by computing and operating on low-dimensional vector
representations of entities, relations, and queries. However, current query
embedding models heavily rely on excessively parameterized neural networks and
cannot explain the knowledge learned from the graph. We propose a novel query
embedding method, AConE, which explains the knowledge learned from the graph in
the form of $SROI^-$ description logic axioms while being more
parameter-efficient than most existing approaches. AConE associates queries to
a $SROI^-$ description logic concept. Every $SROI^-$ concept is embedded as a
cone in complex vector space, and each $SROI^-$ relation is embedded as a
transformation that rotates and scales cones. We show theoretically that AConE
can learn $SROI^-$ axioms, and defines an algebra whose operations correspond
one to one to $SROI^-$ description logic concept constructs. Our empirical
study on multiple query datasets shows that AConE achieves superior results
over previous baselines with fewer parameters. Notably on the WN18RR dataset,
AConE achieves significant improvement over baseline models. We provide
comprehensive analyses showing that the capability to represent axioms
positively impacts the results of query answering.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.09212v4},
File          = {2407.09212v4.pdf}
}
@article{2407.09103v1,
Author        = {Thomas Constum and Pierrick Tranouez and Thierry Paquet},
Title         = {DANIEL: A fast Document Attention Network for Information Extraction and
  Labelling of handwritten documents},
Eprint        = {2407.09103v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Information extraction from handwritten documents involves traditionally
three distinct steps: Document Layout Analysis, Handwritten Text Recognition,
and Named Entity Recognition. Recent approaches have attempted to integrate
these steps into a single process using fully end-to-end architectures. Despite
this, these integrated approaches have not yet matched the performance of
language models, when applied to information extraction in plain text. In this
paper, we introduce DANIEL (Document Attention Network for Information
Extraction and Labelling), a fully end-to-end architecture integrating a
language model and designed for comprehensive handwritten document
understanding. DANIEL performs layout recognition, handwriting recognition, and
named entity recognition on full-page documents. Moreover, it can
simultaneously learn across multiple languages, layouts, and tasks. For named
entity recognition, the ontology to be applied can be specified via the input
prompt. The architecture employs a convolutional encoder capable of processing
images of any size without resizing, paired with an autoregressive decoder
based on a transformer-based language model. DANIEL achieves competitive
results on four datasets, including a new state-of-the-art performance on RIMES
2009 and M-POPP for Handwriting Text Recognition, and IAM NER for Named Entity
Recognition. Furthermore, DANIEL is much faster than existing approaches.
  We provide the source code and the weights of the trained models at
\url{https://github.com/Shulk97/daniel}.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.09103v1},
File          = {2407.09103v1.pdf}
}
@article{2407.08411v1,
Author        = {Shishir Muralidhara and Saqib Bukhari and Georg Schneider and Didier Stricker and RenÃ© Schuster},
Title         = {CLEO: Continual Learning of Evolving Ontologies},
Eprint        = {2407.08411v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Continual learning (CL) addresses the problem of catastrophic forgetting in
neural networks, which occurs when a trained model tends to overwrite
previously learned information, when presented with a new task. CL aims to
instill the lifelong learning characteristic of humans in intelligent systems,
making them capable of learning continuously while retaining what was already
learned. Current CL problems involve either learning new domains
(domain-incremental) or new and previously unseen classes (class-incremental).
However, general learning processes are not just limited to learning
information, but also refinement of existing information. In this paper, we
define CLEO - Continual Learning of Evolving Ontologies, as a new incremental
learning setting under CL to tackle evolving classes. CLEO is motivated by the
need for intelligent systems to adapt to real-world ontologies that change over
time, such as those in autonomous driving. We use Cityscapes, PASCAL VOC, and
Mapillary Vistas to define the task settings and demonstrate the applicability
of CLEO. We highlight the shortcomings of existing CIL methods in adapting to
CLEO and propose a baseline solution, called Modelling Ontologies (MoOn). CLEO
is a promising new approach to CL that addresses the challenge of evolving
ontologies in real-world applications. MoOn surpasses previous CL approaches in
the context of CLEO.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.08411v1},
File          = {2407.08411v1.pdf}
}
@article{2407.07045v1,
Author        = {Christian Riefolo and Nicola Fanizzi and Claudia d'Amato},
Title         = {Simple and Interpretable Probabilistic Classifiers for Knowledge Graphs},
Eprint        = {2407.07045v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Tackling the problem of learning probabilistic classifiers from incomplete
data in the context of Knowledge Graphs expressed in Description Logics, we
describe an inductive approach based on learning simple belief networks.
Specifically, we consider a basic probabilistic model, a Naive Bayes
classifier, based on multivariate Bernoullis and its extension to a two-tier
network in which this classification model is connected to a lower layer
consisting of a mixture of Bernoullis. We show how such models can be converted
into (probabilistic) axioms (or rules) thus ensuring more interpretability.
Moreover they may be also initialized exploiting expert knowledge. We present
and discuss the outcomes of an empirical evaluation which aimed at testing the
effectiveness of the models on a number of random classification problems with
different ontologies.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.07045v1},
File          = {2407.07045v1.pdf}
}
@article{2407.06930v1,
Author        = {Milapji Singh Gill and Tom Westermann and Gernot Steindl and Felix Gehlhoff and Alexander Fay},
Title         = {Integrating Ontology Design with the CRISP-DM in the context of
  Cyber-Physical Systems Maintenance},
Eprint        = {2407.06930v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In the following contribution, a method is introduced that integrates domain
expert-centric ontology design with the Cross-Industry Standard Process for
Data Mining (CRISP-DM). This approach aims to efficiently build an
application-specific ontology tailored to the corrective maintenance of
Cyber-Physical Systems (CPS). The proposed method is divided into three phases.
In phase one, ontology requirements are systematically specified, defining the
relevant knowledge scope. Accordingly, CPS life cycle data is contextualized in
phase two using domain-specific ontological artifacts. This formalized domain
knowledge is then utilized in the CRISP-DM to efficiently extract new insights
from the data. Finally, the newly developed data-driven model is employed to
populate and expand the ontology. Thus, information extracted from this model
is semantically annotated and aligned with the existing ontology in phase
three. The applicability of this method has been evaluated in an anomaly
detection case study for a modular process plant.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.06930v1},
File          = {2407.06930v1.pdf}
}
@article{2407.12851v1,
Author        = {Zixin Shu and Rui Hua and Dengying Yan and Chenxia Lu and Ning Xu and Jun Li and Hui Zhu and Jia Zhang and Dan Zhao and Chenyang Hui and Junqiu Ye and Chu Liao and Qi Hao and Wen Ye and Cheng Luo and Xinyan Wang and Chuang Cheng and Xiaodong Li and Baoyan Liu and Xiaji Zhou and Runshun Zhang and Min Xu and Xuezhong Zhou},
Title         = {ISPO: An Integrated Ontology of Symptom Phenotypes for Semantic
  Integration of Traditional Chinese Medical Data},
Eprint        = {2407.12851v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Symptom phenotypes are one of the key types of manifestations for diagnosis
and treatment of various disease conditions. However, the diversity of symptom
terminologies is one of the major obstacles hindering the analysis and
knowledge sharing of various types of symptom-related medical data particularly
in the fields of Traditional Chinese Medicine (TCM). Objective: This study
aimed to construct an Integrated Ontology of symptom phenotypes (ISPO) to
support the data mining of Chinese EMRs and real-world study in TCM field.
Methods: To construct an integrated ontology of symptom phenotypes (ISPO), we
manually annotated classical TCM textbooks and large-scale Chinese electronic
medical records (EMRs) to collect symptom terms with support from a medical
text annotation system. Furthermore, to facilitate the semantic
interoperability between different terminologies, we incorporated public
available biomedical vocabularies by manual mapping between Chinese terms and
English terms with cross-references to source vocabularies. In addition, we
evaluated the ISPO using independent clinical EMRs to provide a high-usable
medical ontology for clinical data analysis. Results: By integrating 78,696
inpatient cases of EMRs, 5 biomedical vocabularies, 21 TCM books and
dictionaries, ISPO provides 3,147 concepts, 23,475 terms, and 55,552 definition
or contextual texts. Adhering to the taxonomical structure of the related
anatomical systems of symptom phenotypes, ISPO provides 12 top-level categories
and 79 middle-level sub-categories. The validation of data analysis showed the
ISPO has a coverage rate of 95.35%, 98.53% and 92.66% for symptom terms with
occurrence rates of 0.5% in additional three independent curated clinical
datasets, which can demonstrate the significant value of ISPO in mapping
clinical terms to ontologies.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.12851v1},
File          = {2407.12851v1.pdf}
}
@article{2407.09567v4,
Author        = {Wouter van der Wijngaart},
Title         = {Where are the bits in atoms? A perspective on the physical origin and
  evolutionary nature of information},
Eprint        = {2407.09567v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.NC},
Abstract      = {Information is a structural pattern that represents another structural
pattern. This perspective hypothesizes that modelling of structure creation
through causal sets can elucidate the natural origin, evolution and ontology of
information.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.09567v4},
File          = {2407.09567v4.pdf}
}
@article{2407.04486v1,
Author        = {Tianshu Feng and Rohan Gnanaolivu and Abolfazl Safikhani and Yuanhang Liu and Jun Jiang and Nicholas Chia and Alexander Partin and Priyanka Vasanthakumari and Yitan Zhu and Chen Wang},
Title         = {Variational and Explanatory Neural Networks for Encoding Cancer Profiles
  and Predicting Drug Responses},
Eprint        = {2407.04486v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.QM},
Abstract      = {Human cancers present a significant public health challenge and require the
discovery of novel drugs through translational research. Transcriptomics
profiling data that describes molecular activities in tumors and cancer cell
lines are widely utilized for predicting anti-cancer drug responses. However,
existing AI models face challenges due to noise in transcriptomics data and
lack of biological interpretability. To overcome these limitations, we
introduce VETE (Variational and Explanatory Transcriptomics Encoder), a novel
neural network framework that incorporates a variational component to mitigate
noise effects and integrates traceable gene ontology into the neural network
architecture for encoding cancer transcriptomics data. Key innovations include
a local interpretability-guided method for identifying ontology paths, a
visualization tool to elucidate biological mechanisms of drug responses, and
the application of centralized large scale hyperparameter optimization. VETE
demonstrated robust accuracy in cancer cell line classification and drug
response prediction. Additionally, it provided traceable biological
explanations for both tasks and offers insights into the mechanisms underlying
its predictions. VETE bridges the gap between AI-driven predictions and
biologically meaningful insights in cancer research, which represents a
promising advancement in the field.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.04486v1},
File          = {2407.04486v1.pdf}
}
@article{2407.04317v2,
Author        = {SÃ©bastien Guillemin and Ana Roxin and Laurence Dujourdy and Ludovic Journaux},
Title         = {Knowledge-based Drug Samples' Comparison},
Eprint        = {2407.04317v2},
DOI           = {10.1109/SITIS61268.2023.00020},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Drug sample comparison is a process used by the French National police to
identify drug distribution networks. The current approach is based on manual
comparison done by forensic experts. In this article, we present our approach
to acquire, formalise, and specify expert knowledge to improve the current
process. For modelling the underlying knowledge we use an ontology coupled with
logical rules. The different steps of our approach are designed to be reused in
other application domains. The results obtained are explainable making them
usable by experts in different fields.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.04317v2},
File          = {2407.04317v2.pdf}
}
@article{2407.03183v1,
Author        = {Marvin Schieseck and Philip Topalis and Lasse Reinpold and Felix Gehlhoff and Alexander Fay},
Title         = {A Formal Model for Artificial Intelligence Applications in Automation
  Systems},
Eprint        = {2407.03183v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {eess.SY},
Abstract      = {The integration of Artificial Intelligence (AI) into automation systems has
the potential to enhance efficiency and to address currently unsolved existing
technical challenges. However, the industry-wide adoption of AI is hindered by
the lack of standardized documentation for the complex compositions of
automation systems, AI software, production hardware, and their
interdependencies. This paper proposes a formal model using standards and
ontologies to provide clear and structured documentation of AI applications in
automation systems. The proposed information model for artificial intelligence
in automation systems (AIAS) utilizes ontology design patterns to map and link
various aspects of automation systems and AI software. Validated through a
practical example, the model demonstrates its effectiveness in improving
documentation practices and aiding the sustainable implementation of AI in
industrial settings.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.03183v1},
File          = {2407.03183v1.pdf}
}
@article{2407.12830v3,
Author        = {Sai Sathiesh Rajan and Ezekiel Soremekun and Sudipta Chattopadhyay},
Title         = {Knowledge-based Consistency Testing of Large Language Models},
Eprint        = {2407.12830v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this work, we systematically expose and measure the inconsistency and
knowledge gaps of Large Language Models (LLMs). Specifically, we propose an
automated testing framework (called KonTest) which leverages a knowledge graph
to construct test cases. KonTest probes and measures the inconsistencies in the
LLM's knowledge of the world via a combination of semantically-equivalent
queries and test oracles (metamorphic or ontological oracle). KonTest further
mitigates knowledge gaps via a weighted LLM model ensemble. Using four
state-of-the-art LLMs (Falcon, Gemini, GPT3.5, and Llama2), we show that
KonTest generates 19.2% error inducing inputs (1917 errors from 9979 test
inputs). It also reveals a 16.5% knowledge gap across all tested LLMs. A
mitigation method informed by KonTest's test suite reduces LLM knowledge gap by
32.48%. Our ablation study further shows that GPT3.5 is not suitable for
knowledge-based consistency testing because it is only 60%-68% effective in
knowledge construction.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.12830v3},
File          = {2407.12830v3.pdf}
}
@article{2407.02162v1,
Author        = {Somnath Bharech and Yangyiwei Yang and Michael Selzer and Britta Nestler and Bai-Xiang Xu},
Title         = {ML-extendable framework for multiphysics-multiscale simulation workflow
  and data management using Kadi4Mat},
Eprint        = {2407.02162v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cond-mat.mtrl-sci},
Abstract      = {As material modeling and simulation has become vital for modern materials
science, research data with distinctive physical principles and extensive
volume are generally required for full elucidation of the material behavior
across all relevant scales. Effective workflow and data management, with
corresponding metadata descriptions, helps leverage the full potential of
data-driven analyses for computer-aided material design. In this work, we
propose a research workflow and data management (RWDM) framework to manage
complex workflows and resulting research (meta)data, while following FAIR
principles. Multiphysics multiscale simulations for additive manufacturing
investigations are treated as showcase and implemented on Kadi4Mat: an open
source research data infrastructure. The input and output data of the
simulations, together with the associated setups and scripts realizing the
simulation workflow, are curated in corresponding standardized Kadi4Mat records
with extendibility for further research and data-driven analyses. These records
are interlinked to indicate information flow and form an ontology based
knowledge graph. Automation scheme for performing high-throughput simulation
and post-processing integrated with the proposed RWDM framework is also
presented.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.02162v1},
File          = {2407.02162v1.pdf}
}
@article{2407.01406v3,
Author        = {Daniil Gurgurov and Mareike Hartmann and Simon Ostermann},
Title         = {Adapting Multilingual LLMs to Low-Resource Languages with Knowledge
  Graphs via Adapters},
Eprint        = {2407.01406v3},
DOI           = {10.18653/v1/2024.kallm-1.7},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper explores the integration of graph knowledge from linguistic
ontologies into multilingual Large Language Models (LLMs) using adapters to
improve performance for low-resource languages (LRLs) in sentiment analysis
(SA) and named entity recognition (NER). Building upon successful
parameter-efficient fine-tuning techniques, such as K-ADAPTER and MAD-X, we
propose a similar approach for incorporating knowledge from multilingual
graphs, connecting concepts in various languages with each other through
linguistic relationships, into multilingual LLMs for LRLs. Specifically, we
focus on eight LRLs -- Maltese, Bulgarian, Indonesian, Nepali, Javanese,
Uyghur, Tibetan, and Sinhala -- and employ language-specific adapters
fine-tuned on data extracted from the language-specific section of ConceptNet,
aiming to enable knowledge transfer across the languages covered by the
knowledge graph. We compare various fine-tuning objectives, including standard
Masked Language Modeling (MLM), MLM with full-word masking, and MLM with
targeted masking, to analyse their effectiveness in learning and integrating
the extracted graph data. Through empirical evaluation on language-specific
tasks, we assess how structured graph knowledge affects the performance of
multilingual LLMs for LRLs in SA and NER, providing insights into the potential
benefits of adapting language models for low-resource scenarios.},
Year          = {2024},
Month         = {Jul},
Note          = {2024.kallm-1.7},
Url           = {http://arxiv.org/abs/2407.01406v3},
File          = {2407.01406v3.pdf}
}
@article{2406.19537v1,
Author        = {Andrea Bacciu and Marco Damonte and Marco Basaldella and Emilio Monti},
Title         = {Handling Ontology Gaps in Semantic Parsing},
Eprint        = {2406.19537v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The majority of Neural Semantic Parsing (NSP) models are developed with the
assumption that there are no concepts outside the ones such models can
represent with their target symbols (closed-world assumption). This assumption
leads to generate hallucinated outputs rather than admitting their lack of
knowledge. Hallucinations can lead to wrong or potentially offensive responses
to users. Hence, a mechanism to prevent this behavior is crucial to build
trusted NSP-based Question Answering agents. To that end, we propose the
Hallucination Simulation Framework (HSF), a general setting for stimulating and
analyzing NSP model hallucinations. The framework can be applied to any NSP
task with a closed-ontology. Using the proposed framework and KQA Pro as the
benchmark dataset, we assess state-of-the-art techniques for hallucination
detection. We then present a novel hallucination detection strategy that
exploits the computational graph of the NSP model to detect the NSP
hallucinations in the presence of ontology gaps, out-of-domain utterances, and
to recognize NSP errors, improving the F1-Score respectively by ~21, ~24% and
~1%. This is the first work in closed-ontology NSP that addresses the problem
of recognizing ontology gaps. We release our code and checkpoints at
https://github.com/amazon-science/handling-ontology-gaps-in-semantic-parsing.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.19537v1},
File          = {2406.19537v1.pdf}
}
@article{2406.19228v1,
Author        = {Jimin Sun and So Yeon Min and Yingshan Chang and Yonatan Bisk},
Title         = {Tools Fail: Detecting Silent Errors in Faulty Tools},
Eprint        = {2406.19228v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Tools have become a mainstay of LLMs, allowing them to retrieve knowledge not
in their weights, to perform tasks on the web, and even to control robots.
However, most ontologies and surveys of tool-use have assumed the core
challenge for LLMs is choosing the tool. Instead, we introduce a framework for
tools more broadly which guides us to explore a model's ability to detect
"silent" tool errors, and reflect on how to plan. This more directly aligns
with the increasingly popular use of models as tools. We provide an initial
approach to failure recovery with promising results both on a controlled
calculator setting and embodied agent planning.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.19228v1},
File          = {2406.19228v1.pdf}
}
@article{2406.18276v1,
Author        = {Hrishikesh Terdalkar},
Title         = {Sanskrit Knowledge-based Systems: Annotation and Computational Tools},
Eprint        = {2406.18276v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We address the challenges and opportunities in the development of knowledge
systems for Sanskrit, with a focus on question answering. By proposing a
framework for the automated construction of knowledge graphs, introducing
annotation tools for ontology-driven and general-purpose tasks, and offering a
diverse collection of web-interfaces, tools, and software libraries, we have
made significant contributions to the field of computational Sanskrit. These
contributions not only enhance the accessibility and accuracy of Sanskrit text
analysis but also pave the way for further advancements in knowledge
representation and language processing. Ultimately, this research contributes
to the preservation, understanding, and utilization of the rich linguistic
information embodied in Sanskrit texts.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.18276v1},
File          = {2406.18276v1.pdf}
}
@article{2406.17532v2,
Author        = {Keyu Wang and Guilin Qi and Jiaqi Li and Songlin Zhai},
Title         = {Can Large Language Models Understand DL-Lite Ontologies? An Empirical
  Study},
Eprint        = {2406.17532v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large language models (LLMs) have shown significant achievements in solving a
wide range of tasks. Recently, LLMs' capability to store, retrieve and infer
with symbolic knowledge has drawn a great deal of attention, showing their
potential to understand structured information. However, it is not yet known
whether LLMs can understand Description Logic (DL) ontologies. In this work, we
empirically analyze the LLMs' capability of understanding DL-Lite ontologies
covering 6 representative tasks from syntactic and semantic aspects. With
extensive experiments, we demonstrate both the effectiveness and limitations of
LLMs in understanding DL-Lite ontologies. We find that LLMs can understand
formal syntax and model-theoretic semantics of concepts and roles. However,
LLMs struggle with understanding TBox NI transitivity and handling ontologies
with large ABoxes. We hope that our experiments and analyses provide more
insights into LLMs and inspire to build more faithful knowledge engineering
solutions.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.17532v2},
File          = {2406.17532v2.pdf}
}
@article{2406.15268v1,
Author        = {Lynn Vonderhaar and Timothy Elvira and Tyler Procko and Omar Ochoa},
Title         = {Towards Robust Training Datasets for Machine Learning with Ontologies: A
  Case Study for Emergency Road Vehicle Detection},
Eprint        = {2406.15268v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Countless domains rely on Machine Learning (ML) models, including
safety-critical domains, such as autonomous driving, which this paper focuses
on. While the black box nature of ML is simply a nuisance in some domains, in
safety-critical domains, this makes ML models difficult to trust. To fully
utilize ML models in safety-critical domains, it would be beneficial to have a
method to improve trust in model robustness and accuracy without human experts
checking each decision. This research proposes a method to increase trust in ML
models used in safety-critical domains by ensuring the robustness and
completeness of the model's training dataset. Because ML models embody what
they are trained with, ensuring the completeness of training datasets can help
to increase the trust in the training of ML models. To this end, this paper
proposes the use of a domain ontology and an image quality characteristic
ontology to validate the domain completeness and image quality robustness of a
training dataset. This research also presents an experiment as a proof of
concept for this method, where ontologies are built for the emergency road
vehicle domain.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.15268v1},
File          = {2406.15268v1.pdf}
}
@article{2406.14757v1,
Author        = {Syed I. Munzir and Daniel B. Hier and Chelsea Oommen and Michael D. Carrithers},
Title         = {A Large Language Model Outperforms Other Computational Approaches to the
  High-Throughput Phenotyping of Physician Notes},
Eprint        = {2406.14757v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {High-throughput phenotyping, the automated mapping of patient signs and
symptoms to standardized ontology concepts, is essential to gaining value from
electronic health records (EHR) in the support of precision medicine. Despite
technological advances, high-throughput phenotyping remains a challenge. This
study compares three computational approaches to high-throughput phenotyping: a
Large Language Model (LLM) incorporating generative AI, a Natural Language
Processing (NLP) approach utilizing deep learning for span categorization, and
a hybrid approach combining word vectors with machine learning. The approach
that implemented GPT-4 (a Large Language Model) demonstrated superior
performance, suggesting that Large Language Models are poised to be the
preferred method for high-throughput phenotyping of physician notes.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.14757v1},
File          = {2406.14757v1.pdf}
}
@article{2406.14312v1,
Author        = {Abul Hasan and Jinge Wu and Quang Ngoc Nguyen and SalomÃ© Andres and Imane Guellil and Huayu Zhang and Arlene Casey and Beatrice Alex and Bruce Guthrie and Honghan Wu},
Title         = {Infusing clinical knowledge into tokenisers for language models},
Eprint        = {2406.14312v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This study introduces a novel knowledge enhanced tokenisation mechanism,
K-Tokeniser, for clinical text processing. Technically, at initialisation
stage, K-Tokeniser populates global representations of tokens based on semantic
types of domain concepts (such as drugs or diseases) from either a domain
ontology like Unified Medical Language System or the training data of the task
related corpus. At training or inference stage, sentence level localised
context will be utilised for choosing the optimal global token representation
to realise the semantic-based tokenisation. To avoid pretraining using the new
tokeniser, an embedding initialisation approach is proposed to generate
representations for new tokens. Using three transformer-based language models,
a comprehensive set of experiments are conducted on four real-world datasets
for evaluating K-Tokeniser in a wide range of clinical text analytics tasks
including clinical concept and relation extraction, automated clinical coding,
clinical phenotype identification, and clinical research article
classification. Overall, our models demonstrate consistent improvements over
their counterparts in all tasks. In particular, substantial improvements are
observed in the automated clinical coding task with 13\% increase on Micro
$F_1$ score. Furthermore, K-Tokeniser also shows significant capacities in
facilitating quicker converge of language models. Specifically, using
K-Tokeniser, the language models would only require 50\% of the training data
to achieve the best performance of the baseline tokeniser using all training
data in the concept extraction task and less than 20\% of the data for the
automated coding task. It is worth mentioning that all these improvements
require no pre-training process, making the approach generalisable.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.14312v1},
File          = {2406.14312v1.pdf}
}
@article{2406.14307v2,
Author        = {Chao Hui Huang},
Title         = {QuST-LLM: Integrating Large Language Models for Comprehensive Spatial
  Transcriptomics Analysis},
Eprint        = {2406.14307v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.GN},
Abstract      = {In this paper, we introduce QuST-LLM, an innovative extension of QuPath that
utilizes the capabilities of large language models (LLMs) to analyze and
interpret spatial transcriptomics (ST) data. In addition to simplifying the
intricate and high-dimensional nature of ST data by offering a comprehensive
workflow that includes data loading, region selection, gene expression
analysis, and functional annotation, QuST-LLM employs LLMs to transform complex
ST data into understandable and detailed biological narratives based on gene
ontology annotations, thereby significantly improving the interpretability of
ST data. Consequently, users can interact with their own ST data using natural
language. Hence, QuST-LLM provides researchers with a potent functionality to
unravel the spatial and functional complexities of tissues, fostering novel
insights and advancements in biomedical research. QuST-LLM is a part of QuST
project. The source code is hosted on GitHub and documentation is available at
(https://github.com/huangch/qust).},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.14307v2},
File          = {2406.14307v2.pdf}
}
@article{2406.13791v3,
Author        = {Amelie Gyrard and Seyedali Mohammadi and Manas Gaur and Antonio Kung},
Title         = {IoT-Based Preventive Mental Health Using Knowledge Graphs and Standards
  for Better Well-Being},
Eprint        = {2406.13791v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Sustainable Development Goals (SDGs) give the UN a road map for development
with Agenda 2030 as a target. SDG3 "Good Health and Well-Being" ensures healthy
lives and promotes well-being for all ages. Digital technologies can support
SDG3. Burnout and even depression could be reduced by encouraging better
preventive health. Due to the lack of patient knowledge and focus to take care
of their health, it is necessary to help patients before it is too late. New
trends such as positive psychology and mindfulness are highly encouraged in the
USA. Digital Twins (DTs) can help with the continuous monitoring of emotion
using physiological signals (e.g., collected via wearables). DTs facilitate
monitoring and provide constant health insight to improve quality of life and
well-being with better personalization. Healthcare DTs challenges are
standardizing data formats, communication protocols, and data exchange
mechanisms. As an example, ISO has the ISO/IEC JTC 1/SC 41 Internet of Things
(IoT) and DTs Working Group, with standards such as "ISO/IEC 21823-3:2021 IoT -
Interoperability for IoT Systems - Part 3 Semantic interoperability", "ISO/IEC
CD 30178 - IoT - Data format, value and coding". To achieve those data
integration and knowledge challenges, we designed the Mental Health Knowledge
Graph (ontology and dataset) to boost mental health. As an example, explicit
knowledge is described such as chocolate contains magnesium which is
recommended for depression. The Knowledge Graph (KG) acquires knowledge from
ontology-based mental health projects classified within the LOV4IoT ontology
catalog (Emotion, Depression, and Mental Health). Furthermore, the KG is mapped
to standards when possible. Standards from ETSI SmartM2M can be used such as
SAREF4EHAW to represent medical devices and sensors, but also ITU/WHO, ISO,
W3C, NIST, and IEEE standards relevant to mental health can be considered.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.13791v3},
File          = {2406.13791v3.pdf}
}
@article{2406.12420v2,
Author        = {Philipp Seeberger and Dominik Wagner and Korbinian Riedhammer},
Title         = {MMUTF: Multimodal Multimedia Event Argument Extraction with Unified
  Template Filling},
Eprint        = {2406.12420v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {With the advancement of multimedia technologies, news documents and
user-generated content are often represented as multiple modalities, making
Multimedia Event Extraction (MEE) an increasingly important challenge. However,
recent MEE methods employ weak alignment strategies and data augmentation with
simple classification models, which ignore the capabilities of natural
language-formulated event templates for the challenging Event Argument
Extraction (EAE) task. In this work, we focus on EAE and address this issue by
introducing a unified template filling model that connects the textual and
visual modalities via textual prompts. This approach enables the exploitation
of cross-ontology transfer and the incorporation of event-specific semantics.
Experiments on the M2E2 benchmark demonstrate the effectiveness of our
approach. Our system surpasses the current SOTA on textual EAE by +7% F1, and
performs generally better than the second-best systems for multimedia EAE.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.12420v2},
File          = {2406.12420v2.pdf}
}
@article{2406.10964v3,
Author        = {Jiaoyan Chen and Olga Mashkova and Fernando Zhapa-Camacho and Robert Hoehndorf and Yuan He and Ian Horrocks},
Title         = {Ontology Embedding: A Survey of Methods, Applications and Resources},
Eprint        = {2406.10964v3},
DOI           = {10.1109/TKDE.2025.3559023},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontologies are widely used for representing domain knowledge and meta data,
playing an increasingly important role in Information Systems, the Semantic
Web, Bioinformatics and many other domains. However, logical reasoning that
ontologies can directly support are quite limited in learning, approximation
and prediction. One straightforward solution is to integrate statistical
analysis and machine learning. To this end, automatically learning vector
representation for knowledge of an ontology i.e., ontology embedding has been
widely investigated. Numerous papers have been published on ontology embedding,
but a lack of systematic reviews hinders researchers from gaining a
comprehensive understanding of this field. To bridge this gap, we write this
survey paper, which first introduces different kinds of semantics of ontologies
and formally defines ontology embedding as well as its property of
faithfulness. Based on this, it systematically categorizes and analyses a
relatively complete set of over 80 papers, according to the ontologies they aim
at and their technical solutions including geometric modeling, sequence
modeling and graph propagation. This survey also introduces the applications of
ontology embedding in ontology engineering, machine learning augmentation and
life sciences, presents a new library mOWL and discusses the challenges and
future directions.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.10964v3},
File          = {2406.10964v3.pdf}
}
@article{2406.09923v2,
Author        = {Mingyu Derek Ma and Chenchen Ye and Yu Yan and Xiaoxuan Wang and Peipei Ping and Timothy S Chang and Wei Wang},
Title         = {CliBench: A Multifaceted and Multigranular Evaluation of Large Language
  Models for Clinical Decision Making},
Eprint        = {2406.09923v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The integration of Artificial Intelligence (AI), especially Large Language
Models (LLMs), into the clinical diagnosis process offers significant potential
to improve the efficiency and accessibility of medical care. While LLMs have
shown some promise in the medical domain, their application in clinical
diagnosis remains underexplored, especially in real-world clinical practice,
where highly sophisticated, patient-specific decisions need to be made. Current
evaluations of LLMs in this field are often narrow in scope, focusing on
specific diseases or specialties and employing simplified diagnostic tasks. To
bridge this gap, we introduce CliBench, a novel benchmark developed from the
MIMIC IV dataset, offering a comprehensive and realistic assessment of LLMs'
capabilities in clinical diagnosis. This benchmark not only covers diagnoses
from a diverse range of medical cases across various specialties but also
incorporates tasks of clinical significance: treatment procedure
identification, lab test ordering and medication prescriptions. Supported by
structured output ontologies, CliBench enables a precise and multi-granular
evaluation, offering an in-depth understanding of LLM's capability on diverse
clinical tasks of desired granularity. We conduct a zero-shot evaluation of
leading LLMs to assess their proficiency in clinical decision-making. Our
preliminary results shed light on the potential and limitations of current LLMs
in clinical settings, providing valuable insights for future advancements in
LLM-powered healthcare.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.09923v2},
File          = {2406.09923v2.pdf}
}
@article{2406.09320v2,
Author        = {Nimol Thuon},
Title         = {Khmer Semantic Search Engine (KSE): Digital Information Access and
  Document Retrieval},
Eprint        = {2406.09320v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {The search engine process is crucial for document content retrieval. For
Khmer documents, an effective tool is needed to extract essential keywords and
facilitate accurate searches. Despite the daily generation of significant Khmer
content, Cambodians struggle to find necessary documents due to the lack of an
effective semantic searching tool. Even Google does not deliver high accuracy
for Khmer content. Semantic search engines improve search results by employing
advanced algorithms to understand various content types. With the rise in Khmer
digital content such as reports, articles, and social media feedback enhanced
search capabilities are essential. This research proposes the first Khmer
Semantic Search Engine (KSE), designed to enhance traditional Khmer search
methods. Utilizing semantic matching techniques and formally annotated semantic
content, our tool extracts meaningful keywords from user queries, performs
precise matching, and provides the best matching offline documents and online
URLs. We propose three semantic search frameworks: semantic search based on a
keyword dictionary, semantic search based on ontology, and semantic search
based on ranking. Additionally, we developed tools for data preparation,
including document addition and manual keyword extraction. To evaluate
performance, we created a ground truth dataset and addressed issues related to
searching and semantic search. Our findings demonstrate that understanding
search term semantics can lead to significantly more accurate results.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.09320v2},
File          = {2406.09320v2.pdf}
}
@article{2407.00053v1,
Author        = {Habtom Kahsay Gidey and Mario Kesseler and Patrick Stangl and Peter Hillmann and Andreas Karcher},
Title         = {A Document-based Knowledge Discovery with Microservices Architecture},
Eprint        = {2407.00053v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DC},
Abstract      = {The first step towards digitalization within organizations lies in
digitization - the conversion of analog data into digitally stored data. This
basic step is the prerequisite for all following activities like the
digitalization of processes or the servitization of products or offerings.
However, digitization itself often leads to 'data-rich' but 'knowledge-poor'
material. Knowledge discovery and knowledge extraction as approaches try to
increase the usefulness of digitized data. In this paper, we point out the key
challenges in the context of knowledge discovery and present an approach to
addressing these using a microservices architecture. Our solution led to a
conceptual design focusing on keyword extraction, similarity calculation of
documents, database queries in natural language, and programming language
independent provision of the extracted information. In addition, the conceptual
design provides referential design guidelines for integrating processes and
applications for semi-automatic learning, editing, and visualization of
ontologies. The concept also uses a microservices architecture to address
non-functional requirements, such as scalability and resilience. The evaluation
of the specified requirements is performed using a demonstrator that implements
the concept. Furthermore, this modern approach is used in the German patent
office in an extended version.},
Year          = {2024},
Month         = {Jun},
Note          = {International Conference on Intelligent Systems and Pattern
  Recognition 2022},
Url           = {http://arxiv.org/abs/2407.00053v1},
File          = {2407.00053v1.pdf}
}
@article{2406.08223v2,
Author        = {Hanieh Khorashadizadeh and Fatima Zahra Amara and Morteza Ezzabady and FrÃ©dÃ©ric Ieng and Sanju Tiwari and Nandana Mihindukulasooriya and Jinghua Groppe and Soror Sahri and Farah Benamara and Sven Groppe},
Title         = {Research Trends for the Interplay between Large Language Models and
  Knowledge Graphs},
Eprint        = {2406.08223v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This survey investigates the synergistic relationship between Large Language
Models (LLMs) and Knowledge Graphs (KGs), which is crucial for advancing AI's
capabilities in understanding, reasoning, and language processing. It aims to
address gaps in current research by exploring areas such as KG Question
Answering, ontology generation, KG validation, and the enhancement of KG
accuracy and consistency through LLMs. The paper further examines the roles of
LLMs in generating descriptive texts and natural language queries for KGs.
Through a structured analysis that includes categorizing LLM-KG interactions,
examining methodologies, and investigating collaborative uses and potential
biases, this study seeks to provide new insights into the combined potential of
LLMs and KGs. It highlights the importance of their interaction for improving
AI applications and outlines future research directions.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.08223v2},
File          = {2406.08223v2.pdf}
}
@article{2406.07962v2,
Author        = {Luis Miguel Vieira da Silva and Aljosha KÃ¶cher and Felix Gehlhoff and Alexander Fay},
Title         = {Toward a Method to Generate Capability Ontologies from Natural Language
  Descriptions},
Eprint        = {2406.07962v2},
DOI           = {10.1109/ETFA61755.2024.10710783},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {To achieve a flexible and adaptable system, capability ontologies are
increasingly leveraged to describe functions in a machine-interpretable way.
However, modeling such complex ontological descriptions is still a manual and
error-prone task that requires a significant amount of effort and ontology
expertise. This contribution presents an innovative method to automate
capability ontology modeling using Large Language Models (LLMs), which have
proven to be well suited for such tasks. Our approach requires only a natural
language description of a capability, which is then automatically inserted into
a predefined prompt using a few-shot prompting technique. After prompting an
LLM, the resulting capability ontology is automatically verified through
various steps in a loop with the LLM to check the overall correctness of the
capability ontology. First, a syntax check is performed, then a check for
contradictions, and finally a check for hallucinations and missing ontology
elements. Our method greatly reduces manual effort, as only the initial natural
language description and a final human review and possible correction are
necessary, thereby streamlining the capability ontology generation process.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.07962v2},
File          = {2406.07962v2.pdf}
}
@article{2406.07229v1,
Author        = {JinKyu Lee and Jihie Kim},
Title         = {Improving Commonsense Bias Classification by Mitigating the Influence of
  Demographic Terms},
Eprint        = {2406.07229v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Understanding commonsense knowledge is crucial in the field of Natural
Language Processing (NLP). However, the presence of demographic terms in
commonsense knowledge poses a potential risk of compromising the performance of
NLP models. This study aims to investigate and propose methods for enhancing
the performance and effectiveness of a commonsense polarization classifier by
mitigating the influence of demographic terms. Three methods are introduced in
this paper: (1) hierarchical generalization of demographic terms (2)
threshold-based augmentation and (3) integration of hierarchical generalization
and threshold-based augmentation methods (IHTA). The first method involves
replacing demographic terms with more general ones based on a term hierarchy
ontology, aiming to mitigate the influence of specific terms. To address the
limited bias-related information, the second method measures the polarization
of demographic terms by comparing the changes in the model's predictions when
these terms are masked versus unmasked. This method augments commonsense
sentences containing terms with high polarization values by replacing their
predicates with synonyms generated by ChatGPT. The third method combines the
two approaches, starting with threshold-based augmentation followed by
hierarchical generalization. The experiments show that the first method
increases the accuracy over the baseline by 2.33%, and the second one by 0.96%
over standard augmentation methods. The IHTA techniques yielded an 8.82% and
9.96% higher accuracy than threshold-based and standard augmentation methods,
respectively.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.07229v1},
File          = {2406.07229v1.pdf}
}
@article{2406.06469v1,
Author        = {Joongwon Kim and Bhargavi Paranjape and Tushar Khot and Hannaneh Hajishirzi},
Title         = {Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning},
Eprint        = {2406.06469v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Language agents perform complex tasks by using tools to execute each step
precisely. However, most existing agents are based on proprietary models or
designed to target specific tasks, such as mathematics or multi-hop question
answering. We introduce Husky, a holistic, open-source language agent that
learns to reason over a unified action space to address a diverse set of
complex tasks involving numerical, tabular, and knowledge-based reasoning.
Husky iterates between two stages: 1) generating the next action to take
towards solving a given task and 2) executing the action using expert models
and updating the current solution state. We identify a thorough ontology of
actions for addressing complex tasks and curate high-quality data to train
expert models for executing these actions. Our experiments show that Husky
outperforms prior language agents across 14 evaluation datasets. Moreover, we
introduce HuskyQA, a new evaluation set which stress tests language agents for
mixed-tool reasoning, with a focus on retrieving missing knowledge and
performing numerical reasoning. Despite using 7B models, Husky matches or even
exceeds frontier LMs such as GPT-4 on these tasks, showcasing the efficacy of
our holistic approach in addressing complex reasoning problems. Our code and
models are available at https://github.com/agent-husky/Husky-v1.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.06469v1},
File          = {2406.06469v1.pdf}
}
@article{2406.07583v1,
Author        = {Delfina Sol Martinez Pandiani and Valentina Presutti},
Title         = {Situated Ground Truths: Enhancing Bias-Aware AI by Situating Data Labels
  with SituAnnotate},
Eprint        = {2406.07583v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In the contemporary world of AI and data-driven applications, supervised
machines often derive their understanding, which they mimic and reproduce,
through annotations--typically conveyed in the form of words or labels.
However, such annotations are often divorced from or lack contextual
information, and as such hold the potential to inadvertently introduce biases
when subsequently used for training. This paper introduces SituAnnotate, a
novel ontology explicitly crafted for 'situated grounding,' aiming to anchor
the ground truth data employed in training AI systems within the contextual and
culturally-bound situations from which those ground truths emerge. SituAnnotate
offers an ontology-based approach to structured and context-aware data
annotation, addressing potential bias issues associated with isolated
annotations. Its representational power encompasses situational context,
including annotator details, timing, location, remuneration schemes, annotation
roles, and more, ensuring semantic richness. Aligned with the foundational
Dolce Ultralight ontology, it provides a robust and consistent framework for
knowledge representation. As a method to create, query, and compare label-based
datasets, SituAnnotate empowers downstream AI systems to undergo training with
explicit consideration of context and cultural bias, laying the groundwork for
enhanced system interpretability and adaptability, and enabling AI models to
align with a multitude of cultural contexts and viewpoints.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.07583v1},
File          = {2406.07583v1.pdf}
}
@article{2407.02500v1,
Author        = {Osama F. Zaki},
Title         = {Coupling Machine Learning with Ontology for Robotics Applications},
Eprint        = {2407.02500v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {In this paper I present a practical approach for coupling machine learning
(ML) algorithms with knowledge bases (KB) ontology formalism. The lack of
availability of prior knowledge in dynamic scenarios is without doubt a major
barrier for scalable machine intelligence. My view of the interaction between
the two tiers intelligence is based on the idea that when knowledge is not
readily available at the knowledge base tier, more knowledge can be extracted
from the other tier, which has access to trained models from machine learning
algorithms. To analyse this hypothesis, I create two experiments based on
different datasets, which are related directly to risk-awareness of autonomous
systems, analysed by different machine learning algorithms (namely; multi-layer
feedforward backpropagation, Naive Bayes, and J48 decision tree). My analysis
shows that the two-tiers intelligence approach for coupling ML and KB is
computationally valid and the time complexity of the algorithms during the
robot mission is linear with the size of the data and knowledge.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2407.02500v1},
File          = {2407.02500v1.pdf}
}
@article{2406.06610v1,
Author        = {Walid S. Saba},
Title         = {Reinterpreting 'the Company a Word Keeps': Towards Explainable and
  Ontologically Grounded Language Models},
Eprint        = {2406.06610v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We argue that the relative success of large language models (LLMs) is not a
reflection on the symbolic vs. subsymbolic debate but a reflection on employing
a successful bottom-up strategy of a reverse engineering of language at scale.
However, and due to their subsymbolic nature whatever knowledge these systems
acquire about language will always be buried in millions of weights none of
which is meaningful on its own, rendering such systems utterly unexplainable.
Furthermore, and due to their stochastic nature, LLMs will often fail in making
the correct inferences in various linguistic contexts that require reasoning in
intensional, temporal, or modal contexts. To remedy these shortcomings we
suggest employing the same successful bottom-up strategy employed in LLMs but
in a symbolic setting, resulting in explainable, language-agnostic, and
ontologically grounded language models.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.06610v1},
File          = {2406.06610v1.pdf}
}
@article{2406.06608v6,
Author        = {Sander Schulhoff and Michael Ilie and Nishant Balepur and Konstantine Kahadze and Amanda Liu and Chenglei Si and Yinheng Li and Aayush Gupta and HyoJung Han and Sevien Schulhoff and Pranav Sandeep Dulepet and Saurav Vidyadhara and Dayeon Ki and Sweta Agrawal and Chau Pham and Gerson Kroiz and Feileen Li and Hudson Tao and Ashay Srivastava and Hevander Da Costa and Saloni Gupta and Megan L. Rogers and Inna Goncearenco and Giuseppe Sarli and Igor Galynker and Denis Peskoff and Marine Carpuat and Jules White and Shyamal Anadkat and Alexander Hoyle and Philip Resnik},
Title         = {The Prompt Report: A Systematic Survey of Prompt Engineering Techniques},
Eprint        = {2406.06608v6},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Generative Artificial Intelligence (GenAI) systems are increasingly being
deployed across diverse industries and research domains. Developers and
end-users interact with these systems through the use of prompting and prompt
engineering. Although prompt engineering is a widely adopted and extensively
researched area, it suffers from conflicting terminology and a fragmented
ontological understanding of what constitutes an effective prompt due to its
relatively recent emergence. We establish a structured understanding of prompt
engineering by assembling a taxonomy of prompting techniques and analyzing
their applications. We present a detailed vocabulary of 33 vocabulary terms, a
taxonomy of 58 LLM prompting techniques, and 40 techniques for other
modalities. Additionally, we provide best practices and guidelines for prompt
engineering, including advice for prompting state-of-the-art (SOTA) LLMs such
as ChatGPT. We further present a meta-analysis of the entire literature on
natural language prefix-prompting. As a culmination of these efforts, this
paper presents the most comprehensive survey on prompt engineering to date.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.06608v6},
File          = {2406.06608v6.pdf}
}
@article{2406.03062v1,
Author        = {Jinge Wu and Abul Hasan and Honghan Wu},
Title         = {RadBARTsum: Domain Specific Adaption of Denoising Sequence-to-Sequence
  Models for Abstractive Radiology Report Summarization},
Eprint        = {2406.03062v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Radiology report summarization is a crucial task that can help doctors
quickly identify clinically significant findings without the need to review
detailed sections of reports. This study proposes RadBARTsum, a domain-specific
and ontology facilitated adaptation of the BART model for abstractive radiology
report summarization. The approach involves two main steps: 1) re-training the
BART model on a large corpus of radiology reports using a novel entity masking
strategy to improving biomedical domain knowledge learning, and 2) fine-tuning
the model for the summarization task using the Findings and Background sections
to predict the Impression section. Experiments are conducted using different
masking strategies. Results show that the re-training process with domain
knowledge facilitated masking improves performances consistently across various
settings. This work contributes a domain-specific generative language model for
radiology report summarization and a method for utilising medical knowledge to
realise entity masking language model. The proposed approach demonstrates a
promising direction of enhancing the efficiency of language models by deepening
its understanding of clinical knowledge in radiology reports.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.03062v1},
File          = {2406.03062v1.pdf}
}
@article{2406.00537v1,
Author        = {Lucas Valadares Vieira and Mara Abel and Fabricio Henrique Rodrigues and Tiago Prince Sales and Claudenir M. Fonseca},
Title         = {Towards an ontology of portions of matter to support multi-scale
  analysis and provenance tracking},
Eprint        = {2406.00537v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper presents an ontology of portions of matter with practical
implications across scientific and industrial domains. The ontology is
developed under the Unified Foundational Ontology (UFO), which uses the concept
of quantity to represent topologically maximally self-connected portions of
matter. The proposed ontology introduces the granuleOf parthood relation,
holding between objects and portions of matter. It also discusses the
constitution of quantities by collections of granules, the representation of
sub-portions of matter, and the tracking of matter provenance between
quantities using historical relations. Lastly, a case study is presented to
demonstrate the use of the portion of matter ontology in the geology domain for
an Oil & Gas industry application. In the case study, we model how to represent
the historical relation between an original portion of rock and the
sub-portions created during the industrial process. Lastly, future research
directions are outlined, including investigating granularity levels and
defining a taxonomy of events.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.00537v1},
File          = {2406.00537v1.pdf}
}
@article{2405.20527v1,
Author        = {Francesco Ronzano and Jay Nanavati},
Title         = {Towards Ontology-Enhanced Representation Learning for Large Language
  Models},
Eprint        = {2405.20527v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Taking advantage of the widespread use of ontologies to organise and
harmonize knowledge across several distinct domains, this paper proposes a
novel approach to improve an embedding-Large Language Model (embedding-LLM) of
interest by infusing the knowledge formalized by a reference ontology:
ontological knowledge infusion aims at boosting the ability of the considered
LLM to effectively model the knowledge domain described by the infused
ontology. The linguistic information (i.e. concept synonyms and descriptions)
and structural information (i.e. is-a relations) formalized by the ontology are
utilized to compile a comprehensive set of concept definitions, with the
assistance of a powerful generative LLM (i.e. GPT-3.5-turbo). These concept
definitions are then employed to fine-tune the target embedding-LLM using a
contrastive learning framework. To demonstrate and evaluate the proposed
approach, we utilize the biomedical disease ontology MONDO. The results show
that embedding-LLMs enhanced by ontological disease knowledge exhibit an
improved capability to effectively evaluate the similarity of in-domain
sentences from biomedical documents mentioning diseases, without compromising
their out-of-domain performance.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.20527v1},
File          = {2405.20527v1.pdf}
}
@article{2405.20526v1,
Author        = {Steven Moore and Robin Schmucker and Tom Mitchell and John Stamper},
Title         = {Automated Generation and Tagging of Knowledge Components from
  Multiple-Choice Questions},
Eprint        = {2405.20526v1},
DOI           = {10.1145/3657604.3662030},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge Components (KCs) linked to assessments enhance the measurement of
student learning, enrich analytics, and facilitate adaptivity. However,
generating and linking KCs to assessment items requires significant effort and
domain-specific knowledge. To streamline this process for higher-education
courses, we employed GPT-4 to generate KCs for multiple-choice questions (MCQs)
in Chemistry and E-Learning. We analyzed discrepancies between the KCs
generated by the Large Language Model (LLM) and those made by humans through
evaluation from three domain experts in each subject area. This evaluation
aimed to determine whether, in instances of non-matching KCs, evaluators showed
a preference for the LLM-generated KCs over their human-created counterparts.
We also developed an ontology induction algorithm to cluster questions that
assess similar KCs based on their content. Our most effective LLM strategy
accurately matched KCs for 56% of Chemistry and 35% of E-Learning MCQs, with
even higher success when considering the top five KC suggestions. Human
evaluators favored LLM-generated KCs, choosing them over human-assigned ones
approximately two-thirds of the time, a preference that was statistically
significant across both domains. Our clustering algorithm successfully grouped
questions by their underlying KCs without needing explicit labels or contextual
information. This research advances the automation of KC generation and
classification for assessment items, alleviating the need for student data or
predefined KC labels.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.20526v1},
File          = {2405.20526v1.pdf}
}
@article{2405.20163v1,
Author        = {Rosario Uceda-Sosa and Karthikeyan Natesan Ramamurthy and Maria Chang and Moninder Singh},
Title         = {Reasoning about concepts with LLMs: Inconsistencies abound},
Eprint        = {2405.20163v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The ability to summarize and organize knowledge into abstract concepts is key
to learning and reasoning. Many industrial applications rely on the consistent
and systematic use of concepts, especially when dealing with decision-critical
knowledge. However, we demonstrate that, when methodically questioned, large
language models (LLMs) often display and demonstrate significant
inconsistencies in their knowledge. Computationally, the basic aspects of the
conceptualization of a given domain can be represented as Is-A hierarchies in a
knowledge graph (KG) or ontology, together with a few properties or axioms that
enable straightforward reasoning. We show that even simple ontologies can be
used to reveal conceptual inconsistencies across several LLMs. We also propose
strategies that domain experts can use to evaluate and improve the coverage of
key domain concepts in LLMs of various sizes. In particular, we have been able
to significantly enhance the performance of LLMs of various sizes with openly
available weights using simple knowledge-graph (KG) based prompting strategies.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.20163v1},
File          = {2405.20163v1.pdf}
}
@article{2405.19877v1,
Author        = {Arto Bendiken},
Title         = {KNOW: A Real-World Ontology for Knowledge Capture with Large Language
  Models},
Eprint        = {2405.19877v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We present KNOW--the Knowledge Navigator Ontology for the World--the first
ontology designed to capture everyday knowledge to augment large language
models (LLMs) in real-world generative AI use cases such as personal AI
assistants. Our domain is human life, both its everyday concerns and its major
milestones. We have limited the initial scope of the modeled concepts to only
established human universals: spacetime (places, events) plus social (people,
groups, organizations). The inclusion criteria for modeled concepts are
pragmatic, beginning with universality and utility. We compare and contrast
previous work such as Schema.org and Cyc--as well as attempts at a synthesis of
knowledge graphs and language models--noting how LLMs already encode internally
much of the commonsense tacit knowledge that took decades to capture in the Cyc
project. We also make available code-generated software libraries for the 12
most popular programming languages, enabling the direct use of ontology
concepts in software engineering. We emphasize simplicity and developer
experience in promoting AI interoperability.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.19877v1},
File          = {2405.19877v1.pdf}
}
@article{2405.19575v1,
Author        = {Umar Ibrahim and Abubakar Yakubu Zandam and Fatima Muhammad Adam and Aminu Musa},
Title         = {A Deep Convolutional Neural Network-based Model for Aspect and Polarity
  Classification in Hausa Movie Reviews},
Eprint        = {2405.19575v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Aspect-based Sentiment Analysis (ABSA) is crucial for understanding sentiment
nuances in text, especially across diverse languages and cultures. This paper
introduces a novel Deep Convolutional Neural Network (CNN)-based model tailored
for aspect and polarity classification in Hausa movie reviews, an
underrepresented language in sentiment analysis research. A comprehensive Hausa
ABSA dataset is created, filling a significant gap in resource availability.
The dataset, preprocessed using sci-kit-learn for TF-IDF transformation,
includes manually annotated aspect-level feature ontology words and sentiment
polarity assignments. The proposed model combines CNNs with attention
mechanisms for aspect-word prediction, leveraging contextual information and
sentiment polarities. With 91% accuracy on aspect term extraction and 92% on
sentiment polarity classification, the model outperforms traditional machine
models, offering insights into specific aspects and sentiments. This study
advances ABSA research, particularly in underrepresented languages, with
implications for cross-cultural linguistic research.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.19575v1},
File          = {2405.19575v1.pdf}
}
@article{2405.19255v3,
Author        = {Jose Tupayachi and Haowen Xu and Olufemi A. Omitaomu and Mustafa Can Camur and Aliza Sharmin and Xueping Li},
Title         = {Towards Next-Generation Urban Decision Support Systems through
  AI-Powered Construction of Scientific Ontology using Large Language Models --
  A Case in Optimizing Intermodal Freight Transportation},
Eprint        = {2405.19255v3},
DOI           = {10.3390/smartcities7050094},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The incorporation of Artificial Intelligence (AI) models into various
optimization systems is on the rise. Yet, addressing complex urban and
environmental management problems normally requires in-depth domain science and
informatics expertise. This expertise is essential for deriving data and
simulation-driven for informed decision support. In this context, we
investigate the potential of leveraging the pre-trained Large Language Models
(LLMs). By adopting ChatGPT API as the reasoning core, we outline an integrated
workflow that encompasses natural language processing, methontology-based
prompt tuning, and transformers. This workflow automates the creation of
scenario-based ontology using existing research articles and technical manuals
of urban datasets and simulations. The outcomes of our methodology are
knowledge graphs in widely adopted ontology languages (e.g., OWL, RDF, SPARQL).
These facilitate the development of urban decision support systems by enhancing
the data and metadata modeling, the integration of complex datasets, the
coupling of multi-domain simulation models, and the formulation of
decision-making metrics and workflow. The feasibility of our methodology is
evaluated through a comparative analysis that juxtaposes our AI-generated
ontology with the well-known Pizza Ontology employed in tutorials for popular
ontology software (e.g., prot\'eg\'e). We close with a real-world case study of
optimizing the complex urban system of multi-modal freight transportation by
generating anthologies of various domain data and simulations to support
informed decision-making.},
Year          = {2024},
Month         = {May},
Note          = {Smart Cities, 2024, 7(5), 2392-2421},
Url           = {http://arxiv.org/abs/2405.19255v3},
File          = {2405.19255v3.pdf}
}
@article{2405.18181v3,
Author        = {Bianca LÃ¶hnert and Nikolaus Augsten and Cem Okulmus and Magdalena Ortiz},
Title         = {Towards Practicable Algorithms for Rewriting Graph Queries beyond
  DL-Lite},
Eprint        = {2405.18181v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Despite the many advantages that ontology-based data access (OBDA) has
brought to a range of application domains, state-of-the-art OBDA systems still
do not support popular graph database management systems such as Neo4j.
Algorithms for query rewriting focus on languages like conjunctive queries and
their unions, which are fragments of first-order logic and were developed for
relational data. Such query languages are poorly suited for querying graph
data. Moreover, they also limit the expressiveness of the ontology languages
that admit rewritings, restricting them to those where the data complexity of
reasoning is not higher than it is in first-order logic. In this paper, we
propose a technique for rewriting a family of navigational queries for a
suitably restricted fragment of ELHI that extends DL-Lite and that is
NL-complete in data complexity. We implemented a proof-of-concept prototype
that rewrites into Cypher queries, and tested it on a real-world cognitive
neuroscience use case with promising results.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.18181v3},
File          = {2405.18181v3.pdf}
}
@article{2405.18095v2,
Author        = {David W. Hogg and Soledad Villar},
Title         = {Is machine learning good or bad for the natural sciences?},
Eprint        = {2405.18095v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {stat.ML},
Abstract      = {Machine learning (ML) methods are having a huge impact across all of the
sciences. However, ML has a strong ontology - in which only the data exist -
and a strong epistemology - in which a model is considered good if it performs
well on held-out training data. These philosophies are in strong conflict with
both standard practices and key philosophies in the natural sciences. Here we
identify some locations for ML in the natural sciences at which the ontology
and epistemology are valuable. For example, when an expressive machine learning
model is used in a causal inference to represent the effects of confounders,
such as foregrounds, backgrounds, or instrument calibration parameters, the
model capacity and loose philosophy of ML can make the results more
trustworthy. We also show that there are contexts in which the introduction of
ML introduces strong, unwanted statistical biases. For one, when ML models are
used to emulate physical (or first-principles) simulations, they amplify
confirmation biases. For another, when expressive regressions are used to label
datasets, those labels cannot be used in downstream joint or ensemble analyses
without taking on uncontrolled biases. The question in the title is being asked
of all of the natural sciences; that is, we are calling on the scientific
communities to take a step back and consider the role and value of ML in their
fields; the (partial) answers we give here come from the particular perspective
of physics.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.18095v2},
File          = {2405.18095v2.pdf}
}
@article{2405.17691v1,
Author        = {Saeedeh Ghanadbashi and Fatemeh Golpayegani},
Title         = {Ontology-Enhanced Decision-Making for Autonomous Agents in Dynamic and
  Partially Observable Environments},
Eprint        = {2405.17691v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Agents, whether software or hardware, perceive their environment through
sensors and act using actuators, often operating in dynamic, partially
observable settings. They face challenges like incomplete and noisy data,
unforeseen situations, and the need to adapt goals in real-time. Traditional
reasoning and ML methods, including Reinforcement Learning (RL), help but are
limited by data needs, predefined goals, and extensive exploration periods.
Ontologies offer a solution by integrating diverse information sources,
enhancing decision-making in complex environments. This thesis introduces an
ontology-enhanced decision-making model (OntoDeM) for autonomous agents.
OntoDeM enriches agents' domain knowledge, allowing them to interpret
unforeseen events, generate or adapt goals, and make better decisions. Key
contributions include: 1. An ontology-based method to improve agents' real-time
observations using prior knowledge. 2. The OntoDeM model for handling dynamic,
unforeseen situations by evolving or generating new goals. 3. Implementation
and evaluation in four real-world applications, demonstrating its
effectiveness. Compared to traditional and advanced learning algorithms,
OntoDeM shows superior performance in improving agents' observations and
decision-making in dynamic, partially observable environments.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.17691v1},
File          = {2405.17691v1.pdf}
}
@article{2405.16115v1,
Author        = {Mikhail Kulyabin and Gleb Sokolov and Aleksandr Galaida and Andreas Maier and Tomas Arias-Vergara},
Title         = {SNOBERT: A Benchmark for clinical notes entity linking in the SNOMED CT
  clinical terminology},
Eprint        = {2405.16115v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The extraction and analysis of insights from medical data, primarily stored
in free-text formats by healthcare workers, presents significant challenges due
to its unstructured nature. Medical coding, a crucial process in healthcare,
remains minimally automated due to the complexity of medical ontologies and
restricted access to medical texts for training Natural Language Processing
models. In this paper, we proposed a method, "SNOBERT," of linking text spans
in clinical notes to specific concepts in the SNOMED CT using BERT-based
models. The method consists of two stages: candidate selection and candidate
matching. The models were trained on one of the largest publicly available
dataset of labeled clinical notes. SNOBERT outperforms other classical methods
based on deep learning, as confirmed by the results of a challenge in which it
was applied.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.16115v1},
File          = {2405.16115v1.pdf}
}
@article{2405.15134v2,
Author        = {Akshit Achara and Sanand Sasidharan and Gagan N},
Title         = {Efficient Biomedical Entity Linking: Clinical Text Standardization with
  Low-Resource Techniques},
Eprint        = {2405.15134v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Clinical text is rich in information, with mentions of treatment, medication
and anatomy among many other clinical terms. Multiple terms can refer to the
same core concepts which can be referred as a clinical entity. Ontologies like
the Unified Medical Language System (UMLS) are developed and maintained to
store millions of clinical entities including the definitions, relations and
other corresponding information. These ontologies are used for standardization
of clinical text by normalizing varying surface forms of a clinical term
through Biomedical entity linking. With the introduction of transformer-based
language models, there has been significant progress in Biomedical entity
linking. In this work, we focus on learning through synonym pairs associated
with the entities. As compared to the existing approaches, our approach
significantly reduces the training data and resource consumption. Moreover, we
propose a suite of context-based and context-less reranking techniques for
performing the entity disambiguation. Overall, we achieve similar performance
to the state-of-the-art zero-shot and distant supervised entity linking
techniques on the Medmentions dataset, the largest annotated dataset on UMLS,
without any domain-based training. Finally, we show that retrieval performance
alone might not be sufficient as an evaluation metric and introduce an article
level quantitative and qualitative analysis to reveal further insights on the
performance of entity linking methods.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.15134v2},
File          = {2405.15134v2.pdf}
}
@article{2405.14067v1,
Author        = {Eduardo da C. Ramos and Maria Luiza M. Campos and Fernanda BaiÃ£o},
Title         = {ABI Approach: Automatic Bias Identification in Decision-Making Under
  Risk based in an Ontology of Behavioral Economics},
Eprint        = {2405.14067v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {Organizational decision-making is crucial for success, yet cognitive biases
can significantly affect risk preferences, leading to suboptimal outcomes. Risk
seeking preferences for losses, driven by biases such as loss aversion, pose
challenges and can result in severe negative consequences, including financial
losses. This research introduces the ABI approach, a novel solution designed to
support organizational decision-makers by automatically identifying and
explaining risk seeking preferences during decision-making. This research makes
a novel contribution by automating the identification and explanation of risk
seeking preferences using Cumulative Prospect theory (CPT) from Behavioral
Economics. The ABI approach transforms theoretical insights into actionable,
real-time guidance, making them accessible to a broader range of organizations
and decision-makers without requiring specialized personnel. By contextualizing
CPT concepts into business language, the approach facilitates widespread
adoption and enhances decision-making processes with deep behavioral insights.
Our systematic literature review identified significant gaps in existing
methods, especially the lack of automated solutions with a concrete mechanism
for automatically identifying risk seeking preferences, and the absence of
formal knowledge representation, such as ontologies, for identifying and
explaining the risk preferences. The ABI Approach addresses these gaps,
offering a significant contribution to decision-making research and practice.
Furthermore, it enables automatic collection of historical decision data with
risk preferences, providing valuable insights for enhancing strategic
management and long-term organizational performance. An experiment provided
preliminary evidence on its effectiveness in helping decision-makers recognize
their risk seeking preferences during decision-making in the loss domain.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.14067v1},
File          = {2405.14067v1.pdf}
}
@article{2405.14012v1,
Author        = {Tolga ÃÃ¶plÃ¼ and Arto Bendiken and Andrii Skomorokhov and Eduard Bateiko and Stephen Cobb},
Title         = {Prompt-Time Ontology-Driven Symbolic Knowledge Capture with Large
  Language Models},
Eprint        = {2405.14012v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In applications such as personal assistants, large language models (LLMs)
must consider the user's personal information and preferences. However, LLMs
lack the inherent ability to learn from user interactions. This paper explores
capturing personal information from user prompts using ontology and
knowledge-graph approaches. We use a subset of the KNOW ontology, which models
personal information, to train the language model on these concepts. We then
evaluate the success of knowledge capture using a specially constructed
dataset. Our code and datasets are publicly available at
https://github.com/HaltiaAI/paper-PTODSKC},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.14012v1},
File          = {2405.14012v1.pdf}
}
@article{2405.13931v1,
Author        = {Efe Y. Yarbasi and Dimitri N. Mavris},
Title         = {A Methodology to Identify Physical or Computational Experiment
  Conditions for Uncertainty Mitigation},
Eprint        = {2405.13931v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CE},
Abstract      = {Complex engineering systems require integration of simulation of sub-systems
and calculation of metrics to drive design decisions. This paper introduces a
methodology for designing computational or physical experiments for
system-level uncertainty mitigation purposes. The methodology follows a
previously determined problem ontology, where physical, functional and modeling
architectures are decided upon. By carrying out sensitivity analysis techniques
utilizing system-level tools, critical epistemic uncertainties can be
identified. Afterwards, a framework is introduced to design specific
computational and physical experimentation for generating new knowledge about
parameters, and for uncertainty mitigation. The methodology is demonstrated
through a case study on an early-stage design Blended-Wing-Body (BWB) aircraft
concept, showcasing how aerostructures analyses can be leveraged for mitigating
system-level uncertainty, by computer experiments or guiding physical
experimentation. The proposed methodology is versatile enough to tackle
uncertainty management across various design challenges, highlighting the
potential for more risk-informed design processes.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.13931v1},
File          = {2405.13931v1.pdf}
}
@article{2405.12971v3,
Author        = {Theodore Zhao and Yu Gu and Jianwei Yang and Naoto Usuyama and Ho Hin Lee and Tristan Naumann and Jianfeng Gao and Angela Crabtree and Jacob Abel and Christine Moung-Wen and Brian Piening and Carlo Bifulco and Mu Wei and Hoifung Poon and Sheng Wang},
Title         = {BiomedParse: a biomedical foundation model for image parsing of
  everything everywhere all at once},
Eprint        = {2405.12971v3},
DOI           = {10.1038/s41592-024-02499-w},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Biomedical image analysis is fundamental for biomedical discovery in cell
biology, pathology, radiology, and many other biomedical domains. Holistic
image analysis comprises interdependent subtasks such as segmentation,
detection, and recognition of relevant objects. Here, we propose BiomedParse, a
biomedical foundation model for imaging parsing that can jointly conduct
segmentation, detection, and recognition for 82 object types across 9 imaging
modalities. Through joint learning, we can improve accuracy for individual
tasks and enable novel applications such as segmenting all relevant objects in
an image through a text prompt, rather than requiring users to laboriously
specify the bounding box for each object. We leveraged readily available
natural-language labels or descriptions accompanying those datasets and use
GPT-4 to harmonize the noisy, unstructured text information with established
biomedical object ontologies. We created a large dataset comprising over six
million triples of image, segmentation mask, and textual description. On image
segmentation, we showed that BiomedParse is broadly applicable, outperforming
state-of-the-art methods on 102,855 test image-mask-label triples across 9
imaging modalities (everything). On object detection, which aims to locate a
specific object of interest, BiomedParse again attained state-of-the-art
performance, especially on objects with irregular shapes (everywhere). On
object recognition, which aims to identify all objects in a given image along
with their semantic types, we showed that BiomedParse can simultaneously
segment and label all biomedical objects in an image (all at once). In summary,
BiomedParse is an all-in-one tool for biomedical image analysis by jointly
solving segmentation, detection, and recognition for all major biomedical image
modalities, paving the path for efficient and accurate image-based biomedical
discovery.},
Year          = {2024},
Month         = {May},
Note          = {Nat Methods 22, 166-176 (2025)},
Url           = {http://arxiv.org/abs/2405.12971v3},
File          = {2405.12971v3.pdf}
}
@article{2405.11941v1,
Author        = {Fons Hartendorp and Tom Seinen and Erik van Mulligen and Suzan Verberne},
Title         = {Biomedical Entity Linking for Dutch: Fine-tuning a Self-alignment BERT
  Model on an Automatically Generated Wikipedia Corpus},
Eprint        = {2405.11941v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Biomedical entity linking, a main component in automatic information
extraction from health-related texts, plays a pivotal role in connecting
textual entities (such as diseases, drugs and body parts mentioned by patients)
to their corresponding concepts in a structured biomedical knowledge base. The
task remains challenging despite recent developments in natural language
processing. This paper presents the first evaluated biomedical entity linking
model for the Dutch language. We use MedRoBERTa.nl as base model and perform
second-phase pretraining through self-alignment on a Dutch biomedical ontology
extracted from the UMLS and Dutch SNOMED. We derive a corpus from Wikipedia of
ontology-linked Dutch biomedical entities in context and fine-tune our model on
this dataset. We evaluate our model on the Dutch portion of the Mantra
GSC-corpus and achieve 54.7% classification accuracy and 69.8% 1-distance
accuracy. We then perform a case study on a collection of unlabeled,
patient-support forum data and show that our model is hampered by the limited
quality of the preceding entity recognition step. Manual evaluation of small
sample indicates that of the correctly extracted entities, around 65% is linked
to the correct concept in the ontology. Our results indicate that biomedical
entity linking in a language other than English remains challenging, but our
Dutch model can be used to for high-level analysis of patient-generated text.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.11941v1},
File          = {2405.11941v1.pdf}
}
@article{2405.11735v1,
Author        = {Jiayu Shang and Cheng Peng and Yongxin Ji and Jiaojiao Guan and Dehan Cai and Xubo Tang and Yanni Sun},
Title         = {Accurate and efficient protein embedding using multi-teacher
  distillation learning},
Eprint        = {2405.11735v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.GN},
Abstract      = {Motivation: Protein embedding, which represents proteins as numerical
vectors, is a crucial step in various learning-based protein
annotation/classification problems, including gene ontology prediction,
protein-protein interaction prediction, and protein structure prediction.
However, existing protein embedding methods are often computationally expensive
due to their large number of parameters, which can reach millions or even
billions. The growing availability of large-scale protein datasets and the need
for efficient analysis tools have created a pressing demand for efficient
protein embedding methods.
  Results: We propose a novel protein embedding approach based on multi-teacher
distillation learning, which leverages the knowledge of multiple pre-trained
protein embedding models to learn a compact and informative representation of
proteins. Our method achieves comparable performance to state-of-the-art
methods while significantly reducing computational costs and resource
requirements. Specifically, our approach reduces computational time by ~70\%
and maintains almost the same accuracy as the original large models. This makes
our method well-suited for large-scale protein analysis and enables the
bioinformatics community to perform protein embedding tasks more efficiently.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.11735v1},
File          = {2405.11735v1.pdf}
}
@article{2405.11706v1,
Author        = {Dean Allemang and Juan Sequeda},
Title         = {Increasing the LLM Accuracy for Question Answering: Ontologies to the
  Rescue!},
Eprint        = {2405.11706v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {There is increasing evidence that question-answering (QA) systems with Large
Language Models (LLMs), which employ a knowledge graph/semantic representation
of an enterprise SQL database (i.e. Text-to-SPARQL), achieve higher accuracy
compared to systems that answer questions directly on SQL databases (i.e.
Text-to-SQL). Our previous benchmark research showed that by using a knowledge
graph, the accuracy improved from 16% to 54%. The question remains: how can we
further improve the accuracy and reduce the error rate? Building on the
observations of our previous research where the inaccurate LLM-generated SPARQL
queries followed incorrect paths, we present an approach that consists of 1)
Ontology-based Query Check (OBQC): detects errors by leveraging the ontology of
the knowledge graph to check if the LLM-generated SPARQL query matches the
semantic of ontology and 2) LLM Repair: use the error explanations with an LLM
to repair the SPARQL query. Using the chat with the data benchmark, our primary
finding is that our approach increases the overall accuracy to 72% including an
additional 8% of "I don't know" unknown results. Thus, the overall error rate
is 20%. These results provide further evidence that investing knowledge graphs,
namely the ontology, provides higher accuracy for LLM powered question
answering systems.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.11706v1},
File          = {2405.11706v1.pdf}
}
@article{2405.11581v2,
Author        = {Panos Fitsilis and Vyron Damasiotis and Vasileios Kyriatzis and Paraskevi Tsoutsa},
Title         = {DOLLmC: DevOps for Large Language model Customization},
Eprint        = {2405.11581v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {The rapid integration of Large Language Models (LLMs) into various industries
presents both revolutionary opportunities and unique challenges. This research
aims to establish a scalable and efficient framework for LLM customization,
exploring how DevOps practices should be adapted to meet the specific demands
of LLM customization. By integrating ontologies, knowledge maps, and prompt
engineering into the DevOps pipeline, we propose a robust framework that
enhances continuous learning, seamless deployment, and rigorous version control
of LLMs. This methodology is demonstrated through the development of a
domain-specific chatbot for the agricultural sector, utilizing heterogeneous
data to deliver actionable insights. The proposed methodology, so called
DOLLmC, not only addresses the immediate challenges of LLM customization but
also promotes scalability and operational efficiency. However, the
methodology's primary limitation lies in the need for extensive testing,
validation, and broader adoption across different domains.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.11581v2},
File          = {2405.11581v2.pdf}
}
@article{2405.11346v3,
Author        = {Ritesh Chandra and Shashi Shekhar Kumar and Rushil Patra and Sonali Agarwal},
Title         = {Decision support system for Forest fire management using Ontology with
  Big Data and LLMs},
Eprint        = {2405.11346v3},
DOI           = {10.1007/s10586-025-05383-0},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Forests are crucial for ecological balance, but wildfires, a major cause of
forest loss, pose significant risks. Fire weather indices, which assess
wildfire risk and predict resource demands, are vital. With the rise of sensor
networks in fields like healthcare and environmental monitoring, semantic
sensor networks are increasingly used to gather climatic data such as wind
speed, temperature, and humidity. However, processing these data streams to
determine fire weather indices presents challenges, underscoring the growing
importance of effective forest fire detection. This paper discusses using
Apache Spark for early forest fire detection, enhancing fire risk prediction
with meteorological and geographical data. Building on our previous development
of Semantic Sensor Network (SSN) ontologies and Semantic Web Rules Language
(SWRL) for managing forest fires in Monesterial Natural Park, we expanded SWRL
to improve a Decision Support System (DSS) using a Large Language Models (LLMs)
and Spark framework. We implemented real-time alerts with Spark streaming,
tailored to various fire scenarios, and validated our approach using ontology
metrics, query-based evaluations, LLMs score precision, F1 score, and recall
measures.},
Year          = {2024},
Month         = {May},
Note          = {Cluster Computing /2025},
Url           = {http://arxiv.org/abs/2405.11346v3},
File          = {2405.11346v3.pdf}
}
@article{2405.10713v1,
Author        = {A Akanbi},
Title         = {Development of Semantics-Based Distributed Middleware for Heterogeneous
  Data Integration and its Application for Drought},
Eprint        = {2405.10713v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CY},
Abstract      = {Drought is a complex environmental phenomenon that affects millions of people
and communities all over the globe and is too elusive to be accurately
predicted. This is mostly due to the scalability and variability of the web of
environmental parameters that directly/indirectly causes the onset of different
categories of drought. Since the dawn of man, efforts have been made to
uniquely understand the natural indicators that provide signs of likely
environmental events. These indicators/signs in the form of indigenous
knowledge system have been used for generations. The intricate complexity of
drought has, however, always been a major stumbling block for accurate drought
prediction and forecasting systems. Recently, scientists in the field of
agriculture and environmental monitoring have been discussing the integration
of indigenous knowledge and scientific knowledge for a more accurate
environmental forecasting system in order to incorporate diverse environmental
information for a reliable drought forecast. Hence, in this research, the core
objective is the development of a semantics-based data integration middleware
that encompasses and integrates heterogeneous data models of local indigenous
knowledge and sensor data towards an accurate drought forecasting system for
the study areas. The local indigenous knowledge on drought gathered from the
domain experts is transformed into rules to be used for performing deductive
inference in conjunction with sensors data for determining the onset of drought
through an automated inference generation module of the middleware. The
semantic middleware incorporates, inter alia, a distributed architecture that
consists of a streaming data processing engine based on Apache Kafka for
real-time stream processing; a rule-based reasoning module; an ontology module
for semantic representation of the knowledge bases.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.10713v1},
File          = {2405.10713v1.pdf}
}
@article{2405.10440v3,
Author        = {Jinge Wu and Hang Dong and Zexi Li and Haowei Wang and Runci Li and Arijit Patra and Chengliang Dai and Waqar Ali and Phil Scordis and Honghan Wu},
Title         = {A Hybrid Framework with Large Language Models for Rare Disease
  Phenotyping},
Eprint        = {2405.10440v3},
DOI           = {10.1186/s12911-024-02698-7},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Rare diseases pose significant challenges in diagnosis and treatment due to
their low prevalence and heterogeneous clinical presentations. Unstructured
clinical notes contain valuable information for identifying rare diseases, but
manual curation is time-consuming and prone to subjectivity. This study aims to
develop a hybrid approach combining dictionary-based natural language
processing (NLP) tools with large language models (LLMs) to improve rare
disease identification from unstructured clinical reports. We propose a novel
hybrid framework that integrates the Orphanet Rare Disease Ontology (ORDO) and
the Unified Medical Language System (UMLS) to create a comprehensive rare
disease vocabulary. The proposed hybrid approach demonstrates superior
performance compared to traditional NLP systems and standalone LLMs. Notably,
the approach uncovers a significant number of potential rare disease cases not
documented in structured diagnostic records, highlighting its ability to
identify previously unrecognized patients.},
Year          = {2024},
Month         = {May},
Note          = {BMC Med Inform Decis Mak 24, 289 (2024)},
Url           = {http://arxiv.org/abs/2405.10440v3},
File          = {2405.10440v3.pdf}
}
@article{2405.10235v1,
Author        = {Kourosh Malek and Max Dreger and Zirui Tang and Qingshi Tu},
Title         = {Novel Data Models for Inter-operable LCA Frameworks},
Eprint        = {2405.10235v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Life cycle assessment (LCA) plays a critical role in assessing the
environmental impacts of a product, technology, or service throughout its
entire life cycle. Nonetheless, many existing LCA tools and methods lack
adequate metadata management, which can hinder their further development and
wide adoption. In the example of LCA for clean energy technologies, metadata
helps monitor data and the environment that holds the integrity of the energy
assets and sustainability of the materials sources across their entire value
chains. Ontologizing metadata, i.e. a common vocabulary and language to connect
multiple data sources, as well as implementing AI-aware data management, can
have long-lasting, positive, and accelerating effects along with collecting and
utilizing quality data from different sources and across the entire data
lifecycle. The integration of ontologies in life cycle assessments has garnered
significant attention in recent years. We synthesized the existing literature
on ontologies for LCAs, providing insights into this interdisciplinary field's
evolution, current state, and future directions. We also proposed the framework
for a suitable data model and the workflow thereof to warrant the alignment
with existing ontologies, practical frameworks, and industry standards.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.10235v1},
File          = {2405.10235v1.pdf}
}
@article{2405.10094v2,
Author        = {Piotr Ostropolski-Nalewaja and Tim S. Lyon},
Title         = {Decidability of Quasi-Dense Modal Logics},
Eprint        = {2405.10094v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LO},
Abstract      = {The decidability of axiomatic extensions of the modal logic K with modal
reduction principles, i.e. axioms of the form $\Diamond^{k} p \rightarrow
\Diamond^{n} p$, has remained a long-standing open problem. In this paper, we
make significant progress toward solving this problem and show that
decidability holds for a large subclass of these logics, namely, for
'quasi-dense logics.' Such logics are extensions of K with with modal reduction
axioms such that $0 < k < n$ (dubbed 'quasi-density axioms'). To prove
decidability, we define novel proof systems for quasi-dense logics consisting
of disjunctive existential rules, which are first-order formulae typically used
to specify ontologies in the context of database theory. We show that such
proof systems can be used to generate proofs and models of modal formulae, and
provide an intricate model-theoretic argument showing that such generated
models can be encoded as finite objects called 'templates.' By enumerating
templates of bound size, we obtain an EXPSPACE decision procedure as a
consequence.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.10094v2},
File          = {2405.10094v2.pdf}
}
@article{2406.00008v2,
Author        = {Shinnosuke Tanaka and James Barry and Vishnudev Kuruvanthodi and Movina Moses and Maxwell J. Giammona and Nathan Herr and Mohab Elkaref and Geeth De Mel},
Title         = {KnowledgeHub: An end-to-end Tool for Assisted Scientific Discovery},
Eprint        = {2406.00008v2},
DOI           = {10.24963/ijcai.2024/1039},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {This paper describes the KnowledgeHub tool, a scientific literature
Information Extraction (IE) and Question Answering (QA) pipeline. This is
achieved by supporting the ingestion of PDF documents that are converted to
text and structured representations. An ontology can then be constructed where
a user defines the types of entities and relationships they want to capture. A
browser-based annotation tool enables annotating the contents of the PDF
documents according to the ontology. Named Entity Recognition (NER) and
Relation Classification (RC) models can be trained on the resulting annotations
and can be used to annotate the unannotated portion of the documents. A
knowledge graph is constructed from these entity and relation triples which can
be queried to obtain insights from the data. Furthermore, we integrate a suite
of Large Language Models (LLMs) that can be used for QA and summarisation that
is grounded in the included documents via a retrieval component. KnowledgeHub
is a unique tool that supports annotation, IE and QA, which gives the user full
insight into the knowledge discovery pipeline.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2406.00008v2},
File          = {2406.00008v2.pdf}
}
@article{2405.09875v2,
Author        = {Piotr Gorczyca and DÃ¶rthe Arndt and Martin Diller and Pascal Kettmann and Stephan Mennicke and Hannes Strass},
Title         = {A Farewell to Harms: Risk Management for Medical Devices via the Riskman
  Ontology & Shapes},
Eprint        = {2405.09875v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We introduce the Riskman ontology & shapes for representing and analysing
information about risk management for medical devices. Risk management is
concerned with taking necessary precautions so a medical device does not cause
harms for users or the environment. To date, risk management documentation is
submitted to notified bodies (for certification) in the form of semi-structured
natural language text. We propose to use classes from the Riskman ontology to
logically model risk management documentation and to use the included SHACL
constraints to check for syntactic completeness and conformity to relevant
standards. In particular, the ontology is modelled after ISO 14971 and the
recently published VDE Spec 90025. Our proposed methodology has the potential
to save many person-hours for both manufacturers (when creating risk management
documentation) as well as notified bodies (when assessing submitted
applications for certification), and thus offers considerable benefits for
healthcare and, by extension, society as a whole.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.09875v2},
File          = {2405.09875v2.pdf}
}
@article{2405.09269v1,
Author        = {Sabah Al-Fedaghi},
Title         = {Preconceptual Modeling in Software Engineering: Metaphysics of
  Diagrammatic Representations},
Eprint        = {2405.09269v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {According to many researchers, conceptual model (CM) development is a hard
task, and system requirements are difficult to collect, causing many
miscommunication problems. CMs require more than modeling ability alone - they
first require an understanding of the targeted domain that the model attempts
to represent. Accordingly, a preconceptual modeling (pre-CM) stage is intended
to address ontological issues before typical CM development is initiated. It
involves defining a portion of reality when entities and processes are
differentiated and integrated as unified wholes. This pre-CM phase forms the
focus of research in this paper. The purpose is not show how to model; rather,
it is to demonstrate how to establish a metaphysical basis of the involved
portion of reality. To demonstrate such a venture, we employ the so-called
thinging machine (TM) modeling that has been proposed as a high-level CM. A TM
model integrates staticity and dynamism grounded in a fundamental construct
called a thimac (things/machine). It involves two modes of reality, existence
(events) and subsistence (regions - roughly, specifications of things and
processes). Currently, the dominant approach in CM has evolved to limit its
scope of application to develop ontological categorization (types of things).
In the TM approach, pre-CM metaphysics is viewed as a part and parcel of CM
itself. The general research problem is how to map TM constructs to what is out
there in the targeted domain. Discussions involve the nature of thimacs (things
and processes) and subsistence and existence as they are superimposed over each
other in reality. Specifically, we make two claims, (a) the perceptibility of
regions as a phenomenon and (b) the distinctiveness of existence as a construct
for events. The results contribute to further the understanding of TM modeling
in addition to introducing some metaphysical insights.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.09269v1},
File          = {2405.09269v1.pdf}
}
@article{2405.07452v2,
Author        = {Karim Abbasi and Parvin Razzaghi and Amin Ghareyazi and Hamid R. Rabiee},
Title         = {PLA-SGCN: Protein-Ligand Binding Affinity Prediction by Integrating
  Similar Pairs and Semi-supervised Graph Convolutional Network},
Eprint        = {2405.07452v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.QM},
Abstract      = {The protein-ligand binding affinity (PLA) prediction goal is to predict
whether or not the ligand could bind to a protein sequence. Recently, in PLA
prediction, deep learning has received much attention. Two steps are involved
in deep learning-based approaches: feature extraction and task prediction step.
Many deep learning-based approaches concentrate on introducing new feature
extraction networks or integrating auxiliary knowledge like protein-protein
interaction networks or gene ontology knowledge. Then, a task prediction
network is designed simply using some fully connected layers. This paper aims
to integrate retrieved similar hard protein-ligand pairs in PLA prediction
(i.e., task prediction step) using a semi-supervised graph convolutional
network (GCN). Hard protein-ligand pairs are retrieved for each input query
sample based on the manifold smoothness constraint. Then, a graph is learned
automatically in which each node is a protein-ligand pair, and each edge
represents the similarity between pairs. In other words, an end-to-end
framework is proposed that simultaneously retrieves hard similar samples,
learns protein-ligand descriptor, learns the graph topology of the input sample
with retrieved similar hard samples (learn adjacency matrix), and learns a
semi-supervised GCN to predict the binding affinity (as task predictor). The
training step adjusts the parameter values, and in the inference step, the
learned model is fine-tuned for each input sample. To evaluate the proposed
approach, it is applied to the four well-known PDBbind, Davis, KIBA, and
BindingDB datasets. The results show that the proposed method significantly
performs better than the comparable approaches.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.07452v2},
File          = {2405.07452v2.pdf}
}
@article{2405.04868v2,
Author        = {Olga Mashkova and Fernando Zhapa-Camacho and Robert Hoehndorf},
Title         = {Enhancing Geometric Ontology Embeddings for $\mathcal{EL}^{++}$ with
  Negative Sampling and Deductive Closure Filtering},
Eprint        = {2405.04868v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontology embeddings map classes, relations, and individuals in ontologies
into $\mathbb{R}^n$, and within $\mathbb{R}^n$ similarity between entities can
be computed or new axioms inferred. For ontologies in the Description Logic
$\mathcal{EL}^{++}$, several embedding methods have been developed that
explicitly generate models of an ontology. However, these methods suffer from
some limitations; they do not distinguish between statements that are
unprovable and provably false, and therefore they may use entailed statements
as negatives. Furthermore, they do not utilize the deductive closure of an
ontology to identify statements that are inferred but not asserted. We
evaluated a set of embedding methods for $\mathcal{EL}^{++}$ ontologies based
on high-dimensional ball representation of concept descriptions, incorporating
several modifications that aim to make use of the ontology deductive closure.
In particular, we designed novel negative losses that account both for the
deductive closure and different types of negatives. We demonstrate that our
embedding methods improve over the baseline ontology embedding in the task of
knowledge base or ontology completion.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.04868v2},
File          = {2405.04868v2.pdf}
}
@article{2405.03345v1,
Author        = {Lars Vogt and Philip StrÃ¶mert and Nicolas Matentzoglu and Naouel Karam and Marcel Konrad and Manuel Prinz and Roman Baum},
Title         = {FAIR 2.0: Extending the FAIR Guiding Principles to Address Semantic
  Interoperability},
Eprint        = {2405.03345v1},
DOI           = {10.1038/s41597-025-05011-x},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {FAIR data presupposes their successful communication between machines and
humans while preserving their meaning and reference, requiring all parties
involved to share the same background knowledge. Inspired by English as a
natural language, we investigate the linguistic structure that ensures reliable
communication of information and draw parallels with data structures,
understanding both as models of systems of interest. We conceptualize semantic
interoperability as comprising terminological and propositional
interoperability. The former includes ontological (i.e., same meaning) and
referential (i.e., same referent/extension) interoperability and the latter
schema (i.e., same data schema) and logical (i.e., same logical framework)
interoperability. Since no best ontology and no best data schema exists,
establishing semantic interoperability and FAIRness of data and metadata
requires the provision of a comprehensive set of relevant ontological and
referential entity mappings and schema crosswalks. We therefore propose
appropriate additions to the FAIR Guiding Principles, leading to FAIR 2.0.
Furthermore, achieving FAIRness of data requires the provision of FAIR services
in addition to organizing data into FAIR Digital Objects. FAIR services include
a terminology, a schema, and an operations service.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.03345v1},
File          = {2405.03345v1.pdf}
}
@article{2405.02458v1,
Author        = {Gianluca Cima and Domenico Lembo and Lorenzo Marconi and Riccardo Rosati and Domenico Fabio Savo},
Title         = {Controlled Query Evaluation through Epistemic Dependencies},
Eprint        = {2405.02458v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In this paper, we propose the use of epistemic dependencies to express data
protection policies in Controlled Query Evaluation (CQE), which is a form of
confidentiality-preserving query answering over ontologies and databases. The
resulting policy language goes significantly beyond those proposed in the
literature on CQE so far, allowing for very rich and practically interesting
forms of data protection rules. We show the expressive abilities of our
framework and study the data complexity of CQE for (unions of) conjunctive
queries when ontologies are specified in the Description Logic DL-Lite_R.
Interestingly, while we show that the problem is in general intractable, we
prove tractability for the case of acyclic epistemic dependencies by providing
a suitable query rewriting algorithm. The latter result paves the way towards
the implementation and practical application of this new approach to CQE.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.02458v1},
File          = {2405.02458v1.pdf}
}
@article{2405.02083v2,
Author        = {Simon FlÃ¼gel and Martin Glauer and Till Mossakowski and Fabian Neuhaus},
Title         = {A fuzzy loss for ontology classification},
Eprint        = {2405.02083v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Deep learning models are often unaware of the inherent constraints of the
task they are applied to. However, many downstream tasks require logical
consistency. For ontology classification tasks, such constraints include
subsumption and disjointness relations between classes.
  In order to increase the consistency of deep learning models, we propose a
fuzzy loss that combines label-based loss with terms penalising subsumption- or
disjointness-violations. Our evaluation on the ChEBI ontology shows that the
fuzzy loss is able to decrease the number of consistency violations by several
orders of magnitude without decreasing the classification performance. In
addition, we use the fuzzy loss for unsupervised learning. We show that this
can further improve consistency on data from a},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.02083v2},
File          = {2405.02083v2.pdf}
}
@article{2405.00186v1,
Author        = {John Beverley and Robin McGill and Sam Smith and Jie Zheng and Giacomo De Colle and Finn Wilson and Matthew Diller and William D. Duncan and William R. Hogan and Yongqun He},
Title         = {Credentials in the Occupation Ontology},
Eprint        = {2405.00186v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The term credential encompasses educational certificates, degrees,
certifications, and government-issued licenses. An occupational credential is a
verification of an individuals qualification or competence issued by a third
party with relevant authority. Job seekers often leverage such credentials as
evidence that desired qualifications are satisfied by their holders. Many U.S.
education and workforce development organizations have recognized the
importance of credentials for employment and the challenges of understanding
the value of credentials. In this study, we identified and ontologically
defined credential and credential-related terms at the textual and semantic
levels based on the Occupation Ontology (OccO), a BFO-based ontology. Different
credential types and their authorization logic are modeled. We additionally
defined a high-level hierarchy of credential related terms and relations among
many terms, which were initiated in concert with the Alabama Talent Triad (ATT)
program, which aims to connect learners, earners, employers and
education/training providers through credentials and skills. To our knowledge,
our research provides for the first time systematic ontological modeling of the
important domain of credentials and related contents, supporting enhanced
credential data and knowledge integration in the future.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2405.00186v1},
File          = {2405.00186v1.pdf}
}
@article{2407.01553v2,
Author        = {JingHong Li and Huy Phan and Wen Gu and Koichi Ota and Shinobu Hasegawa},
Title         = {Fish-bone diagram of research issue: Gain a bird's-eye view on a
  specific research topic},
Eprint        = {2407.01553v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Novice researchers often face difficulties in understanding a multitude of
academic papers and grasping the fundamentals of a new research field. To solve
such problems, the knowledge graph supporting research survey is gradually
being developed. Existing keyword-based knowledge graphs make it difficult for
researchers to deeply understand abstract concepts. Meanwhile, novice
researchers may find it difficult to use ChatGPT effectively for research
surveys due to their limited understanding of the research field. Without the
ability to ask proficient questions that align with key concepts, obtaining
desired and accurate answers from this large language model (LLM) could be
inefficient. This study aims to help novice researchers by providing a
fish-bone diagram that includes causal relationships, offering an overview of
the research topic. The diagram is constructed using the issue ontology from
academic papers, and it offers a broad, highly generalized perspective of the
research field, based on relevance and logical factors. Furthermore, we
evaluate the strengths and improvable points of the fish-bone diagram derived
from this study's development pattern, emphasizing its potential as a viable
tool for supporting research survey.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2407.01553v2},
File          = {2407.01553v2.pdf}
}
@article{2404.19146v1,
Author        = {Linyi Ding and Sizhe Zhou and Jinfeng Xiao and Jiawei Han},
Title         = {Automated Construction of Theme-specific Knowledge Graphs},
Eprint        = {2404.19146v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Despite widespread applications of knowledge graphs (KGs) in various tasks
such as question answering and intelligent conversational systems, existing KGs
face two major challenges: information granularity and deficiency in
timeliness. These hinder considerably the retrieval and analysis of in-context,
fine-grained, and up-to-date knowledge from KGs, particularly in highly
specialized themes (e.g., specialized scientific research) and rapidly evolving
contexts (e.g., breaking news or disaster tracking). To tackle such challenges,
we propose a theme-specific knowledge graph (i.e., ThemeKG), a KG constructed
from a theme-specific corpus, and design an unsupervised framework for ThemeKG
construction (named TKGCon). The framework takes raw theme-specific corpus and
generates a high-quality KG that includes salient entities and relations under
the theme. Specifically, we start with an entity ontology of the theme from
Wikipedia, based on which we then generate candidate relations by Large
Language Models (LLMs) to construct a relation ontology. To parse the documents
from the theme corpus, we first map the extracted entity pairs to the ontology
and retrieve the candidate relations. Finally, we incorporate the context and
ontology to consolidate the relations for entity pairs. We observe that
directly prompting GPT-4 for theme-specific KG leads to inaccurate entities
(such as "two main types" as one entity in the query result) and unclear (such
as "is", "has") or wrong relations (such as "have due to", "to start"). In
contrast, by constructing the theme-specific KG step by step, our model
outperforms GPT-4 and could consistently identify accurate entities and
relations. Experimental results also show that our framework excels in
evaluations compared with various KG construction baselines.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.19146v1},
File          = {2404.19146v1.pdf}
}
@article{2404.18681v1,
Author        = {Fabian Biester and Mohamed Abdelaal and Daniel Del Gaudio},
Title         = {LLMClean: Context-Aware Tabular Data Cleaning via LLM-Generated OFDs},
Eprint        = {2404.18681v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Machine learning's influence is expanding rapidly, now integral to
decision-making processes from corporate strategy to the advancements in
Industry 4.0. The efficacy of Artificial Intelligence broadly hinges on the
caliber of data used during its training phase; optimal performance is tied to
exceptional data quality. Data cleaning tools, particularly those that exploit
functional dependencies within ontological frameworks or context models, are
instrumental in augmenting data quality. Nevertheless, crafting these context
models is a demanding task, both in terms of resources and expertise, often
necessitating specialized knowledge from domain experts.
  In light of these challenges, this paper introduces an innovative approach,
called LLMClean, for the automated generation of context models, utilizing
Large Language Models to analyze and understand various datasets. LLMClean
encompasses a sequence of actions, starting with categorizing the dataset,
extracting or mapping relevant models, and ultimately synthesizing the context
model. To demonstrate its potential, we have developed and tested a prototype
that applies our approach to three distinct datasets from the Internet of
Things, healthcare, and Industry 4.0 sectors. The results of our evaluation
indicate that our automated approach can achieve data cleaning efficacy
comparable with that of context models crafted by human experts.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.18681v1},
File          = {2404.18681v1.pdf}
}
@article{2404.18542v1,
Author        = {Sven Hertling and Ebrahim Norouzi and Harald Sack},
Title         = {OAEI Machine Learning Dataset for Online Model Generation},
Eprint        = {2404.18542v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Ontology and knowledge graph matching systems are evaluated annually by the
Ontology Alignment Evaluation Initiative (OAEI). More and more systems use
machine learning-based approaches, including large language models. The
training and validation datasets are usually determined by the system developer
and often a subset of the reference alignments are used. This sampling is
against the OAEI rules and makes a fair comparison impossible. Furthermore,
those models are trained offline (a trained and optimized model is packaged
into the matcher) and therefore the systems are specifically trained for those
tasks. In this paper, we introduce a dataset that contains training,
validation, and test sets for most of the OAEI tracks. Thus, online model
learning (the systems must adapt to the given input alignment without human
intervention) is made possible to enable a fair comparison for ML-based
systems. We showcase the usefulness of the dataset by fine-tuning the
confidence thresholds of popular systems.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.18542v1},
File          = {2404.18542v1.pdf}
}
@article{2404.18386v1,
Author        = {Yao Wang and Yijun Yu and Yexing Li and Dong Li and Xiaoxue Zhao and Chungang Yang},
Title         = {Network Intent Decomposition and Optimization for Energy-Aware Radio
  Access Network},
Eprint        = {2404.18386v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.NI},
Abstract      = {With recent advancements in the sixth generation (6G) communication
technologies, more vertical industries have encountered diverse network
services. How to reduce energy consumption is critical to meet the expectation
of the quality of diverse network services. In particular, the number of base
stations in 6G is huge with coupled adjustable network parameters. However, the
problem is complex with multiple network objectives and parameters. Network
intents are difficult to map to individual network elements and require
enhanced automation capabilities. In this paper, we present a network intent
decomposition and optimization mechanism in an energy-aware radio access
network scenario. By characterizing the intent ontology with a standard
template, we present a generic network intent representation framework. Then we
propose a novel intent modeling method using Knowledge Acquisition in automated
Specification language, which can model the network ontology. To clarify the
number and types of network objectives and energy-saving operations, we develop
a Softgoal Interdependency Graph-based network intent decomposition model, and
thus, a network intent decomposition algorithm is presented. Simulation results
demonstrate that the proposed algorithm outperforms without conflict analysis
in intent decomposition time. Moreover, we design a deep Q-network-assisted
intent optimization scheme to validate the performance gain.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.18386v1},
File          = {2404.18386v1.pdf}
}
@article{2404.17809v1,
Author        = {Guozheng Li and Peng Wang and Wenjun Ke and Yikai Guo and Ke Ji and Ziyu Shang and Jiajun Liu and Zijie Xu},
Title         = {Recall, Retrieve and Reason: Towards Better In-Context Relation
  Extraction},
Eprint        = {2404.17809v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Relation extraction (RE) aims to identify relations between entities
mentioned in texts. Although large language models (LLMs) have demonstrated
impressive in-context learning (ICL) abilities in various tasks, they still
suffer from poor performances compared to most supervised fine-tuned RE
methods. Utilizing ICL for RE with LLMs encounters two challenges: (1)
retrieving good demonstrations from training examples, and (2) enabling LLMs
exhibit strong ICL abilities in RE. On the one hand, retrieving good
demonstrations is a non-trivial process in RE, which easily results in low
relevance regarding entities and relations. On the other hand, ICL with an LLM
achieves poor performance in RE while RE is different from language modeling in
nature or the LLM is not large enough. In this work, we propose a novel
recall-retrieve-reason RE framework that synergizes LLMs with retrieval corpora
(training examples) to enable relevant retrieving and reliable in-context
reasoning. Specifically, we distill the consistently ontological knowledge from
training datasets to let LLMs generate relevant entity pairs grounded by
retrieval corpora as valid queries. These entity pairs are then used to
retrieve relevant training examples from the retrieval corpora as
demonstrations for LLMs to conduct better ICL via instruction tuning. Extensive
experiments on different LLMs and RE datasets demonstrate that our method
generates relevant and valid entity pairs and boosts ICL abilities of LLMs,
achieving competitive or new state-of-the-art performance on sentence-level RE
compared to previous supervised fine-tuning methods and ICL-based methods.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.17809v1},
File          = {2404.17809v1.pdf}
}
@article{2405.01581v1,
Author        = {Nele KÃ¶hler and Fabian Neuhaus},
Title         = {The Mercurial Top-Level Ontology of Large Language Models},
Eprint        = {2405.01581v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In our work, we systematize and analyze implicit ontological commitments in
the responses generated by large language models (LLMs), focusing on ChatGPT
3.5 as a case study. We investigate how LLMs, despite having no explicit
ontology, exhibit implicit ontological categorizations that are reflected in
the texts they generate. The paper proposes an approach to understanding the
ontological commitments of LLMs by defining ontology as a theory that provides
a systematic account of the ontological commitments of some text. We
investigate the ontological assumptions of ChatGPT and present a systematized
account, i.e., GPT's top-level ontology. This includes a taxonomy, which is
available as an OWL file, as well as a discussion about ontological assumptions
(e.g., about its mereology or presentism). We show that in some aspects GPT's
top-level ontology is quite similar to existing top-level ontologies. However,
there are significant challenges arising from the flexible nature of
LLM-generated texts, including ontological overload, ambiguity, and
inconsistency.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2405.01581v1},
File          = {2405.01581v1.pdf}
}
@article{2404.17524v4,
Author        = {Luis Miguel Vieira da Silva and Aljosha KÃ¶cher and Felix Gehlhoff and Alexander Fay},
Title         = {On the Use of Large Language Models to Generate Capability Ontologies},
Eprint        = {2404.17524v4},
DOI           = {10.1109/ETFA61755.2024.10710775},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Capability ontologies are increasingly used to model functionalities of
systems or machines. The creation of such ontological models with all
properties and constraints of capabilities is very complex and can only be done
by ontology experts. However, Large Language Models (LLMs) have shown that they
can generate machine-interpretable models from natural language text input and
thus support engineers / ontology experts. Therefore, this paper investigates
how LLMs can be used to create capability ontologies. We present a study with a
series of experiments in which capabilities with varying complexities are
generated using different prompting techniques and with different LLMs. Errors
in the generated ontologies are recorded and compared. To analyze the quality
of the generated ontologies, a semi-automated approach based on RDF syntax
checking, OWL reasoning, and SHACL constraints is used. The results of this
study are very promising because even for complex capabilities, the generated
ontologies are almost free of errors.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.17524v4},
File          = {2404.17524v4.pdf}
}
@article{2404.16198v1,
Author        = {Mojdeh Rahmanian and Seyed Mostafa Fakhrahmad and Seyedeh Zahra Mousavi},
Title         = {Towards Efficient Patient Recruitment for Clinical Trials: Application
  of a Prompt-Based Learning Model},
Eprint        = {2404.16198v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Objective: Clinical trials are essential for advancing pharmaceutical
interventions, but they face a bottleneck in selecting eligible participants.
Although leveraging electronic health records (EHR) for recruitment has gained
popularity, the complex nature of unstructured medical texts presents
challenges in efficiently identifying participants. Natural Language Processing
(NLP) techniques have emerged as a solution with a recent focus on transformer
models. In this study, we aimed to evaluate the performance of a prompt-based
large language model for the cohort selection task from unstructured medical
notes collected in the EHR. Methods: To process the medical records, we
selected the most related sentences of the records to the eligibility criteria
needed for the trial. The SNOMED CT concepts related to each eligibility
criterion were collected. Medical records were also annotated with MedCAT based
on the SNOMED CT ontology. Annotated sentences including concepts matched with
the criteria-relevant terms were extracted. A prompt-based large language model
(Generative Pre-trained Transformer (GPT) in this study) was then used with the
extracted sentences as the training set. To assess its effectiveness, we
evaluated the model's performance using the dataset from the 2018 n2c2
challenge, which aimed to classify medical records of 311 patients based on 13
eligibility criteria through NLP techniques. Results: Our proposed model showed
the overall micro and macro F measures of 0.9061 and 0.8060 which were among
the highest scores achieved by the experiments performed with this dataset.
Conclusion: The application of a prompt-based large language model in this
study to classify patients based on eligibility criteria received promising
scores. Besides, we proposed a method of extractive summarization with the aid
of SNOMED CT ontology that can be also applied to other medical texts.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.16198v1},
File          = {2404.16198v1.pdf}
}
@article{2404.16030v1,
Author        = {Jiawei Ma and Po-Yao Huang and Saining Xie and Shang-Wen Li and Luke Zettlemoyer and Shih-Fu Chang and Wen-Tau Yih and Hu Xu},
Title         = {MoDE: CLIP Data Experts via Clustering},
Eprint        = {2404.16030v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {The success of contrastive language-image pretraining (CLIP) relies on the
supervision from the pairing between images and captions, which tends to be
noisy in web-crawled data. We present Mixture of Data Experts (MoDE) and learn
a system of CLIP data experts via clustering. Each data expert is trained on
one data cluster, being less sensitive to false negative noises in other
clusters. At inference time, we ensemble their outputs by applying weights
determined through the correlation between task metadata and cluster
conditions. To estimate the correlation precisely, the samples in one cluster
should be semantically similar, but the number of data experts should still be
reasonable for training and inference. As such, we consider the ontology in
human language and propose to use fine-grained cluster centers to represent
each data expert at a coarse-grained level. Experimental studies show that four
CLIP data experts on ViT-B/16 outperform the ViT-L/14 by OpenAI CLIP and
OpenCLIP on zero-shot image classification but with less ($<$35\%) training
cost. Meanwhile, MoDE can train all data expert asynchronously and can flexibly
include new data experts. The code is available at
https://github.com/facebookresearch/MetaCLIP/tree/main/mode.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.16030v1},
File          = {2404.16030v1.pdf}
}
@article{2404.16884v1,
Author        = {Tangrui Li and Jun Zhou},
Title         = {Aligning Knowledge Graphs Provided by Humans and Generated from Neural
  Networks in Specific Tasks},
Eprint        = {2404.16884v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {This paper develops an innovative method that enables neural networks to
generate and utilize knowledge graphs, which describe their concept-level
knowledge and optimize network parameters through alignment with human-provided
knowledge. This research addresses a gap where traditionally, network-generated
knowledge has been limited to applications in downstream symbolic analysis or
enhancing network transparency. By integrating a novel autoencoder design with
the Vector Symbolic Architecture (VSA), we have introduced auxiliary tasks that
support end-to-end training. Our approach eschews traditional dependencies on
ontologies or word embedding models, mining concepts from neural networks and
directly aligning them with human knowledge. Experiments show that our method
consistently captures network-generated concepts that align closely with human
knowledge and can even uncover new, useful concepts not previously identified
by humans. This plug-and-play strategy not only enhances the interpretability
of neural networks but also facilitates the integration of symbolic logical
reasoning within these systems.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.16884v1},
File          = {2404.16884v1.pdf}
}
@article{2404.14991v2,
Author        = {Rick Du and Huilong An and Keyu Wang and Weidong Liu},
Title         = {A Short Review for Ontology Learning: Stride to Large Language Models
  Trend},
Eprint        = {2404.14991v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Ontologies provide formal representation of knowledge shared within Semantic
Web applications. Ontology learning involves the construction of ontologies
from a given corpus. In the past years, ontology learning has traversed through
shallow learning and deep learning methodologies, each offering distinct
advantages and limitations in the quest for knowledge extraction and
representation. A new trend of these approaches is relying on large language
models (LLMs) to enhance ontology learning. This paper gives a review in
approaches and challenges of ontology learning. It analyzes the methodologies
and limitations of shallow-learning-based and deep-learning-based techniques
for ontology learning, and provides comprehensive knowledge for the frontier
work of using LLMs to enhance ontology learning. In addition, it proposes
several noteworthy future directions for further exploration into the
integration of LLMs with ontology learning tasks.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.14991v2},
File          = {2404.14991v2.pdf}
}
@article{2404.14209v1,
Author        = {Po-Ting Lai and Elisabeth Coudert and Lucila Aimo and Kristian Axelsen and Lionel Breuza and Edouard de Castro and Marc Feuermann and Anne Morgat and Lucille Pourcel and Ivo Pedruzzi and Sylvain Poux and Nicole Redaschi and Catherine Rivoire and Anastasia Sveshnikova and Chih-Hsuan Wei and Robert Leaman and Ling Luo and Zhiyong Lu and Alan Bridge},
Title         = {EnzChemRED, a rich enzyme chemistry relation extraction dataset},
Eprint        = {2404.14209v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Expert curation is essential to capture knowledge of enzyme functions from
the scientific literature in FAIR open knowledgebases but cannot keep pace with
the rate of new discoveries and new publications. In this work we present
EnzChemRED, for Enzyme Chemistry Relation Extraction Dataset, a new training
and benchmarking dataset to support the development of Natural Language
Processing (NLP) methods such as (large) language models that can assist enzyme
curation. EnzChemRED consists of 1,210 expert curated PubMed abstracts in which
enzymes and the chemical reactions they catalyze are annotated using
identifiers from the UniProt Knowledgebase (UniProtKB) and the ontology of
Chemical Entities of Biological Interest (ChEBI). We show that fine-tuning
pre-trained language models with EnzChemRED can significantly boost their
ability to identify mentions of proteins and chemicals in text (Named Entity
Recognition, or NER) and to extract the chemical conversions in which they
participate (Relation Extraction, or RE), with average F1 score of 86.30% for
NER, 86.66% for RE for chemical conversion pairs, and 83.79% for RE for
chemical conversion pairs and linked enzymes. We combine the best performing
methods after fine-tuning using EnzChemRED to create an end-to-end pipeline for
knowledge extraction from text and apply this to abstracts at PubMed scale to
create a draft map of enzyme functions in literature to guide curation efforts
in UniProtKB and the reaction knowledgebase Rhea. The EnzChemRED corpus is
freely available at https://ftp.expasy.org/databases/rhea/nlp/.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.14209v1},
File          = {2404.14209v1.pdf}
}
@article{2404.12827v3,
Author        = {Anthony Yazdani and Alban Bornet and Philipp Khlebnikov and Boya Zhang and Hossein Rouhizadeh and Poorya Amini and Douglas Teodoro},
Title         = {An Evaluation Benchmark for Adverse Drug Event Prediction from Clinical
  Trial Results},
Eprint        = {2404.12827v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Adverse drug events (ADEs) are a major safety issue in clinical trials. Thus,
predicting ADEs is key to developing safer medications and enhancing patient
outcomes. To support this effort, we introduce CT-ADE, a dataset for multilabel
ADE prediction in monopharmacy treatments. CT-ADE encompasses 2,497 drugs and
168,984 drug-ADE pairs from clinical trial results, annotated using the MedDRA
ontology. Unlike existing resources, CT-ADE integrates treatment and target
population data, enabling comparative analyses under varying conditions, such
as dosage, administration route, and demographics. In addition, CT-ADE
systematically collects all ADEs in the study population, including positive
and negative cases. To provide a baseline for ADE prediction performance using
the CT-ADE dataset, we conducted analyses using large language models (LLMs).
The best LLM achieved an F1-score of 56%, with models incorporating treatment
and patient information outperforming by 21%-38% those relying solely on the
chemical structure. These findings underscore the importance of contextual
information in ADE prediction and establish CT-ADE as a robust resource for
safety risk assessment in pharmaceutical research and development.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.12827v3},
File          = {2404.12827v3.pdf}
}
@article{2404.12698v2,
Author        = {Xiao Zhang and Gosse Bouma and Johan Bos},
Title         = {Neural Semantic Parsing with Extremely Rich Symbolic Meaning
  Representations},
Eprint        = {2404.12698v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Current open-domain neural semantics parsers show impressive performance.
However, closer inspection of the symbolic meaning representations they produce
reveals significant weaknesses: sometimes they tend to merely copy character
sequences from the source text to form symbolic concepts, defaulting to the
most frequent word sense based in the training distribution. By leveraging the
hierarchical structure of a lexical ontology, we introduce a novel
compositional symbolic representation for concepts based on their position in
the taxonomical hierarchy. This representation provides richer semantic
information and enhances interpretability. We introduce a neural "taxonomical"
semantic parser to utilize this new representation system of predicates, and
compare it with a standard neural semantic parser trained on the traditional
meaning representation format, employing a novel challenge set and evaluation
metric for evaluation. Our experimental findings demonstrate that the
taxonomical model, trained on much richer and complex meaning representations,
is slightly subordinate in performance to the traditional model using the
standard metrics for evaluation, but outperforms it when dealing with
out-of-vocabulary concepts. This finding is encouraging for research in
computational semantics that aims to combine data-driven distributional
meanings with knowledge-based symbolic representations.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.12698v2},
File          = {2404.12698v2.pdf}
}
@article{2404.12447v3,
Author        = {Yoonsang Lee and Xi Ye and Eunsol Choi},
Title         = {AmbigDocs: Reasoning across Documents on Different Entities under the
  Same Name},
Eprint        = {2404.12447v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Different entities with the same name can be difficult to distinguish.
Handling confusing entity mentions is a crucial skill for language models
(LMs). For example, given the question "Where was Michael Jordan educated?" and
a set of documents discussing different people named Michael Jordan, can LMs
distinguish entity mentions to generate a cohesive answer to the question? To
test this ability, we introduce a new benchmark, AmbigDocs. By leveraging
Wikipedia's disambiguation pages, we identify a set of documents, belonging to
different entities who share an ambiguous name. From these documents, we
generate questions containing an ambiguous name and their corresponding sets of
answers. Our analysis reveals that current state-of-the-art models often yield
ambiguous answers or incorrectly merge information belonging to different
entities. We establish an ontology categorizing four types of incomplete
answers and automatic evaluation metrics to identify such categories. We lay
the foundation for future work on reasoning across multiple documents with
ambiguous entities.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.12447v3},
File          = {2404.12447v3.pdf}
}
@article{2404.12069v2,
Author        = {Sebastian Barzaghi and Ivan Heibi and Arianna Moretti and Silvio Peroni},
Title         = {Developing Application Profiles for Enhancing Data and Workflows in
  Cultural Heritage Digitisation Processes},
Eprint        = {2404.12069v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {As a result of the proliferation of 3D digitisation in the context of
cultural heritage projects, digital assets and digitisation processes - being
considered as proper research objects - must prioritise adherence to FAIR
principles. Existing standards and ontologies, such as CIDOC CRM, play a
crucial role in this regard, but they are often over-engineered for the need of
a particular application context, thus making their understanding and adoption
difficult. Application profiles of a given standard - defined as sets of
ontological entities drawn from one or more semantic artefacts for a particular
context or application - are usually proposed as tools for promoting
interoperability and reuse while being tied entirely to the particular
application context they refer to. In this paper, we present an adaptation and
application of an ontology development methodology, i.e. SAMOD, to guide the
creation of robust, semantically sound application profiles of large standard
models. Using an existing pilot study we have developed in a project dedicated
to leveraging virtual technologies to preserve and valorise cultural heritage,
we introduce an application profile named CHAD-AP, that we have developed
following our customised version of SAMOD. We reflect on the use of SAMOD and
similar ontology development methodologies for this purpose, highlighting its
strengths and current limitations, future developments, and possible adoption
in other similar projects.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.12069v2},
File          = {2404.12069v2.pdf}
}
@article{2404.11744v1,
Author        = {Luca Buoncompagni and Fulvio Mastrogiovanni},
Title         = {Incremental Bootstrapping and Classification of Structured Scenes in a
  Fuzzy Ontology},
Eprint        = {2404.11744v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We foresee robots that bootstrap knowledge representations and use them for
classifying relevant situations and making decisions based on future
observations. Particularly for assistive robots, the bootstrapping mechanism
might be supervised by humans who should not repeat a training phase several
times and should be able to refine the taught representation. We consider
robots that bootstrap structured representations to classify some intelligible
categories. Such a structure should be incrementally bootstrapped, i.e.,
without invalidating the identified category models when a new additional
category is considered. To tackle this scenario, we presented the Scene
Identification and Tagging (SIT) algorithm, which bootstraps structured
knowledge representation in a crisp OWL-DL ontology. Over time, SIT bootstraps
a graph representing scenes, sub-scenes and similar scenes. Then, SIT can
classify new scenes within the bootstrapped graph through logic-based
reasoning. However, SIT has issues with sensory data because its crisp
implementation is not robust to perception noises. This paper presents a
reformulation of SIT within the fuzzy domain, which exploits a fuzzy DL
ontology to overcome the robustness issues. By comparing the performances of
fuzzy and crisp implementations of SIT, we show that fuzzy SIT is robust,
preserves the properties of its crisp formulation, and enhances the
bootstrapped representations. On the contrary, the fuzzy implementation of SIT
leads to less intelligible knowledge representations than the one bootstrapped
in the crisp domain.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.11744v1},
File          = {2404.11744v1.pdf}
}
@article{2404.10329v2,
Author        = {Reihaneh Amini and Sanaz Saki Norouzi and Pascal Hitzler and Reza Amini},
Title         = {Towards Complex Ontology Alignment using Large Language Models},
Eprint        = {2404.10329v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontology alignment, a critical process in the Semantic Web for detecting
relationships between different ontologies, has traditionally focused on
identifying so-called "simple" 1-to-1 relationships through class labels and
properties comparison. The more practically useful exploration of more complex
alignments remains a hard problem to automate, and as such is largely
underexplored, i.e. in application practice it is usually done manually by
ontology and domain experts. Recently, the surge in Natural Language Processing
(NLP) capabilities, driven by advancements in Large Language Models (LLMs),
presents new opportunities for enhancing ontology engineering practices,
including ontology alignment tasks. This paper investigates the application of
LLM technologies to tackle the complex ontology alignment challenge. Leveraging
a prompt-based approach and integrating rich ontology content so-called modules
our work constitutes a significant advance towards automating the complex
alignment task.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.10329v2},
File          = {2404.10329v2.pdf}
}
@article{2404.10317v2,
Author        = {Hamed Babaei Giglou and Jennifer D'Souza and Felix Engel and SÃ¶ren Auer},
Title         = {LLMs4OM: Matching Ontologies with Large Language Models},
Eprint        = {2404.10317v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontology Matching (OM), is a critical task in knowledge integration, where
aligning heterogeneous ontologies facilitates data interoperability and
knowledge sharing. Traditional OM systems often rely on expert knowledge or
predictive models, with limited exploration of the potential of Large Language
Models (LLMs). We present the LLMs4OM framework, a novel approach to evaluate
the effectiveness of LLMs in OM tasks. This framework utilizes two modules for
retrieval and matching, respectively, enhanced by zero-shot prompting across
three ontology representations: concept, concept-parent, and concept-children.
Through comprehensive evaluations using 20 OM datasets from various domains, we
demonstrate that LLMs, under the LLMs4OM framework, can match and even surpass
the performance of traditional OM systems, particularly in complex matching
scenarios. Our results highlight the potential of LLMs to significantly
contribute to the field of OM.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.10317v2},
File          = {2404.10317v2.pdf}
}
@article{2404.09305v2,
Author        = {Luca Buoncompagni and Fulvio Mastrogiovanni},
Title         = {OWLOOP: Interfaces for Mapping OWL Axioms into OOP Hierarchies},
Eprint        = {2404.09305v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The paper tackles the issue of mapping logic axioms formalised in the
Ontology Web Language (OWL) within the Object-Oriented Programming (OOP)
paradigm. The issues of mapping OWL axioms hierarchies and OOP objects
hierarchies are due to OWL-based reasoning algorithms, which might change an
OWL hierarchy at runtime; instead, OOP hierarchies are usually defined as
static structures. Although programming paradigms based on reflection allow
changing the OOP hierarchies at runtime and mapping OWL axioms dynamically,
there are no currently available mechanisms that do not limit the reasoning
algorithms. Thus, the factory-based paradigm is typically used since it
decouples the OWL and OOP hierarchies. However, the factory inhibits OOP
polymorphism and introduces a paradigm shift with respect to widely accepted
OOP paradigms. We present the OWLOOP API, which exploits the factory to not
limit reasoning algorithms, and it provides novel OOP interfaces concerning the
axioms in an ontology. OWLOOP is designed to limit the paradigm shift required
for using ontologies while improving, through OOP-like polymorphism, the
modularity of software architectures that exploit logic reasoning. The paper
details our OWL to OOP mapping mechanism, and it shows the benefits and
limitations of OWLOOP through examples concerning a robot in a smart
environment.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.09305v2},
File          = {2404.09305v2.pdf}
}
@article{2405.01392v1,
Author        = {David Maranto},
Title         = {LLMSat: A Large Language Model-Based Goal-Oriented Agent for Autonomous
  Space Exploration},
Eprint        = {2405.01392v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {As spacecraft journey further from Earth with more complex missions, systems
of greater autonomy and onboard intelligence are called for. Reducing reliance
on human-based mission control becomes increasingly critical if we are to
increase our rate of solar-system-wide exploration. Recent work has explored
AI-based goal-oriented systems to increase the level of autonomy in mission
execution. These systems make use of symbolic reasoning managers to make
inferences from the state of a spacecraft and a handcrafted knowledge base,
enabling autonomous generation of tasks and re-planning. Such systems have
proven to be successful in controlled cases, but they are difficult to
implement as they require human-crafted ontological models to allow the
spacecraft to understand the world. Reinforcement learning has been applied to
train robotic agents to pursue a goal. A new architecture for autonomy is
called for. This work explores the application of Large Language Models (LLMs)
as the high-level control system of a spacecraft. Using a systems engineering
approach, this work presents the design and development of an agentic
spacecraft controller by leveraging an LLM as a reasoning engine, to evaluate
the utility of such an architecture in achieving higher levels of spacecraft
autonomy. A series of deep space mission scenarios simulated within the popular
game engine Kerbal Space Program (KSP) are used as case studies to evaluate the
implementation against the requirements. It is shown the reasoning and planning
abilities of present-day LLMs do not scale well as the complexity of a mission
increases, but this can be alleviated with adequate prompting frameworks and
strategic selection of the agent's level of authority over the host spacecraft.
This research evaluates the potential of LLMs in augmenting autonomous
decision-making systems for future robotic space applications.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2405.01392v1},
File          = {2405.01392v1.pdf}
}
@article{2404.08579v1,
Author        = {William Gantt and Aaron Steven White},
Title         = {Small Models Are (Still) Effective Cross-Domain Argument Extractors},
Eprint        = {2404.08579v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Effective ontology transfer has been a major goal of recent work on event
argument extraction (EAE). Two methods in particular -- question answering (QA)
and template infilling (TI) -- have emerged as promising approaches to this
problem. However, detailed explorations of these techniques' ability to
actually enable this transfer are lacking. In this work, we provide such a
study, exploring zero-shot transfer using both techniques on six major EAE
datasets at both the sentence and document levels. Further, we challenge the
growing reliance on LLMs for zero-shot extraction, showing that vastly smaller
models trained on an appropriate source ontology can yield zero-shot
performance superior to that of GPT-3.5 or GPT-4.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.08579v1},
File          = {2404.08579v1.pdf}
}
@article{2404.08443v1,
Author        = {Raia Abu Ahmad and Jennifer D'Souza and MatthÃ¤us Zloch and Wolfgang Otto and Georg Rehm and Allard Oelen and Stefan Dietze and SÃ¶ren Auer},
Title         = {Toward FAIR Semantic Publishing of Research Dataset Metadata in the Open
  Research Knowledge Graph},
Eprint        = {2404.08443v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {Search engines these days can serve datasets as search results. Datasets get
picked up by search technologies based on structured descriptions on their
official web pages, informed by metadata ontologies such as the Dataset content
type of schema.org. Despite this promotion of the content type dataset as a
first-class citizen of search results, a vast proportion of datasets,
particularly research datasets, still need to be made discoverable and,
therefore, largely remain unused. This is due to the sheer volume of datasets
released every day and the inability of metadata to reflect a dataset's content
and context accurately. This work seeks to improve this situation for a
specific class of datasets, namely research datasets, which are the result of
research endeavors and are accompanied by a scholarly publication. We propose
the ORKG-Dataset content type, a specialized branch of the Open Research
Knowledge Graoh (ORKG) platform, which provides descriptive information and a
semantic model for research datasets, integrating them with their accompanying
scholarly publications. This work aims to establish a standardized framework
for recording and reporting research datasets within the ORKG-Dataset content
type. This, in turn, increases research dataset transparency on the web for
their improved discoverability and applied use. In this paper, we present a
proposal -- the minimum FAIR, comparable, semantic description of research
datasets in terms of salient properties of their supporting publication. We
design a specific application of the ORKG-Dataset semantic model based on 40
diverse research datasets on scientific information extraction.},
Year          = {2024},
Month         = {Apr},
Note          = {In Joint Proceedings of the Onto4FAIR 2023 Workshops: Collocated
  with FOIS 2023 and SEMANTICS 2023. pp.23-31. https://hal.science/hal-04312604},
Url           = {http://arxiv.org/abs/2404.08443v1},
File          = {2404.08443v1.pdf}
}
@article{2404.06571v1,
Author        = {Yunqing Li and Binil Starly},
Title         = {Building A Knowledge Graph to Enrich ChatGPT Responses in Manufacturing
  Service Discovery},
Eprint        = {2404.06571v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Sourcing and identification of new manufacturing partners is crucial for
manufacturing system integrators to enhance agility and reduce risk through
supply chain diversification in the global economy. The advent of advanced
large language models has captured significant interest, due to their ability
to generate comprehensive and articulate responses across a wide range of
knowledge domains. However, the system often falls short in accuracy and
completeness when responding to domain-specific inquiries, particularly in
areas like manufacturing service discovery. This research explores the
potential of leveraging Knowledge Graphs in conjunction with ChatGPT to
streamline the process for prospective clients in identifying small
manufacturing enterprises. In this study, we propose a method that integrates
bottom-up ontology with advanced machine learning models to develop a
Manufacturing Service Knowledge Graph from an array of structured and
unstructured data sources, including the digital footprints of small-scale
manufacturers throughout North America. The Knowledge Graph and the learned
graph embedding vectors are leveraged to tackle intricate queries within the
digital supply chain network, responding with enhanced reliability and greater
interpretability. The approach highlighted is scalable to millions of entities
that can be distributed to form a global Manufacturing Service Knowledge
Network Graph that can potentially interconnect multiple types of Knowledge
Graphs that span industry sectors, geopolitical boundaries, and business
domains. The dataset developed for this study, now publicly accessible,
encompasses more than 13,000 manufacturers' weblinks, manufacturing services,
certifications, and location entity types.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.06571v1},
File          = {2404.06571v1.pdf}
}
@article{2404.05012v1,
Author        = {Kunyao Lan and Cong Ming and Binwei Yao and Lu Chen and Mengyue Wu},
Title         = {Towards Reliable and Empathetic Depression-Diagnosis-Oriented Chats},
Eprint        = {2404.05012v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Chatbots can serve as a viable tool for preliminary depression diagnosis via
interactive conversations with potential patients. Nevertheless, the blend of
task-oriented and chit-chat in diagnosis-related dialogues necessitates
professional expertise and empathy. Such unique requirements challenge
traditional dialogue frameworks geared towards single optimization goals. To
address this, we propose an innovative ontology definition and generation
framework tailored explicitly for depression diagnosis dialogues, combining the
reliability of task-oriented conversations with the appeal of empathy-related
chit-chat. We further apply the framework to D$^4$, the only existing public
dialogue dataset on depression diagnosis-oriented chats. Exhaustive
experimental results indicate significant improvements in task completion and
emotional support generation in depression diagnosis, fostering a more
comprehensive approach to task-oriented chat dialogue system development and
its applications in digital mental health.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.05012v1},
File          = {2404.05012v1.pdf}
}
@article{2404.04108v2,
Author        = {Giovanni Ciatto and Andrea Agiollo and Matteo Magnini and Andrea Omicini},
Title         = {Large language models as oracles for instantiating ontologies with
  domain-specific knowledge},
Eprint        = {2404.04108v2},
DOI           = {10.1016/j.knosys.2024.112940},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Background. Endowing intelligent systems with semantic data commonly requires
designing and instantiating ontologies with domain-specific knowledge.
Especially in the early phases, those activities are typically performed
manually by human experts possibly leveraging on their own experience. The
resulting process is therefore time-consuming, error-prone, and often biased by
the personal background of the ontology designer. Objective. To mitigate that
issue, we propose a novel domain-independent approach to automatically
instantiate ontologies with domain-specific knowledge, by leveraging on large
language models (LLMs) as oracles. Method. Starting from (i) an initial schema
composed by inter-related classes and properties and (ii) a set of query
templates, our method queries the LLM multiple times, and generates instances
for both classes and properties from its replies. Thus, the ontology is
automatically filled with domain-specific knowledge, compliant to the initial
schema. As a result, the ontology is quickly and automatically enriched with
manifold instances, which experts may consider to keep, adjust, discard, or
complement according to their own needs and expertise. Contribution. We
formalise our method in general way and instantiate it over various LLMs, as
well as on a concrete case study. We report experiments rooted in the
nutritional domain where an ontology of food meals and their ingredients is
automatically instantiated from scratch, starting from a categorisation of
meals and their relationships. There, we analyse the quality of the generated
ontologies and compare ontologies attained by exploiting different LLMs.
Experimentally, our approach achieves a quality metric that is up to five times
higher than the state-of-the-art, while reducing erroneous entities and
relations by up to ten times. Finally, we provide a SWOT analysis of the
proposed method.},
Year          = {2024},
Month         = {Apr},
Note          = {Knowledge-Based Systems 310 (2025) 112940},
Url           = {http://arxiv.org/abs/2404.04108v2},
File          = {2404.04108v2.pdf}
}
@article{2404.03080v5,
Author        = {Yanpeng Ye and Jie Ren and Shaozhou Wang and Yuwei Wan and Imran Razzak and Bram Hoex and Haofen Wang and Tong Xie and Wenjie Zhang},
Title         = {Construction and Application of Materials Knowledge Graph in
  Multidisciplinary Materials Science via Large Language Model},
Eprint        = {2404.03080v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge in materials science is widely dispersed across extensive
scientific literature, posing significant challenges to the efficient discovery
and integration of new materials. Traditional methods, often reliant on costly
and time-consuming experimental approaches, further complicate rapid
innovation. Addressing these challenges, the integration of artificial
intelligence with materials science has opened avenues for accelerating the
discovery process, though it also demands precise annotation, data extraction,
and traceability of information. To tackle these issues, this article
introduces the Materials Knowledge Graph (MKG), which utilizes advanced natural
language processing techniques integrated with large language models to extract
and systematically organize a decade's worth of high-quality research into
structured triples, contains 162,605 nodes and 731,772 edges. MKG categorizes
information into comprehensive labels such as Name, Formula, and Application,
structured around a meticulously designed ontology, thus enhancing data
usability and integration. By implementing network-based algorithms, MKG not
only facilitates efficient link prediction but also significantly reduces
reliance on traditional experimental methods. This structured approach not only
streamlines materials research but also lays the groundwork for more
sophisticated science knowledge graphs.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.03080v5},
File          = {2404.03080v5.pdf}
}
@article{2404.03044v1,
Author        = {Marcin P. Joachimiak and Mark A. Miller and J. Harry Caufield and Ryan Ly and Nomi L. Harris and Andrew Tritt and Christopher J. Mungall and Kristofer E. Bouchard},
Title         = {The Artificial Intelligence Ontology: LLM-assisted construction of AI
  concept hierarchies},
Eprint        = {2404.03044v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {The Artificial Intelligence Ontology (AIO) is a systematization of artificial
intelligence (AI) concepts, methodologies, and their interrelations. Developed
via manual curation, with the additional assistance of large language models
(LLMs), AIO aims to address the rapidly evolving landscape of AI by providing a
comprehensive framework that encompasses both technical and ethical aspects of
AI technologies. The primary audience for AIO includes AI researchers,
developers, and educators seeking standardized terminology and concepts within
the AI domain. The ontology is structured around six top-level branches:
Networks, Layers, Functions, LLMs, Preprocessing, and Bias, each designed to
support the modular composition of AI methods and facilitate a deeper
understanding of deep learning architectures and ethical considerations in AI.
  AIO's development utilized the Ontology Development Kit (ODK) for its
creation and maintenance, with its content being dynamically updated through
AI-driven curation support. This approach not only ensures the ontology's
relevance amidst the fast-paced advancements in AI but also significantly
enhances its utility for researchers, developers, and educators by simplifying
the integration of new AI concepts and methodologies.
  The ontology's utility is demonstrated through the annotation of AI methods
data in a catalog of AI research publications and the integration into the
BioPortal ontology resource, highlighting its potential for cross-disciplinary
research. The AIO ontology is open source and is available on GitHub
(https://github.com/berkeleybop/artificial-intelligence-ontology) and BioPortal
(https://bioportal.bioontology.org/ontologies/AIO).},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.03044v1},
File          = {2404.03044v1.pdf}
}
@article{2404.01679v2,
Author        = {Tanmay Parekh and Anh Mac and Jiarui Yu and Yuxuan Dong and Syed Shahriar and Bonnie Liu and Eric Yang and Kuan-Hao Huang and Wei Wang and Nanyun Peng and Kai-Wei Chang},
Title         = {Event Detection from Social Media for Epidemic Prediction},
Eprint        = {2404.01679v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Social media is an easy-to-access platform providing timely updates about
societal trends and events. Discussions regarding epidemic-related events such
as infections, symptoms, and social interactions can be crucial for informing
policymaking during epidemic outbreaks. In our work, we pioneer exploiting
Event Detection (ED) for better preparedness and early warnings of any upcoming
epidemic by developing a framework to extract and analyze epidemic-related
events from social media posts. To this end, we curate an epidemic event
ontology comprising seven disease-agnostic event types and construct a Twitter
dataset SPEED with human-annotated events focused on the COVID-19 pandemic.
Experimentation reveals how ED models trained on COVID-based SPEED can
effectively detect epidemic events for three unseen epidemics of Monkeypox,
Zika, and Dengue; while models trained on existing ED datasets fail miserably.
Furthermore, we show that reporting sharp increases in the extracted events by
our framework can provide warnings 4-9 weeks earlier than the WHO epidemic
declaration for Monkeypox. This utility of our framework lays the foundations
for better preparedness against emerging epidemics.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.01679v2},
File          = {2404.01679v2.pdf}
}
@article{2404.00756v1,
Author        = {Cristina Cornelio and Mohammed Diab},
Title         = {Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery},
Eprint        = {2404.00756v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Recognizing failures during task execution and implementing recovery
procedures is challenging in robotics. Traditional approaches rely on the
availability of extensive data or a tight set of constraints, while more recent
approaches leverage large language models (LLMs) to verify task steps and
replan accordingly. However, these methods often operate offline, necessitating
scene resets and incurring in high costs. This paper introduces Recover, a
neuro-symbolic framework for online failure identification and recovery. By
integrating ontologies, logical rules, and LLM-based planners, Recover exploits
symbolic information to enhance the ability of LLMs to generate recovery plans
and also to decrease the associated costs. In order to demonstrate the
capabilities of our method in a simulated kitchen environment, we introduce
OntoThor, an ontology describing the AI2Thor simulator setting. Empirical
evaluation shows that OntoThor's logical rules accurately detect all failures
in the analyzed tasks, and that Recover considerably outperforms, for both
failure detection and recovery, a baseline method reliant solely on LLMs.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2404.00756v1},
File          = {2404.00756v1.pdf}
}
@article{2404.00395v1,
Author        = {Manuele Veggi},
Title         = {A First Ontological Model for the Description of the Art Market in the
  Semantic Web},
Eprint        = {2404.00395v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {This dissertation presents the first version of a project at the Fondazione
Federico Zeri, aimed at modelling the art market starting from the recognition
of the peculiarities of this sector and relying on the data collected by this
institute during its research activities on its documentary collection.
Specifically, this study describes the development of an ontology, able to
describe agents, events and sources which define the art market and enable its
investigation. The recognition of existing conceptual models is hence followed
by the description of the adopted methodology, based on the protocol SAMOD. The
central section provides a general overview of the final ontology, integrating
the results of a preliminary study. Lastly, the appendix lists motivating
scenarios, examples and competency questions collected during the first SAMOD
iterations, as well as a first alignment with existing models.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2404.00395v1},
File          = {2404.00395v1.pdf}
}
@article{2404.00341v1,
Author        = {Ahmed R. Sadik and Bodo Urban},
Title         = {Ontology in Holonic Cooperative Manufacturing: A Solution to Share and
  Exchange the Knowledge},
Eprint        = {2404.00341v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Cooperative manufacturing is a new trend in industry, which depends on the
existence of a collaborative robot. A collaborative robot is usually a
light-weight robot which is capable of operating safely with a human co-worker
in a shared work environment. During this cooperation, a vast amount of
information is exchanged between the collaborative robot and the worker. This
information constructs the cooperative manufacturing knowledge, which describes
the production components and environment. In this research, we propose a
holonic control solution, which uses the ontology concept to represent the
cooperative manufacturing knowledge. The holonic control solution is
implemented as an autonomous multi-agent system that exchanges the
manufacturing knowledge based on an ontology model. Ultimately, the research
illustrates and implements the proposed solution over a cooperative assembly
scenario, which involves two workers and one collaborative robot, whom
cooperate together to assemble a customized product.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2404.00341v1},
File          = {2404.00341v1.pdf}
}
@article{2403.20282v2,
Author        = {Johannes Branahl},
Title         = {Deriving Ontological Statements from the Unnatural Higgs Mass},
Eprint        = {2403.20282v2},
DOI           = {10.1007/s10701-025-00852-3},
ArchivePrefix = {arXiv},
PrimaryClass  = {physics.hist-ph},
Abstract      = {We provide novel, metatheoretical arguments strengthening the position that
the naturalness problem of the light Higgs mass is a pseudo-problem: No physics
beyond the standard model of particle physics is needed to explain the small
value of the Higgs boson. By evaluating previous successes of the guiding
principle of technical naturalness, we restrict its applicability to
non-fundamental phenomena in the realm of provisional theories within limited
energy scales. In view of further breaches of autonomy of scales in apparently
fundamental phenomena outside particle physics, the hierarchy problem of the
Higgs mass is instead reinterpreted as an indication of the ontologically
fundamental status of the Higgs boson. Applying the concept of Selective
Realism justifies this seemingly contradictory attribution within the effective
theories of the standard model of particle physics. Moreover, we argue that the
ongoing naturalness debate about the Higgs mass is partly based on the
adherence to the methodology of effective theories (often claimed to be
universally applicable), for which there is no justification when dealing with
presumably fundamental phenomena such as the Higgs mechanism, even if it is
embedded into an effective theory.},
Year          = {2024},
Month         = {Mar},
Note          = {Foundations of Physics Vol. 55, Article 39 (2025)},
Url           = {http://arxiv.org/abs/2403.20282v2},
File          = {2403.20282v2.pdf}
}
@article{2403.17778v2,
Author        = {Marco Reidelbach and BjÃ¶rn Schembera and Marcus Weber},
Title         = {Towards a FAIR Documentation of Workflows and Models in Applied
  Mathematics},
Eprint        = {2403.17778v2},
DOI           = {10.1007/978-3-031-64529-7_27},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Modeling-Simulation-Optimization workflows play a fundamental role in applied
mathematics. The Mathematical Research Data Initiative, MaRDI, responded to
this by developing a FAIR and machine-interpretable template for a
comprehensive documentation of such workflows. MaRDMO, a Plugin for the
Research Data Management Organiser, enables scientists from diverse fields to
document and publish their workflows on the MaRDI Portal seamlessly using the
MaRDI template. Central to these workflows are mathematical models. MaRDI
addresses them with the MathModDB ontology, offering a structured formal model
description. Here, we showcase the interaction between MaRDMO and the MathModDB
Knowledge Graph through an algebraic modeling workflow from the Digital
Humanities. This demonstration underscores the versatility of both services
beyond their original numerical domain.},
Year          = {2024},
Month         = {Mar},
Note          = {International Congress on Mathematical Software (pp. 254-262).
  Cham: Springer Nature Switzerland (2024, July)},
Url           = {http://arxiv.org/abs/2403.17778v2},
File          = {2403.17778v2.pdf}
}
@article{2404.00054v1,
Author        = {Siyuan Peng and Kate Ladenheim and Snehesh Shrestha and Cornelia FermÃ¼ller},
Title         = {Choreographing the Digital Canvas: A Machine Learning Approach to
  Artistic Performance},
Eprint        = {2404.00054v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {This paper introduces the concept of a design tool for artistic performances
based on attribute descriptions. To do so, we used a specific performance of
falling actions. The platform integrates a novel machine-learning (ML) model
with an interactive interface to generate and visualize artistic movements. Our
approach's core is a cyclic Attribute-Conditioned Variational Autoencoder
(AC-VAE) model developed to address the challenge of capturing and generating
realistic 3D human body motions from motion capture (MoCap) data. We created a
unique dataset focused on the dynamics of falling movements, characterized by a
new ontology that divides motion into three distinct phases: Impact, Glitch,
and Fall. The ML model's innovation lies in its ability to learn these phases
separately. It is achieved by applying comprehensive data augmentation
techniques and an initial pose loss function to generate natural and plausible
motion. Our web-based interface provides an intuitive platform for artists to
engage with this technology, offering fine-grained control over motion
attributes and interactive visualization tools, including a 360-degree view and
a dynamic timeline for playback manipulation. Our research paves the way for a
future where technology amplifies the creative potential of human expression,
making sophisticated motion generation accessible to a wider artistic
community.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2404.00054v1},
File          = {2404.00054v1.pdf}
}
@article{2403.17216v1,
Author        = {Na Li and Thomas Bailleux and Zied Bouraoui and Steven Schockaert},
Title         = {Ontology Completion with Natural Language Inference and Concept
  Embeddings: An Analysis},
Eprint        = {2403.17216v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We consider the problem of finding plausible knowledge that is missing from a
given ontology, as a generalisation of the well-studied taxonomy expansion
task. One line of work treats this task as a Natural Language Inference (NLI)
problem, thus relying on the knowledge captured by language models to identify
the missing knowledge. Another line of work uses concept embeddings to identify
what different concepts have in common, taking inspiration from cognitive
models for category based induction. These two approaches are intuitively
complementary, but their effectiveness has not yet been compared. In this
paper, we introduce a benchmark for evaluating ontology completion methods and
thoroughly analyse the strengths and weaknesses of both approaches. We find
that both approaches are indeed complementary, with hybrid strategies achieving
the best overall results. We also find that the task is highly challenging for
Large Language Models, even after fine-tuning.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.17216v1},
File          = {2403.17216v1.pdf}
}
@article{2403.16984v2,
Author        = {Hanane Kteich and Na Li and Usashi Chatterjee and Zied Bouraoui and Steven Schockaert},
Title         = {Modelling Commonsense Commonalities with Multi-Facet Concept Embeddings},
Eprint        = {2403.16984v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Concept embeddings offer a practical and efficient mechanism for injecting
commonsense knowledge into downstream tasks. Their core purpose is often not to
predict the commonsense properties of concepts themselves, but rather to
identify commonalities, i.e.\ sets of concepts which share some property of
interest. Such commonalities are the basis for inductive generalisation, hence
high-quality concept embeddings can make learning easier and more robust.
Unfortunately, standard embeddings primarily reflect basic taxonomic
categories, making them unsuitable for finding commonalities that refer to more
specific aspects (e.g.\ the colour of objects or the materials they are made
of). In this paper, we address this limitation by explicitly modelling the
different facets of interest when learning concept embeddings. We show that
this leads to embeddings which capture a more diverse range of commonsense
properties, and consistently improves results in downstream tasks such as
ultra-fine entity typing and ontology completion.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.16984v2},
File          = {2403.16984v2.pdf}
}
@article{2403.16778v1,
Author        = {Raul Palma and Bogusz Janiak and LuÃ­s Moreira de Sousa and Kathi Schleidt and TomÃ¡Å¡ ÅeznÃ­k and Fenny van Egmond and Johan Leenaars and Dimitrios Moshou and Abdul Mouazen and Peter Wilson and David Medyckyj-Scott and Alistair Ritchie and Yusuf Yigini and Ronald Vargas},
Title         = {GloSIS: The Global Soil Information System Web Ontology},
Eprint        = {2403.16778v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Established in 2012 by members of the Food and Agriculture Organisation
(FAO), the Global Soil Partnership (GSP) is a global network of stakeholders
promoting sound land and soil management practices towards a sustainable world
food system. However, soil survey largely remains a local or regional activity,
bound to heterogeneous methods and conventions. Recognising the relevance of
global and trans-national policies towards sustainable land management
practices, the GSP elected data harmonisation and exchange as one of its key
lines of action. Building upon international standards and previous work
towards a global soil data ontology, an improved domain model was eventually
developed within the GSP [54], the basis for a Global Soil Information System
(GloSIS). This work also identified the Semantic Web as a possible avenue to
operationalise the domain model. This article presents the GloSIS web ontology,
an implementation of the GloSIS domain model with the Web Ontology Language
(OWL). Thoroughly employing a host of Semantic Web standards (SOSA, SKOS,
GeoSPARQL, QUDT), GloSIS lays out not only a soil data ontology but also an
extensive set of ready-to-use code-lists for soil description and
physio-chemical analysis. Various examples are provided on the provision and
use of GloSIS-compliant linked data, showcasing the contribution of this
ontology to the discovery, exploration, integration and access of soil data.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.16778v1},
File          = {2403.16778v1.pdf}
}
@article{2403.16712v2,
Author        = {Philipp Hanisch and Markus KrÃ¶tzsch},
Title         = {Chase Termination Beyond Polynomial Time},
Eprint        = {2403.16712v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {The chase is a widely implemented approach to reason with tuple-generating
dependencies (tgds), used in data exchange, data integration, and
ontology-based query answering. However, it is merely a semi-decision
procedure, which may fail to terminate. Many decidable conditions have been
proposed for tgds to ensure chase termination, typically by forbidding some
kind of "cycle" in the chase process. We propose a new criterion that
explicitly allows some such cycles, and yet ensures termination of the standard
chase under reasonable conditions. This leads to new decidable fragments of
tgds that are not only syntactically more general but also strictly more
expressive than the fragments defined by prior acyclicity conditions. Indeed,
while known terminating fragments are restricted to PTime data complexity, our
conditions yield decidable languages for any k-ExpTime. We further refine our
syntactic conditions to obtain fragments of tgds for which an optimised chase
procedure decides query entailment in PSpace or k-ExpSpace, respectively.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.16712v2},
File          = {2403.16712v2.pdf}
}
@article{2403.15864v1,
Author        = {Yihang Zhao and Neil Vetter and Kaveh Aryan},
Title         = {Using Large Language Models for OntoClean-based Ontology Refinement},
Eprint        = {2403.15864v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper explores the integration of Large Language Models (LLMs) such as
GPT-3.5 and GPT-4 into the ontology refinement process, specifically focusing
on the OntoClean methodology. OntoClean, critical for assessing the
metaphysical quality of ontologies, involves a two-step process of assigning
meta-properties to classes and verifying a set of constraints. Manually
conducting the first step proves difficult in practice, due to the need for
philosophical expertise and lack of consensus among ontologists. By employing
LLMs with two prompting strategies, the study demonstrates that high accuracy
in the labelling process can be achieved. The findings suggest the potential
for LLMs to enhance ontology refinement, proposing the development of plugin
software for ontology tools to facilitate this integration.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.15864v1},
File          = {2403.15864v1.pdf}
}
@article{2403.15587v2,
Author        = {David Herrera-Poyatos and Cristina Zuheros and Rosana Montes and Francisco Herrera},
Title         = {Large language models for crowd decision making based on prompt design
  strategies using ChatGPT: models, analysis and challenges},
Eprint        = {2403.15587v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Social Media and Internet have the potential to be exploited as a source of
opinion to enrich Decision Making solutions. Crowd Decision Making (CDM) is a
methodology able to infer opinions and decisions from plain texts, such as
reviews published in social media platforms, by means of Sentiment Analysis.
Currently, the emergence and potential of Large Language Models (LLMs) lead us
to explore new scenarios of automatically understand written texts, also known
as natural language processing. This paper analyzes the use of ChatGPT based on
prompt design strategies to assist in CDM processes to extract opinions and
make decisions. We integrate ChatGPT in CDM processes as a flexible tool that
infer the opinions expressed in texts, providing numerical or linguistic
evaluations where the decision making models are based on the prompt design
strategies. We include a multi-criteria decision making scenario with a
category ontology for criteria. We also consider ChatGPT as an end-to-end CDM
model able to provide a general opinion and score on the alternatives. We
conduct empirical experiments on real data extracted from TripAdvisor, the
TripR-2020Large dataset. The analysis of results show a promising branch for
developing quality decision making models using ChatGPT. Finally, we discuss
the challenges of consistency, sensitivity and explainability associated to the
use of LLMs in CDM processes, raising open questions for future studies.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.15587v2},
File          = {2403.15587v2.pdf}
}
@article{2403.15504v1,
Author        = {Brandon Curtis Colelough},
Title         = {SymboSLAM: Semantic Map Generation in a Multi-Agent System},
Eprint        = {2403.15504v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Sub-symbolic artificial intelligence methods dominate the fields of
environment-type classification and Simultaneous Localisation and Mapping.
However, a significant area overlooked within these fields is solution
transparency for the human-machine interaction space, as the sub-symbolic
methods employed for map generation do not account for the explainability of
the solutions generated. This paper proposes a novel approach to
environment-type classification through Symbolic Simultaneous Localisation and
Mapping, SymboSLAM, to bridge the explainability gap. Our method for
environment-type classification observes ontological reasoning used to
synthesise the context of an environment through the features found within. We
achieve explainability within the model by presenting operators with
environment-type classifications overlayed by a semantically labelled occupancy
map of landmarks and features. We evaluate SymboSLAM with ground-truth maps of
the Canberra region, demonstrating method effectiveness. We assessed the system
through both simulations and real-world trials.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.15504v1},
File          = {2403.15504v1.pdf}
}
@article{2403.12666v1,
Author        = {Dojun Park and Sebastian PadÃ³},
Title         = {Multi-Dimensional Machine Translation Evaluation: Model Evaluation and
  Resource for Korean},
Eprint        = {2403.12666v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Almost all frameworks for the manual or automatic evaluation of machine
translation characterize the quality of an MT output with a single number. An
exception is the Multidimensional Quality Metrics (MQM) framework which offers
a fine-grained ontology of quality dimensions for scoring (such as style,
fluency, accuracy, and terminology). Previous studies have demonstrated the
feasibility of MQM annotation but there are, to our knowledge, no computational
models that predict MQM scores for novel texts, due to a lack of resources. In
this paper, we address these shortcomings by (a) providing a 1200-sentence MQM
evaluation benchmark for the language pair English-Korean and (b) reframing MT
evaluation as the multi-task problem of simultaneously predicting several MQM
scores using SOTA language models, both in a reference-based MT evaluation
setup and a reference-free quality estimation (QE) setup. We find that
reference-free setup outperforms its counterpart in the style dimension while
reference-based models retain an edge regarding accuracy. Overall, RemBERT
emerges as the most promising model. Through our evaluation, we offer an
insight into the translation quality in a more fine-grained, interpretable
manner.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.12666v1},
File          = {2403.12666v1.pdf}
}
@article{2403.11996v3,
Author        = {Markus J. Buehler},
Title         = {Accelerating Scientific Discovery with Generative Knowledge Extraction,
  Graph-Based Representation, and Multimodal Intelligent Graph Reasoning},
Eprint        = {2403.11996v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Leveraging generative Artificial Intelligence (AI), we have transformed a
dataset comprising 1,000 scientific papers into an ontological knowledge graph.
Through an in-depth structural analysis, we have calculated node degrees,
identified communities and connectivities, and evaluated clustering
coefficients and betweenness centrality of pivotal nodes, uncovering
fascinating knowledge architectures. The graph has an inherently scale-free
nature, is highly connected, and can be used for graph reasoning by taking
advantage of transitive and isomorphic properties that reveal unprecedented
interdisciplinary relationships that can be used to answer queries, identify
gaps in knowledge, propose never-before-seen material designs, and predict
material behaviors. We compute deep node embeddings for combinatorial node
similarity ranking for use in a path sampling strategy links dissimilar
concepts that have previously not been related. One comparison revealed
structural parallels between biological materials and Beethoven's 9th Symphony,
highlighting shared patterns of complexity through isomorphic mapping. In
another example, the algorithm proposed a hierarchical mycelium-based composite
based on integrating path sampling with principles extracted from Kandinsky's
'Composition VII' painting. The resulting material integrates an innovative set
of concepts that include a balance of chaos/order, adjustable porosity,
mechanical strength, and complex patterned chemical functionalization. We
uncover other isomorphisms across science, technology and art, revealing a
nuanced ontology of immanence that reveal a context-dependent heterarchical
interplay of constituents. Graph-based generative AI achieves a far higher
degree of novelty, explorative capacity, and technical detail, than
conventional approaches and establishes a widely useful framework for
innovation by revealing hidden connections.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.11996v3},
File          = {2403.11996v3.pdf}
}
@article{2403.11669v1,
Author        = {Peter Å vec and Å tefan Balogh and Martin Homola and JÃ¡n KÄ¾uka and TomÃ¡Å¡ BistÃ¡k},
Title         = {Semantic Data Representation for Explainable Windows Malware Detection
  Models},
Eprint        = {2403.11669v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {Ontologies are a standard tool for creating semantic schemata in many
knowledge intensive domains of human interest. They are becoming increasingly
important also in the areas that have been until very recently dominated by
subsymbolic knowledge representation and machine-learning (ML) based data
processing. One such area is information security, and specifically, malware
detection. We thus propose PE Malware Ontology that offers a reusable semantic
schema for Portable Executable (PE - the Windows binary format) malware files.
This ontology is inspired by the structure of the EMBER dataset, which focuses
on the static malware analysis of PE files. With this proposal, we hope to
provide a unified semantic representation for the existing and future
PE-malware datasets and facilitate the application of symbolic, neuro-symbolic,
or otherwise explainable approaches in the PE-malware-detection domain, which
may produce interpretable results described by the terms defined in our
ontology. In addition, we also publish semantically treated EMBER data,
including fractional datasets, to support the reproducibility of experiments on
EMBER. We supplement our work with a preliminary case study, conducted using
concept learning, to show the general feasibility of our approach. While we
were not able to match the precision of the state-of-the-art ML tools, the
learned malware discriminators were interesting and highly interpretable.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.11669v1},
File          = {2403.11669v1.pdf}
}
@article{2403.10822v3,
Author        = {Simon A. Lee and Timothy Lindsey},
Title         = {Can Large Language Models abstract Medical Coded Language?},
Eprint        = {2403.10822v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have become a pivotal research area, potentially
making beneficial contributions in fields like healthcare where they can
streamline automated billing and decision support. However, the frequent use of
specialized coded languages like ICD-10, which are regularly updated and
deviate from natural language formats, presents potential challenges for LLMs
in creating accurate and meaningful latent representations. This raises
concerns among healthcare professionals about potential inaccuracies or
``hallucinations" that could result in the direct impact of a patient.
Therefore, this study evaluates whether large language models (LLMs) are aware
of medical code ontologies and can accurately generate names from these codes.
We assess the capabilities and limitations of both general and
biomedical-specific generative models, such as GPT, LLaMA-2, and Meditron,
focusing on their proficiency with domain-specific terminologies. While the
results indicate that LLMs struggle with coded language, we offer insights on
how to adapt these models to reason more effectively.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.10822v3},
File          = {2403.10822v3.pdf}
}
@article{2403.08536v1,
Author        = {Francesco Dibitonto and Fabio Garcea and AndrÃ© Panisson and Alan Perotti and Lia Morra},
Title         = {HOLMES: HOLonym-MEronym based Semantic inspection for Convolutional
  Image Classifiers},
Eprint        = {2403.08536v1},
DOI           = {10.1007/978-3-031-44067-0_25},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Convolutional Neural Networks (CNNs) are nowadays the model of choice in
Computer Vision, thanks to their ability to automatize the feature extraction
process in visual tasks. However, the knowledge acquired during training is
fully subsymbolic, and hence difficult to understand and explain to end users.
In this paper, we propose a new technique called HOLMES (HOLonym-MEronym based
Semantic inspection) that decomposes a label into a set of related concepts,
and provides component-level explanations for an image classification model.
Specifically, HOLMES leverages ontologies, web scraping and transfer learning
to automatically construct meronym (parts)-based detectors for a given holonym
(class). Then, it produces heatmaps at the meronym level and finally, by
probing the holonym CNN with occluded images, it highlights the importance of
each part on the classification output. Compared to state-of-the-art saliency
methods, HOLMES takes a step further and provides information about both where
and what the holonym CNN is looking at, without relying on densely annotated
datasets and without forcing concepts to be associated to single computational
units. Extensive experimental evaluation on different categories of objects
(animals, tools and vehicles) shows the feasibility of our approach. On
average, HOLMES explanations include at least two meronyms, and the ablation of
a single meronym roughly halves the holonym model confidence. The resulting
heatmaps were quantitatively evaluated using the
deletion/insertion/preservation curves. All metrics were comparable to those
achieved by GradCAM, while offering the advantage of further decomposing the
heatmap in human-understandable concepts, thus highlighting both the relevance
of meronyms to object classification, as well as HOLMES ability to capture it.
The code is available at https://github.com/FrancesC0de/HOLMES.},
Year          = {2024},
Month         = {Mar},
Note          = {Longo, L. (eds) Explainable Artificial Intelligence. xAI 2023.
  Communications in Computer and Information Science, vol 1902. Springer, Cham},
Url           = {http://arxiv.org/abs/2403.08536v1},
File          = {2403.08536v1.pdf}
}
@article{2403.08438v2,
Author        = {Tobias Hille and Maximilian Stubbemann and Tom Hanika},
Title         = {Reproducibility and Geometric Intrinsic Dimensionality: An Investigation
  on Graph Neural Network Research},
Eprint        = {2403.08438v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Difficulties in replication and reproducibility of empirical evidences in
machine learning research have become a prominent topic in recent years.
Ensuring that machine learning research results are sound and reliable requires
reproducibility, which verifies the reliability of research findings using the
same code and data. This promotes open and accessible research, robust
experimental workflows, and the rapid integration of new findings. Evaluating
the degree to which research publications support these different aspects of
reproducibility is one goal of the present work. For this we introduce an
ontology of reproducibility in machine learning and apply it to methods for
graph neural networks. Building on these efforts we turn towards another
critical challenge in machine learning, namely the curse of dimensionality,
which poses challenges in data collection, representation, and analysis, making
it harder to find representative data and impeding the training and inference
processes. Using the closely linked concept of geometric intrinsic dimension we
investigate to which extend the used machine learning models are influenced by
the intrinsic dimension of the data sets they are trained on.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.08438v2},
File          = {2403.08438v2.pdf}
}
@article{2403.08345v1,
Author        = {Vamsi Krishna Kommineni and Birgitta KÃ¶nig-Ries and Sheeba Samuel},
Title         = {From human experts to machines: An LLM supported approach to ontology
  and knowledge graph construction},
Eprint        = {2403.08345v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The conventional process of building Ontologies and Knowledge Graphs (KGs)
heavily relies on human domain experts to define entities and relationship
types, establish hierarchies, maintain relevance to the domain, fill the ABox
(or populate with instances), and ensure data quality (including amongst others
accuracy and completeness). On the other hand, Large Language Models (LLMs)
have recently gained popularity for their ability to understand and generate
human-like natural language, offering promising ways to automate aspects of
this process. This work explores the (semi-)automatic construction of KGs
facilitated by open-source LLMs. Our pipeline involves formulating competency
questions (CQs), developing an ontology (TBox) based on these CQs, constructing
KGs using the developed ontology, and evaluating the resultant KG with minimal
to no involvement of human experts. We showcase the feasibility of our
semi-automated pipeline by creating a KG on deep learning methodologies by
exploiting scholarly publications. To evaluate the answers generated via
Retrieval-Augmented-Generation (RAG) as well as the KG concepts automatically
extracted using LLMs, we design a judge LLM, which rates the generated content
based on ground truth. Our findings suggest that employing LLMs could
potentially reduce the human effort involved in the construction of KGs,
although a human-in-the-loop approach is recommended to evaluate automatically
generated KGs.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.08345v1},
File          = {2403.08345v1.pdf}
}
@article{2403.08264v1,
Author        = {Raza Nowrozy and Khandakar Ahmed and Hua Wang},
Title         = {GPT, Ontology, and CAABAC: A Tripartite Personalized Access Control
  Model Anchored by Compliance, Context and Attribute},
Eprint        = {2403.08264v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CY},
Abstract      = {As digital healthcare evolves, the security of electronic health records
(EHR) becomes increasingly crucial. This study presents the GPT-Onto-CAABAC
framework, integrating Generative Pretrained Transformer (GPT), medical-legal
ontologies and Context-Aware Attribute-Based Access Control (CAABAC) to enhance
EHR access security. Unlike traditional models, GPT-Onto-CAABAC dynamically
interprets policies and adapts to changing healthcare and legal environments,
offering customized access control solutions. Through empirical evaluation,
this framework is shown to be effective in improving EHR security by accurately
aligning access decisions with complex regulatory and situational requirements.
The findings suggest its broader applicability in sectors where access control
must meet stringent compliance and adaptability standards.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.08264v1},
File          = {2403.08264v1.pdf}
}
@article{2403.06586v2,
Author        = {Luca Arrotta and Claudio Bettini and Gabriele Civitarese and Michele Fiori},
Title         = {ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity
  Recognition Models},
Eprint        = {2403.06586v2},
DOI           = {10.1145/3675094.3679000},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Context-aware Human Activity Recognition (HAR) is a hot research area in
mobile computing, and the most effective solutions in the literature are based
on supervised deep learning models. However, the actual deployment of these
systems is limited by the scarcity of labeled data that is required for
training. Neuro-Symbolic AI (NeSy) provides an interesting research direction
to mitigate this issue, by infusing common-sense knowledge about human
activities and the contexts in which they can be performed into HAR deep
learning classifiers. Existing NeSy methods for context-aware HAR rely on
knowledge encoded in logic-based models (e.g., ontologies) whose design,
implementation, and maintenance to capture new activities and contexts require
significant human engineering efforts, technical knowledge, and domain
expertise. Recent works show that pre-trained Large Language Models (LLMs)
effectively encode common-sense knowledge about human activities. In this work,
we propose ContextGPT: a novel prompt engineering approach to retrieve from
LLMs common-sense knowledge about the relationship between human activities and
the context in which they are performed. Unlike ontologies, ContextGPT requires
limited human effort and expertise. An extensive evaluation carried out on two
public datasets shows how a NeSy model obtained by infusing common-sense
knowledge from ContextGPT is effective in data scarcity scenarios, leading to
similar (and sometimes better) recognition rates than logic-based approaches
with a fraction of the effort.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.06586v2},
File          = {2403.06586v2.pdf}
}
@article{2403.05921v2,
Author        = {Bohui Zhang and Valentina Anita Carriero and Katrin Schreiberhuber and Stefani Tsaneva and LucÃ­a SÃ¡nchez GonzÃ¡lez and Jongmo Kim and Jacopo de Berardinis},
Title         = {OntoChat: a Framework for Conversational Ontology Engineering using
  Language Models},
Eprint        = {2403.05921v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontology engineering (OE) in large projects poses a number of challenges
arising from the heterogeneous backgrounds of the various stakeholders, domain
experts, and their complex interactions with ontology designers. This
multi-party interaction often creates systematic ambiguities and biases from
the elicitation of ontology requirements, which directly affect the design,
evaluation and may jeopardise the target reuse. Meanwhile, current OE
methodologies strongly rely on manual activities (e.g., interviews, discussion
pages). After collecting evidence on the most crucial OE activities, we
introduce \textbf{OntoChat}, a framework for conversational ontology
engineering that supports requirement elicitation, analysis, and testing. By
interacting with a conversational agent, users can steer the creation of user
stories and the extraction of competency questions, while receiving
computational support to analyse the overall requirements and test early
versions of the resulting ontologies. We evaluate OntoChat by replicating the
engineering of the Music Meta Ontology, and collecting preliminary metrics on
the effectiveness of each component from users. We release all code at
https://github.com/King-s-Knowledge-Graph-Lab/OntoChat.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.05921v2},
File          = {2403.05921v2.pdf}
}
@article{2403.05920v1,
Author        = {Syed I. Munzir and Daniel B. Hier and Michael D. Carrithers},
Title         = {High Throughput Phenotyping of Physician Notes with Large Language and
  Hybrid NLP Models},
Eprint        = {2403.05920v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Deep phenotyping is the detailed description of patient signs and symptoms
using concepts from an ontology. The deep phenotyping of the numerous physician
notes in electronic health records requires high throughput methods. Over the
past thirty years, progress toward making high throughput phenotyping feasible.
In this study, we demonstrate that a large language model and a hybrid NLP
model (combining word vectors with a machine learning classifier) can perform
high throughput phenotyping on physician notes with high accuracy. Large
language models will likely emerge as the preferred method for high throughput
deep phenotyping of physician notes.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.05920v1},
File          = {2403.05920v1.pdf}
}
@article{2403.05221v1,
Author        = {Wolfgang HÃ¶hl},
Title         = {Understanding Hybrid Spaces: Designing a Spacetime Model to Represent
  Dynamic Topologies of Hybrid Spaces},
Eprint        = {2403.05221v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CY},
Abstract      = {This paper develops a spatiotemporal model for the visualization of dynamic
topologies of hybrid spaces. The visualization of spatiotemporal data is a
well-known problem, for example in digital twins in urban planning. There is
also a lack of a basic ontology for understanding hybrid spaces. The developed
spatiotemporal model has three levels: a level of places and media types, a
level of perception and a level of time and interaction. Existing concepts and
types of representation of hybrid spaces are presented. The space-time model is
tested on the basis of an art exhibition. Two hypotheses guide the accompanying
online survey: (A) there are correlations between media use (modality), the
participants' interactions (creativity) and their perception (understanding of
art) and (B) individual parameters (demographic data, location and situation,
individual knowledge) influence perception (understanding of art). The range,
the number of interactions and the response rate were also evaluated.
  The online survey generally showed a positive correlation between media use
(modality) and individual activity (creativity). However, due to the low
participation rate ($P_{TN} = 14$), the survey is unfortunately not very
representative. Various dynamic topologies of hybrid spaces were successfully
visualized. The joint representation of real and virtual places and media types
conveys a new basic understanding of place, range and urban density.
Relationships between modality, Mobility and communicative interaction become
visible. The current phenomenon of multilocality has been successfully mapped.
The space-time model enables more precise class and structure formation, for
example in the development of digital twins. Dynamic topologies of hybrid
spaces, such as in social media, at events or in urban development, can thus be
better represented and compared.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.05221v1},
File          = {2403.05221v1.pdf}
}
@article{2403.04326v1,
Author        = {Zhongjun Ni and Chi Zhang and Magnus Karlsson and Shaofang Gong},
Title         = {Edge-based Parametric Digital Twins for Intelligent Building Indoor
  Climate Modeling},
Eprint        = {2403.04326v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {eess.SY},
Abstract      = {Digital transformation in the built environment generates vast data for
developing data-driven models to optimize building operations. This study
presents an integrated solution utilizing edge computing, digital twins, and
deep learning to enhance the understanding of climate in buildings. Parametric
digital twins, created using an ontology, ensure consistent data representation
across diverse service systems equipped by different buildings. Based on
created digital twins and collected data, deep learning methods are employed to
develop predictive models for identifying patterns in indoor climate and
providing insights. Both the parametric digital twin and deep learning models
are deployed on edge for low latency and privacy compliance. As a
demonstration, a case study was conducted in a historic building in
\"Osterg\"otland, Sweden, to compare the performance of five deep learning
architectures. The results indicate that the time-series dense encoder model
exhibited strong competitiveness in performing multi-horizon forecasts of
indoor temperature and relative humidity with low computational costs.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.04326v1},
File          = {2403.04326v1.pdf}
}
@article{2403.02586v1,
Author        = {Zefan Cai and Po-Nien Kung and Ashima Suvarna and Mingyu Derek Ma and Hritik Bansal and Baobao Chang and P. Jeffrey Brantingham and Wei Wang and Nanyun Peng},
Title         = {Improving Event Definition Following For Zero-Shot Event Detection},
Eprint        = {2403.02586v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Existing approaches on zero-shot event detection usually train models on
datasets annotated with known event types, and prompt them with unseen event
definitions. These approaches yield sporadic successes, yet generally fall
short of expectations. In this work, we aim to improve zero-shot event
detection by training models to better follow event definitions. We hypothesize
that a diverse set of event types and definitions are the key for models to
learn to follow event definitions while existing event extraction datasets
focus on annotating many high-quality examples for a few event types. To verify
our hypothesis, we construct an automatically generated Diverse Event
Definition (DivED) dataset and conduct comparative studies. Our experiments
reveal that a large number of event types (200) and diverse event definitions
can significantly boost event extraction performance; on the other hand, the
performance does not scale with over ten examples per event type. Beyond
scaling, we incorporate event ontology information and hard-negative samples
during training, further boosting the performance. Based on these findings, we
fine-tuned a LLaMA-2-7B model on our DivED dataset, yielding performance that
surpasses SOTA large language models like GPT-3.5 across three open benchmarks
on zero-shot event detection.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.02586v1},
File          = {2403.02586v1.pdf}
}
@article{2403.01630v1,
Author        = {Ryan Wisnesky and Daniel Filonik},
Title         = {Relational to RDF Data Migration by Query Co-Evaluation},
Eprint        = {2403.01630v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {In this paper we define a new algorithm to convert an input relational
database to an output set of RDF triples. The algorithm can be used to e.g.
load CSV data into a financial OWL ontology such as FIBO. The algorithm takes
as input a set of relational conjunctive (select-from-where) queries, one for
each input table, from the three column (subject, predicate, object) output RDF
schema to the input table's relational schema. The algorithm's output is the
only set of RDF triples for which a unique round-trip of the input data under
the relational queries exists. The output may contain blank nodes, is unique up
to unique isomorphism, and can be obtained using elementary formal methods
(equational theorem proving and term model construction specifically). We also
describe how (generalized) homomorphisms between graphs can be used to write
such relational conjunctive (select-from-where) queries, which, due to the lack
of structure in the three-column RDF schema, tend to be large in practice. We
demonstrate examples of both the algorithm and mapping language on the FIBO
financial ontology.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.01630v1},
File          = {2403.01630v1.pdf}
}
@article{2403.00953v4,
Author        = {Lang Cao and Jimeng Sun and Adam Cross},
Title         = {AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge
  Graph Construction Based on Ontologies-enhanced Large Language Models},
Eprint        = {2403.00953v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Rare diseases affect millions worldwide but often face limited research focus
due to their low prevalence. This results in prolonged diagnoses and a lack of
approved therapies. Recent advancements in Large Language Models (LLMs) have
shown promise in automating the extraction of medical information, offering
potential to improve medical diagnosis and management. However, most LLMs lack
professional medical knowledge, especially concerning rare diseases, and
struggle to handle the latest rare disease information. They also cannot
effectively manage rare disease data and are not directly suitable for
diagnosis and management tasks. Our objective is to create an end-to-end system
called AutoRD, which automates the extraction of information from medical texts
about rare diseases, focusing on entities and their relations. AutoRD
integrates up-to-date structured knowledge and demonstrates superior
performance in rare disease extraction tasks. We conduct various experiments to
evaluate AutoRD's performance, aiming to surpass common LLMs and traditional
methods.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.00953v4},
File          = {2403.00953v4.pdf}
}
@article{2403.00685v2,
Author        = {Gabriele Sacco and Loris Bozzato and Oliver Kutz},
Title         = {Know your exceptions: Towards an Ontology of Exceptions in Knowledge
  Representation},
Eprint        = {2403.00685v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Defeasible reasoning is a kind of reasoning where some generalisations may
not be valid in all circumstances, that is general conclusions may fail in some
cases. Various formalisms have been developed to model this kind of reasoning,
which is characteristic of common-sense contexts. However, it is not easy for a
modeller to choose among these systems the one that better fits its domain from
an ontological point of view. In this paper we first propose a framework based
on the notions of exceptionality and defeasibility in order to be able to
compare formalisms and reveal their ontological commitments. Then, we apply
this framework to compare four systems, showing the differences that may occur
from an ontological perspective.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.00685v2},
File          = {2403.00685v2.pdf}
}
@article{2402.18715v1,
Author        = {Andrew Eells and Brandon Dave and Pascal Hitzler and Cogan Shimizu},
Title         = {Commonsense Ontology Micropatterns},
Eprint        = {2402.18715v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The previously introduced Modular Ontology Modeling methodology (MOMo)
attempts to mimic the human analogical process by using modular patterns to
assemble more complex concepts. To support this, MOMo organizes organizes
ontology design patterns into design libraries, which are programmatically
queryable, to support accelerated ontology development, for both human and
automated processes. However, a major bottleneck to large-scale deployment of
MOMo is the (to-date) limited availability of ready-to-use ontology design
patterns. At the same time, Large Language Models have quickly become a source
of common knowledge and, in some cases, replacing search engines for questions.
In this paper, we thus present a collection of 104 ontology design patterns
representing often occurring nouns, curated from the common-sense knowledge
available in LLMs, organized into a fully-annotated modular ontology design
library ready for use with MOMo.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.18715v1},
File          = {2402.18715v1.pdf}
}
@article{2402.18312v2,
Author        = {Subhabrata Dutta and Joykirat Singh and Soumen Chakrabarti and Tanmoy Chakraborty},
Title         = {How to think step-by-step: A mechanistic understanding of
  chain-of-thought reasoning},
Eprint        = {2402.18312v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Despite superior reasoning prowess demonstrated by Large Language Models
(LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails
around the internal mechanisms of the models that facilitate CoT generation.
This work investigates the neural sub-structures within LLMs that manifest CoT
reasoning from a mechanistic point of view. From an analysis of Llama-2 7B
applied to multistep reasoning over fictional ontologies, we demonstrate that
LLMs deploy multiple parallel pathways of answer generation for step-by-step
reasoning. These parallel pathways provide sequential answers from the input
question context as well as the generated CoT. We observe a functional rift in
the middle layers of the LLM. Token representations in the initial half remain
strongly biased towards the pretraining prior, with the in-context prior taking
over in the later half. This internal phase shift manifests in different
functional components: attention heads that write the answer token appear in
the later half, attention heads that move information along ontological
relationships appear in the initial half, and so on. To the best of our
knowledge, this is the first attempt towards mechanistic investigation of CoT
reasoning in LLMs.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.18312v2},
File          = {2402.18312v2.pdf}
}
@article{2402.18258v1,
Author        = {Hongshen Xu and Ruisheng Cao and Su Zhu and Sheng Jiang and Hanchong Zhang and Lu Chen and Kai Yu},
Title         = {A BiRGAT Model for Multi-intent Spoken Language Understanding with
  Hierarchical Semantic Frames},
Eprint        = {2402.18258v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Previous work on spoken language understanding (SLU) mainly focuses on
single-intent settings, where each input utterance merely contains one user
intent. This configuration significantly limits the surface form of user
utterances and the capacity of output semantics. In this work, we first propose
a Multi-Intent dataset which is collected from a realistic in-Vehicle dialogue
System, called MIVS. The target semantic frame is organized in a 3-layer
hierarchical structure to tackle the alignment and assignment problems in
multi-intent cases. Accordingly, we devise a BiRGAT model to encode the
hierarchy of ontology items, the backbone of which is a dual relational graph
attention network. Coupled with the 3-way pointer-generator decoder, our method
outperforms traditional sequence labeling and classification-based schemes by a
large margin.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.18258v1},
File          = {2402.18258v1.pdf}
}
@article{2402.17897v2,
Author        = {Hang Dong and Jiaoyan Chen and Yuan He and Yongsheng Gao and Ian Horrocks},
Title         = {A Language Model based Framework for New Concept Placement in Ontologies},
Eprint        = {2402.17897v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We investigate the task of inserting new concepts extracted from texts into
an ontology using language models. We explore an approach with three steps:
edge search which is to find a set of candidate locations to insert (i.e.,
subsumptions between concepts), edge formation and enrichment which leverages
the ontological structure to produce and enhance the edge candidates, and edge
selection which eventually locates the edge to be placed into. In all steps, we
propose to leverage neural methods, where we apply embedding-based methods and
contrastive learning with Pre-trained Language Models (PLMs) such as BERT for
edge search, and adapt a BERT fine-tuning-based multi-label Edge-Cross-encoder,
and Large Language Models (LLMs) such as GPT series, FLAN-T5, and Llama 2, for
edge selection. We evaluate the methods on recent datasets created using the
SNOMED CT ontology and the MedMentions entity linking benchmark. The best
settings in our framework use fine-tuned PLM for search and a multi-label
Cross-encoder for selection. Zero-shot prompting of LLMs is still not adequate
for the task, and we propose explainable instruction tuning of LLMs for
improved performance. Our study shows the advantages of PLMs and highlights the
encouraging performance of LLMs that motivates future studies.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.17897v2},
File          = {2402.17897v2.pdf}
}
@article{2402.17066v2,
Author        = {Per Ãstborn},
Title         = {Born's rule from epistemic assumptions},
Eprint        = {2402.17066v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {quant-ph},
Abstract      = {Born's rule is the recipe for calculating probabilities from quantum
mechanical amplitudes. There is no generally accepted derivation of Born's rule
from first principles. In this paper, it is motivated from assumptions that
link the ontological content of a proper physical model to the epistemic
conditions of the experimental context. More precisely, it is assumed that all
knowable distinctions should correspond to distinctions in a proper model. This
principle of "ontological completeness" means, for example, that the
probabilistic treatment of the double slit experiment with and without path
information should differ. Further, it is assumed that the model should rely
only on knowable ontological elements, and that failure to fulfill this
principle of "ontological minimalism" gives rise to wrong predictions.
Consequently, probabilities should be assigned only to observable experimental
outcomes. Also, the method to calculate such probabilities should not rely on
the existence of a precise path of the observed object if this path is not
knowable. A similar principle was promoted by Born, even though he did not
apply it to probability. Another crucial assumption is that the proper rule to
calculate probabilities should be generally valid. It should be applicable in
all experimental contexts, regardless the setup that determines which
attributes of the studied object are observed, together with the probability to
observe each of the associated attribute values. There is no need to refer to
the Hilbert space structure of quantum mechanics in the present treatment.
Rather, some elements of this structure emerge from the analysis.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.17066v2},
File          = {2402.17066v2.pdf}
}
@article{2402.16278v3,
Author        = {Yukihiro Shiraishi and Ken Kaneiwa},
Title         = {A Self-matching Training Method with Annotation Embedding Models for
  Ontology Subsumption Prediction},
Eprint        = {2402.16278v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Recently, ontology embeddings representing entities in a low-dimensional
space have been proposed for ontology completion. However, the ontology
embeddings for concept subsumption prediction do not address the difficulties
of similar and isolated entities and fail to extract the global information of
annotation axioms from an ontology. In this paper, we propose a self-matching
training method for the two ontology embedding models: Inverted-index Matrix
Embedding (InME) and Co-occurrence Matrix Embedding (CoME). The two embeddings
capture the global and local information in annotation axioms by means of the
occurring locations of each word in a set of axioms and the co-occurrences of
words in each axiom. The self-matching training method increases the robustness
of the concept subsumption prediction when predicted superclasses are similar
to subclasses and are isolated to other entities in an ontology. Our evaluation
experiments show that the self-matching training method with InME outperforms
the existing ontology embeddings for the GO and FoodOn ontologies and that the
method with the concatenation of CoME and OWL2Vec* outperforms them for the
HeLiS ontology.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.16278v3},
File          = {2402.16278v3.pdf}
}
@article{2402.14901v2,
Author        = {Wendi Zhou and Tianyi Li and Pavlos Vougiouklis and Mark Steedman and Jeff Z. Pan},
Title         = {A Usage-centric Take on Intent Understanding in E-Commerce},
Eprint        = {2402.14901v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Identifying and understanding user intents is a pivotal task for E-Commerce.
Despite its essential role in product recommendation and business user
profiling analysis, intent understanding has not been consistently defined or
accurately benchmarked. In this paper, we focus on predicative user intents as
"how a customer uses a product", and pose intent understanding as a natural
language reasoning task, independent of product ontologies. We identify two
weaknesses of FolkScope, the SOTA E-Commerce Intent Knowledge Graph:
category-rigidity and property-ambiguity. They limit its ability to strongly
align user intents with products having the most desirable property, and to
recommend useful products across diverse categories. Following these
observations, we introduce a Product Recovery Benchmark featuring a novel
evaluation framework and an example dataset. We further validate the above
FolkScope weaknesses on this benchmark. Our code and dataset are available at
https://github.com/stayones/Usgae-Centric-Intent-Understanding.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.14901v2},
File          = {2402.14901v2.pdf}
}
@article{2405.01549v1,
Author        = {Sabah Al-Fedaghi},
Title         = {Exploring Conceptual Modeling Metaphysics: Existence Containers,
  Leibniz's Monads and Avicenna's Essence},
Eprint        = {2405.01549v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {Requirement specifications in software engineering involve developing a
conceptual model of a target domain. The model is based on ontological
exploration of things in reality. Many things in such a process closely tie to
problems in metaphysics, the field of inquiry of what reality fundamentally is.
According to some researchers, metaphysicians are trying to develop an account
of the world that properly conceptualizes the way it is, and software design is
similar. Notions such as classes, object orientation, properties,
instantiation, algorithms, etc. are metaphysical concepts developed many years
ago. Exploring the metaphysics of such notions aims to establish quality
assurance though some objective foundation not subject to misapprehensions and
conventions. Much metaphysical work might best be understood as a
model-building process. Here, a model is viewed as a hypothetical structure
that we describe and investigate to understand more complex, real-world
systems. The purpose of this paper is to enhance understanding of the
metaphysical origins of conceptual modeling as exemplified by a specific
proposed high-level model called thinging machines (TMs). The focus is on
thimacs (things/machine) as a single category of TM modeling in the context of
a two-phase world of staticity and dynamics. The general idea of this reality
has been inspired by Deleuze s the virtual and related to the classical notions
of Leibniz's monads and Avicenna's essence. The analysis of TMs leads to
several interesting results about a thimac s nature at the static and existence
levels.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2405.01549v1},
File          = {2405.01549v1.pdf}
}
@article{2402.13140v1,
Author        = {Laurens Walleghem and Shashaank Khanna and Rutvij Bhavsar},
Title         = {Comment on a no-go theorem for $Ï$-ontic models},
Eprint        = {2402.13140v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {quant-ph},
Abstract      = {In a recent paper [Carcassi, Oldofredi and Aidala, Found Phys 54, 14 (2024)]
it is claimed that the whole Harrigan--Spekkens framework of ontological models
is inconsistent with quantum theory. They show this by showing that all pure
quantum states in $\psi$-ontic models must be orthogonal. In this note, we
identify some crucial mistakes in their argument to the extent that the main
claim is incorrect.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.13140v1},
File          = {2402.13140v1.pdf}
}
@article{2402.12282v1,
Author        = {Zehra Melce HÃ¼sÃ¼nbeyi and Tatjana Scheffler},
Title         = {Ontology Enhanced Claim Detection},
Eprint        = {2402.12282v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We propose an ontology enhanced model for sentence based claim detection. We
fused ontology embeddings from a knowledge base with BERT sentence embeddings
to perform claim detection for the ClaimBuster and the NewsClaims datasets. Our
ontology enhanced approach showed the best results with these small-sized
unbalanced datasets, compared to other statistical and neural machine learning
models. The experiments demonstrate that adding domain specific features
(either trained word embeddings or knowledge graph metadata) can improve
traditional ML methods. In addition, adding domain knowledge in the form of
ontology embeddings helps avoid the bias encountered in neural network based
models, for example the pure BERT model bias towards larger classes in our
small corpus.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.12282v1},
File          = {2402.12282v1.pdf}
}
@article{2404.03662v1,
Author        = {Drishti Goel and Fiza Husain and Aditya Singh and Supriyo Ghosh and Anjaly Parayil and Chetan Bansal and Xuchao Zhang and Saravan Rajmohan},
Title         = {X-lifecycle Learning for Cloud Incident Management using LLMs},
Eprint        = {2404.03662v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.NI},
Abstract      = {Incident management for large cloud services is a complex and tedious process
and requires significant amount of manual efforts from on-call engineers
(OCEs). OCEs typically leverage data from different stages of the software
development lifecycle [SDLC] (e.g., codes, configuration, monitor data, service
properties, service dependencies, trouble-shooting documents, etc.) to generate
insights for detection, root causing and mitigating of incidents. Recent
advancements in large language models [LLMs] (e.g., ChatGPT, GPT-4, Gemini)
created opportunities to automatically generate contextual recommendations to
the OCEs assisting them to quickly identify and mitigate critical issues.
However, existing research typically takes a silo-ed view for solving a certain
task in incident management by leveraging data from a single stage of SDLC. In
this paper, we demonstrate that augmenting additional contextual data from
different stages of SDLC improves the performance of two critically important
and practically challenging tasks: (1) automatically generating root cause
recommendations for dependency failure related incidents, and (2) identifying
ontology of service monitors used for automatically detecting incidents. By
leveraging 353 incident and 260 monitor dataset from Microsoft, we demonstrate
that augmenting contextual information from different stages of the SDLC
improves the performance over State-of-The-Art methods.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2404.03662v1},
File          = {2404.03662v1.pdf}
}
@article{2402.10967v1,
Author        = {JosÃ© Alberto BenÃ­tez-Andrades and IsaÃ­as GarcÃ­a-RodrÃ­guez and Carmen Benavides and HÃ©ctor Alaiz-MoretÃ³n and Alejandro RodrÃ­guez-GonzÃ¡lez},
Title         = {Social network analysis for personalized characterization and risk
  assessment of alcohol use disorders in adolescents using semantic
  technologies},
Eprint        = {2402.10967v1},
DOI           = {10.1016/j.future.2020.01.002},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Alcohol Use Disorder (AUD) is a major concern for public health organizations
worldwide, especially as regards the adolescent population. The consumption of
alcohol in adolescents is known to be influenced by seeing friends and even
parents drinking alcohol. Building on this fact, a number of studies into
alcohol consumption among adolescents have made use of Social Network Analysis
(SNA) techniques to study the different social networks (peers, friends,
family, etc.) with whom the adolescent is involved. These kinds of studies need
an initial phase of data gathering by means of questionnaires and a subsequent
analysis phase using the SNA techniques. The process involves a number of
manual data handling stages that are time consuming and error-prone. The use of
knowledge engineering techniques (including the construction of a domain
ontology) to represent the information, allows the automation of all the
activities, from the initial data collection to the results of the SNA study.
This paper shows how a knowledge model is constructed, and compares the results
obtained using the traditional method with this, fully automated model,
detailing the main advantages of the latter. In the case of the SNA analysis,
the validity of the results obtained with the knowledge engineering approach
are compared to those obtained manually using the UCINET, Cytoscape, Pajek and
Gephi to test the accuracy of the knowledge model.},
Year          = {2024},
Month         = {Feb},
Note          = {Future Generation Computer Systems, Volume 106, May 2020, Pages
  154-170},
Url           = {http://arxiv.org/abs/2402.10967v1},
File          = {2402.10967v1.pdf}
}
@article{2402.07351v1,
Author        = {Valentina Alberti and Cinzia Cocco and Sergio Consoli and Valentina Montalto and Francesco Panella},
Title         = {Ontology Engineering to Model the European Cultural Heritage: The Case
  of Cultural Gems},
Eprint        = {2402.07351v1},
DOI           = {10.1007/978-981-99-3236-8_1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CY},
Abstract      = {Cultural gems is a web application conceived by the European Commission's
Joint Research Centre (DG JRC), which aims at engaging people and organisations
across Europe to create a unique repository of cultural and creative places.
The main goal is to provide a vision of European culture in order to strengthen
a sense of identity within a single European cultural realm. Cultural gems maps
more than 130,000 physical places in over 300 European cities and towns, and
since 2020 it also lists online cultural initiatives. The new release aims,
among other, to increase the interoperability of the application. At this
purpose, we provide an overview on the current development of an ontology for
Cultural gems used to map cultural heritage in European cities by using Linked
Open Data (LOD) standards, and making the data FAIR, that is Findable,
Accessible, Interoperable, and Reusable. We provide an overview of the
methodology, presenting the structure of the ontology, and the services and
tools we are currently building on top.},
Year          = {2024},
Month         = {Feb},
Note          = {Lecture Notes in Networks and Systems, vol 696, Springer Nature
  (2023)},
Url           = {http://arxiv.org/abs/2402.07351v1},
File          = {2402.07351v1.pdf}
}
@article{2402.07335v1,
Author        = {Sergio Consoli and Valentina Alberti and Cinzia Cocco and Francesco Panella and Valentina Montalto},
Title         = {Cultural gems linked open data: Mapping culture and intangible heritage
  in European cities},
Eprint        = {2402.07335v1},
DOI           = {10.1016/j.dib.2023.109375},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {The recovery and resilience of the cultural and creative sectors after the
COVID-19 pandemic is a current topic with priority for the European Commission.
Cultural gems is a crowdsourced web platform managed by the Joint Research
Centre of the European Commission aimed at creating community-led maps as well
as a common repository for cultural and creative places across European cities
and towns. More than 130,000 physical locations and online cultural activities
in more than 300 European cities and towns are currently tracked by the
application. The main objective of Cultural gems consists in raising a holistic
vision of European culture, reinforcing a sense of belonging to a common
European cultural space. This data article describes the ontology developed for
Cultural gems, adopted to represent the domain of knowledge of the application
by means of FAIR (Findable, Accessible, Interoperable, Reusable) principles and
following the paradigms of Linked Open Data (LOD). We provide an overview of
this dataset, and describe the ontology model, along with the services used to
access and consume the data.},
Year          = {2024},
Month         = {Feb},
Note          = {Data in Brief, Volume 49, 2023, 109375},
Url           = {http://arxiv.org/abs/2402.07335v1},
File          = {2402.07335v1.pdf}
}
@article{2402.07148v2,
Author        = {Eric L. Buehler and Markus J. Buehler},
Title         = {X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for
  Large Language Models with Applications in Protein Mechanics and Molecular
  Design},
Eprint        = {2402.07148v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cond-mat.soft},
Abstract      = {We report a mixture of expert strategy to create fine-tuned large language
models using a deep layer-wise token-level approach based on low-rank
adaptation (LoRA). Starting with a set of pre-trained LoRA adapters, our gating
strategy uses the hidden states to dynamically mix adapted layers, allowing the
resulting X-LoRA model to draw upon different capabilities and create
never-before-used deep layer-wise combinations to solve tasks. The design is
inspired by the biological principles of universality and diversity, where
neural network building blocks are reused in different hierarchical
manifestations. Hence, the X-LoRA model can be easily implemented for any
existing large language model (LLM) without a need for modifications of the
underlying structure. We develop a tailored X-LoRA model that offers scientific
capabilities including forward/inverse analysis tasks and enhanced reasoning
capability, focused on biomaterial analysis, protein mechanics and design. The
impact of this work include access to readily expandable and adaptable models
with strong domain knowledge and the capability to integrate across areas of
knowledge. Featuring experts in biology, mathematics, reasoning, bio-inspired
materials, mechanics and materials, chemistry, protein biophysics, mechanics
and quantum-mechanics based molecular properties, we conduct a series of
physics-focused case studies. We examine knowledge recall, protein mechanics
forward/inverse tasks, protein design, adversarial agentic modeling including
ontological knowledge graph construction, as well as molecular design. The
model is capable not only of making quantitative predictions of nanomechanical
properties of proteins or quantum mechanical molecular properties, but also
reasons over the results and correctly predicts likely mechanisms that explain
distinct molecular behaviors.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.07148v2},
File          = {2402.07148v2.pdf}
}
@article{2402.05211v5,
Author        = {Mark S. Fox and Bart Gajderowicz and Dishu Lyu},
Title         = {A Capability Maturity Model for Urban Dataset Meta-data},
Eprint        = {2402.05211v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {In the current environment of data generation and publication, there is an
ever-growing number of datasets available for download. This growth
precipitates an existing challenge: sourcing and integrating relevant datasets
for analysis is becoming more complex. Despite efforts by open data platforms,
obstacles remain, predominantly rooted in inadequate metadata, unsuitable data
presentation, complications in pinpointing desired data, and data integration.
This paper delves into the intricacies of dataset retrieval, emphasizing the
pivotal role of metadata in aligning datasets with user queries. Through an
exploration of existing literature, it underscores prevailing issues such as
the identification of valuable metadata and the development of tools to
maintain and annotate them effectively. The central contribution of this
research is the proposition of a dataset metadata maturity model. Deriving
inspiration from software engineering maturity models, this framework
delineates a progression from rudimentary metadata documentation to advanced
levels, aiding dataset creators in their documentation efforts. The model
encompasses seven pivotal dimensions, spanning content to quality information,
each stratified across five maturity levels to guide the optimal documentation
of datasets, ensuring ease of discovery, relevance assessment, and
comprehensive dataset understanding. This paper also incorporates the maturity
model into a data cataloguing tool called CKAN through a custom plugin,
CKANext-udc. The plugin introduces custom fields based on different maturity
levels, allows for user interface customisation, and integrates with a graph
database, converting catalogue data into a knowledge graph based on the
Maturity Model ontology.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.05211v5},
File          = {2402.05211v5.pdf}
}
@article{2402.02978v1,
Author        = {Haya Majid Qureshi and Wolfgang Faber},
Title         = {Evaluating Datalog Tools for Meta-reasoning over OWL 2 QL},
Eprint        = {2402.02978v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Metamodeling is a general approach to expressing knowledge about classes and
properties in an ontology. It is a desirable modeling feature in multiple
applications that simplifies the extension and reuse of ontologies.
Nevertheless, allowing metamodeling without restrictions is problematic for
several reasons, mainly due to undecidability issues. Practical languages,
therefore, forbid classes to occur as instances of other classes or treat such
occurrences as semantically different objects. Specifically, meta-querying in
SPARQL under the Direct Semantic Entailment Regime (DSER) uses the latter
approach, thereby effectively not supporting meta-queries. However, several
extensions enabling different metamodeling features have been proposed over the
last decade. This paper deals with the Metamodeling Semantics (MS) over OWL 2
QL and the Metamodeling Semantic Entailment Regime (MSER), as proposed in
Lenzerini et al. (2015) and Lenzerini et al. (2020); Cima et al. (2017). A
reduction from OWL 2 QL to Datalog for meta-querying was proposed in Cima et
al. (2017). In this paper, we experiment with various logic programming tools
that support Datalog querying to determine their suitability as back-ends to
MSER query answering. These tools stem from different logic programming
paradigms (Prolog, pure Datalog, Answer Set Programming, Hybrid Knowledge
Bases). Our work shows that the Datalog approach to MSER querying is practical
also for sizeable ontologies with limited resources (time and memory). This
paper significantly extends Qureshi & Faber (2021) by a more detailed
experimental analysis and more background. Under consideration in Theory and
Practice of Logic Programming (TPLP).},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.02978v1},
File          = {2402.02978v1.pdf}
}
@article{2402.03380v1,
Author        = {Shreyash Rawat and V. Vijayarajan and V. B. Surya Prasath},
Title         = {Modified K-means with Cluster Assignment -- Application to COVID-19 Data},
Eprint        = {2402.03380v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Text extraction is a highly subjective problem which depends on the dataset
that one is working on and the kind of summarization details that needs to be
extracted out. All the steps ranging from preprocessing of the data, to the
choice of an optimal model for predictions, depends on the problem and the
corpus at hand. In this paper, we describe a text extraction model where the
aim is to extract word specified information relating to the semantics such
that we can get all related and meaningful information about that word in a
succinct format. This model can obtain meaningful results and can augment
ubiquitous search model or a normal clustering or topic modelling algorithms.
By utilizing new technique called two cluster assignment technique with K-means
model, we improved the ontology of the retrieved text. We further apply the
vector average damping technique for flexible movement of clusters. Our
experimental results on a recent corpus of Covid-19 shows that we obtain good
results based on main keywords.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.03380v1},
File          = {2402.03380v1.pdf}
}
@article{2402.02181v1,
Author        = {JosÃ© Alberto BenÃ­tez-Andrades and IsaÃ­as GarcÃ­a-RodrÃ­guez and Carmen Benavides and HÃ©ctor AlÃ¡iz-MoretÃ³n and JosÃ© Emilio Labra Gayo},
Title         = {An Ontology-Based multi-domain model in Social Network Analysis:
  Experimental validation and case study},
Eprint        = {2402.02181v1},
DOI           = {10.1016/j.ins.2020.06.008},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SI},
Abstract      = {The use of social network theory and methods of analysis have been applied to
different domains in recent years, including public health. The complete
procedure for carrying out a social network analysis (SNA) is a time-consuming
task that entails a series of steps in which the expert in social network
analysis could make mistakes. This research presents a multi-domain knowledge
model capable of automatically gathering data and carrying out different social
network analyses in different domains, without errors and obtaining the same
conclusions that an expert in SNA would obtain. The model is represented in an
ontology called OntoSNAQA, which is made up of classes, properties and rules
representing the domains of People, Questionnaires and Social Network Analysis.
Besides the ontology itself, different rules are represented by SWRL and SPARQL
queries. A Knowledge Based System was created using OntoSNAQA and applied to a
real case study in order to show the advantages of the approach. Finally, the
results of an SNA analysis obtained through the model were compared to those
obtained from some of the most widely used SNA applications: UCINET, Pajek,
Cytoscape and Gephi, to test and confirm the validity of the model.},
Year          = {2024},
Month         = {Feb},
Note          = {Information Sciences, Volume 540, November 2020, Pages 390-413},
Url           = {http://arxiv.org/abs/2402.02181v1},
File          = {2402.02181v1.pdf}
}
@article{2402.00591v3,
Author        = {Nicolas Lazzari and Stefano De Giorgis and Aldo Gangemi and Valentina Presutti},
Title         = {Sandra -- A Neuro-Symbolic Reasoner Based On Descriptions And Situations},
Eprint        = {2402.00591v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper presents sandra, a neuro-symbolic reasoner combining vectorial
representations with deductive reasoning. Sandra builds a vector space
constrained by an ontology and performs reasoning over it. The geometric nature
of the reasoner allows its combination with neural networks, bridging the gap
with symbolic knowledge representations. Sandra is based on the Description and
Situation (DnS) ontology design pattern, a formalization of frame semantics.
Given a set of facts (a situation) it allows to infer all possible perspectives
(descriptions) that can provide a plausible interpretation for it, even in
presence of incomplete information. We prove that our method is correct with
respect to the DnS model. We experiment with two different tasks and their
standard benchmarks, demonstrating that, without increasing complexity, sandra
(i) outperforms all the baselines (ii) provides interpretability in the
classification process, and (iii) allows control over the vector space, which
is designed a priori.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.00591v3},
File          = {2402.00591v3.pdf}
}
@article{2402.00325v2,
Author        = {Jennifer Whyte and Ranjith Soman and Rafael Sacks and Neda Mohammadi and Nader Naderpajouh and Wei-Ting Hong and Ghang Lee},
Title         = {Using digital twins for managing change in complex projects},
Eprint        = {2402.00325v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {eess.SY},
Abstract      = {Complex systems are not entirely decomposable, hence interdependences arise
at the interfaces in complex projects. When changes occur, significant risks
arise at these interfaces as it is hard to identify, manage and visualise the
systemic consequences of changes. Particularly problematic are the interfaces
in which there are multiple interdependencies, which occur where the boundaries
between design components, contracts and organisation coincide, such as between
design disciplines. In this paper, we propose an approach to digital twin-based
interface management, through an underpinning state-of-the-art review of the
existing technical literature and a small pilot to identify the characteristics
of future data-driven solutions. We set out an approach to digital twin-based
interface management and an agenda for research on advanced methodologies for
managing change in complex projects. This agenda includes the need to integrate
work on identifying systems interfaces, change propagation and visualisation,
and the potential to significantly extend the limitations of existing solutions
by using developments in the digital twin, such as linked data, semantic
enrichment, network analyses, natural language processing (NLP)-enhanced
ontology and machine learning.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.00325v2},
File          = {2402.00325v2.pdf}
}
@article{2403.08795v1,
Author        = {Bruna AraÃºjo de Castro Oliveira},
Title         = {Ontologia para monitorar a deficiÃªncia mental em seus dÃ©ficts no
  processamento da informaÃ§Ã£o por declÃ­nio cognitivo e evitar
  agressÃµes psicolÃ³gicas e fÃ­sicas em ambientes educacionais com ajuda da
  I.A*},
Eprint        = {2403.08795v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {The intention of this article is to propose the use of artificial
intelligence to detect through analysis by UFO ontology the emergence of verbal
and physical aggression related to psychosocial deficiencies and their
provoking agents, in an attempt to prevent catastrophic consequences within
school environments.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2403.08795v1},
File          = {2403.08795v1.pdf}
}
@article{2401.17344v1,
Author        = {Hans Christian Ãttinger},
Title         = {Elementary particles with nonzero spin must be massless},
Eprint        = {2401.17344v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {physics.gen-ph},
Abstract      = {We present an ontological argument why elementary particles with nonzero spin
must be massless. This argument implies that, from an ontological perspective,
the massive quarks and leptons of the standard model cannot be elementary
particles. This conclusion is less disquieting than it might seem at first
sight because the Higgs mechanism entails that not only the masses of the W and
Z bosons but also the masses of quarks and leptons arise from the interaction
of massless elementary particles with the vacuum expectation value of the Higgs
field, which is a result of symmetry breaking.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.17344v1},
File          = {2401.17344v1.pdf}
}
@article{2402.03362v1,
Author        = {Martin Lentschat and Cyril LabbÃ© and Ran Cheng},
Title         = {NanoNER: Named Entity Recognition for nanobiology using experts'
  knowledge and distant supervision},
Eprint        = {2402.03362v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Here we present the training and evaluation of NanoNER, a Named Entity
Recognition (NER) model for Nanobiology. NER consists in the identification of
specific entities in spans of unstructured texts and is often a primary task in
Natural Language Processing (NLP) and Information Extraction. The aim of our
model is to recognise entities previously identified by domain experts as
constituting the essential knowledge of the domain. Relying on ontologies,
which provide us with a domain vocabulary and taxonomy, we implemented an
iterative process enabling experts to determine the entities relevant to the
domain at hand. We then delve into the potential of distant supervision
learning in NER, supporting how this method can increase the quantity of
annotated data with minimal additional manpower. On our full corpus of 728
full-text nanobiology articles, containing more than 120k entity occurrences,
NanoNER obtained a F1-score of 0.98 on the recognition of previously known
entities. Our model also demonstrated its ability to discover new entities in
the text, with precision scores ranging from 0.77 to 0.81. Ablation experiments
further confirmed this and allowed us to assess the dependency of our approach
on the external resources. It highlighted the dependency of the approach to the
resource, while also confirming its ability to rediscover up to 30% of the
ablated terms. This paper details the methodology employed, experimental
design, and key findings, providing valuable insights and directions for future
related researches on NER in specialized domain. Furthermore, since our
approach require minimal manpower , we believe that it can be generalized to
other specialized fields.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2402.03362v1},
File          = {2402.03362v1.pdf}
}
@article{2401.15814v2,
Author        = {Weicong Tan and Weiqing Wang and Xin Zhou and Wray Buntine and Gordon Bingham and Hongzhi Yin},
Title         = {OntoMedRec: Logically-Pretrained Model-Agnostic Ontology Encoders for
  Medication Recommendation},
Eprint        = {2401.15814v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Most existing medication recommendation models learn representations for
medical concepts based on electronic health records (EHRs) and make
recommendations with learnt representations. However, most medications appear
in the dataset for limited times, resulting in insufficient learning of their
representations. Medical ontologies are the hierarchical classification systems
for medical terms where similar terms are in the same class on a certain level.
In this paper, we propose OntoMedRec, the logically-pretrained and
model-agnostic medical Ontology Encoders for Medication Recommendation that
addresses data sparsity problem with medical ontologies. We conduct
comprehensive experiments on benchmark datasets to evaluate the effectiveness
of OntoMedRec, and the result shows the integration of OntoMedRec improves the
performance of various models in both the entire EHR datasets and the
admissions with few-shot medications. We provide the GitHub repository for the
source code on https://anonymous.4open.science/r/OntoMedRec-D123},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.15814v2},
File          = {2401.15814v2.pdf}
}
@article{2401.14933v1,
Author        = {Idoia Berges and JesÃºs BermÃºdez and Arantza Illarramendi},
Title         = {SSDOnt: an Ontology for representing Single-Subject Design Studies},
Eprint        = {2401.14933v1},
DOI           = {10.3414/ME17-01-0109},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Background: Single-Subject Design is used in several areas such as education
and biomedicine. However, no suited formal vocabulary exists for annotating the
detailed configuration and the results of this type of research studies with
the appropriate granularity for looking for information about them. Therefore,
the search for those study designs relies heavily on a syntactical search on
the abstract, keywords or full text of the publications about the study, which
entails some limitations. Objective: To present SSDOnt, a specific purpose
ontology for describing and annotating single-subject design studies, so that
complex questions can be asked about them afterwards. Methods: The ontology was
developed following the NeOn methodology. Once the requirements of the ontology
were defined, a formal model was described in a Description Logic and later
implemented in the ontology language OWL 2 DL. Results: We show how the
ontology provides a reference model with a suitable terminology for the
annotation and searching of single-subject design studies and their main
components, such as the phases, the intervention types, the outcomes and the
results. Some mappings with terms of related ontologies have been established.
We show as proof-of-concept that classes in the ontology can be easily extended
to annotate more precise information about specific interventions and outcomes
such as those related to autism. Moreover, we provide examples of some types of
queries that can be posed to the ontology. Conclusions: SSDOnt has achieved the
purpose of covering the descriptions of the domain of single-subject research
studies.},
Year          = {2024},
Month         = {Jan},
Note          = {Methods of Information in Medicine 57(01/02) : 55-61 (2018)},
Url           = {http://arxiv.org/abs/2401.14933v1},
File          = {2401.14933v1.pdf}
}
@article{2401.14931v2,
Author        = {Marco Bombieri and Paolo Fiorini and Simone Paolo Ponzetto and Marco Rospocher},
Title         = {Do LLMs Dream of Ontologies?},
Eprint        = {2401.14931v2},
DOI           = {10.1145/3725852},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have demonstrated remarkable performance across
diverse natural language processing tasks, yet their ability to memorize
structured knowledge remains underexplored. In this paper, we investigate the
extent to which general-purpose pre-trained LLMs retain and correctly reproduce
concept identifier (ID)-label associations from publicly available ontologies.
We conduct a systematic evaluation across multiple ontological resources,
including the Gene Ontology, Uberon, Wikidata, and ICD-10, using LLMs such as
Pythia-12B, Gemini-1.5-Flash, GPT-3.5, and GPT-4. Our findings reveal that only
a small fraction of ontological concepts is accurately memorized, with GPT-4
demonstrating the highest performance. To understand why certain concepts are
memorized more effectively than others, we analyze the relationship between
memorization accuracy and concept popularity on the Web. Our results indicate a
strong correlation between the frequency of a concept's occurrence online and
the likelihood of accurately retrieving its ID from the label. This suggests
that LLMs primarily acquire such knowledge through indirect textual exposure
rather than directly from structured ontological resources. Furthermore, we
introduce new metrics to quantify prediction invariance, demonstrating that the
stability of model responses across variations in prompt language and
temperature settings can serve as a proxy for estimating memorization
robustness.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.14931v2},
File          = {2401.14931v2.pdf}
}
@article{2401.14153v1,
Author        = {J. Carbo and N. Sanchez and J. M. Molina},
Title         = {Agent-based Simulation with Netlogo to Evaluate AmI Scenarios},
Eprint        = {2401.14153v1},
DOI           = {10.1057/jos.2016.10},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In this paper an agent-based simulation is developed in order to evaluate an
AmI scenario based on agents. Many AmI applications are implemented through
agents but they are not compared to any other existing alternative in order to
evaluate the relative benefits of using them. The proposal simulation
environment developed in Netlogo analyse such benefits using two evaluation
criteria: First, measuring agent satisfaction of different types of desires
along the execution. Second, measuring time savings obtained through a correct
use of context information.
  So, here, a previously suggested agent architecture, an ontology and a
12-steps protocol to provide AmI services in airports, is evaluated using a
NetLogo simulation environment. The present work uses a NetLogo model
considering scalability problems of this application domain but using FIPA and
BDI extensions to be coherent with our previous works and our previous JADE
implementation of them.
  The NetLogo model presented simulates an airport with agent users passing
through several zones located in a specific order in a map: passport controls,
check-in counters of airline companies, boarding gates, different types of
shopping. Although initial data in simulations are generated randomly, and the
model is just an approximation of real-world airports, the definition of this
case of use of Ambient Intelligence through NetLogo agents opens an interesting
way to evaluate the benefits of using Ambient Intelligence, which is a
significant contribution to the final development of them.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.14153v1},
File          = {2401.14153v1.pdf}
}
@article{2401.11865v1,
Author        = {Idoia Berges and JesÃºs BermÃºdez and Arantza Illarramendi},
Title         = {Toward Semantic Interoperability of Electronic Health Records},
Eprint        = {2401.11865v1},
DOI           = {10.1109/TITB.2011.2180917},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Although the goal of achieving semantic interoperability of electronic health
records (EHRs) is pursued by many researchers, it has not been accomplished
yet. In this paper, we present a proposal that smoothes out the way toward the
achievement of that goal. In particular, our study focuses on medical diagnoses
statements. In summary, the main contributions of our ontology-based proposal
are the following: first, it includes a canonical ontology whose EHR-related
terms focus on semantic aspects. As a result, their descriptions are
independent of languages and technology aspects used in different organizations
to represent EHRs. Moreover, those terms are related to their corresponding
codes in well-known medical terminologies. Second, it deals with modules that
allow obtaining rich ontological representations of EHR information managed by
proprietary models of health information systems. The features of one specific
module are shown as reference. Third, it considers the necessary mapping axioms
between ontological terms enhanced with so-called path mappings. This feature
smoothes out structural differences between heterogeneous EHR representations,
allowing proper alignment of information.},
Year          = {2024},
Month         = {Jan},
Note          = {IEEE Trans. Inf. Technol. Biomed. 16(3): 424-431 (2012)},
Url           = {http://arxiv.org/abs/2401.11865v1},
File          = {2401.11865v1.pdf}
}
@article{2401.11848v1,
Author        = {VÃ­ctor Julio RamÃ­rez-DurÃ¡n and Idoia Berges and Arantza Illarramendi},
Title         = {ExtruOnt: An ontology for describing a type of manufacturing machine for
  Industry 4.0 systems},
Eprint        = {2401.11848v1},
DOI           = {10.3233/sw-200376},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Semantically rich descriptions of manufacturing machines, offered in a
machine-interpretable code, can provide interesting benefits in Industry 4.0
scenarios. However, the lack of that type of descriptions is evident. In this
paper we present the development effort made to build an ontology, called
ExtruOnt, for describing a type of manufacturing machine, more precisely, a
type that performs an extrusion process (extruder). Although the scope of the
ontology is restricted to a concrete domain, it could be used as a model for
the development of other ontologies for describing manufacturing machines in
Industry 4.0 scenarios. The terms of the ExtruOnt ontology provide different
types of information related with an extruder, which are reflected in distinct
modules that constitute the ontology. Thus, it contains classes and properties
for expressing descriptions about components of an extruder, spatial
connections, features, and 3D representations of those components, and finally
the sensors used to capture indicators about the performance of this type of
machine. The ontology development process has been carried out in close
collaboration with domain experts.},
Year          = {2024},
Month         = {Jan},
Note          = {Semantic Web 11(6): 887-909 (2020)},
Url           = {http://arxiv.org/abs/2401.11848v1},
File          = {2401.11848v1.pdf}
}
@article{2401.11823v1,
Author        = {Idoia Berges and JesÃºs BermÃºdez and Alfredo GoÃ±i and Arantza Illarramendi},
Title         = {Towards a satisfactory conversion of messages among agent-based
  information systems},
Eprint        = {2401.11823v1},
DOI           = {10.1016/j.eswa.2012.10.055},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.MA},
Abstract      = {Over the last years, there has been a change of perspective concerning the
management of information systems, since they are no longer isolated and need
to communicate with others. However, from a semantic point of view, real
communication is difficult to achieve due to the heterogeneity of the systems.
We present a proposal which, considering information systems are represented by
software agents, provides a framework that favours a semantic communication
among them, overcoming the heterogeneity of their agent communication
languages. The main components of the framework are a suite of ontologies --
conceptualizing communication acts -- that will be used for generating the
communication conversion, and an Event Calculus interpretation of the
communications, which will be used for formalizing the notion of a satisfactory
conversion. Moreover, we present a motivating example in order to complete the
explanation of the whole picture.},
Year          = {2024},
Month         = {Jan},
Note          = {Expert Syst. Appl.40(7) : 2462-2475 (2013)},
Url           = {http://arxiv.org/abs/2401.11823v1},
File          = {2401.11823v1.pdf}
}
@article{2401.11648v5,
Author        = {Heejoon Koo},
Title         = {Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal
  Contrastive EHR Modelling with Hierarchical Regularisation},
Eprint        = {2401.11648v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Predicting next visit diagnosis using Electronic Health Records (EHR) is an
essential task in healthcare, critical for devising proactive future plans for
both healthcare providers and patients. Nonetheless, many preceding studies
have not sufficiently addressed the heterogeneous and hierarchical
characteristics inherent in EHR data, inevitably leading to sub-optimal
performance. To this end, we propose NECHO, a novel medical code-centric
multimodal contrastive EHR learning framework with hierarchical regularisation.
First, we integrate multifaceted information encompassing medical codes,
demographics, and clinical notes using a tailored network design and a pair of
bimodal contrastive losses, all of which pivot around a medical codes
representation. We also regularise modality-specific encoders using a parental
level information in medical ontology to learn hierarchical structure of EHR
data. A series of experiments on MIMIC-III data demonstrates effectiveness of
our approach.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.11648v5},
File          = {2401.11648v5.pdf}
}
@article{2402.01677v5,
Author        = {Keyu Wang and Guilin Qi and Jiaoyan Chen and Yi Huang and Tianxing Wu},
Title         = {Embedding Ontologies via Incorporating Extensional and Intensional
  Knowledge},
Eprint        = {2402.01677v5},
DOI           = {10.3724/2096-7004.di.2024.0088},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontologies contain rich knowledge within domain, which can be divided into
two categories, namely extensional knowledge and intensional knowledge.
Extensional knowledge provides information about the concrete instances that
belong to specific concepts in the ontology, while intensional knowledge
details inherent properties, characteristics, and semantic associations among
concepts. However, existing ontology embedding approaches fail to take both
extensional knowledge and intensional knowledge into fine consideration
simultaneously. In this paper, we propose a novel ontology embedding approach
named EIKE (Extensional and Intensional Knowledge Embedding) by representing
ontologies in two spaces, called extensional space and intensional space. EIKE
presents a unified framework for embedding instances, concepts and their
relations in an ontology, applying a geometry-based method to model extensional
knowledge and a pretrained language model to model intensional knowledge, which
can capture both structure information and textual information. Experimental
results show that EIKE significantly outperforms state-of-the-art methods in
three datasets for both triple classification and link prediction, indicating
that EIKE provides a more comprehensive and representative perspective of the
domain.},
Year          = {2024},
Month         = {Jan},
Note          = {Data Intelligence, Vol. 7, Issue 4, 1222-1241, 2024},
Url           = {http://arxiv.org/abs/2402.01677v5},
File          = {2402.01677v5.pdf}
}
@article{2401.10751v1,
Author        = {Stefano De Giorgis and Aldo Gangemi},
Title         = {EFO: the Emotion Frame Ontology},
Eprint        = {2401.10751v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Emotions are a subject of intense debate in various disciplines. Despite the
proliferation of theories and definitions, there is still no consensus on what
emotions are, and how to model the different concepts involved when we talk
about - or categorize - them. In this paper, we propose an OWL frame-based
ontology of emotions: the Emotion Frames Ontology (EFO). EFO treats emotions as
semantic frames, with a set of semantic roles that capture the different
aspects of emotional experience. EFO follows pattern-based ontology design, and
is aligned to the DOLCE foundational ontology. EFO is used to model multiple
emotion theories, which can be cross-linked as modules in an Emotion Ontology
Network. In this paper, we exemplify it by modeling Ekman's Basic Emotions (BE)
Theory as an EFO-BE module, and demonstrate how to perform automated inferences
on the representation of emotion situations. EFO-BE has been evaluated by
lexicalizing the BE emotion frames from within the Framester knowledge graph,
and implementing a graph-based emotion detector from text. In addition, an EFO
integration of multimodal datasets, including emotional speech and emotional
face expressions, has been performed to enable further inquiry into crossmodal
emotion semantics.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.10751v1},
File          = {2401.10751v1.pdf}
}
@article{2401.10558v1,
Author        = {Rex W. Douglass and Thomas Leo Scherer and J. AndrÃ©s Gannon and Erik Gartzke},
Title         = {ICBeLLM: High Quality International Events Data with Open Source Large
  Language Models on Consumer Hardware},
Eprint        = {2401.10558v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {stat.AP},
Abstract      = {The International Crises Behavior Events (ICBe) ontology provides high
coverage over the thoughts, communications, and actions that constitute
international relations. A major disadvantage of that level of detail is that
it requires large human capital costs to apply it manually to new texts.
Whether such an ontolgy is practical for international relations research given
limited human and financial resources is a pressing concern. We introduce a
working proof of concept showing that ICBe codings can be reliably extracted
from new texts using the current generation of open source large language
models (LLM) running on consumer grade computer hardware. Our solution requires
no finetuning and only limited prompt engineering. We detail our solution and
present benchmarks against the original ICBe codings. We conclude by discussing
the implications of very high quality event coding of any text being within
reach of individual researchers with limited resources.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.10558v1},
File          = {2401.10558v1.pdf}
}
@article{2401.09789v1,
Author        = {Idoia Berges and VÃ­ctor Julio RamÃ­rez-DurÃ¡n and Arantza Illarramendi},
Title         = {A Semantic Approach for Big Data Exploration in Industry 4.0},
Eprint        = {2401.09789v1},
DOI           = {10.1016/j.bdr.2021.100222},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The growing trends in automation, Internet of Things, big data and cloud
computing technologies have led to the fourth industrial revolution (Industry
4.0), where it is possible to visualize and identify patterns and insights,
which results in a better understanding of the data and can improve the
manufacturing process. However, many times, the task of data exploration
results difficult for manufacturing experts because they might be interested in
analyzing also data that does not appear in pre-designed visualizations and
therefore they must be assisted by Information Technology experts. In this
paper, we present a proposal materialized in a semantic-based visual query
system developed for a real Industry 4.0 scenario that allows domain experts to
explore and visualize data in a friendly way. The main novelty of the system is
the combined use that it makes of captured data that are semantically annotated
first, and a 2D customized digital representation of a machine that is also
linked with semantic descriptions. Those descriptions are expressed using terms
of an ontology, where, among others, the sensors that are used to capture
indicators about the performance of a machine that belongs to a Industry 4.0
scenario have been modeled. Moreover, this semantic description allows to:
formulate queries at a higher level of abstraction, provide customized
graphical visualizations of the results based on the format and nature of the
data, and download enriched data enabling further types of analysis.},
Year          = {2024},
Month         = {Jan},
Note          = {Big Data Res. 25: 100222 (2021)},
Url           = {http://arxiv.org/abs/2401.09789v1},
File          = {2401.09789v1.pdf}
}
@article{2401.09395v6,
Author        = {Pengfei Hong and Navonil Majumder and Deepanway Ghosal and Somak Aditya and Rada Mihalcea and Soujanya Poria},
Title         = {Evaluating LLMs' Mathematical and Coding Competency through
  Ontology-guided Interventions},
Eprint        = {2401.09395v6},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent advancements in Large Language Models (LLMs) have showcased striking
results on existing logical reasoning benchmarks, with some models even
surpassing human performance. However, the true depth of their competencies and
robustness in reasoning tasks remains an open question. To this end, in this
paper, we focus on two popular reasoning tasks: arithmetic reasoning and code
generation. Particularly, we introduce (i) a general ontology of perturbations
for math and coding questions, (ii) a semi-automatic method to apply these
perturbations, and (iii) two datasets, GSMORE and HUMANEVAL-CORE, respectively,
of perturbed math and coding problems to probe LLM capabilities in numeric
reasoning and coding tasks. Through comprehensive evaluations of both
closed-source and open-source LLMs, we show a significant performance drop
across all the models against the perturbed questions, suggesting that the
current LLMs lack robust problem solving skills and structured reasoning
abilities in many areas, as defined by our ontology. We open-source the
datasets and source codes at: https://github.com/declare-lab/LLM-ReasoningTest.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.09395v6},
File          = {2401.09395v6.pdf}
}
@article{2401.08253v2,
Author        = {Hans-Thomas Elze},
Title         = {Cellular automaton ontology, bits, qubits, and the Dirac equation},
Eprint        = {2401.08253v2},
DOI           = {10.1142/S0219749924500138},
ArchivePrefix = {arXiv},
PrimaryClass  = {quant-ph},
Abstract      = {Cornerstones of the Cellular Automaton Interpretation of Quantum Mechanics
are its ontological states that evolve by permutations, in this way never
creating would-be quantum mechanical superposition states. We review and
illustrate this with a classical Ising spin chain. It is shown that it can be
related to the Weyl equation in the continuum limit. Yet, the model of discrete
spins or bits unavoidably becomes a model of qubits by generating
superpositions, if only slightly deformed. We study modifications of its signal
velocity which, however, do not relate to mass terms. To incorporate the
latter, we consider the Dirac equation in 1+1 dimensions and sketch an
underlying discrete deterministic "necklace of necklaces" automaton that
qualifies as ontological.},
Year          = {2024},
Month         = {Jan},
Note          = {Intern. Journ. Qu. Info. (IJQI), Vol. 22, No. 06, 2450013 (2024)},
Url           = {http://arxiv.org/abs/2401.08253v2},
File          = {2401.08253v2.pdf}
}
@article{2401.07890v2,
Author        = {Alireza Shahbazi and Seyyed Ahmad Mirsanei and Malikeh Haj Khan Mirzaye Sarraf and Behrouz Minaei Bidgoli},
Title         = {A Strategy for Implementing description Temporal Dynamic Algorithms in
  Dynamic Knowledge Graphs by SPIN},
Eprint        = {2401.07890v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Planning and reasoning about actions and processes, in addition to reasoning
about propositions, are important issues in recent logical and computer science
studies. The widespread use of actions in everyday life such as IoT, semantic
web services, etc., and the limitations and issues in the action formalisms are
two factors that lead us to study how actions are represented.
  Since 2007, there have been some ideas to integrate Description Logic (DL)
and action formalisms for representing both static and dynamic knowledge.
Meanwhile, time is an important factor in dynamic situations, and actions
change states over time. In this study, on the one hand, we examined related
logical structures such as extensions of description logics (DLs), temporal
formalisms, and action formalisms. On the other hand, we analyzed possible
tools for designing and developing the Knowledge and Action Base (KAB).
  For representation and reasoning about actions, we embedded actions into DLs
(such as Dynamic-ALC and its extensions). We propose a terminable algorithm for
action projection, planning, checking the satisfiability, consistency,
realizability, and executability, and also querying from KAB. Actions in this
framework were modeled with SPIN and added to state space. This framework has
also been implemented as a plugin for the Prot\'eg\'e ontology editor.
  During the last two decades, various algorithms have been presented, but due
to the high computational complexity, we face many problems in implementing
dynamic ontologies. In addition, an algorithm to detect the inconsistency of
actions' effects was not explicitly stated. In the proposed strategy, the
interactions of actions with other parts of modeled knowledge, and a method to
check consistency between the effects of actions are presented. With this
framework, the ramification problem can be well handled in future works.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.07890v2},
File          = {2401.07890v2.pdf}
}
@article{2401.07447v1,
Author        = {Claire NÃ©dellec and Clara Sauvion and Robert Bossy and Mariya Borovikova and Louise DelÃ©ger},
Title         = {Taec: a Manually annotated text dataset for trait and phenotype
  extraction and entity linking in wheat breeding literature},
Eprint        = {2401.07447v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Wheat varieties show a large diversity of traits and phenotypes. Linking them
to genetic variability is essential for shorter and more efficient wheat
breeding programs. Newly desirable wheat variety traits include disease
resistance to reduce pesticide use, adaptation to climate change, resistance to
heat and drought stresses, or low gluten content of grains. Wheat breeding
experiments are documented by a large body of scientific literature and
observational data obtained in-field and under controlled conditions. The
cross-referencing of complementary information from the literature and
observational data is essential to the study of the genotype-phenotype
relationship and to the improvement of wheat selection. The scientific
literature on genetic marker-assisted selection describes much information
about the genotype-phenotype relationship. However, the variety of expressions
used to refer to traits and phenotype values in scientific articles is a hinder
to finding information and cross-referencing it. When trained adequately by
annotated examples, recent text mining methods perform highly in named entity
recognition and linking in the scientific domain. While several corpora contain
annotations of human and animal phenotypes, currently, no corpus is available
for training and evaluating named entity recognition and entity-linking methods
in plant phenotype literature. The Triticum aestivum trait Corpus is a new gold
standard for traits and phenotypes of wheat. It consists of 540 PubMed
references fully annotated for trait, phenotype, and species named entities
using the Wheat Trait and Phenotype Ontology and the species taxonomy of the
National Center for Biotechnology Information. A study of the performance of
tools trained on the Triticum aestivum trait Corpus shows that the corpus is
suitable for the training and evaluation of named entity recognition and
linking.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.07447v1},
File          = {2401.07447v1.pdf}
}
@article{2401.06667v1,
Author        = {Marco Arazzi and Antonino Nocera and Emanuele Storti},
Title         = {The SemIoE Ontology: A Semantic Model Solution for an IoE-based Industry},
Eprint        = {2401.06667v1},
DOI           = {10.1109/JIOT.2024.3452945},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Recently, the Industry 5.0 is gaining attention as a novel paradigm, defining
the next concrete steps toward more and more intelligent, green-aware and
user-centric digital systems. In an era in which smart devices typically
adopted in the industry domain are more and more sophisticated and autonomous,
the Internet of Things and its evolution, known as the Internet of Everything
(IoE, for short), involving also people, robots, processes and data in the
network, represent the main driver to allow industries to put the experiences
and needs of human beings at the center of their ecosystems. However, due to
the extreme heterogeneity of the involved entities, their intrinsic need and
capability to cooperate, and the aim to adapt to a dynamic user-centric
context, special attention is required for the integration and processing of
the data produced by such an IoE. This is the objective of the present paper,
in which we propose a novel semantic model that formalizes the fundamental
actors, elements and information of an IoE, along with their relationships. In
our design, we focus on state-of-the-art design principles, in particular
reuse, and abstraction, to build ``SemIoE'', a lightweight ontology inheriting
and extending concepts from well-known and consolidated reference ontologies.
The defined semantic layer represents a core data model that can be extended to
embrace any modern industrial scenario. It represents the base of an IoE
Knowledge Graph, on top of which, as an additional contribution, we analyze and
define some essential services for an IoE-based industry.},
Year          = {2024},
Month         = {Jan},
Note          = {IEEE Internet of Things Journal 2024},
Url           = {http://arxiv.org/abs/2401.06667v1},
File          = {2401.06667v1.pdf}
}
@article{2401.06337v2,
Author        = {Zhaoming Lv},
Title         = {An ontology alignment method with user intervention using compact
  differential evolution with adaptive parameter control},
Eprint        = {2401.06337v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.NE},
Abstract      = {User interaction is one of the most effective ways to improve the ontology
alignment quality. However, this approach faces the challenge of how users can
participate effectively in the matching process. To solve this challenge. In
this paper, an interactive ontology alignment approach using compact
differential evolution algorithm with adaptive parameter control (IOACDE) is
proposed. In this method, the ontology alignment process is modeled as an
interactive optimization problem and users are allowed to intervene in matching
in two ways. One is that the mapping suggestions generated by IOACDE as a
complete candidate alignment is evaluated by user during optimization process.
The other is that the user ameliorates the alignment results by evaluating
single mapping after the automatic matching process. To demonstrate the
effectiveness of the proposed algorithm, the neural embedding model and K
nearest neighbor (KNN) is employed to simulate user for the ontologies of the
real world. The experimental results show that the proposed interactive
approach can improve the alignment quality compared to the non-interactive.
Compared with the state-of-the-art methods from OAEI, the results show that the
proposed algorithm has a better performance under the same error rate.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.06337v2},
File          = {2401.06337v2.pdf}
}
@article{2401.06256v3,
Author        = {Artem Sukhobokov and Evgeny Belousov and Danila Gromozdov and Anna Zenger and Ilya Popov},
Title         = {A Universal Knowledge Model and Cognitive Architecture for Prototyping
  AGI},
Eprint        = {2401.06256v3},
DOI           = {10.1016/j.cogsys.2024.101279},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The article identified 42 cognitive architectures for creating general
artificial intelligence (AGI) and proposed a set of interrelated functional
blocks that an agent approaching AGI in its capabilities should possess. Since
the required set of blocks is not found in any of the existing architectures,
the article proposes a new cognitive architecture for intelligent systems
approaching AGI in their capabilities. As one of the key solutions within the
framework of the architecture, a universal method of knowledge representation
is proposed, which allows combining various non-formalized, partially and fully
formalized methods of knowledge representation in a single knowledge base, such
as texts in natural languages, images, audio and video recordings, graphs,
algorithms, databases, neural networks, knowledge graphs, ontologies, frames,
essence-property-relation models, production systems, predicate calculus
models, conceptual models, and others. To combine and structure various
fragments of knowledge, archigraph models are used, constructed as a
development of annotated metagraphs. As components, the cognitive architecture
being developed includes machine consciousness, machine subconsciousness,
blocks of interaction with the external environment, a goal management block,
an emotional control system, a block of social interaction, a block of
reflection, an ethics block and a worldview block, a learning block, a
monitoring block, blocks of statement and solving problems, self-organization
and meta learning block.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.06256v3},
File          = {2401.06256v3.pdf}
}
@article{2401.04474v1,
Author        = {Ngoc Luyen Le and Marie-HÃ©lÃ¨ne Abel and Philippe Gouspillou},
Title         = {Combining Embedding-Based and Semantic-Based Models for Post-hoc
  Explanations in Recommender Systems},
Eprint        = {2401.04474v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {In today's data-rich environment, recommender systems play a crucial role in
decision support systems. They provide to users personalized recommendations
and explanations about these recommendations. Embedding-based models, despite
their widespread use, often suffer from a lack of interpretability, which can
undermine trust and user engagement. This paper presents an approach that
combines embedding-based and semantic-based models to generate post-hoc
explanations in recommender systems, leveraging ontology-based knowledge graphs
to improve interpretability and explainability. By organizing data within a
structured framework, ontologies enable the modeling of intricate relationships
between entities, which is essential for generating explanations. By combining
embedding-based and semantic based models for post-hoc explanations in
recommender systems, the framework we defined aims at producing meaningful and
easy-to-understand explanations, enhancing user trust and satisfaction, and
potentially promoting the adoption of recommender systems across the e-commerce
sector.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.04474v1},
File          = {2401.04474v1.pdf}
}
@article{2401.03093v4,
Author        = {V. L. Kalmykov and L. V. Kalmykov},
Title         = {XXAI: Towards eXplicitly eXplainable Artificial Intelligence},
Eprint        = {2401.03093v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {There are concerns about the reliability and safety of artificial
intelligence (AI) based on sub-symbolic neural networks because its decisions
cannot be explained explicitly. This is the black box problem of modern AI. At
the same time, symbolic AI has the nature of a white box and is able to ensure
the reliability and safety of its decisions. However, several problems prevent
the widespread use of symbolic AI: the opacity of mathematical models and
natural language terms, the lack of a unified ontology, and the combinatorial
explosion of search capabilities. To solve the black-box problem of AI, we
propose eXplicitly eXplainable AI (XXAI) - a fully transparent white-box AI
based on deterministic logical cellular automata whose rules are derived from
the first principles of the general theory of the relevant domain. In this
case, the general theory of the domain plays the role of a knowledge base for
deriving the inferences of the cellular automata. A cellular automaton
implements parallel multi-level logical inference at all levels of organization
- from local interactions of the element base to the system as a whole. Our
verification of several ecological hypotheses sets a precedent for the
successful implementation of the proposed solution. XXAI is able to
automatically verify the reliability, security and ethics of sub-symbolic
neural network solutions in both the final and training phases. In this
article, we present precedents for the successful implementation of XXAI, the
theoretical and methodological foundations for its further development, and
discuss prospects for the future.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.03093v4},
File          = {2401.03093v4.pdf}
}
@article{2401.02726v1,
Author        = {Nathan Aky and Denis Payet and Sylvain Giroux and RÃ©my Courdier},
Title         = {Une ontologie pour les syst{Ã¨}mes multi-agents ambiants dans les
  villes intelligentes},
Eprint        = {2401.02726v1},
DOI           = {10.57726/mhdg-s181},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Towns and cities are currently equipping themselves with a host of connected
devices, with a view to transforming themselves into ''smart cities''. To
manage this mass of connected objects, autonomous software entities, known as
agents, can be attached to them to cooperate and use these devices to offer
personalized services. However, this object infrastructure needs to be
semantically structured in order to be exploited. This is why the proposal of
this article is an ontology, formatted in OWL, describing the object
infrastructures, their links with the organization of the multi-agent system
and the services to be delivered according to the users of the system. The
ontology is applied to smart mobility for people with reduced mobility, and
could be adapted to other smart city axes.},
Year          = {2024},
Month         = {Jan},
Note          = {TOTh 2020, Christophe Roche, Nov 2020, Chamb{\'e}ry (73), France.
  pp.199-216},
Url           = {http://arxiv.org/abs/2401.02726v1},
File          = {2401.02726v1.pdf}
}
@article{2401.02540v1,
Author        = {Ahmad Zainul Ihsan and Said Fathalla and Stefan Sandfeld},
Title         = {DISO: A Domain Ontology for Modeling Dislocations in Crystalline
  Materials},
Eprint        = {2401.02540v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cond-mat.mtrl-sci},
Abstract      = {Crystalline materials, such as metals and semiconductors, nearly always
contain a special defect type called dislocation. This defect decisively
determines many important material properties, e.g., strength, fracture
toughness, or ductility. Over the past years, significant effort has been put
into understanding dislocation behavior across different length scales via
experimental characterization techniques and simulations. This paper introduces
the dislocation ontology (DISO), which defines the concepts and relationships
related to linear defects in crystalline materials. We developed DISO using a
top-down approach in which we start defining the most general concepts in the
dislocation domain and subsequent specialization of them. DISO is published
through a persistent URL following W3C best practices for publishing Linked
Data. Two potential use cases for DISO are presented to illustrate its
usefulness in the dislocation dynamics domain. The evaluation of the ontology
is performed in two directions, evaluating the success of the ontology in
modeling a real-world domain and the richness of the ontology.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.02540v1},
File          = {2401.02540v1.pdf}
}
@article{2401.00062v1,
Author        = {Mena Rizk and Daniela Rosu and Mark Fox},
Title         = {Semantic Computing for Organizational Effectiveness: From Organization
  Theory to Practice through Semantics-Based Modelling},
Eprint        = {2401.00062v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {A critical function of an organization is to foster the level of integration
(coordination and cooperation) necessary to achieve its objectives. The need to
coordinate and motivation to cooperate emerges from the myriad dependencies
between an organization's members and their work. Therefore, to reason about
solutions to coordination and cooperation problems requires a robust
representation that includes the underlying dependencies. We find that such a
representation remains missing from formal organizational models, and we
leverage semantics to bridge this gap. Drawing on well-established
organizational research and our extensive fieldwork with one of North America's
largest municipalities, (1) we introduce an ontology, formalized in first-order
logic, that operationalizes concepts like outcome, reward, and epistemic
dependence, and their links to potential integration risks; and (2) present
real-world applications of this ontology to analyze and support integration in
complex government infrastructure projects. Our ontology is implemented and
validated in both Z3 and OWL. Key features of our model include inferable
dependencies, explainable coordination and cooperation risks, and actionable
insights on how dependency structures within an organization can be altered to
mitigate the risks. Conceptualizing real-world challenges like incentive
misalignment, free-riding, and subgoal optimization in terms of dependency
structures, our semantics-based approach represents a novel method for
modelling and enhancing coordination and cooperation. Integrated within a
decision-support system, our model may serve as an impactful aid for
organizational design and effectiveness. More broadly, our approach underscores
the transformative potential of semantics in deriving tangible, real-world
value from existing organization theory.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2401.00062v1},
File          = {2401.00062v1.pdf}
}
@article{2312.16378v1,
Author        = {Sanjay Oruganti and Sergei Nirenburg and Jesse English and Marjorie McShane},
Title         = {Automating Knowledge Acquisition for Content-Centric Cognitive Agents
  Using LLMs},
Eprint        = {2312.16378v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The paper describes a system that uses large language model (LLM) technology
to support the automatic learning of new entries in an intelligent agent's
semantic lexicon. The process is bootstrapped by an existing non-toy lexicon
and a natural language generator that converts formal, ontologically-grounded
representations of meaning into natural language sentences. The learning method
involves a sequence of LLM requests and includes an automatic quality control
step. To date, this learning method has been applied to learning multiword
expressions whose meanings are equivalent to those of transitive verbs in the
agent's lexicon. The experiment demonstrates the benefits of a hybrid learning
architecture that integrates knowledge-based methods and resources with both
traditional data analytics and LLMs.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.16378v1},
File          = {2312.16378v1.pdf}
}
@article{2312.16309v1,
Author        = {MarÃ­a JosÃ© DomÃ­nguez VÃ¡zquez},
Title         = {ContribuciÃ³n de la semÃ¡ntica combinatoria al desarrollo de
  herramientas digitales multilingÃ¼es},
Eprint        = {2312.16309v1},
DOI           = {10.5209/clac.73849},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper describes how the field of Combinatorial Semantics has contributed
to the design of three prototypes for the automatic generation of argument
patterns in nominal phrases in Spanish, French and German (Xera, Combinatoria
and CombiContext). It also shows the importance of knowing about the argument
syntactic-semantic interface in a production situation in the context of
foreign languages. After a descriptive section on the design, typologie and
information levels of the resources, there follows an explanation of the
central role of the combinatorial meaning (roles and ontological features). The
study deals with different semantic f ilters applied in the selection,
organization and expansion of the lexicon, being these key pieces for the
generation of grammatically correct and semantically acceptable mono- and
biargumental nominal phrases.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.16309v1},
File          = {2312.16309v1.pdf}
}
@article{2312.15381v1,
Author        = {Marcin Åyczak},
Title         = {Classical Mereology is Axiomatizable Using Primitive Fusion in Two
  Sorted Logic},
Eprint        = {2312.15381v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {math.LO},
Abstract      = {\textit{Mereological fusion}, also known as \textit{composition} and
\textit{sum}, was originally used by me as a primitive notion to axiomatize
\textit{Extensional Mereology} wih \textit{atoms} in \cite{Ly22}. Here, I
extend this idea to axiomatize \textit{General Extensional Mereology}, also
called \textit{Classical Mereology}, which is neutral regarding the existence
of atoms. I give a proof that classical mereology is axiomatizable using
primitive mereological fusion within the framework of two-sorted logic, as I
announced in \cite{Ly22}. The use of the primitive notion of fusion instead of
primitive notion of \textit{part} was considered by G. Leibniz
\cite[50]{CoVa21}, S. Le\'sniewski \cite[CCLXIV, CCLXIV]{Le92}, K. Fine
\cite{Fi10}, J. Ketland, and T. Schindler \cite{KeSh16}, and S. Kleishmid
\cite{Kl17}. C. Lejewski formulated mereology using a single axiom with
primitive mereological sum \cite[222]{Sb84}. However, Lejewski's theory is
formulated in non-classical language of Le\'sniewski's ontology and contains
complicated quantification over function symbols, including quantification on
\textit{part of} and \textit{sum of}. Lejewski's approach is not expressible in
modern mereologies. The presented theory is the first contemporary
axiomatization of classical mereology in two-sorted logic with primitive
mereological fusion.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.15381v1},
File          = {2312.15381v1.pdf}
}
@article{2312.13881v1,
Author        = {Juraj Vladika and Alexander Fichtl and Florian Matthes},
Title         = {Diversifying Knowledge Enhancement of Biomedical Language Models using
  Adapter Modules and Knowledge Graphs},
Eprint        = {2312.13881v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent advances in natural language processing (NLP) owe their success to
pre-training language models on large amounts of unstructured data. Still,
there is an increasing effort to combine the unstructured nature of LMs with
structured knowledge and reasoning. Particularly in the rapidly evolving field
of biomedical NLP, knowledge-enhanced language models (KELMs) have emerged as
promising tools to bridge the gap between large language models and
domain-specific knowledge, considering the available biomedical knowledge
graphs (KGs) curated by experts over the decades. In this paper, we develop an
approach that uses lightweight adapter modules to inject structured biomedical
knowledge into pre-trained language models (PLMs). We use two large KGs, the
biomedical knowledge system UMLS and the novel biochemical ontology OntoChem,
with two prominent biomedical PLMs, PubMedBERT and BioLinkBERT. The approach
includes partitioning knowledge graphs into smaller subgraphs, fine-tuning
adapter modules for each subgraph, and combining the knowledge in a fusion
layer. We test the performance on three downstream tasks: document
classification,question answering, and natural language inference. We show that
our methodology leads to performance improvements in several instances while
keeping requirements in computing power low. Finally, we provide a detailed
interpretation of the results and report valuable insights for future work.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.13881v1},
File          = {2312.13881v1.pdf}
}
@article{2312.13530v1,
Author        = {Yu-Zheng Lin and Muntasir Mamun and Muhtasim Alam Chowdhury and Shuyu Cai and Mingyu Zhu and Banafsheh Saber Latibari and Kevin Immanuel Gubbi and Najmeh Nazari Bavarsad and Arjun Caputo and Avesta Sasan and Houman Homayoun and Setareh Rafatirad and Pratik Satam and Soheil Salehi},
Title         = {HW-V2W-Map: Hardware Vulnerability to Weakness Mapping Framework for
  Root Cause Analysis with GPT-assisted Mitigation Suggestion},
Eprint        = {2312.13530v1},
DOI           = {10.1145/3737459},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {The escalating complexity of modern computing frameworks has resulted in a
surge in the cybersecurity vulnerabilities reported to the National
Vulnerability Database (NVD) by practitioners. Despite the fact that the
stature of NVD is one of the most significant databases for the latest insights
into vulnerabilities, extracting meaningful trends from such a large amount of
unstructured data is still challenging without the application of suitable
technological methodologies. Previous efforts have mostly concentrated on
software vulnerabilities; however, a holistic strategy incorporates approaches
for mitigating vulnerabilities, score prediction, and a knowledge-generating
system that may extract relevant insights from the Common Weakness Enumeration
(CWE) and Common Vulnerability Exchange (CVE) databases is notably absent. As
the number of hardware attacks on Internet of Things (IoT) devices continues to
rapidly increase, we present the Hardware Vulnerability to Weakness Mapping
(HW-V2W-Map) Framework, which is a Machine Learning (ML) framework focusing on
hardware vulnerabilities and IoT security. The architecture that we have
proposed incorporates an Ontology-driven Storytelling framework, which
automates the process of updating the ontology in order to recognize patterns
and evolution of vulnerabilities over time and provides approaches for
mitigating the vulnerabilities. The repercussions of vulnerabilities can be
mitigated as a result of this, and conversely, future exposures can be
predicted and prevented. Furthermore, our proposed framework utilized
Generative Pre-trained Transformer (GPT) Large Language Models (LLMs) to
provide mitigation suggestions.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.13530v1},
File          = {2312.13530v1.pdf}
}
@article{2312.12989v1,
Author        = {Emily Groves and Minhong Wang and Yusuf Abdulle and Holger Kunz and Jason Hoelscher-Obermaier and Ronin Wu and Honghan Wu},
Title         = {Benchmarking and Analyzing In-context Learning, Fine-tuning and
  Supervised Learning for Biomedical Knowledge Curation: a focused study on
  chemical entities of biological interest},
Eprint        = {2312.12989v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Automated knowledge curation for biomedical ontologies is key to ensure that
they remain comprehensive, high-quality and up-to-date. In the era of
foundational language models, this study compares and analyzes three NLP
paradigms for curation tasks: in-context learning (ICL), fine-tuning (FT), and
supervised learning (ML). Using the Chemical Entities of Biological Interest
(ChEBI) database as a model ontology, three curation tasks were devised. For
ICL, three prompting strategies were employed with GPT-4, GPT-3.5, BioGPT.
PubmedBERT was chosen for the FT paradigm. For ML, six embedding models were
utilized for training Random Forest and Long-Short Term Memory models. Five
setups were designed to assess ML and FT model performance across different
data availability scenarios.Datasets for curation tasks included: task 1
(620,386), task 2 (611,430), and task 3 (617,381), maintaining a 50:50 positive
versus negative ratio. For ICL models, GPT-4 achieved best accuracy scores of
0.916, 0.766 and 0.874 for tasks 1-3 respectively. In a direct comparison, ML
(trained on ~260,000 triples) outperformed ICL in accuracy across all tasks.
(accuracy differences: +.11, +.22 and +.17). Fine-tuned PubmedBERT performed
similarly to leading ML models in tasks 1 & 2 (F1 differences: -.014 and
+.002), but worse in task 3 (-.048). Simulations revealed performance declines
in both ML and FT models with smaller and higher imbalanced training data.
where ICL (particularly GPT-4) excelled in tasks 1 & 3. GPT-4 excelled in tasks
1 and 3 with less than 6,000 triples, surpassing ML/FT. ICL underperformed
ML/FT in task 2.ICL-augmented foundation models can be good assistants for
knowledge curation with correct prompting, however, not making ML and FT
paradigms obsolete. The latter two require task-specific data to beat ICL. In
such cases, ML relies on small pretrained embeddings, minimizing computational
demands.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.12989v1},
File          = {2312.12989v1.pdf}
}
@article{2312.11713v2,
Author        = {Jared Strader and Nathan Hughes and William Chen and Alberto Speranzon and Luca Carlone},
Title         = {Indoor and Outdoor 3D Scene Graph Generation via Language-Enabled
  Spatial Ontologies},
Eprint        = {2312.11713v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {This paper proposes an approach to build 3D scene graphs in arbitrary indoor
and outdoor environments. Such extension is challenging; the hierarchy of
concepts that describe an outdoor environment is more complex than for indoors,
and manually defining such hierarchy is time-consuming and does not scale.
Furthermore, the lack of training data prevents the straightforward application
of learning-based tools used in indoor settings. To address these challenges,
we propose two novel extensions. First, we develop methods to build a spatial
ontology defining concepts and relations relevant for indoor and outdoor robot
operation. In particular, we use a Large Language Model (LLM) to build such an
ontology, thus largely reducing the amount of manual effort required. Second,
we leverage the spatial ontology for 3D scene graph construction using Logic
Tensor Networks (LTN) to add logical rules, or axioms (e.g., "a beach contains
sand"), which provide additional supervisory signals at training time thus
reducing the need for labelled data, providing better predictions, and even
allowing predicting concepts unseen at training time. We test our approach in a
variety of datasets, including indoor, rural, and coastal environments, and
show that it leads to a significant increase in the quality of the 3D scene
graph generation with sparsely annotated data.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.11713v2},
File          = {2312.11713v2.pdf}
}
@article{2312.10904v2,
Author        = {Sabrina Toro and Anna V Anagnostopoulos and Sue Bello and Kai Blumberg and Rhiannon Cameron and Leigh Carmody and Alexander D Diehl and Damion Dooley and William Duncan and Petra Fey and Pascale Gaudet and Nomi L Harris and Marcin Joachimiak and Leila Kiani and Tiago Lubiana and Monica C Munoz-Torres and Shawn O'Neil and David Osumi-Sutherland and Aleix Puig and Justin P Reese and Leonore Reiser and Sofia Robb and Troy Ruemping and James Seager and Eric Sid and Ray Stefancsik and Magalie Weber and Valerie Wood and Melissa A Haendel and Christopher J Mungall},
Title         = {Dynamic Retrieval Augmented Generation of Ontologies using Artificial
  Intelligence (DRAGON-AI)},
Eprint        = {2312.10904v2},
DOI           = {10.1186/s13326-024-00320-3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Background: Ontologies are fundamental components of informatics
infrastructure in domains such as biomedical, environmental, and food sciences,
representing consensus knowledge in an accurate and computable form. However,
their construction and maintenance demand substantial resources and necessitate
substantial collaboration between domain experts, curators, and ontology
experts. We present Dynamic Retrieval Augmented Generation of Ontologies using
AI (DRAGON-AI), an ontology generation method employing Large Language Models
(LLMs) and Retrieval Augmented Generation (RAG). DRAGON-AI can generate textual
and logical ontology components, drawing from existing knowledge in multiple
ontologies and unstructured text sources.
  Results: We assessed performance of DRAGON-AI on de novo term construction
across ten diverse ontologies, making use of extensive manual evaluation of
results. Our method has high precision for relationship generation, but has
slightly lower precision than from logic-based reasoning. Our method is also
able to generate definitions deemed acceptable by expert evaluators, but these
scored worse than human-authored definitions. Notably, evaluators with the
highest level of confidence in a domain were better able to discern flaws in
AI-generated definitions. We also demonstrated the ability of DRAGON-AI to
incorporate natural language instructions in the form of GitHub issues.
  Conclusions: These findings suggest DRAGON-AI's potential to substantially
aid the manual ontology construction process. However, our results also
underscore the importance of having expert curators and ontology editors drive
the ontology generation process.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.10904v2},
File          = {2312.10904v2.pdf}
}
@article{2312.10857v1,
Author        = {Christian Kindermann and Anne-Marie George and Bijan Parsia and Uli Sattler},
Title         = {Minimal Macro-Based Rewritings of Formal Languages: Theory and
  Applications in Ontology Engineering (and beyond)},
Eprint        = {2312.10857v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In this paper, we introduce the problem of rewriting finite formal languages
using syntactic macros such that the rewriting is minimal in size. We present
polynomial-time algorithms to solve variants of this problem and show their
correctness. To demonstrate the practical relevance of the proposed problems
and the feasibility and effectiveness of our algorithms in practice, we apply
these to biomedical ontologies authored in OWL. We find that such rewritings
can significantly reduce the size of ontologies by capturing repeated
expressions with macros. In addition to offering valuable assistance in
enhancing ontology quality and comprehension, the presented approach introduces
a systematic way of analysing and evaluating features of rewriting systems
(including syntactic macros, templates, or other forms of rewriting rules) in
terms of their influence on computational problems.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.10857v1},
File          = {2312.10857v1.pdf}
}
@article{2312.10519v1,
Author        = {Vishal Rana and Jianhao Peng and Chao Pan and Hanbaek Lyu and Albert Cheng and Minji Kim and Olgica Milenkovic},
Title         = {Interpretable Online Network Dictionary Learning for Inferring
  Long-Range Chromatin Interactions},
Eprint        = {2312.10519v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.GN},
Abstract      = {Dictionary learning (DL) is commonly used in computational biology to tackle
ubiquitous clustering problems due to its conceptual simplicity and relatively
low computational complexity. However, DL algorithms produce results that lack
interpretability and are not optimized for large-scale graph-structured data.
We propose a novel DL algorithm called online convex network dictionary
learning (online cvxNDL) that can handle extremely large datasets and enables
the interpretation of dictionary elements, which serve as cluster
representatives, through convex combinations of real measurements. Moreover,
the algorithm can be applied to network-structured data via specialized
subnetwork sampling techniques.
  To demonstrate the utility of our approach, we apply cvxNDL on 3D-genome
RNAPII ChIA-Drop data to identify important long-range interaction patterns.
ChIA-Drop probes higher-order interactions, and produces hypergraphs whose
nodes represent genomic fragments. The hyperedges represent observed physical
contacts. Our hypergraph model analysis creates an interpretable dictionary of
long-range interaction patterns that accurately represent global chromatin
physical contact maps. Using dictionary information, one can also associate the
contact maps with RNA transcripts and infer cellular functions.
  Our results offer two key insights. First, we demonstrate that online cvxNDL
retains the accuracy of classical DL methods while simultaneously ensuring
unique interpretability and scalability. Second, we identify distinct
collections of proximal and distal interaction patterns involving chromatin
elements shared by related processes across different chromosomes, as well as
patterns unique to specific chromosomes. To associate the dictionary elements
with biological properties of the corresponding chromatin regions, we employ
Gene Ontology enrichment analysis and perform RNA coexpression studies.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.10519v1},
File          = {2312.10519v1.pdf}
}
@article{2401.02863v1,
Author        = {Yi-Chun Chen and Arnav Jhala},
Title         = {A Customizable Generator for Comic-Style Visual Narrative},
Eprint        = {2401.02863v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We present a theory-inspired visual narrative generator that incorporates
comic-authoring idioms, which transfers the conceptual principles of comics
into system layers that integrate the theories to create comic content. The
generator creates comics through sequential decision-making across layers from
panel composition, object positions, panel transitions, and narrative elements.
Each layer's decisions are based on narrative goals and follow the respective
layer idioms of the medium. Cohn's narrative grammar provides the overall story
arc. Photographic compositions inspired by the rule of thirds is used to
provide panel compositions. McCloud's proposed panel transitions based on focus
shifts between scene, character, and temporal changes are encoded in the
transition layer. Finally, common overlay symbols (such as the exclamation) are
added based on analyzing action verbs using an action-verb ontology. We
demonstrate the variety of generated comics through various settings with
example outputs. The generator and associated modules could be a useful system
for visual narrative authoring and for further research into computational
models of visual narrative understanding.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2401.02863v1},
File          = {2401.02863v1.pdf}
}
@article{2312.08036v1,
Author        = {Huaiyuan Ying and Zhengyun Zhao and Yang Zhao and Sihang Zeng and Sheng Yu},
Title         = {CoRTEx: Contrastive Learning for Representing Terms via Explanations
  with Applications on Constructing Biomedical Knowledge Graphs},
Eprint        = {2312.08036v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Objective: Biomedical Knowledge Graphs play a pivotal role in various
biomedical research domains. Concurrently, term clustering emerges as a crucial
step in constructing these knowledge graphs, aiming to identify synonymous
terms. Due to a lack of knowledge, previous contrastive learning models trained
with Unified Medical Language System (UMLS) synonyms struggle at clustering
difficult terms and do not generalize well beyond UMLS terms. In this work, we
leverage the world knowledge from Large Language Models (LLMs) and propose
Contrastive Learning for Representing Terms via Explanations (CoRTEx) to
enhance term representation and significantly improves term clustering.
Materials and Methods: The model training involves generating explanations for
a cleaned subset of UMLS terms using ChatGPT. We employ contrastive learning,
considering term and explanation embeddings simultaneously, and progressively
introduce hard negative samples. Additionally, a ChatGPT-assisted BIRCH
algorithm is designed for efficient clustering of a new ontology. Results: We
established a clustering test set and a hard negative test set, where our model
consistently achieves the highest F1 score. With CoRTEx embeddings and the
modified BIRCH algorithm, we grouped 35,580,932 terms from the Biomedical
Informatics Ontology System (BIOS) into 22,104,559 clusters with O(N) queries
to ChatGPT. Case studies highlight the model's efficacy in handling challenging
samples, aided by information from explanations. Conclusion: By aligning terms
to their explanations, CoRTEx demonstrates superior accuracy over benchmark
models and robustness beyond its training set, and it is suitable for
clustering terms for large-scale biomedical ontologies.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.08036v1},
File          = {2312.08036v1.pdf}
}
@article{2312.07250v2,
Author        = {Lifeng Han and Serge Gladkoff and Gleb Erofeev and Irina Sorokina and Betty Galiano and Goran Nenadic},
Title         = {Neural Machine Translation of Clinical Text: An Empirical Investigation
  into Multilingual Pre-Trained Language Models and Transfer-Learning},
Eprint        = {2312.07250v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We conduct investigations on clinical text machine translation by examining
multilingual neural network models using deep learning such as Transformer
based structures. Furthermore, to address the language resource imbalance
issue, we also carry out experiments using a transfer learning methodology
based on massive multilingual pre-trained language models (MMPLMs). The
experimental results on three subtasks including 1) clinical case (CC), 2)
clinical terminology (CT), and 3) ontological concept (OC) show that our models
achieved top-level performances in the ClinSpEn-2022 shared task on
English-Spanish clinical domain data. Furthermore, our expert-based human
evaluations demonstrate that the small-sized pre-trained language model (PLM)
won over the other two extra-large language models by a large margin, in the
clinical domain fine-tuning, which finding was never reported in the field.
Finally, the transfer learning method works well in our experimental setting
using the WMT21fb model to accommodate a new language space Spanish that was
not seen at the pre-training stage within WMT21fb itself, which deserves more
exploitation for clinical knowledge transformation, e.g. to investigate into
more languages. These research findings can shed some light on domain-specific
machine translation development, especially in clinical and healthcare fields.
Further research projects can be carried out based on our work to improve
healthcare text analytics and knowledge transformation. Our data will be openly
available for research purposes at https://github.com/HECTA-UoM/ClinicalNMT},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.07250v2},
File          = {2312.07250v2.pdf}
}
@article{2312.06355v3,
Author        = {L. Siddharth and Jianxi Luo},
Title         = {Linguistic and Structural Basis of Engineering Design Knowledge},
Eprint        = {2312.06355v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Natural language artefact descriptions are primary carriers of engineering
design knowledge, whose retrieval, representation, and reuse are fundamental to
supporting knowledge-intensive tasks in the design process. In this paper, we
explicate design knowledge from patented artefact descriptions as knowledge
graphs and examine these to understand the linguistic and structural basis. The
purpose of our work is to advance the traditional and ontological perspectives
of design knowledge and to guide Large-Language Models (LLMs) on how to
articulate natural language responses that reflect knowledge that is valuable
in a design environment. We populate 33,881 knowledge graphs from a sample of
patents stratified according to technology classes. For linguistic basis, we
conduct Zipf distribution analyses on the frequencies of unique entities and
relationships to identify 64 and 37 generalisable linguistic syntaxes
respectively. The relationships largely represent attributes ('of'), structure
('in', 'with'), purpose ('to', 'for'), hierarchy ('include'), exemplification
('such as'), and behaviour ('to', 'from'). For structural basis, we draw
inspiration from various studies on biological/ecological networks and discover
motifs from patent knowledge graphs. We identify four 3-node and four 4-node
subgraph patterns that could be converged and simplified into sequence
[->...->], aggregation [->...<-], and hierarchy [<-...->]. Based on these
results, we suggest concretisation strategies for entities and relationships
and explicating hierarchical structures, potentially aiding the construction
and modularisation of design knowledge.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.06355v3},
File          = {2312.06355v3.pdf}
}
@article{2312.05805v1,
Author        = {Kelley Ann Yohe},
Title         = {Towards Global, Socio-Economic, and Culturally Aware Recommender Systems},
Eprint        = {2312.05805v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Recommender systems have gained increasing attention to personalise consumer
preferences. While these systems have primarily focused on applications such as
advertisement recommendations (e.g., Google), personalized suggestions (e.g.,
Netflix and Spotify), and retail selection (e.g., Amazon), there is potential
for these systems to benefit from a more global, socio-economic, and culturally
aware approach, particularly as companies seek to expand into diverse markets.
This paper aims to investigate the potential of a recommender system that
considers cultural identity and socio-economic factors. We review the most
recent developments in recommender systems and explore the impact of cultural
identity and socio-economic factors on consumer preferences. We then propose an
ontology and approach for incorporating these factors into recommender systems.
To illustrate the potential of our approach, we present a scenario in consumer
subscription plan selection within the entertainment industry. We argue that
existing recommender systems have limited ability to precisely understand user
preferences due to a lack of awareness of socio-economic factors and cultural
identity. They also fail to update recommendations in response to changing
socio-economic conditions. We explore various machine learning models and
develop a final artificial neural network model (ANN) that addresses this gap.
We evaluate the effectiveness of socio-economic and culturally aware
recommender systems across four dimensions: Precision, Accuracy, F1, and
Recall. We find that a highly tuned ANN model incorporating domain-specific
data, select cultural indices and relevant socio-economic factors predicts user
preference in subscriptions with an accuracy of 95%, a precision of 94%, a F1
Score of 92\%, and a Recall of 90\%.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.05805v1},
File          = {2312.05805v1.pdf}
}
@article{2312.05209v2,
Author        = {Navapat Nananukul and Mayank Kejriwal},
Title         = {HALO: An Ontology for Representing and Categorizing Hallucinations in
  Large Language Models},
Eprint        = {2312.05209v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Recent progress in generative AI, including large language models (LLMs) like
ChatGPT, has opened up significant opportunities in fields ranging from natural
language processing to knowledge discovery and data mining. However, there is
also a growing awareness that the models can be prone to problems such as
making information up or `hallucinations', and faulty reasoning on seemingly
simple problems. Because of the popularity of models like ChatGPT, both
academic scholars and citizen scientists have documented hallucinations of
several different types and severity. Despite this body of work, a formal model
for describing and representing these hallucinations (with relevant meta-data)
at a fine-grained level, is still lacking. In this paper, we address this gap
by presenting the Hallucination Ontology or HALO, a formal, extensible ontology
written in OWL that currently offers support for six different types of
hallucinations known to arise in LLMs, along with support for provenance and
experimental metadata. We also collect and publish a dataset containing
hallucinations that we inductively gathered across multiple independent Web
sources, and show that HALO can be successfully used to model this dataset and
answer competency questions.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.05209v2},
File          = {2312.05209v2.pdf}
}
@article{2312.00353v1,
Author        = {Pei-Chi Lo and Yi-Hang Tsai and Ee-Peng Lim and San-Yih Hwang},
Title         = {On Exploring the Reasoning Capability of Large Language Models with
  Knowledge Graphs},
Eprint        = {2312.00353v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper examines the capacity of LLMs to reason with knowledge graphs
using their internal knowledge graph, i.e., the knowledge graph they learned
during pre-training. Two research questions are formulated to investigate the
accuracy of LLMs in recalling information from pre-training knowledge graphs
and their ability to infer knowledge graph relations from context. To address
these questions, we employ LLMs to perform four distinct knowledge graph
reasoning tasks. Furthermore, we identify two types of hallucinations that may
occur during knowledge reasoning with LLMs: content and ontology hallucination.
Our experimental results demonstrate that LLMs can successfully tackle both
simple and complex knowledge graph reasoning tasks from their own memory, as
well as infer from input context.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.00353v1},
File          = {2312.00353v1.pdf}
}
@article{2312.00332v1,
Author        = {Peng Wang},
Title         = {Matching Weak Informative Ontologies},
Eprint        = {2312.00332v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Most existing ontology matching methods utilize the literal information to
discover alignments. However, some literal information in ontologies may be
opaque and some ontologies may not have sufficient literal information. In this
paper, these ontologies are named as weak informative ontologies (WIOs) and it
is challenging for existing methods to matching WIOs. On one hand, string-based
and linguistic-based matching methods cannot work well for WIOs. On the other
hand, some matching methods use external resources to improve their
performance, but collecting and processing external resources is still
time-consuming. To address this issue, this paper proposes a practical method
for matching WIOs by employing the ontology structure information to discover
alignments. First, the semantic subgraphs are extracted from the ontology graph
to capture the precise meanings of ontology elements. Then, a new similarity
propagation model is designed for matching WIOs. Meanwhile, in order to avoid
meaningless propagation, the similarity propagation is constrained by semantic
subgraphs and other conditions. Consequently, the similarity propagation model
ensures a balance between efficiency and quality during matching. Finally, the
similarity propagation model uses a few credible alignments as seeds to find
more alignments, and some useful strategies are adopted to improve the
performance. This matching method for WIOs has been implemented in the ontology
matching system Lily. Experimental results on public OAEI benchmark datasets
demonstrate that Lily significantly outperforms most of the state-of-the-art
works in both WIO matching tasks and general ontology matching tasks. In
particular, Lily increases the recall by a large margin, while it still obtains
high precision of matching results.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.00332v1},
File          = {2312.00332v1.pdf}
}
@article{2312.00326v13,
Author        = {Zhangcheng Qiang and Weiqing Wang and Kerry Taylor},
Title         = {Agent-OM: Leveraging LLM Agents for Ontology Matching},
Eprint        = {2312.00326v13},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontology matching (OM) enables semantic interoperability between different
ontologies and resolves their conceptual heterogeneity by aligning related
entities. OM systems currently have two prevailing design paradigms:
conventional knowledge-based expert systems and newer machine learning-based
predictive systems. While large language models (LLMs) and LLM agents have
revolutionised data engineering and have been applied creatively in many
domains, their potential for OM remains underexplored. This study introduces a
novel agent-powered LLM-based design paradigm for OM systems. With
consideration of several specific challenges in leveraging LLM agents for OM,
we propose a generic framework, namely Agent-OM (Agent for Ontology Matching),
consisting of two Siamese agents for retrieval and matching, with a set of OM
tools. Our framework is implemented in a proof-of-concept system. Evaluations
of three Ontology Alignment Evaluation Initiative (OAEI) tracks over
state-of-the-art OM systems show that our system can achieve results very close
to the long-standing best performance on simple OM tasks and can significantly
improve the performance on complex and few-shot OM tasks.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.00326v13},
File          = {2312.00326v13.pdf}
}
@article{2312.00183v1,
Author        = {Emanuele Cavalleri and Alberto Cabri and Mauricio Soto-Gomez and Sara Bonfitto and Paolo Perlasca and Jessica Gliozzo and Tiffany J. Callahan and Justin Reese and Peter N Robinson and Elena Casiraghi and Giorgio Valentini and Marco Mesiti},
Title         = {RNA-KG: An ontology-based knowledge graph for representing interactions
  involving RNA molecules},
Eprint        = {2312.00183v1},
DOI           = {10.1038/s41597-024-03673-7},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CE},
Abstract      = {The "RNA world" represents a novel frontier for the study of fundamental
biological processes and human diseases and is paving the way for the
development of new drugs tailored to the patient's biomolecular
characteristics. Although scientific data about coding and non-coding RNA
molecules are continuously produced and available from public repositories,
they are scattered across different databases and a centralized, uniform, and
semantically consistent representation of the "RNA world" is still lacking. We
propose RNA-KG, a knowledge graph encompassing biological knowledge about RNAs
gathered from more than 50 public databases, integrating functional
relationships with genes, proteins, and chemicals and ontologically grounded
biomedical concepts. To develop RNA-KG, we first identified, pre-processed, and
characterized each data source; next, we built a meta-graph that provides an
ontological description of the KG by representing all the bio-molecular
entities and medical concepts of interest in this domain, as well as the types
of interactions connecting them. Finally, we leveraged an instance-based
semantically abstracted knowledge model to specify the ontological alignment
according to which RNA-KG was generated. RNA-KG can be downloaded in different
formats and also queried by a SPARQL endpoint. A thorough topological analysis
of the resulting heterogeneous graph provides further insights into the
characteristics of the "RNA world". RNA-KG can be both directly explored and
visualized, and/or analyzed by applying computational methods to infer
bio-medical knowledge from its heterogeneous nodes and edges. The resource can
be easily updated with new experimental data, and specific views of the overall
KG can be extracted according to the bio-medical problem to be studied.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2312.00183v1},
File          = {2312.00183v1.pdf}
}
@article{2312.03742v1,
Author        = {Angeela Acharya and Sulabh Shrestha and Anyi Chen and Joseph Conte and Sanja Avramovic and Siddhartha Sikdar and Antonios Anastasopoulos and Sanmay Das},
Title         = {Clinical Risk Prediction Using Language Models: Benefits And
  Considerations},
Eprint        = {2312.03742v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The utilization of Electronic Health Records (EHRs) for clinical risk
prediction is on the rise. However, strict privacy regulations limit access to
comprehensive health records, making it challenging to apply standard machine
learning algorithms in practical real-world scenarios. Previous research has
addressed this data limitation by incorporating medical ontologies and
employing transfer learning methods. In this study, we investigate the
potential of leveraging language models (LMs) as a means to incorporate
supplementary domain knowledge for improving the performance of various
EHR-based risk prediction tasks. Unlike applying LMs to unstructured EHR data
such as clinical notes, this study focuses on using textual descriptions within
structured EHR to make predictions exclusively based on that information. We
extensively compare against previous approaches across various data types and
sizes. We find that employing LMs to represent structured EHRs, such as
diagnostic histories, leads to improved or at least comparable performance in
diverse risk prediction tasks. Furthermore, LM-based approaches offer numerous
advantages, including few-shot learning, the capability to handle previously
unseen medical concepts, and adaptability to various medical vocabularies.
Nevertheless, we underscore, through various experiments, the importance of
being cautious when employing such models, as concerns regarding the
reliability of LMs persist.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2312.03742v1},
File          = {2312.03742v1.pdf}
}
@article{2311.14540v3,
Author        = {Piotr Sowinski and Pawel Szmeja and Maria Ganzha and Marcin Paprzycki},
Title         = {RDF Stream Taxonomy: Systematizing RDF Stream Types in Research and
  Practice},
Eprint        = {2311.14540v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Over the years, RDF streaming was explored in research and practice from many
angles, resulting in a wide range of RDF stream definitions. This variety
presents a major challenge in discussing and integrating streaming systems, due
to the lack of a common language. This work attempts to address this critical
research gap, by systematizing RDF stream types present in the literature in a
novel taxonomy. The proposed RDF Stream Taxonomy (RDF-STaX) is embodied in an
OWL 2 DL ontology that follows the FAIR principles, making it readily
applicable in practice. Extensive documentation and additional resources are
provided, to foster the adoption of the ontology. Three use cases for the
ontology are presented with accompanying competency questions, demonstrating
the usefulness of the resource. Additionally, this work introduces a novel
nanopublications dataset, which serves as a collaborative, living
state-of-the-art review of RDF streaming. The results of a multifaceted
evaluation of the resource are presented, testing its logical validity, use
case coverage, and adherence to the community's best practices, while also
comparing it to other works. RDF-STaX is expected to help drive innovation in
RDF streaming, by fostering scientific discussion, cooperation, and tool
interoperability.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.14540v3},
File          = {2311.14540v3.pdf}
}
@article{2311.17928v1,
Author        = {Ahmet Ãevik},
Title         = {Hierarchical Multiverse of Sets},
Eprint        = {2311.17928v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {math.LO},
Abstract      = {In this paper, I develop a novel version of the multiverse theory of sets
called hierarchical pluralism by introducing the notion of `degrees of
intentionality' of theories. The presented view is articulated for the purpose
of reconciling epistemological realism and the multiverse theory of sets so as
to preserve a considerable amount of epistemic objectivity when working with
the multiverse theory. I give some arguments in favour of a hierarchical
picture of the multiverse in which theories or models are thought to be ordered
with respect to their plausibility, as a manifestation of endorsing the idea
that some set theories are more plausible than others. The proposed multiverse
account settles the pluralist's dilemma, the dichotomy that there is a
trade-off between the richness of mathematical ontology and the objectivity of
mathematical truth. The view also extends and serves as an alternative position
to Balaguer's intention-based Platonism from which he claims that a certain
version of mathematical pluralism follows.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.17928v1},
File          = {2311.17928v1.pdf}
}
@article{2311.13852v4,
Author        = {Sumit Dalal and Deepa Tilwani and Kaushik Roy and Manas Gaur and Sarika Jain and Valerie Shalin and Amit Sheth},
Title         = {A Cross Attention Approach to Diagnostic Explainability using Clinical
  Practice Guidelines for Depression},
Eprint        = {2311.13852v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The lack of explainability using relevant clinical knowledge hinders the
adoption of Artificial Intelligence-powered analysis of unstructured clinical
dialogue. A wealth of relevant, untapped Mental Health (MH) data is available
in online communities, providing the opportunity to address the explainability
problem with substantial potential impact as a screening tool for both online
and offline applications. We develop a method to enhance attention in popular
transformer models and generate clinician-understandable explanations for
classification by incorporating external clinical knowledge. Inspired by how
clinicians rely on their expertise when interacting with patients, we leverage
relevant clinical knowledge to model patient inputs, providing meaningful
explanations for classification. This will save manual review time and engender
trust. We develop such a system in the context of MH using clinical practice
guidelines (CPG) for diagnosing depression, a mental health disorder of global
concern. We propose an application-specific language model called ProcesS
knowledge-infused cross ATtention (PSAT), which incorporates CPGs when
computing attention. Through rigorous evaluation on three expert-curated
datasets related to depression, we demonstrate application-relevant
explainability of PSAT. PSAT also surpasses the performance of nine baseline
models and can provide explanations where other baselines fall short. We
transform a CPG resource focused on depression, such as the Patient Health
Questionnaire (e.g. PHQ-9) and related questions, into a machine-readable
ontology using SNOMED-CT. With this resource, PSAT enhances the ability of
models like GPT-3.5 to generate application-relevant explanations.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.13852v4},
File          = {2311.13852v4.pdf}
}
@article{2401.03343v2,
Author        = {B. Dutta and S. Arzoo},
Title         = {Rediscovering Ranganathan: A Prismatic View of His Life through the
  Knowledge Graph Spectrum},
Eprint        = {2401.03343v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {The present study puts forward a novel biographical knowledge graph (KG) on
Prof. S. R. Ranganathan, one of the pioneering figures in the Library and
Information Science (LIS) domain. It has been found that most of the relevant
facts about Ranganathan exist in a variety of resources (e.g., books, essays,
journal articles, websites, blogs, etc.), offering information in a fragmented
and piecemeal way. With this dedicated KG (henceforth known as RKG), we hope to
furnish a 360-degree view of his life and achievements. To the best of our
knowledge, such a dedicated representation is unparalleled in its scope and
coverage: using state-of-the-art technology for anyone to openly access,
use/re-use, and contribute. Inspired by Ranganathan's theories and ideas, the
KG was developed using a "facet-based methodology" at two levels: in the
identification of the vital biographical aspects and the development of the
ontological model. Finally, with this study, we call for a community-driven
effort to enhance the KG and pay homage to the Father of Library Science on the
hundredth anniversary of his revitalizing the LIS domain through his enduring
participation.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2401.03343v2},
File          = {2401.03343v2.pdf}
}
@article{2311.13474v2,
Author        = {Massy Khoshbin and Lorenzo Catani and Matthew Leifer},
Title         = {Alternative robust ways of witnessing nonclassicality in the simplest
  scenario},
Eprint        = {2311.13474v2},
DOI           = {10.1103/PhysRevA.109.032212},
ArchivePrefix = {arXiv},
PrimaryClass  = {quant-ph},
Abstract      = {In this paper we relate notions of nonclassicality in the simplest nontrivial
scenario (a prepare and measure scenario composed of four preparations and two
binary-outcome tomographically complete measurements). Specifically, we relate
the established method developed in [Pusey, PRA 98,022112(2018)] to witness a
violation of preparation noncontextuality, that is not suitable in experiments
where the operational equivalences to be tested are specified in advance, with
an approach based on the notion of bounded ontological distinctness for
preparations, defined in [Chaturvedi and Saha, Quantum 4, 345 (2020)]. In our
approach, we test bounded ontological distinctness for two particular
preparations that are relevant in certain information processing tasks in that
they are associated with the even- and odd-parity of the bits to communicate.
When there exists an ontological model where this distance is preserved we talk
of parity preservation. Our main result provides a noise threshold under which
violating parity preservation (and so bounded ontological distinctness) agrees
with the established method for witnessing preparation contextuality in the
simplest nontrivial scenario. This is achieved by first relating the violation
of parity preservation to the quantification of contextuality in terms of
inaccessible information as developed in [Marvian, arXiv:2003.05984(2020)],
that we also show, given the way we quantify noise, to be more robust in
witnessing contextuality than Pusey's noncontextuality inequality. As an
application of our findings, we treat the case of two-bit parity-oblivious
multiplexing in the presence of noise. In particular, we provide a condition
for which the result establishing preparation contextuality as a resource for
the quantum advantage of the protocol in the noiseless case still holds in the
noisy case.},
Year          = {2023},
Month         = {Nov},
Note          = {Phys. Rev. A, 2024, Volume 3, pp. 032212},
Url           = {http://arxiv.org/abs/2311.13474v2},
File          = {2311.13474v2.pdf}
}
@article{2311.12465v1,
Author        = {Mattia Fumagalli and Marco Boffo and Daqian Shi and Mayukh Bagchi and Fausto Giunchiglia},
Title         = {Towards a Gateway for Knowledge Graph Schemas Collection, Analysis, and
  Embedding},
Eprint        = {2311.12465v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {One of the significant barriers to the training of statistical models on
knowledge graphs is the difficulty that scientists have in finding the best
input data to address their prediction goal. In addition to this, a key
challenge is to determine how to manipulate these relational data, which are
often in the form of particular triples (i.e., subject, predicate, object), to
enable the learning process. Currently, many high-quality catalogs of knowledge
graphs, are available. However, their primary goal is the re-usability of these
resources, and their interconnection, in the context of the Semantic Web. This
paper describes the LiveSchema initiative, namely, a first version of a gateway
that has the main scope of leveraging the gold mine of data collected by many
existing catalogs collecting relational data like ontologies and knowledge
graphs. At the current state, LiveSchema contains - 1000 datasets from 4 main
sources and offers some key facilities, which allow to: i) evolving LiveSchema,
by aggregating other source catalogs and repositories as input sources; ii)
querying all the collected resources; iii) transforming each given dataset into
formal concept analysis matrices that enable analysis and visualization
services; iv) generating models and tensors from each given dataset.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.12465v1},
File          = {2311.12465v1.pdf}
}
@article{2311.12236v1,
Author        = {Teodoro Baldazzi and Luigi Bellomarini and Marco Favorito and Emanuel Sallinger},
Title         = {Ontological Reasoning over Shy and Warded Datalog$+/-$ for
  Streaming-based Architectures (technical report)},
Eprint        = {2311.12236v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Recent years witnessed a rising interest towards Datalog-based ontological
reasoning systems, both in academia and industry. These systems adopt
languages, often shared under the collective name of Datalog$+/-$, that extend
Datalog with the essential feature of existential quantification, while
introducing syntactic limitations to sustain reasoning decidability and achieve
a good trade-off between expressive power and computational complexity. From an
implementation perspective, modern reasoners borrow the vast experience of the
database community in developing streaming-based data processing systems, such
as volcano-iterator architectures, that sustain a limited memory footprint and
good scalability. In this paper, we focus on two extremely promising,
expressive, and tractable languages, namely, Shy and Warded Datalog$+/-$. We
leverage their theoretical underpinnings to introduce novel reasoning
techniques, technically, "chase variants", that are particularly fit for
efficient reasoning in streaming-based architectures. We then implement them in
Vadalog, our reference streaming-based engine, to efficiently solve ontological
reasoning tasks over real-world settings.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.12236v1},
File          = {2311.12236v1.pdf}
}
@article{2311.10937v1,
Author        = {Kunkun Hao and Lu Liu and Wen Cui and Jianxing Zhang and Songyang Yan and Yuxi Pan and Zijiang Yang},
Title         = {Bridging Data-Driven and Knowledge-Driven Approaches for Safety-Critical
  Scenario Generation in Automated Vehicle Validation},
Eprint        = {2311.10937v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Automated driving vehicles~(ADV) promise to enhance driving efficiency and
safety, yet they face intricate challenges in safety-critical scenarios. As a
result, validating ADV within generated safety-critical scenarios is essential
for both development and performance evaluations. This paper investigates the
complexities of employing two major scenario-generation solutions: data-driven
and knowledge-driven methods. Data-driven methods derive scenarios from
recorded datasets, efficiently generating scenarios by altering the existing
behavior or trajectories of traffic participants but often falling short in
considering ADV perception; knowledge-driven methods provide effective coverage
through expert-designed rules, but they may lead to inefficiency in generating
safety-critical scenarios within that coverage. To overcome these challenges,
we introduce BridgeGen, a safety-critical scenario generation framework,
designed to bridge the benefits of both methodologies. Specifically, by
utilizing ontology-based techniques, BridgeGen models the five scenario layers
in the operational design domain (ODD) from knowledge-driven methods, ensuring
broad coverage, and incorporating data-driven strategies to efficiently
generate safety-critical scenarios. An optimized scenario generation toolkit is
developed within BridgeGen. This expedites the crafting of safety-critical
scenarios through a combination of traditional optimization and reinforcement
learning schemes. Extensive experiments conducted using Carla simulator
demonstrate the effectiveness of BridgeGen in generating diverse
safety-critical scenarios.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.10937v1},
File          = {2311.10937v1.pdf}
}
@article{2311.10247v1,
Author        = {S. M. M. Rasouli},
Title         = {Noncompactified Kaluza--Klein theories and Anisotropic Kantowski-Sachs
  Universe},
Eprint        = {2311.10247v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {gr-qc},
Abstract      = {We provide an overview of noncompactified Kaluza-Klein theories. The
space-time-matter theory (or induced-matter theory) and the modified
Brans-Dicke theory are discussed. Finally, an extended version of the
Kantowski-Sachs anisotropic model is investigated as a cosmological application
of the latter.},
Year          = {2023},
Month         = {Nov},
Note          = {Proceeding of the Sixth International Conference on the Nature and
  Ontology of Spacetime 12-15 September 2022, Albena, Bulgaria},
Url           = {http://arxiv.org/abs/2311.10247v1},
File          = {2311.10247v1.pdf}
}
@article{2311.08546v1,
Author        = {Yanying Wu},
Title         = {A Category of Genes},
Eprint        = {2311.08546v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.OT},
Abstract      = {Understanding how genes interact and relate to each other is a fundamental
question in biology. However, current practices for describing these
relationships, such as drawing diagrams or graphs in a somewhat arbitrary
manner, limit our ability to integrate various aspects of the gene functions
and view the genome holistically. To overcome these limitations, we need a more
appropriate way to describe the intricate relationships between genes.
Interestingly, category theory, an abstract field of mathematics seemingly
unrelated to biology, has emerged as a powerful language for describing
relations in general. We propose that category theory could provide a framework
for unifying our knowledge of genes and their relationships.
  As a starting point, we construct a category of genes, with its morphisms
abstracting various aspects of the relationships betweens genes. These
relationships include, but not limited to, the order of genes on the
chromosomes, the physical or genetic interactions, the signalling pathways, the
gene ontology causal activity models (GO-CAM) and gene groups. Previously, they
were encoded by miscellaneous networks or graphs, while our work unifies them
in a consistent manner as a category. By doing so, we hope to view the
relationships between genes systematically. In the long run, this paves a
promising way for us to understand the fundamental principles that govern gene
regulation and function.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.08546v1},
File          = {2311.08546v1.pdf}
}
@article{2311.07509v1,
Author        = {Juan Sequeda and Dean Allemang and Bryon Jacob},
Title         = {A Benchmark to Understand the Role of Knowledge Graphs on Large Language
  Model's Accuracy for Question Answering on Enterprise SQL Databases},
Eprint        = {2311.07509v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Enterprise applications of Large Language Models (LLMs) hold promise for
question answering on enterprise SQL databases. However, the extent to which
LLMs can accurately respond to enterprise questions in such databases remains
unclear, given the absence of suitable Text-to-SQL benchmarks tailored to
enterprise settings. Additionally, the potential of Knowledge Graphs (KGs) to
enhance LLM-based question answering by providing business context is not well
understood. This study aims to evaluate the accuracy of LLM-powered question
answering systems in the context of enterprise questions and SQL databases,
while also exploring the role of knowledge graphs in improving accuracy. To
achieve this, we introduce a benchmark comprising an enterprise SQL schema in
the insurance domain, a range of enterprise queries encompassing reporting to
metrics, and a contextual layer incorporating an ontology and mappings that
define a knowledge graph. Our primary finding reveals that question answering
using GPT-4, with zero-shot prompts directly on SQL databases, achieves an
accuracy of 16%. Notably, this accuracy increases to 54% when questions are
posed over a Knowledge Graph representation of the enterprise SQL database.
Therefore, investing in Knowledge Graph provides higher accuracy for LLM
powered question answering systems.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.07509v1},
File          = {2311.07509v1.pdf}
}
@article{2311.07396v1,
Author        = {Nele Kadastik and Thomas A. Pederson and Luis Emilio Bruni and Rossana Damiano and Antonio Lieto and Manuel Striani and Tsvi Kuflik and Alan Wecker},
Title         = {Exploring Values in Museum Artifacts in the SPICE project: a Preliminary
  Study},
Eprint        = {2311.07396v1},
DOI           = {10.1145/3511047.3537662},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This document describes the rationale, the implementation and a preliminary
evaluation of a semantic reasoning tool developed in the EU H2020 SPICE project
to enhance the diversity of perspectives experienced by museum visitors. The
tool, called DEGARI 2.0 for values, relies on the commonsense reasoning
framework TCL, and exploits an ontological model formalizingthe Haidt's theory
of moral values to associate museum items with combined values and emotions.
Within a museum exhibition, this tool can suggest cultural items that are
associated not only with the values of already experienced or preferred
objects, but also with novel items with different value stances, opening the
visit experience to more inclusive interpretations of cultural content. The
system has been preliminarily tested, in the context of the SPICE project, on
the collection of the Hecht Museum of Haifa.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.07396v1},
File          = {2311.07396v1.pdf}
}
@article{2311.07180v1,
Author        = {Samyak Jain and Manuel Burger and Gunnar RÃ¤tsch and Rita Kuznetsova},
Title         = {Knowledge Graph Representations to enhance Intensive Care Time-Series
  Predictions},
Eprint        = {2311.07180v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Intensive Care Units (ICU) require comprehensive patient data integration for
enhanced clinical outcome predictions, crucial for assessing patient
conditions. Recent deep learning advances have utilized patient time series
data, and fusion models have incorporated unstructured clinical reports,
improving predictive performance. However, integrating established medical
knowledge into these models has not yet been explored. The medical domain's
data, rich in structural relationships, can be harnessed through knowledge
graphs derived from clinical ontologies like the Unified Medical Language
System (UMLS) for better predictions. Our proposed methodology integrates this
knowledge with ICU data, improving clinical decision modeling. It combines
graph representations with vital signs and clinical reports, enhancing
performance, especially when data is missing. Additionally, our model includes
an interpretability component to understand how knowledge graph nodes affect
predictions.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.07180v1},
File          = {2311.07180v1.pdf}
}
@article{2311.06858v1,
Author        = {Antonio Zaitoun and Tomer Sagi and Szymon Wilk and Mor Peleg},
Title         = {Can Large Language Models Augment a Biomedical Ontology with missing
  Concepts and Relations?},
Eprint        = {2311.06858v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Ontologies play a crucial role in organizing and representing knowledge.
However, even current ontologies do not encompass all relevant concepts and
relationships. Here, we explore the potential of large language models (LLM) to
expand an existing ontology in a semi-automated fashion. We demonstrate our
approach on the biomedical ontology SNOMED-CT utilizing semantic relation types
from the widely used UMLS semantic network. We propose a method that uses
conversational interactions with an LLM to analyze clinical practice guidelines
(CPGs) and detect the relationships among the new medical concepts that are not
present in SNOMED-CT. Our initial experimentation with the conversational
prompts yielded promising preliminary results given a manually generated gold
standard, directing our future potential improvements.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.06858v1},
File          = {2311.06858v1.pdf}
}
@article{2311.06729v1,
Author        = {Salim Sazzed},
Title         = {Comprehending Lexical and Affective Ontologies in the Demographically
  Diverse Spatial Social Media Discourse},
Eprint        = {2311.06729v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This study aims to comprehend linguistic and socio-demographic features,
encompassing English language styles, conveyed sentiments, and lexical
diversity within spatial online social media review data. To this end, we
undertake a case study that scrutinizes reviews composed by two distinct and
demographically diverse groups. Our analysis entails the extraction and
examination of various statistical, grammatical, and sentimental features from
these two groups. Subsequently, we leverage these features with machine
learning (ML) classifiers to discern their potential in effectively
differentiating between the groups. Our investigation unveils substantial
disparities in certain linguistic attributes between the two groups. When
integrated into ML classifiers, these attributes exhibit a marked efficacy in
distinguishing the groups, yielding a macro F1 score of approximately 0.85.
Furthermore, we conduct a comparative evaluation of these linguistic features
with word n-gram-based lexical features in discerning demographically diverse
review data. As expected, the n-gram lexical features, coupled with fine-tuned
transformer-based models, show superior performance, attaining accuracies
surpassing 95\% and macro F1 scores exceeding 0.96. Our meticulous analysis and
comprehensive evaluations substantiate the efficacy of linguistic and
sentimental features in effectively discerning demographically diverse review
data. The findings of this study provide valuable guidelines for future
research endeavors concerning the analysis of demographic patterns in textual
content across various social media platforms.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.06729v1},
File          = {2311.06729v1.pdf}
}
@article{2311.06183v1,
Author        = {Ganesh Kumar and Shuib Basri and Abdullahi Abubakar Imam and Abdullateef Oluwaqbemiga Balogun and Hussaini Mamman and Luiz Fernando Capretz},
Title         = {A Novel Multidimensional Reference Model For Heterogeneous Textual
  Datasets Using Context, Semantic And Syntactic Clues},
Eprint        = {2311.06183v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {With the advent of technology and use of latest devices, they produces
voluminous data. Out of it, 80% of the data are unstructured and remaining 20%
are structured and semi-structured. The produced data are in heterogeneous
format and without following any standards. Among heterogeneous (structured,
semi-structured and unstructured) data, textual data are nowadays used by
industries for prediction and visualization of future challenges. Extracting
useful information from it is really challenging for stakeholders due to
lexical and semantic matching. Few studies have been solving this issue by
using ontologies and semantic tools, but the main limitations of proposed work
were the less coverage of multidimensional terms. To solve this problem, this
study aims to produce a novel multidimensional reference model using
linguistics categories for heterogeneous textual datasets. The categories such
context, semantic and syntactic clues are focused along with their score. The
main contribution of MRM is that it checks each tokens with each term based on
indexing of linguistic categories such as synonym, antonym, formal, lexical
word order and co-occurrence. The experiments show that the percentage of MRM
is better than the state-of-the-art single dimension reference model in terms
of more coverage, linguistics categories and heterogeneous datasets.},
Year          = {2023},
Month         = {Nov},
Note          = {International Journal of Advanced Science and Applications, Volume
  14, Issue 10, pp. 754-763, 2023},
Url           = {http://arxiv.org/abs/2311.06183v1},
File          = {2311.06183v1.pdf}
}
@article{2311.07595v2,
Author        = {Ritesh Chandra and Sadhana Tiwari and Satyam Rastogi and Sonali Agarwal},
Title         = {A Diagnosis and Treatment of Liver Diseases: Integrating Batch
  Processing, Rule-Based Event Detection and Explainable Artificial
  Intelligence},
Eprint        = {2311.07595v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Liver diseases pose a significant global health burden, impacting many
individuals and having substantial economic and social consequences. Rising
liver problems are considered a fatal disease in many countries, such as Egypt
and Moldova. This study aims to develop a diagnosis and treatment model for
liver disease using Basic Formal Ontology (BFO), Patient Clinical Data (PCD)
ontology, and detection rules derived from a decision tree algorithm. For the
development of the ontology, the National Viral Hepatitis Control Program
(NVHCP) guidelines were used, which made the ontology more accurate and
reliable. The Apache Jena framework uses batch processing to detect events
based on these rules. Based on the event detected, queries can be directly
processed using SPARQL. We convert these Decision Tree (DT) and medical
guidelines-based rules into Semantic Web Rule Language (SWRL) to operationalize
the ontology. Using this SWRL in the ontology to predict different types of
liver disease with the help of the Pellet and Drools inference engines in
Protege Tools, a total of 615 records were taken from different liver diseases.
After inferring the rules, the result can be generated for the patient
according to the rules, and other patient-related details, along with different
precautionary suggestions, can be obtained based on these results. These rules
can make suggestions more accurate with the help of Explainable Artificial
Intelligence (XAI) with open API-based suggestions. When the patient has
prescribed a medical test, the model accommodates this result using optical
character recognition (OCR), and the same process applies when the patient has
prescribed a further medical suggestion according to the test report. These
models combine to form a comprehensive Decision Support System (DSS) for the
diagnosis of liver disease.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.07595v2},
File          = {2311.07595v2.pdf}
}
@article{2311.14699v1,
Author        = {Bryar A. Hassan},
Title         = {Ontology Learning Using Formal Concept Analysis and WordNet},
Eprint        = {2311.14699v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Manual ontology construction takes time, resources, and domain specialists.
Supporting a component of this process for automation or semi-automation would
be good. This project and dissertation provide a Formal Concept Analysis and
WordNet framework for learning concept hierarchies from free texts. The process
has steps. First, the document is Part-Of-Speech labeled, then parsed to
produce sentence parse trees. Verb/noun dependencies are derived from parse
trees next. After lemmatizing, pruning, and filtering the word pairings, the
formal context is created. The formal context may contain some erroneous and
uninteresting pairs because the parser output may be erroneous, not all derived
pairs are interesting, and it may be large due to constructing it from a large
free text corpus. Deriving lattice from the formal context may take longer,
depending on the size and complexity of the data. Thus, decreasing formal
context may eliminate erroneous and uninteresting pairs and speed up idea
lattice derivation. WordNet-based and Frequency-based approaches are tested.
Finally, we compute formal idea lattice and create a classical concept
hierarchy. The reduced concept lattice is compared to the original to evaluate
the outcomes. Despite several system constraints and component discrepancies
that may prevent logical conclusion, the following data imply idea hierarchies
in this project and dissertation are promising. First, the reduced idea lattice
and original concept have commonalities. Second, alternative language or
statistical methods can reduce formal context size. Finally, WordNet-based and
Frequency-based approaches reduce formal context differently, and the order of
applying them is examined to reduce context efficiently.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.14699v1},
File          = {2311.14699v1.pdf}
}
@article{2311.05662v1,
Author        = {Reham Alharbi and Valentina Tamma and Floriana Grasso and Terry Payne},
Title         = {An Experiment in Retrofitting Competency Questions for Existing
  Ontologies},
Eprint        = {2311.05662v1},
DOI           = {10.1145/3605098.3636053},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Competency Questions (CQs) are a form of ontology functional requirements
expressed as natural language questions. Inspecting CQs together with the
axioms in an ontology provides critical insights into the intended scope and
applicability of the ontology. CQs also underpin a number of tasks in the
development of ontologies e.g. ontology reuse, ontology testing, requirement
specification, and the definition of patterns that implement such requirements.
Although CQs are integral to the majority of ontology engineering
methodologies, the practice of publishing CQs alongside the ontological
artefacts is not widely observed by the community. In this context, we present
an experiment in retrofitting CQs from existing ontologies. We propose
RETROFIT-CQs, a method to extract candidate CQs directly from ontologies using
Generative AI. In the paper we present the pipeline that facilitates the
extraction of CQs by leveraging Large Language Models (LLMs) and we discuss its
application to a number of existing ontologies.},
Year          = {2023},
Month         = {Nov},
Note          = {2024},
Url           = {http://arxiv.org/abs/2311.05662v1},
File          = {2311.05662v1.pdf}
}
@article{2311.04778v1,
Author        = {Roberto Confalonieri and Giancarlo Guizzardi},
Title         = {On the Multiple Roles of Ontologies in Explainable AI},
Eprint        = {2311.04778v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper discusses the different roles that explicit knowledge, in
particular ontologies, can play in Explainable AI and in the development of
human-centric explainable systems and intelligible explanations. We consider
three main perspectives in which ontologies can contribute significantly,
namely reference modelling, common-sense reasoning, and knowledge refinement
and complexity management. We overview some of the existing approaches in the
literature, and we position them according to these three proposed
perspectives. The paper concludes by discussing what challenges still need to
be addressed to enable ontology-based approaches to explanation and to evaluate
their human-understandability and effectiveness.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.04778v1},
File          = {2311.04778v1.pdf}
}
@article{2311.04490v1,
Author        = {Prabuddha Roy and A. K. Pan},
Title         = {Generalized parity-oblivious communication games powered by quantum
  preparation contextuality},
Eprint        = {2311.04490v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {quant-ph},
Abstract      = {The parity-oblivious random-access-code (PORAC) is a class of communication
games involving a sender (Alice) and a receiver (Bob). In such games, Alice's
amount of communication to Bob is constraint by the parity-oblivious (PO)
conditions, so that the parity information of her inputs remains oblivious to
Bob. The PO condition in an operational theory is equivalently represented in
an ontological model that satisfies the preparation noncontextuality. In this
paper, we provide a nontrivial generalization of the existing two-level PORAC
and derive the winning probability of the game in the preparation noncontextual
ontological model. We demonstrate that the quantum theory outperforms the
preparation noncontextual model by predicting higher winning probability in our
generalized PORAC.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.04490v1},
File          = {2311.04490v1.pdf}
}
@article{2311.03942v1,
Author        = {Jacopo de Berardinis and Valentina Anita Carriero and Albert MeroÃ±o-PeÃ±uela and Andrea Poltronieri and Valentina Presutti},
Title         = {The Music Meta Ontology: a flexible semantic model for the
  interoperability of music metadata},
Eprint        = {2311.03942v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {The semantic description of music metadata is a key requirement for the
creation of music datasets that can be aligned, integrated, and accessed for
information retrieval and knowledge discovery. It is nonetheless an open
challenge due to the complexity of musical concepts arising from different
genres, styles, and periods -- standing to benefit from a lingua franca to
accommodate various stakeholders (musicologists, librarians, data engineers,
etc.). To initiate this transition, we introduce the Music Meta ontology, a
rich and flexible semantic model to describe music metadata related to artists,
compositions, performances, recordings, and links. We follow eXtreme Design
methodologies and best practices for data engineering, to reflect the
perspectives and the requirements of various stakeholders into the design of
the model, while leveraging ontology design patterns and accounting for
provenance at different levels (claims, links). After presenting the main
features of Music Meta, we provide a first evaluation of the model, alignments
to other schema (Music Ontology, DOREMUS, Wikidata), and support for data
transformation.},
Year          = {2023},
Month         = {Nov},
Note          = {Proceedings of the the 24th International Society for Music
  Information Retrieval Conference, ISMIR 2023},
Url           = {http://arxiv.org/abs/2311.03942v1},
File          = {2311.03942v1.pdf}
}
@article{2311.03837v1,
Author        = {Sven Hertling and Heiko Paulheim},
Title         = {OLaLa: Ontology Matching with Large Language Models},
Eprint        = {2311.03837v1},
DOI           = {10.1145/3587259.3627571},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Ontology (and more generally: Knowledge Graph) Matching is a challenging task
where information in natural language is one of the most important signals to
process. With the rise of Large Language Models, it is possible to incorporate
this knowledge in a better way into the matching pipeline. A number of
decisions still need to be taken, e.g., how to generate a prompt that is useful
to the model, how information in the KG can be formulated in prompts, which
Large Language Model to choose, how to provide existing correspondences to the
model, how to generate candidates, etc. In this paper, we present a prototype
that explores these questions by applying zero-shot and few-shot prompting with
multiple open Large Language Models to different tasks of the Ontology
Alignment Evaluation Initiative (OAEI). We show that with only a handful of
examples and a well-designed prompt, it is possible to achieve results that are
en par with supervised matching systems which use a much larger portion of the
ground truth.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.03837v1},
File          = {2311.03837v1.pdf}
}
@article{2311.02462v4,
Author        = {Meredith Ringel Morris and Jascha Sohl-dickstein and Noah Fiedel and Tris Warkentin and Allan Dafoe and Aleksandra Faust and Clement Farabet and Shane Legg},
Title         = {Levels of AGI for Operationalizing Progress on the Path to AGI},
Eprint        = {2311.02462v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We propose a framework for classifying the capabilities and behavior of
Artificial General Intelligence (AGI) models and their precursors. This
framework introduces levels of AGI performance, generality, and autonomy,
providing a common language to compare models, assess risks, and measure
progress along the path to AGI. To develop our framework, we analyze existing
definitions of AGI, and distill six principles that a useful ontology for AGI
should satisfy. With these principles in mind, we propose "Levels of AGI" based
on depth (performance) and breadth (generality) of capabilities, and reflect on
how current systems fit into this ontology. We discuss the challenging
requirements for future benchmarks that quantify the behavior and capabilities
of AGI models against these levels. Finally, we discuss how these levels of AGI
interact with deployment considerations such as autonomy and risk, and
emphasize the importance of carefully selecting Human-AI Interaction paradigms
for responsible and safe deployment of highly capable AI systems.},
Year          = {2023},
Month         = {Nov},
Note          = {Proceedings of ICML 2024},
Url           = {http://arxiv.org/abs/2311.02462v4},
File          = {2311.02462v4.pdf}
}
@article{2311.00668v1,
Author        = {Oriol Barbany and Xiaofan Lin and Muhammet Bastan and Arnab Dhua},
Title         = {ProcSim: Proxy-based Confidence for Robust Similarity Learning},
Eprint        = {2311.00668v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Deep Metric Learning (DML) methods aim at learning an embedding space in
which distances are closely related to the inherent semantic similarity of the
inputs. Previous studies have shown that popular benchmark datasets often
contain numerous wrong labels, and DML methods are susceptible to them.
Intending to study the effect of realistic noise, we create an ontology of the
classes in a dataset and use it to simulate semantically coherent labeling
mistakes. To train robust DML models, we propose ProcSim, a simple framework
that assigns a confidence score to each sample using the normalized distance to
its class representative. The experimental results show that the proposed
method achieves state-of-the-art performance on the DML benchmark datasets
injected with uniform and the proposed semantically coherent noise.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.00668v1},
File          = {2311.00668v1.pdf}
}
@article{2311.04910v1,
Author        = {Oleksandr Palagin and Mykola Petrenko and Sergii Kryvyi and Mykola Boyko and Kyrylo Malakhov},
Title         = {Ontology-Driven Processing of Transdisciplinary Domain Knowledge},
Eprint        = {2311.04910v1},
DOI           = {10.31274/isudp.2023.140},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {The monograph discusses certain aspects of modern real-world problems facing
humanity, which are much more challenging than scientific ones. Modern science
is unable to solve them in a fundamental way. Vernadsky's noosphere thesis, in
fact, appeals to the scientific worldview that needs to be built in a way that
overcomes the interdisciplinary barriers and increases the effectiveness of
interdisciplinary interaction and modern science overall. We are talking about
the general transdisciplinary knowledge. In world practice, there is still no
systematic methodology and a specific form of generally accepted valid
scientific theory that would provide transdisciplinary knowledge. Non-linear
interdisciplinary interaction is the standard of evolution of modern science.
At the same time, a new transdisciplinary theory (domain of scientific
research) is being de facto created and the process is repeated many times:
from an individual or group of disciplines, through interdisciplinary
interaction, in a direction that brings us closer to creating a holistic
general scientific worldview.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.04910v1},
File          = {2311.04910v1.pdf}
}
@article{2310.20443v2,
Author        = {BjÃ¶rn Schembera and Frank WÃ¼bbeling and Hendrik Kleikamp and Christine Biedinger and Jochen Fiedler and Marco Reidelbach and Aurela Shehu and Burkhard Schmidt and Thomas Koprucki and Dorothea Iglezakis and Dominik GÃ¶ddeke},
Title         = {Ontologies for Models and Algorithms in Applied Mathematics and Related
  Disciplines},
Eprint        = {2310.20443v2},
DOI           = {10.1007/978-3-031-65990-4_14},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In applied mathematics and related disciplines, the
modeling-simulation-optimization workflow is a prominent scheme, with
mathematical models and numerical algorithms playing a crucial role. For these
types of mathematical research data, the Mathematical Research Data Initiative
has developed, merged and implemented ontologies and knowledge graphs. This
contributes to making mathematical research data FAIR by introducing semantic
technology and documenting the mathematical foundations accordingly. Using the
concrete example of microfracture analysis of porous media, it is shown how the
knowledge of the underlying mathematical model and the corresponding numerical
algorithms for its solution can be represented by the ontologies.},
Year          = {2023},
Month         = {Oct},
Note          = {In: Metadata and Semantic Research. MTSR 2023. Communications in
  Computer and Information Science, vol 2048. Springer, Cham (2024)},
Url           = {http://arxiv.org/abs/2310.20443v2},
File          = {2310.20443v2.pdf}
}
@article{2310.19998v1,
Author        = {Markus J. Buehler},
Title         = {Generative retrieval-augmented ontologic graph and multi-agent
  strategies for interpretive large language model-based materials design},
Eprint        = {2310.19998v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Transformer neural networks show promising capabilities, in particular for
uses in materials analysis, design and manufacturing, including their capacity
to work effectively with both human language, symbols, code, and numerical
data. Here we explore the use of large language models (LLMs) as a tool that
can support engineering analysis of materials, applied to retrieving key
information about subject areas, developing research hypotheses, discovery of
mechanistic relationships across disparate areas of knowledge, and writing and
executing simulation codes for active knowledge generation based on physical
ground truths. When used as sets of AI agents with specific features,
capabilities, and instructions, LLMs can provide powerful problem solution
strategies for applications in analysis and design problems. Our experiments
focus on using a fine-tuned model, MechGPT, developed based on training data in
the mechanics of materials domain. We first affirm how finetuning endows LLMs
with reasonable understanding of domain knowledge. However, when queried
outside the context of learned matter, LLMs can have difficulty to recall
correct information. We show how this can be addressed using
retrieval-augmented Ontological Knowledge Graph strategies that discern how the
model understands what concepts are important and how they are related.
Illustrated for a use case of relating distinct areas of knowledge - here,
music and proteins - such strategies can also provide an interpretable graph
structure with rich information at the node, edge and subgraph level. We
discuss nonlinear sampling strategies and agent-based modeling applied to
complex question answering, code generation and execution in the context of
automated force field development from actively learned Density Functional
Theory (DFT) modeling, and data analysis.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.19998v1},
File          = {2310.19998v1.pdf}
}
@article{2310.18979v1,
Author        = {Jan Mendling and Henrik Leopold and Henning Meyerhenke and BenoÃ®t Depaire},
Title         = {Methodology of Algorithm Engineering},
Eprint        = {2310.18979v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DS},
Abstract      = {Research on algorithms has drastically increased in recent years. Various
sub-disciplines of computer science investigate algorithms according to
different objectives and standards. This plurality of the field has led to
various methodological advances that have not yet been transferred to
neighboring sub-disciplines. The central roadblock for a better knowledge
exchange is the lack of a common methodological framework integrating the
perspectives of these sub-disciplines. It is the objective of this paper to
develop a research framework for algorithm engineering. Our framework builds on
three areas discussed in the philosophy of science: ontology, epistemology and
methodology. In essence, ontology describes algorithm engineering as being
concerned with algorithmic problems, algorithmic tasks, algorithm designs and
algorithm implementations. Epistemology describes the body of knowledge of
algorithm engineering as a collection of prescriptive and descriptive
knowledge, residing in World 3 of Popper's Three Worlds model. Methodology
refers to the steps how we can systematically enhance our knowledge of specific
algorithms. The framework helps us to identify and discuss various validity
concerns relevant to any algorithm engineering contribution. In this way, our
framework has important implications for researching algorithms in various
areas of computer science.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.18979v1},
File          = {2310.18979v1.pdf}
}
@article{2310.18929v1,
Author        = {Mona Abdel-Keream and Daniel BeÃler and Ayden Janssen and Sascha Jongebloed and Robin Nolte and Mihai Pomarlan and Robert Porzel},
Title         = {An Ontological Model of User Preferences},
Eprint        = {2310.18929v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {The notion of preferences plays an important role in many disciplines
including service robotics which is concerned with scenarios in which robots
interact with humans. These interactions can be favored by robots taking human
preferences into account. This raises the issue of how preferences should be
represented to support such preference-aware decision making. Several formal
accounts for a notion of preferences exist. However, these approaches fall
short on defining the nature and structure of the options that a robot has in a
given situation. In this work, we thus investigate a formal model of
preferences where options are non-atomic entities that are defined by the
complex situations they bring about.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.18929v1},
File          = {2310.18929v1.pdf}
}
@article{2310.18378v2,
Author        = {Qiu Ji and Guilin Qi and Yuxin Ye and Jiaye Li and Site Li and Jianjie Ren and Songtao Lu},
Title         = {Ontology Revision based on Pre-trained Language Models},
Eprint        = {2310.18378v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontology revision aims to seamlessly incorporate a new ontology into an
existing ontology and plays a crucial role in tasks such as ontology evolution,
ontology maintenance, and ontology alignment. Similar to repair single
ontologies, resolving logical incoherence in the task of ontology revision is
also important and meaningful, because incoherence is a main potential factor
to cause inconsistency and reasoning with an inconsistent ontology will obtain
meaningless answers.To deal with this problem, various ontology revision
approaches have been proposed to define revision operators and design ranking
strategies for axioms in an ontology. However, they rarely consider axiom
semantics which provides important information to differentiate axioms. In
addition, pre-trained models can be utilized to encode axiom semantics, and
have been widely applied in many natural language processing tasks and
ontology-related ones in recent years.Therefore, in this paper, we study how to
apply pre-trained models to revise ontologies. We first define four scoring
functions to rank axioms based on a pre-trained model by considering various
information from an ontology. Based on the functions, an ontology revision
algorithm is then proposed to deal with unsatisfiable concepts at once. To
improve efficiency, an adapted revision algorithm is designed to deal with
unsatisfiable concepts group by group. We conduct experiments over 19 ontology
pairs and compare our algorithms and scoring functions with existing ones.
According to the experiments, our algorithms could achieve promising
performance.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.18378v2},
File          = {2310.18378v2.pdf}
}
@article{2310.16788v2,
Author        = {Peter Mortimer and Raphael Hagmanns and Miguel Granero and Thorsten Luettel and Janko Petereit and Hans-Joachim Wuensche},
Title         = {The GOOSE Dataset for Perception in Unstructured Environments},
Eprint        = {2310.16788v2},
DOI           = {10.1109/ICRA57147.2024},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {The potential for deploying autonomous systems can be significantly increased
by improving the perception and interpretation of the environment. However, the
development of deep learning-based techniques for autonomous systems in
unstructured outdoor environments poses challenges due to limited data
availability for training and testing. To address this gap, we present the
German Outdoor and Offroad Dataset (GOOSE), a comprehensive dataset
specifically designed for unstructured outdoor environments. The GOOSE dataset
incorporates 10 000 labeled pairs of images and point clouds, which are
utilized to train a range of state-of-the-art segmentation models on both image
and point cloud data. We open source the dataset, along with an ontology for
unstructured terrain, as well as dataset standards and guidelines. This
initiative aims to establish a common framework, enabling the seamless
inclusion of existing datasets and a fast way to enhance the perception
capabilities of various robots operating in unstructured environments. The
dataset, pre-trained models for offroad perception, and additional
documentation can be found at https://goose-dataset.de/.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.16788v2},
File          = {2310.16788v2.pdf}
}
@article{2310.16760v1,
Author        = {Andrea Calvagna and Arabinda Ghosh and Sadegh Soudjani},
Title         = {Using Knowledge Awareness to improve Safety of Autonomous Driving},
Eprint        = {2310.16760v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {eess.SY},
Abstract      = {We present a method, which incorporates knowledge awareness into the symbolic
computation of discrete controllers for reactive cyber physical systems, to
improve decision making about the unknown operating environment under
uncertain/incomplete inputs. Assuming an abstract model of the system and the
environment, we translate the knowledge awareness of the operating context into
linear temporal logic formulas and incorporate them into the system
specifications to synthesize a controller. The knowledge base is built upon an
ontology model of the environment objects and behavioural rules, which includes
also symbolic models of partial input features. The resulting symbolic
controller support smoother, early reactions, which improves the security of
the system over existing approaches based on incremental symbolic perception. A
motion planning case study for an autonomous vehicle has been implemented to
validate the approach, and presented results show significant improvements with
respect to safety of state-of-the-art symbolic controllers for reactive
systems.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.16760v1},
File          = {2310.16760v1.pdf}
}
@article{2310.16472v3,
Author        = {Camille Bourgaux and Ana Ozaki and Rafael PeÃ±aloza},
Title         = {Semiring Provenance for Lightweight Description Logics},
Eprint        = {2310.16472v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LO},
Abstract      = {We investigate semiring provenance--a successful framework originally defined
in the relational database setting--for description logics. In this context,
the ontology axioms are annotated with elements of a commutative semiring and
these annotations are propagated to the ontology consequences in a way that
reflects how they are derived. We define a provenance semantics for a language
that encompasses several lightweight description logics and show its
relationships with semantics that have been defined for ontologies annotated
with a specific kind of annotation (such as fuzzy degrees). We show that under
some restrictions on the semiring, the semantics satisfies desirable properties
(such as extending the semiring provenance defined for databases). We then
focus on the well-known why-provenance, for which we study the complexity of
problems related to the provenance of an assertion or a conjunctive query
answer. Finally, we consider two more restricted cases which correspond to the
so-called positive Boolean provenance and lineage in the database setting. For
these cases, we exhibit relationships with well-known notions related to
explanations in description logics and complete our complexity analysis. As a
side contribution, we provide conditions on an $\mathcal{ELHI}_\bot$ ontology
that guarantee tractable reasoning.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.16472v3},
File          = {2310.16472v3.pdf}
}
@article{2311.16119v3,
Author        = {Sander Schulhoff and Jeremy Pinto and Anaum Khan and Louis-FranÃ§ois Bouchard and Chenglei Si and Svetlina Anati and Valen Tagliabue and Anson Liu Kost and Christopher Carnahan and Jordan Boyd-Graber},
Title         = {Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of
  LLMs through a Global Scale Prompt Hacking Competition},
Eprint        = {2311.16119v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {Large Language Models (LLMs) are deployed in interactive contexts with direct
user engagement, such as chatbots and writing assistants. These deployments are
vulnerable to prompt injection and jailbreaking (collectively, prompt hacking),
in which models are manipulated to ignore their original instructions and
follow potentially malicious ones. Although widely acknowledged as a
significant security threat, there is a dearth of large-scale resources and
quantitative studies on prompt hacking. To address this lacuna, we launch a
global prompt hacking competition, which allows for free-form human input
attacks. We elicit 600K+ adversarial prompts against three state-of-the-art
LLMs. We describe the dataset, which empirically verifies that current LLMs can
indeed be manipulated via prompt hacking. We also present a comprehensive
taxonomical ontology of the types of adversarial prompts.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2311.16119v3},
File          = {2311.16119v3.pdf}
}
@article{2310.15572v1,
Author        = {J. Charles G. Jeynes and Tim James and Matthew Corney},
Title         = {Natural Language Processing for Drug Discovery Knowledge Graphs:
  promises and pitfalls},
Eprint        = {2310.15572v1},
DOI           = {10.1007/978-1-0716-3449-3_10},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Building and analysing knowledge graphs (KGs) to aid drug discovery is a
topical area of research. A salient feature of KGs is their ability to combine
many heterogeneous data sources in a format that facilitates discovering
connections. The utility of KGs has been exemplified in areas such as drug
repurposing, with insights made through manual exploration and modelling of the
data. In this article, we discuss promises and pitfalls of using natural
language processing (NLP) to mine unstructured text typically from scientific
literature as a data source for KGs. This draws on our experience of initially
parsing structured data sources such as ChEMBL as the basis for data within a
KG, and then enriching or expanding upon them using NLP. The fundamental
promise of NLP for KGs is the automated extraction of data from millions of
documents a task practically impossible to do via human curation alone.
However, there are many potential pitfalls in NLP-KG pipelines such as
incorrect named entity recognition and ontology linking all of which could
ultimately lead to erroneous inferences and conclusions.},
Year          = {2023},
Month         = {Oct},
Note          = {Methods Mol Biol . 2024:2716:223-240},
Url           = {http://arxiv.org/abs/2310.15572v1},
File          = {2310.15572v1.pdf}
}
@article{2310.15373v1,
Author        = {Sayed Hoseini and Johannes Theissen-Lipp and Christoph Quix},
Title         = {Semantic Data Management in Data Lakes},
Eprint        = {2310.15373v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {In recent years, data lakes emerged as away to manage large amounts of
heterogeneous data for modern data analytics. One way to prevent data lakes
from turning into inoperable data swamps is semantic data management. Some
approaches propose the linkage of metadata to knowledge graphs based on the
Linked Data principles to provide more meaning and semantics to the data in the
lake. Such a semantic layer may be utilized not only for data management but
also to tackle the problem of data integration from heterogeneous sources, in
order to make data access more expressive and interoperable. In this survey, we
review recent approaches with a specific focus on the application within data
lake systems and scalability to Big Data. We classify the approaches into (i)
basic semantic data management, (ii) semantic modeling approaches for enriching
metadata in data lakes, and (iii) methods for ontologybased data access. In
each category, we cover the main techniques and their background, and compare
latest research. Finally, we point out challenges for future work in this
research area, which needs a closer integration of Big Data and Semantic Web
technologies.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.15373v1},
File          = {2310.15373v1.pdf}
}
@article{2310.14969v1,
Author        = {Angelo Bassi and Mauro Dorato and Hendrik Ulbricht},
Title         = {Collapse Models: a theoretical, experimental and philosophical review},
Eprint        = {2310.14969v1},
DOI           = {10.3390/e25040645},
ArchivePrefix = {arXiv},
PrimaryClass  = {quant-ph},
Abstract      = {n this paper, we review and connect the three essential conditions needed by
the collapse model to achieve a complete and exact formulation, namely the
theoretical, the experimental, and the ontological ones. These features
correspond to the three parts of the paper. In any empirical science, the first
two features are obviously connected but, as is well known, among the different
formulations and interpretations of non-relativistic quantum mechanics, only
collapse models, as the paper well illustrates with a richness of details, have
experimental consequences. Finally, we show that a clarification of the
ontological intimations of collapse models is needed for at least three
reasons: (1) to respond to the indispensable task of answering the question
`what are collapse models (and in general any physical theory) about?'; (2) to
achieve a deeper understanding of their different formulations; (3) to enlarge
the panorama of possible readings of a theory, which historically has often
played a fundamental heuristic role.},
Year          = {2023},
Month         = {Oct},
Note          = {Entropy 25, 645 (2023)},
Url           = {http://arxiv.org/abs/2310.14969v1},
File          = {2310.14969v1.pdf}
}
@article{2310.14684v1,
Author        = {Hassan S. Shavarani and Anoop Sarkar},
Title         = {SpEL: Structured Prediction for Entity Linking},
Eprint        = {2310.14684v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Entity linking is a prominent thread of research focused on structured data
creation by linking spans of text to an ontology or knowledge source. We
revisit the use of structured prediction for entity linking which classifies
each individual input token as an entity, and aggregates the token predictions.
Our system, called SpEL (Structured prediction for Entity Linking) is a
state-of-the-art entity linking system that uses some new ideas to apply
structured prediction to the task of entity linking including: two refined
fine-tuning steps; a context sensitive prediction aggregation strategy;
reduction of the size of the model's output vocabulary, and; we address a
common problem in entity-linking systems where there is a training vs.
inference tokenization mismatch. Our experiments show that we can outperform
the state-of-the-art on the commonly used AIDA benchmark dataset for entity
linking to Wikipedia. Our method is also very compute efficient in terms of
number of parameters and speed of inference.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.14684v1},
File          = {2310.14684v1.pdf}
}
@article{2311.02082v3,
Author        = {Miguel AP Oliveira and Stephane Manara and Bruno MolÃ© and Thomas Muller and AurÃ©lien Guillouche and Lysann Hesske and Bruce Jordan and Gilles Hubert and Chinmay Kulkarni and Pralipta Jagdev and Cedric R. Berger},
Title         = {Semantic Modelling of Organizational Knowledge as a Basis for Enterprise
  Data Governance 4.0 -- Application to a Unified Clinical Data Model},
Eprint        = {2311.02082v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Individuals and organizations cope with an always-growing amount of data,
which is heterogeneous in its contents and formats. An adequate data management
process yielding data quality and control over its lifecycle is a prerequisite
to getting value out of this data and minimizing inherent risks related to
multiple usages. Common data governance frameworks rely on people, policies,
and processes that fall short of the overwhelming complexity of data. Yet,
harnessing this complexity is necessary to achieve high-quality standards. The
latter will condition any downstream data usage outcome, including generative
artificial intelligence trained on this data. In this paper, we report our
concrete experience establishing a simple, cost-efficient framework that
enables metadata-driven, agile and (semi-)automated data governance (i.e. Data
Governance 4.0). We explain how we implement and use this framework to
integrate 25 years of clinical study data at an enterprise scale in a fully
productive environment. The framework encompasses both methodologies and
technologies leveraging semantic web principles. We built a knowledge graph
describing avatars of data assets in their business context, including
governance principles. Multiple ontologies articulated by an enterprise upper
ontology enable key governance actions such as FAIRification, lifecycle
management, definition of roles and responsibilities, lineage across
transformations and provenance from source systems. This metadata model is the
keystone to data governance 4.0: a semi-automatised data management process
that considers the business context in an agile manner to adapt governance
constraints to each use case and dynamically tune it based on business changes.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2311.02082v3},
File          = {2311.02082v3.pdf}
}
@article{2310.13595v2,
Author        = {Nathan Lambert and Thomas Krendl Gilbert and Tom Zick},
Title         = {The History and Risks of Reinforcement Learning and Human Feedback},
Eprint        = {2310.13595v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CY},
Abstract      = {Reinforcement learning from human feedback (RLHF) has emerged as a powerful
technique to make large language models (LLMs) easier to use and more
effective. A core piece of the RLHF process is the training and utilization of
a model of human preferences that acts as a reward function for optimization.
This approach, which operates at the intersection of many stakeholders and
academic disciplines, remains poorly understood. RLHF reward models are often
cited as being central to achieving performance, yet very few descriptors of
capabilities, evaluations, training methods, or open-source models exist. Given
this lack of information, further study and transparency is needed for learned
RLHF reward models. In this paper, we illustrate the complex history of
optimizing preferences, and articulate lines of inquiry to understand the
sociotechnical context of reward models. In particular, we highlight the
ontological differences between costs, rewards, and preferences at stake in
RLHF's foundations, related methodological tensions, and possible research
directions to improve general understanding of how reward models function.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.13595v2},
File          = {2310.13595v2.pdf}
}
@article{2311.08412v1,
Author        = {Felix Ocker and JÃ¶rg DeigmÃ¶ller and Julian Eggert},
Title         = {Exploring Large Language Models as a Source of Common-Sense Knowledge
  for Robots},
Eprint        = {2311.08412v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Service robots need common-sense knowledge to help humans in everyday
situations as it enables them to understand the context of their actions.
However, approaches that use ontologies face a challenge because common-sense
knowledge is often implicit, i.e., it is obvious to humans but not explicitly
stated. This paper investigates if Large Language Models (LLMs) can fill this
gap. Our experiments reveal limited effectiveness in the selective extraction
of contextual action knowledge, suggesting that LLMs may not be sufficient on
their own. However, the large-scale extraction of general, actionable knowledge
shows potential, indicating that LLMs can be a suitable tool for efficiently
creating ontologies for robots. This paper shows that the technique used for
knowledge extraction can be applied to populate a minimalist ontology,
showcasing the potential of LLMs in synergy with formal knowledge
representation.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2311.08412v1},
File          = {2311.08412v1.pdf}
}
@article{2310.11555v1,
Author        = {Linfang Ding and Guohui Xiao and Albulen Pano and Mattia Fumagalli and Dongsheng Chen and Yu Feng and Diego Calvanese and Hongchao Fan and Liqiu Meng},
Title         = {Integrating 3D City Data through Knowledge Graphs},
Eprint        = {2310.11555v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {CityGML is a widely adopted standard by the Open Geospatial Consortium (OGC)
for representing and exchanging 3D city models. The representation of semantic
and topological properties in CityGML makes it possible to query such 3D city
data to perform analysis in various applications, e.g., security management and
emergency response, energy consumption and estimation, and occupancy
measurement. However, the potential of querying CityGML data has not been fully
exploited. The official GML/XML encoding of CityGML is only intended as an
exchange format but is not suitable for query answering. The most common way of
dealing with CityGML data is to store them in the 3DCityDB system as relational
tables and then query them with the standard SQL query language. Nevertheless,
for end users, it remains a challenging task to formulate queries over 3DCityDB
directly for their ad-hoc analytical tasks, because there is a gap between the
conceptual semantics of CityGML and the relational schema adopted in 3DCityDB.
In fact, the semantics of CityGML itself can be modeled as a suitable ontology.
The technology of Knowledge Graphs (KGs), where an ontology is at the core, is
a good solution to bridge such a gap. Moreover, embracing KGs makes it easier
to integrate with other spatial data sources, e.g., OpenStreetMap and existing
(Geo)KGs (e.g., Wikidata, DBPedia, and GeoNames), and to perform queries
combining information from multiple data sources. In this work, we describe a
CityGML KG framework to populate the concepts in the CityGML ontology using
declarative mappings to 3DCityDB, thus exposing the CityGML data therein as a
KG. To demonstrate the feasibility of our approach, we use CityGML data from
the city of Munich as test data and integrate OpenStreeMap data in the same
area.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.11555v1},
File          = {2310.11555v1.pdf}
}
@article{2310.11140v1,
Author        = {InÃªs Koch and Carla Teixeira Lopes and Cristina Ribeiro},
Title         = {Moving from ISAD(G) to a CIDOC CRM-based Linked Data Model in the
  Portuguese Archives},
Eprint        = {2310.11140v1},
DOI           = {10.1145/3605910},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {Archives are facing numerous challenges. On the one hand, archival assets are
evolving to encompass digitized documents and increasing quantities of
born-digital information in diverse formats. On the other hand, the audience is
changing along with how it wishes to access archival material. Moreover, the
interoperability requirements of cultural heritage repositories are growing. In
this context, the Portuguese Archives started an ambitious program aiming to
evolve its data model, migrate existing records, and build a new archival
management system appropriate to both archival tasks and public access. The
overall goal is to have a fine-grained and flexible description, more
machine-actionable than the current one. This work describes ArchOnto, a linked
open data model for archives, and rules for its automatic population from
existing records. ArchOnto adopts a semantic web approach and encompasses the
CIDOC Conceptual Reference Model and additional ontologies, envisioning
interoperability with datasets curated by multiple communities of practice.
Existing ISAD(G)-conforming descriptions are being migrated to the new model
using the direct mappings provided here. We used a sample of 25 records
associated with different description levels to validate the completeness and
conformity of ArchOnto to existing data. This work is in progress and is
original in several respects: (1) it is one of the first approaches to use
CIDOC CRM in the context of archives, identifying problems and questions that
emerged during the process and pinpointing possible solutions; (2) it addresses
the balance in the model between the migration of existing records and the
construction of new ones by archive professionals; and (3) it adopts an open
world view on linking archival data to global information sources.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.11140v1},
File          = {2310.11140v1.pdf}
}
@article{2310.10445v1,
Author        = {Markus J. Buehler},
Title         = {MechGPT, a language-based strategy for mechanics and materials modeling
  that connects knowledge across scales, disciplines and modalities},
Eprint        = {2310.10445v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {For centuries, researchers have sought out ways to connect disparate areas of
knowledge. While early scholars (Galileo, da Vinci, etc.) were experts across
fields, specialization has taken hold later. With the advent of Artificial
Intelligence, we can now explore relationships across areas (e.g.,
mechanics-biology) or disparate domains (e.g., failure mechanics-art). To
achieve this, we use a fine-tuned Large Language Model (LLM), here for a subset
of knowledge in multiscale materials failure. The approach includes the use of
a general-purpose LLM to distill question-answer pairs from raw sources
followed by LLM fine-tuning. The resulting MechGPT LLM foundation model is used
in a series of computational experiments to explore its capacity for knowledge
retrieval, various language tasks, hypothesis generation, and connecting
knowledge across disparate areas. While the model has some ability to recall
knowledge from training, we find that LLMs are particularly useful to extract
structural insights through Ontological Knowledge Graphs. These interpretable
graph structures provide explanatory insights, frameworks for new research
questions, and visual representations of knowledge that also can be used in
retrieval-augmented generation. Three versions of MechGPT are discussed,
featuring different sizes from 13 billion to 70 billion parameters, and
reaching context lengths of more than 10,000 tokens. This provides ample
capacity for sophisticated retrieval augmented strategies, as well as
agent-based modeling where multiple LLMs interact collaboratively and/or
adversarially, the incorporation of new data from the literature or web
searches, as well as multimodality.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.10445v1},
File          = {2310.10445v1.pdf}
}
@article{2310.08365v2,
Author        = {Md. Rezaul Karim and Lina Molinas Comet and Md Shajalal and Oya Deniz Beyan and Dietrich Rebholz-Schuhmann and Stefan Decker},
Title         = {From Large Language Models to Knowledge Graphs for Biomarker Discovery
  in Cancer},
Eprint        = {2310.08365v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Domain experts often rely on most recent knowledge for apprehending and
disseminating specific biological processes that help them design strategies
for developing prevention and therapeutic decision-making in various disease
scenarios. A challenging scenarios for artificial intelligence (AI) is using
biomedical data (e.g., texts, imaging, omics, and clinical) to provide
diagnosis and treatment recommendations for cancerous conditions.~Data and
knowledge about biomedical entities like cancer, drugs, genes, proteins, and
their mechanism is spread across structured (knowledge bases (KBs)) and
unstructured (e.g., scientific articles) sources. A large-scale knowledge graph
(KG) can be constructed by integrating and extracting facts about semantically
interrelated entities and relations. Such a KG not only allows exploration and
question answering (QA) but also enables domain experts to deduce new
knowledge. However, exploring and querying large-scale KGs is tedious for
non-domain users due to their lack of understanding of the data assets and
semantic technologies. In this paper, we develop a domain KG to leverage
cancer-specific biomarker discovery and interactive QA. For this, we
constructed a domain ontology called OncoNet Ontology (ONO), which enables
semantic reasoning for validating gene-disease (different types of cancer)
relations. The KG is further enriched by harmonizing the ONO, metadata,
controlled vocabularies, and biomedical concepts from scientific articles by
employing BioBERT- and SciBERT-based information extractors. Further, since the
biomedical domain is evolving, where new findings often replace old ones,
without having access to up-to-date scientific findings, there is a high chance
an AI system exhibits concept drift while providing diagnosis and treatment.
Therefore, we fine-tune the KG using large language models (LLMs) based on more
recent articles and KBs.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.08365v2},
File          = {2310.08365v2.pdf}
}
@article{2310.07795v1,
Author        = {Siru Ouyang and Jiaxin Huang and Pranav Pillai and Yunyi Zhang and Yu Zhang and Jiawei Han},
Title         = {Ontology Enrichment for Effective Fine-grained Entity Typing},
Eprint        = {2310.07795v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Fine-grained entity typing (FET) is the task of identifying specific entity
types at a fine-grained level for entity mentions based on their contextual
information. Conventional methods for FET require extensive human annotation,
which is time-consuming and costly. Recent studies have been developing weakly
supervised or zero-shot approaches. We study the setting of zero-shot FET where
only an ontology is provided. However, most existing ontology structures lack
rich supporting information and even contain ambiguous relations, making them
ineffective in guiding FET. Recently developed language models, though
promising in various few-shot and zero-shot NLP tasks, may face challenges in
zero-shot FET due to their lack of interaction with task-specific ontology. In
this study, we propose OnEFET, where we (1) enrich each node in the ontology
structure with two types of extra information: instance information for
training sample augmentation and topic information to relate types to contexts,
and (2) develop a coarse-to-fine typing algorithm that exploits the enriched
information by training an entailment model with contrasting topics and
instance-based augmented training samples. Our experiments show that OnEFET
achieves high-quality fine-grained entity typing without human annotation,
outperforming existing zero-shot methods by a large margin and rivaling
supervised methods.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.07795v1},
File          = {2310.07795v1.pdf}
}
@article{2310.06552v3,
Author        = {Joseph S. Boyle and Antanas Kascenas and Pat Lok and Maria Liakata and Alison Q. O'Neil},
Title         = {Automated clinical coding using off-the-shelf large language models},
Eprint        = {2310.06552v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The task of assigning diagnostic ICD codes to patient hospital admissions is
typically performed by expert human coders. Efforts towards automated ICD
coding are dominated by supervised deep learning models. However, difficulties
in learning to predict the large number of rare codes remain a barrier to
adoption in clinical practice. In this work, we leverage off-the-shelf
pre-trained generative large language models (LLMs) to develop a practical
solution that is suitable for zero-shot and few-shot code assignment, with no
need for further task-specific training. Unsupervised pre-training alone does
not guarantee precise knowledge of the ICD ontology and specialist clinical
coding task, therefore we frame the task as information extraction, providing a
description of each coded concept and asking the model to retrieve related
mentions. For efficiency, rather than iterating over all codes, we leverage the
hierarchical nature of the ICD ontology to sparsely search for relevant codes.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.06552v3},
File          = {2310.06552v3.pdf}
}
@article{2310.06309v1,
Author        = {Yuchen Yang},
Title         = {Encoding and Decoding Narratives: Datafication and Alternative Access
  Models for Audiovisual Archives},
Eprint        = {2310.06309v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.MM},
Abstract      = {Situated in the intersection of audiovisual archives, computational methods,
and immersive interactions, this work probes the increasingly important
accessibility issues from a two-fold approach. Firstly, the work proposes an
ontological data model to handle complex descriptors (metadata, feature
vectors, etc.) with regard to user interactions. Secondly, this work examines
text-to-video retrieval from an implementation perspective by proposing a
classifier-enhanced workflow to deal with complex and hybrid queries and a
training data augmentation workflow to improve performance. This work serves as
the foundation for experimenting with novel public-facing access models to
large audiovisual archives},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.06309v1},
File          = {2310.06309v1.pdf}
}
@article{2310.05381v1,
Author        = {Yang Liu and Melissa Xiaohui Qin and Long Wang and Chao Huang},
Title         = {CCAE: A Corpus of Chinese-based Asian Englishes},
Eprint        = {2310.05381v1},
DOI           = {10.1007/978-3-031-44696-2_48},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Language models have been foundations in various scenarios of NLP
applications, but it has not been well applied in language variety studies,
even for the most popular language like English. This paper represents one of
the few initial efforts to utilize the NLP technology in the paradigm of World
Englishes, specifically in creating a multi-variety corpus for studying Asian
Englishes. We present an overview of the CCAE -- Corpus of Chinese-based Asian
English, a suite of corpora comprising six Chinese-based Asian English
varieties. It is based on 340 million tokens in 448 thousand web documents from
six regions. The ontology of data would make the corpus a helpful resource with
enormous research potential for Asian Englishes (especially for Chinese
Englishes for which there has not been a publicly accessible corpus yet so far)
and an ideal source for variety-specific language modeling and downstream
tasks, thus setting the stage for NLP-based World Englishes studies. And
preliminary experiments on this corpus reveal the practical value of CCAE.
Finally, we make CCAE available at
\href{https://huggingface.co/datasets/CCAE/CCAE-Corpus}{this https URL}.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.05381v1},
File          = {2310.05381v1.pdf}
}
@article{2310.04304v1,
Author        = {Ahmed R. Sadik and Sebastian Brulin and Markus Olhofer},
Title         = {Coding by Design: GPT-4 empowers Agile Model Driven Development},
Eprint        = {2310.04304v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {Generating code from a natural language using Large Language Models (LLMs)
such as ChatGPT, seems groundbreaking. Yet, with more extensive use, it's
evident that this approach has its own limitations. The inherent ambiguity of
natural language presents challenges for complex software designs. Accordingly,
our research offers an Agile Model-Driven Development (MDD) approach that
enhances code auto-generation using OpenAI's GPT-4. Our work emphasizes
"Agility" as a significant contribution to the current MDD method, particularly
when the model undergoes changes or needs deployment in a different programming
language. Thus, we present a case-study showcasing a multi-agent simulation
system of an Unmanned Vehicle Fleet. In the first and second layer of our
approach, we constructed a textual representation of the case-study using
Unified Model Language (UML) diagrams. In the next layer, we introduced two
sets of constraints that minimize model ambiguity. Object Constraints Language
(OCL) is applied to fine-tune the code constructions details, while FIPA
ontology is used to shape communication semantics and protocols. Ultimately,
leveraging GPT-4, our last layer auto-generates code in both Java and Python.
The Java code is deployed within the JADE framework, while the Python code is
deployed in PADE framework. Concluding our research, we engaged in a
comprehensive evaluation of the generated code. From a behavioural standpoint,
the auto-generated code aligned perfectly with the expected UML sequence
diagram. Structurally, we compared the complexity of code derived from UML
diagrams constrained solely by OCL to that influenced by both OCL and
FIPA-ontology. Results indicate that ontology-constrained model produce
inherently more intricate code, but it remains manageable and low-risk for
further testing and maintenance.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.04304v1},
File          = {2310.04304v1.pdf}
}
@article{2310.03840v1,
Author        = {Zhu Wang},
Title         = {Contextualized Structural Self-supervised Learning for Ontology Matching},
Eprint        = {2310.03840v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Ontology matching (OM) entails the identification of semantic relationships
between concepts within two or more knowledge graphs (KGs) and serves as a
critical step in integrating KGs from various sources. Recent advancements in
deep OM models have harnessed the power of transformer-based language models
and the advantages of knowledge graph embedding. Nevertheless, these OM models
still face persistent challenges, such as a lack of reference alignments,
runtime latency, and unexplored different graph structures within an end-to-end
framework. In this study, we introduce a novel self-supervised learning OM
framework with input ontologies, called LaKERMap. This framework capitalizes on
the contextual and structural information of concepts by integrating implicit
knowledge into transformers. Specifically, we aim to capture multiple
structural contexts, encompassing both local and global interactions, by
employing distinct training objectives. To assess our methods, we utilize the
Bio-ML datasets and tasks. The findings from our innovative approach reveal
that LaKERMap surpasses state-of-the-art systems in terms of alignment quality
and inference time. Our models and codes are available here:
https://github.com/ellenzhuwang/lakermap.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.03840v1},
File          = {2310.03840v1.pdf}
}
@article{2310.03666v1,
Author        = {Nicolas Matentzoglu and J. Harry Caufield and Harshad B. Hegde and Justin T. Reese and Sierra Moxon and Hyeongsik Kim and Nomi L. Harris and Melissa A Haendel and Christopher J. Mungall},
Title         = {MapperGPT: Large Language Models for Linking and Mapping Entities},
Eprint        = {2310.03666v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Aligning terminological resources, including ontologies, controlled
vocabularies, taxonomies, and value sets is a critical part of data integration
in many domains such as healthcare, chemistry, and biomedical research. Entity
mapping is the process of determining correspondences between entities across
these resources, such as gene identifiers, disease concepts, or chemical entity
identifiers. Many tools have been developed to compute such mappings based on
common structural features and lexical information such as labels and synonyms.
Lexical approaches in particular often provide very high recall, but low
precision, due to lexical ambiguity. As a consequence of this, mapping efforts
often resort to a labor intensive manual mapping refinement through a human
curator.
  Large Language Models (LLMs), such as the ones employed by ChatGPT, have
generalizable abilities to perform a wide range of tasks, including
question-answering and information extraction. Here we present MapperGPT, an
approach that uses LLMs to review and refine mapping relationships as a
post-processing step, in concert with existing high-recall methods that are
based on lexical and structural heuristics.
  We evaluated MapperGPT on a series of alignment tasks from different domains,
including anatomy, developmental biology, and renal diseases. We devised a
collection of tasks that are designed to be particularly challenging for
lexical methods. We show that when used in combination with high-recall
methods, MapperGPT can provide a substantial improvement in accuracy, beating
state-of-the-art (SOTA) methods such as LogMap.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.03666v1},
File          = {2310.03666v1.pdf}
}
@article{2310.03659v1,
Author        = {Thorsten HÃ¤ndler},
Title         = {Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for
  Autonomous LLM-powered Multi-Agent Architectures},
Eprint        = {2310.03659v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large language models (LLMs) have revolutionized the field of artificial
intelligence, endowing it with sophisticated language understanding and
generation capabilities. However, when faced with more complex and
interconnected tasks that demand a profound and iterative thought process, LLMs
reveal their inherent limitations. Autonomous LLM-powered multi-agent systems
represent a strategic response to these challenges. Such systems strive for
autonomously tackling user-prompted goals by decomposing them into manageable
tasks and orchestrating their execution and result synthesis through a
collective of specialized intelligent agents. Equipped with LLM-powered
reasoning capabilities, these agents harness the cognitive synergy of
collaborating with their peers, enhanced by leveraging contextual resources
such as tools and datasets. While these architectures hold promising potential
in amplifying AI capabilities, striking the right balance between different
levels of autonomy and alignment remains the crucial challenge for their
effective operation. This paper proposes a comprehensive multi-dimensional
taxonomy, engineered to analyze how autonomous LLM-powered multi-agent systems
balance the dynamic interplay between autonomy and alignment across various
aspects inherent to architectural viewpoints such as goal-driven task
management, agent composition, multi-agent collaboration, and context
interaction. It also includes a domain-ontology model specifying fundamental
architectural concepts. Our taxonomy aims to empower researchers, engineers,
and AI practitioners to systematically analyze the architectural dynamics and
balancing strategies employed by these increasingly prevalent AI systems. The
exploratory taxonomic classification of selected representative LLM-powered
multi-agent systems illustrates its practical utility and reveals potential for
future research and development.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.03659v1},
File          = {2310.03659v1.pdf}
}
@article{2310.03376v1,
Author        = {Anisa Rula and Jennifer D'Souza},
Title         = {Procedural Text Mining with Large Language Models},
Eprint        = {2310.03376v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent advancements in the field of Natural Language Processing, particularly
the development of large-scale language models that are pretrained on vast
amounts of knowledge, are creating novel opportunities within the realm of
Knowledge Engineering. In this paper, we investigate the usage of large
language models (LLMs) in both zero-shot and in-context learning settings to
tackle the problem of extracting procedures from unstructured PDF text in an
incremental question-answering fashion. In particular, we leverage the current
state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model,
accompanied by two variations of in-context learning that involve an ontology
with definitions of procedures and steps and a limited number of samples of
few-shot learning. The findings highlight both the promise of this approach and
the value of the in-context learning customisations. These modifications have
the potential to significantly address the challenge of obtaining sufficient
training data, a hurdle often encountered in deep learning-based Natural
Language Processing techniques for procedure extraction.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.03376v1},
File          = {2310.03376v1.pdf}
}
@article{2310.02198v2,
Author        = {Victor Lacerda and Ana Ozaki and Ricardo GuimarÃ£es},
Title         = {Strong Faithfulness for ELH Ontology Embeddings},
Eprint        = {2310.02198v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LO},
Abstract      = {Ontology embedding methods are powerful approaches to represent and reason
over structured knowledge in various domains. One advantage of ontology
embeddings over knowledge graph embeddings is their ability to capture and
impose an underlying schema to which the model must conform. Despite advances,
most current approaches do not guarantee that the resulting embedding respects
the axioms the ontology entails. In this work, we formally prove that
normalized ${\cal ELH}$ has the strong faithfulness property on convex
geometric models, which means that there is an embedding that precisely
captures the original ontology. We present a region-based geometric model for
embedding normalized ${\cal ELH}$ ontologies into a continuous vector space. To
prove strong faithfulness, our construction takes advantage of the fact that
normalized ${\cal ELH}$ has a finite canonical model. We first prove the
statement assuming (possibly) non-convex regions, allowing us to keep the
required dimensions low. Then, we impose convexity on the regions and show the
property still holds. Finally, we consider reasoning tasks on geometric models
and analyze the complexity in the class of convex geometric models used for
proving strong faithfulness.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.02198v2},
File          = {2310.02198v2.pdf}
}
@article{2310.01929v3,
Author        = {Mor Ventura and Eyal Ben-David and Anna Korhonen and Roi Reichart},
Title         = {Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of
  Text-To-Image Models},
Eprint        = {2310.01929v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Text-To-Image (TTI) models, such as DALL-E and StableDiffusion, have
demonstrated remarkable prompt-based image generation capabilities.
Multilingual encoders may have a substantial impact on the cultural agency of
these models, as language is a conduit of culture. In this study, we explore
the cultural perception embedded in TTI models by characterizing culture across
three hierarchical tiers: cultural dimensions, cultural domains, and cultural
concepts. Based on this ontology, we derive prompt templates to unlock the
cultural knowledge in TTI models, and propose a comprehensive suite of
evaluation techniques, including intrinsic evaluations using the CLIP space,
extrinsic evaluations with a Visual-Question-Answer (VQA) model and human
assessments, to evaluate the cultural content of TTI-generated images. To
bolster our research, we introduce the CulText2I dataset, derived from six
diverse TTI models and spanning ten languages. Our experiments provide insights
regarding Do, What, Which and How research questions about the nature of
cultural encoding in TTI models, paving the way for cross-cultural applications
of these models.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.01929v3},
File          = {2310.01929v3.pdf}
}
@article{2310.00078v2,
Author        = {Scott McClellan and Yuan An and Xintong Zhao and Xia Lin and Jane Greenberg},
Title         = {Characterizing Semantic Ambiguity of the Materials Science Ontologies},
Eprint        = {2310.00078v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {Growth in computational materials science and initiatives such as the
Materials Genome Initiative (MGI) and the European Materials Modelling Council
(EMMC) has motivated the development and application of ontologies. A key
factor has been increased adoption of the FAIR principles, making research data
findable, accessible, interoperable, and reusable (Wilkinson et al. 2016). This
paper characterizes semantic interoperability among a subset of materials
science ontologies in the MatPortal repository. Background context covers
semantic interoperability, ontological commitment, and the materials science
ontology landscape. The research focused on MatPortal's two interoperability
protocols: LOOM term matching and URI matching. Results report the degree of
overlap and demonstrate the different types of ambiguity among ontologies. The
discussion considers implications for FAIR and AI, and the conclusion highlight
key findings and next steps.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2310.00078v2},
File          = {2310.00078v2.pdf}
}
@article{2309.17169v2,
Author        = {Tudor Groza and Harry Caufield and Dylan Gration and Gareth Baynam and Melissa A Haendel and Peter N Robinson and Christopher J Mungall and Justin T Reese},
Title         = {An evaluation of GPT models for phenotype concept recognition},
Eprint        = {2309.17169v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Objective: Clinical deep phenotyping and phenotype annotation play a critical
role in both the diagnosis of patients with rare disorders as well as in
building computationally-tractable knowledge in the rare disorders field. These
processes rely on using ontology concepts, often from the Human Phenotype
Ontology, in conjunction with a phenotype concept recognition task (supported
usually by machine learning methods) to curate patient profiles or existing
scientific literature. With the significant shift in the use of large language
models (LLMs) for most NLP tasks, we examine the performance of the latest
Generative Pre-trained Transformer (GPT) models underpinning ChatGPT as a
foundation for the tasks of clinical phenotyping and phenotype annotation.
Materials and Methods: The experimental setup of the study included seven
prompts of various levels of specificity, two GPT models (gpt-3.5-turbo and
gpt-4.0) and two established gold standard corpora for phenotype recognition,
one consisting of publication abstracts and the other clinical observations.
Results: Our results show that, with an appropriate setup, these models can
achieve state of the art performance. The best run, using few-shot learning,
achieved 0.58 macro F1 score on publication abstracts and 0.75 macro F1 score
on clinical observations, the former being comparable with the state of the
art, while the latter surpassing the current best in class tool. Conclusion:
While the results are promising, the non-deterministic nature of the outcomes,
the high cost and the lack of concordance between different runs using the same
prompt and input make the use of these LLMs challenging for this particular
task.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.17169v2},
File          = {2309.17169v2.pdf}
}
@article{2309.16265v2,
Author        = {Wuyang Liu and Yanzhen Ren},
Title         = {Semantic Proximity Alignment: Towards Human Perception-consistent Audio
  Tagging by Aligning with Label Text Description},
Eprint        = {2309.16265v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SD},
Abstract      = {Most audio tagging models are trained with one-hot labels as supervised
information. However, one-hot labels treat all sound events equally, ignoring
the semantic hierarchy and proximity relationships between sound events. In
contrast, the event descriptions contains richer information, describing the
distance between different sound events with semantic proximity. In this paper,
we explore the impact of training audio tagging models with auxiliary text
descriptions of sound events. By aligning the audio features with the text
features of corresponding labels, we inject the hierarchy and proximity
information of sound events into audio encoders, improving the performance
while making the prediction more consistent with human perception. We refer to
this approach as Semantic Proximity Alignment (SPA). We use Ontology-aware mean
Average Precision (OmAP) as the main evaluation metric for the models. OmAP
reweights the false positives based on Audioset ontology distance and is more
consistent with human perception compared to mAP. Experimental results show
that the audio tagging models trained with SPA achieve higher OmAP compared to
models trained with one-hot labels solely (+1.8 OmAP). Human evaluations also
demonstrate that the predictions of SPA models are more consistent with human
perception.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.16265v2},
File          = {2309.16265v2.pdf}
}
@article{2309.16248v2,
Author        = {Catherine Kosten and Philippe CudrÃ©-Mauroux and Kurt Stockinger},
Title         = {Spider4SPARQL: A Complex Benchmark for Evaluating Knowledge Graph
  Question Answering Systems},
Eprint        = {2309.16248v2},
DOI           = {10.1109/BigData59044.2023.10386182},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {With the recent spike in the number and availability of Large Language Models
(LLMs), it has become increasingly important to provide large and realistic
benchmarks for evaluating Knowledge Graph Question Answering (KGQA) systems. So
far the majority of benchmarks rely on pattern-based SPARQL query generation
approaches. The subsequent natural language (NL) question generation is
conducted through crowdsourcing or other automated methods, such as rule-based
paraphrasing or NL question templates. Although some of these datasets are of
considerable size, their pitfall lies in their pattern-based generation
approaches, which do not always generalize well to the vague and linguistically
diverse questions asked by humans in real-world contexts. In this paper, we
introduce Spider4SPARQL - a new SPARQL benchmark dataset featuring 9,693
previously existing manually generated NL questions and 4,721 unique, novel,
and complex SPARQL queries of varying complexity. In addition to the NL/SPARQL
pairs, we also provide their corresponding 166 knowledge graphs and ontologies,
which cover 138 different domains. Our complex benchmark enables novel ways of
evaluating the strengths and weaknesses of modern KGQA systems. We evaluate the
system with state-of-the-art KGQA systems as well as LLMs, which achieve only
up to 45\% execution accuracy, demonstrating that Spider4SPARQL is a
challenging benchmark for future research.},
Year          = {2023},
Month         = {Sep},
Note          = {IEEE International Conference on Big Data 2023},
Url           = {http://arxiv.org/abs/2309.16248v2},
File          = {2309.16248v2.pdf}
}
@article{2309.15971v1,
Author        = {Sanonda Datta Gupta and Torsten Hahmann},
Title         = {OPPO: An Ontology for Describing Fine-Grained Data Practices in Privacy
  Policies of Online Social Networks},
Eprint        = {2309.15971v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {Privacy policies outline the data practices of Online Social Networks (OSN)
to comply with privacy regulations such as the EU-GDPR and CCPA. Several
ontologies for modeling privacy regulations, policies, and compliance have
emerged in recent years. However, they are limited in various ways: (1) they
specifically model what is required of privacy policies according to one
specific privacy regulation such as GDPR; (2) they provide taxonomies of
concepts but are not sufficiently axiomatized to afford automated reasoning
with them; and (3) they do not model data practices of privacy policies in
sufficient detail to allow assessing the transparency of policies. This paper
presents an OWL Ontology for Privacy Policies of OSNs, OPPO, that aims to fill
these gaps by formalizing detailed data practices from OSNS' privacy policies.
OPPO is grounded in BFO, IAO, OMRSE, and OBI, and its design is guided by the
use case of representing and reasoning over the content of OSNs' privacy
policies and evaluating policies' transparency in greater detail.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.15971v1},
File          = {2309.15971v1.pdf}
}
@article{2309.15779v1,
Author        = {Dhiraj Amin and Sharvari Govilkar and Sagar Kulkarni},
Title         = {Question answering using deep learning in low resource Indian language
  Marathi},
Eprint        = {2309.15779v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Precise answers are extracted from a text for a given input question in a
question answering system. Marathi question answering system is created in
recent studies by using ontology, rule base and machine learning based
approaches. Recently transformer models and transfer learning approaches are
used to solve question answering challenges. In this paper we investigate
different transformer models for creating a reading comprehension-based Marathi
question answering system. We have experimented on different pretrained Marathi
language multilingual and monolingual models like Multilingual Representations
for Indian Languages (MuRIL), MahaBERT, Indic Bidirectional Encoder
Representations from Transformers (IndicBERT) and fine-tuned it on a Marathi
reading comprehension-based data set. We got the best accuracy in a MuRIL
multilingual model with an EM score of 0.64 and F1 score of 0.74 by fine tuning
the model on the Marathi dataset.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.15779v1},
File          = {2309.15779v1.pdf}
}
@article{2309.14841v1,
Author        = {Florian Ahrens and Mihai Pomarlan and Daniel BeÃler and Thorsten Fehr and Michael Beetz and Manfred Herrmann},
Title         = {Towards a Neuronally Consistent Ontology for Robotic Agents},
Eprint        = {2309.14841v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.NC},
Abstract      = {The Collaborative Research Center for Everyday Activity Science & Engineering
(CRC EASE) aims to enable robots to perform environmental interaction tasks
with close to human capacity. It therefore employs a shared ontology to model
the activity of both kinds of agents, empowering robots to learn from human
experiences. To properly describe these human experiences, the ontology will
strongly benefit from incorporating characteristics of neuronal information
processing which are not accessible from a behavioral perspective alone. We,
therefore, propose the analysis of human neuroimaging data for evaluation and
validation of concepts and events defined in the ontology model underlying most
of the CRC projects. In an exploratory analysis, we employed an Independent
Component Analysis (ICA) on functional Magnetic Resonance Imaging (fMRI) data
from participants who were presented with the same complex video stimuli of
activities as robotic and human agents in different environments and contexts.
We then correlated the activity patterns of brain networks represented by
derived components with timings of annotated event categories as defined by the
ontology model. The present results demonstrate a subset of common networks
with stable correlations and specificity towards particular event classes and
groups, associated with environmental and contextual factors. These neuronal
characteristics will open up avenues for adapting the ontology model to be more
consistent with human information processing.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.14841v1},
File          = {2309.14841v1.pdf}
}
@article{2309.15074v2,
Author        = {Haoyi Xiong and Jiang Bian and Sijia Yang and Xiaofei Zhang and Linghe Kong and Daqing Zhang},
Title         = {Natural Language based Context Modeling and Reasoning for Ubiquitous
  Computing with Large Language Models: A Tutorial},
Eprint        = {2309.15074v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) have become phenomenally surging, since
2018--two decades after introducing context-awareness into computing systems.
Through taking into account the situations of ubiquitous devices, users and the
societies, context-aware computing has enabled a wide spectrum of innovative
applications, such as assisted living, location-based social network services
and so on. To recognize contexts and make decisions for actions accordingly,
various artificial intelligence technologies, such as Ontology and OWL, have
been adopted as representations for context modeling and reasoning. Recently,
with the rise of LLMs and their improved natural language understanding and
reasoning capabilities, it has become feasible to model contexts using natural
language and perform context reasoning by interacting with LLMs such as ChatGPT
and GPT-4. In this tutorial, we demonstrate the use of texts, prompts, and
autonomous agents (AutoAgents) that enable LLMs to perform context modeling and
reasoning without requiring fine-tuning of the model. We organize and introduce
works in the related field, and name this computing paradigm as the LLM-driven
Context-aware Computing (LCaC). In the LCaC paradigm, users' requests, sensors
reading data, and the command to actuators are supposed to be represented as
texts. Given the text of users' request and sensor data, the AutoAgent models
the context by prompting and sends to the LLM for context reasoning. LLM
generates a plan of actions and responds to the AutoAgent, which later follows
the action plan to foster context-awareness. To prove the concepts, we use two
showcases--(1) operating a mobile z-arm in an apartment for assisted living,
and (2) planning a trip and scheduling the itinerary in a context-aware and
personalized manner.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.15074v2},
File          = {2309.15074v2.pdf}
}
@article{2309.13130v1,
Author        = {Moritz Blum and Basil Ell and Philipp Cimiano},
Title         = {Insights from an OTTR-centric Ontology Engineering Methodology},
Eprint        = {2309.13130v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {OTTR is a language for representing ontology modeling patterns, which enables
to build ontologies or knowledge bases by instantiating templates. Thereby,
particularities of the ontological representation language are hidden from the
domain experts, and it enables ontology engineers to, to some extent, separate
the processes of deciding about what information to model from deciding about
how to model the information, e.g., which design patterns to use. Certain
decisions can thus be postponed for the benefit of focusing on one of these
processes. To date, only few works on ontology engineering where ontology
templates are applied are described in the literature.
  In this paper, we outline our methodology and report findings from our
ontology engineering activities in the domain of Material Science. In these
activities, OTTR templates play a key role. Our ontology engineering process is
bottom-up, as we begin modeling activities from existing data that is then, via
templates, fed into a knowledge graph, and it is top-down, as we first focus on
which data to model and postpone the decision of how to model the data.
  We find, among other things, that OTTR templates are especially useful as a
means of communication with domain experts. Furthermore, we find that because
OTTR templates encapsulate modeling decisions, the engineering process becomes
flexible, meaning that design decisions can be changed at little cost.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.13130v1},
File          = {2309.13130v1.pdf}
}
@article{2309.12731v1,
Author        = {Dave Raggett},
Title         = {Defeasible Reasoning with Knowledge Graphs},
Eprint        = {2309.12731v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Human knowledge is subject to uncertainties, imprecision, incompleteness and
inconsistencies. Moreover, the meaning of many everyday terms is dependent on
the context. That poses a huge challenge for the Semantic Web. This paper
introduces work on an intuitive notation and model for defeasible reasoning
with imperfect knowledge, and relates it to previous work on argumentation
theory. PKN is to N3 as defeasible reasoning is to deductive logic. Further
work is needed on an intuitive syntax for describing reasoning strategies and
tactics in declarative terms, drawing upon the AIF ontology for inspiration.
The paper closes with observations on symbolic approaches in the era of large
language models.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.12731v1},
File          = {2309.12731v1.pdf}
}
@article{2309.13091v2,
Author        = {Mirko Navara and Karl Svozil},
Title         = {Form of Contextuality Predicting Probabilistic Equivalence between Two
  Sets of Three Mutually Noncommuting Observables},
Eprint        = {2309.13091v2},
DOI           = {10.1103/PhysRevA.109.022222},
ArchivePrefix = {arXiv},
PrimaryClass  = {quant-ph},
Abstract      = {We introduce a contextual quantum system comprising mutually complementary
observables organized into two or more collections of pseudocontexts with the
same probability sums of outcomes. These pseudocontexts constitute
non-orthogonal bases within the Hilbert space, featuring a state-independent
sum of probabilities. In other words, regardless of the initial state
preparation, the total probability remains constant but may be distinct from
unity. The measurement contextuality in this setup arises from the quantum
realizations of the hypergraph, which adhere to a specific bound on the linear
combination of probabilities. In contrast, classical realizations can surpass
this bound. The violation of quantum bounds stems from the inability of
classical ontological models, specifically the set-theoretic representation of
the hypergraph corresponding to the quantum observables' collections, to adhere
to and explain the observed statistics.},
Year          = {2023},
Month         = {Sep},
Note          = {Phys. Rev. A 109, 022222 (20 February 2024)},
Url           = {http://arxiv.org/abs/2309.13091v2},
File          = {2309.13091v2.pdf}
}
@article{2309.11791v2,
Author        = {Zhaoyi Wang and Zhenyang Zhang and Jiaxin Qin and Mizuho Iwaihara},
Title         = {SLHCat: Mapping Wikipedia Categories and Lists to DBpedia by Leveraging
  Semantic, Lexical, and Hierarchical Features},
Eprint        = {2309.11791v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {Wikipedia articles are hierarchically organized through categories and lists,
providing one of the most comprehensive and universal taxonomy, but its open
creation is causing redundancies and inconsistencies. Assigning DBPedia classes
to Wikipedia categories and lists can alleviate the problem, realizing a large
knowledge graph which is essential for categorizing digital contents through
entity linking and typing. However, the existing approach of CaLiGraph is
producing incomplete and non-fine grained mappings. In this paper, we tackle
the problem as ontology alignment, where structural information of knowledge
graphs and lexical and semantic features of ontology class names are utilized
to discover confident mappings, which are in turn utilized for finetuing
pretrained language models in a distant supervision fashion. Our method SLHCat
consists of two main parts: 1) Automatically generating training data by
leveraging knowledge graph structure, semantic similarities, and named entity
typing. 2) Finetuning and prompt-tuning of the pre-trained language model BERT
are carried out over the training data, to capture semantic and syntactic
properties of class names. Our model SLHCat is evaluated over a benchmark
dataset constructed by annotating 3000 fine-grained CaLiGraph-DBpedia mapping
pairs. SLHCat is outperforming the baseline model by a large margin of 25% in
accuracy, offering a practical solution for large-scale ontology mapping.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.11791v2},
File          = {2309.11791v2.pdf}
}
@article{2309.11669v1,
Author        = {Ali Mousavi and Xin Zhan and He Bai and Peng Shi and Theo Rekatsinas and Benjamin Han and Yunyao Li and Jeff Pound and Josh Susskind and Natalie Schluter and Ihab Ilyas and Navdeep Jaitly},
Title         = {Construction of Paired Knowledge Graph-Text Datasets Informed by Cyclic
  Evaluation},
Eprint        = {2309.11669v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Datasets that pair Knowledge Graphs (KG) and text together (KG-T) can be used
to train forward and reverse neural models that generate text from KG and vice
versa. However models trained on datasets where KG and text pairs are not
equivalent can suffer from more hallucination and poorer recall. In this paper,
we verify this empirically by generating datasets with different levels of
noise and find that noisier datasets do indeed lead to more hallucination. We
argue that the ability of forward and reverse models trained on a dataset to
cyclically regenerate source KG or text is a proxy for the equivalence between
the KG and the text in the dataset. Using cyclic evaluation we find that
manually created WebNLG is much better than automatically created TeKGen and
T-REx. Guided by these observations, we construct a new, improved dataset
called LAGRANGE using heuristics meant to improve equivalence between KG and
text and show the impact of each of the heuristics on cyclic evaluation. We
also construct two synthetic datasets using large language models (LLMs), and
observe that these are conducive to models that perform significantly well on
cyclic generation of text, but less so on cyclic generation of KGs, probably
because of a lack of a consistent underlying ontology.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.11669v1},
File          = {2309.11669v1.pdf}
}
@article{2309.10880v1,
Author        = {Tianyu Jiang and Sonia Vinogradova and Nathan Stringham and E. Louise Earl and Allan D. Hollander and Patrick R. Huber and Ellen Riloff and R. Sandra Schillo and Giorgio A. Ubbiali and Matthew Lange},
Title         = {Classifying Organizations for Food System Ontologies using Natural
  Language Processing},
Eprint        = {2309.10880v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Our research explores the use of natural language processing (NLP) methods to
automatically classify entities for the purpose of knowledge graph population
and integration with food system ontologies. We have created NLP models that
can automatically classify organizations with respect to categories associated
with environmental issues as well as Standard Industrial Classification (SIC)
codes, which are used by the U.S. government to characterize business
activities. As input, the NLP models are provided with text snippets retrieved
by the Google search engine for each organization, which serves as a textual
description of the organization that is used for learning. Our experimental
results show that NLP models can achieve reasonably good performance for these
two classification tasks, and they rely on a general framework that could be
applied to many other classification problems as well. We believe that NLP
models represent a promising approach for automatically harvesting information
to populate knowledge graphs and aligning the information with existing
ontologies through shared categories and concepts.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.10880v1},
File          = {2309.10880v1.pdf}
}
@article{2309.10132v1,
Author        = {Jonghan Lim and Leander Pfeiffer and Felix Ocker and Birgit Vogel-Heuser and Ilya Kovalenko},
Title         = {Ontology-Based Feedback to Improve Runtime Control for Multi-Agent
  Manufacturing Systems},
Eprint        = {2309.10132v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.MA},
Abstract      = {Improving the overall equipment effectiveness (OEE) of machines on the shop
floor is crucial to ensure the productivity and efficiency of manufacturing
systems. To achieve the goal of increased OEE, there is a need to develop
flexible runtime control strategies for the system. Decentralized strategies,
such as multi-agent systems, have proven effective in improving system
flexibility. However, runtime multi-agent control of complex manufacturing
systems can be challenging as the agents require extensive communication and
computational efforts to coordinate agent activities. One way to improve
communication speed and cooperation capabilities between system agents is by
providing a common language between these agents to represent knowledge about
system behavior. The integration of ontology into multi-agent systems in
manufacturing provides agents with the capability to continuously update and
refine their knowledge in a global context. This paper contributes to the
design of an ontology for multi-agent systems in manufacturing, introducing an
extendable knowledge base and a methodology for continuously updating the
production data by agents during runtime. To demonstrate the effectiveness of
the proposed framework, a case study is conducted in a simulated environment,
which shows improvements in OEE during runtime.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.10132v1},
File          = {2309.10132v1.pdf}
}
@article{2309.09898v1,
Author        = {Maurice Funk and Simon Hosemann and Jean Christoph Jung and Carsten Lutz},
Title         = {Towards Ontology Construction with Language Models},
Eprint        = {2309.09898v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We present a method for automatically constructing a concept hierarchy for a
given domain by querying a large language model. We apply this method to
various domains using OpenAI's GPT 3.5. Our experiments indicate that LLMs can
be of considerable help for constructing concept hierarchies.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.09898v1},
File          = {2309.09898v1.pdf}
}
@article{2309.09640v1,
Author        = {Colin M. Gray and Cristiana Santos and Nataliia Bielova and Thomas Mildner},
Title         = {An Ontology of Dark Patterns Knowledge: Foundations, Definitions, and a
  Pathway for Shared Knowledge-Building},
Eprint        = {2309.09640v1},
DOI           = {10.1145/3613904.3642436},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {Deceptive and coercive design practices are increasingly used by companies to
extract profit, harvest data, and limit consumer choice. Dark patterns
represent the most common contemporary amalgamation of these problematic
practices, connecting designers, technologists, scholars, regulators, and legal
professionals in transdisciplinary dialogue. However, a lack of universally
accepted definitions across the academic, legislative and regulatory space has
likely limited the impact that scholarship on dark patterns might have in
supporting sanctions and evolved design practices. In this paper, we seek to
support the development of a shared language of dark patterns, harmonizing ten
existing regulatory and academic taxonomies of dark patterns and proposing a
three-level ontology with standardized definitions for 65 synthesized dark
patterns types across low-, meso-, and high-level patterns. We illustrate how
this ontology can support translational research and regulatory action,
including pathways to extend our initial types through new empirical work and
map across application domains.},
Year          = {2023},
Month         = {Sep},
Note          = {CHI '24: Proceedings of the CHI Conference on Human Factors in
  Computing Systems May 2024},
Url           = {http://arxiv.org/abs/2309.09640v1},
File          = {2309.09640v1.pdf}
}
@article{2309.08754v1,
Author        = {Samira Babalou and Sheeba Samuel and Birgitta KÃ¶nig-Ries},
Title         = {Reproducible Domain-Specific Knowledge Graphs in the Life Sciences: a
  Systematic Literature Review},
Eprint        = {2309.08754v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Knowledge graphs (KGs) are widely used for representing and organizing
structured knowledge in diverse domains. However, the creation and upkeep of
KGs pose substantial challenges. Developing a KG demands extensive expertise in
data modeling, ontology design, and data curation. Furthermore, KGs are
dynamic, requiring continuous updates and quality control to ensure accuracy
and relevance. These intricacies contribute to the considerable effort required
for their development and maintenance. One critical dimension of KGs that
warrants attention is reproducibility. The ability to replicate and validate
KGs is fundamental for ensuring the trustworthiness and sustainability of the
knowledge they represent. Reproducible KGs not only support open science by
allowing others to build upon existing knowledge but also enhance transparency
and reliability in disseminating information. Despite the growing number of
domain-specific KGs, a comprehensive analysis concerning their reproducibility
has been lacking. This paper addresses this gap by offering a general overview
of domain-specific KGs and comparing them based on various reproducibility
criteria. Our study over 19 different domains shows only eight out of 250
domain-specific KGs (3.2%) provide publicly available source code. Among these,
only one system could successfully pass our reproducibility assessment (14.3%).
These findings highlight the challenges and gaps in achieving reproducibility
across domain-specific KGs. Our finding that only 0.4% of published
domain-specific KGs are reproducible shows a clear need for further research
and a shift in cultural practices.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.08754v1},
File          = {2309.08754v1.pdf}
}
@article{2310.18315v1,
Author        = {Debanjali Bain and Biswanath Dutta},
Title         = {Systematic Analysis of COVID-19 Ontologies},
Eprint        = {2310.18315v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {This comprehensive study conducts an in-depth analysis of existing COVID-19
ontologies, scrutinizing their objectives, classifications, design
methodologies, and domain focal points. The study is conducted through a
dual-stage approach, commencing with a systematic review of relevant literature
and followed by an ontological assessment utilizing a parametric methodology.
Through this meticulous process, twenty-four COVID-19 Ontologies (CovOs) are
selected and examined. The findings highlight the scope, intended purpose,
granularity of ontology, modularity, formalism, vocabulary reuse, and extent of
domain coverage. The analysis reveals varying levels of formality in ontology
development, a prevalent preference for utilizing OWL as the representational
language, and diverse approaches to constructing class hierarchies within the
models. Noteworthy is the recurrent reuse of ontologies like OBO models (CIDO,
GO, etc.) alongside CODO. The METHONTOLOGY approach emerges as a favored design
methodology, often coupled with application-based or data-centric evaluation
methods. Our study provides valuable insights for the scientific community and
COVID-19 ontology developers, supplemented by comprehensive ontology metrics.
By meticulously evaluating and documenting COVID-19 information-driven
ontological models, this research offers a comparative cross-domain
perspective, shedding light on knowledge representation variations. The present
study significantly enhances understanding of CovOs, serving as a consolidated
resource for comparative analysis and future development, while also
pinpointing research gaps and domain emphases, thereby guiding the trajectory
of future ontological advancements.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2310.18315v1},
File          = {2310.18315v1.pdf}
}
@article{2309.06930v1,
Author        = {Ahmad Zainul Ihsan and Said Fathalla and Stefan Sandfeld},
Title         = {Modeling Dislocation Dynamics Data Using Semantic Web Technologies},
Eprint        = {2309.06930v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cond-mat.mtrl-sci},
Abstract      = {Research in the field of Materials Science and Engineering focuses on the
design, synthesis, properties, and performance of materials. An important class
of materials that is widely investigated are crystalline materials, including
metals and semiconductors. Crystalline material typically contains a distinct
type of defect called "dislocation". This defect significantly affects various
material properties, including strength, fracture toughness, and ductility.
Researchers have devoted a significant effort in recent years to understanding
dislocation behavior through experimental characterization techniques and
simulations, e.g., dislocation dynamics simulations. This paper presents how
data from dislocation dynamics simulations can be modeled using semantic web
technologies through annotating data with ontologies. We extend the already
existing Dislocation Ontology by adding missing concepts and aligning it with
two other domain-related ontologies (i.e., the Elementary Multi-perspective
Material Ontology and the Materials Design Ontology) allowing for representing
the dislocation simulation data efficiently. Moreover, we show a real-world use
case by representing the discrete dislocation dynamics data as a knowledge
graph (DisLocKG) that illustrates the relationship between them. We also
developed a SPARQL endpoint that brings extensive flexibility to query
DisLocKG.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.06930v1},
File          = {2309.06930v1.pdf}
}
@article{2309.06814v1,
Author        = {R. Priyadharshini and G. Jeyakodi and P. Shanthi Bala},
Title         = {Comparative Analysis of Contextual Relation Extraction based on Deep
  Learning Models},
Eprint        = {2309.06814v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Contextual Relation Extraction (CRE) is mainly used for constructing a
knowledge graph with a help of ontology. It performs various tasks such as
semantic search, query answering, and textual entailment. Relation extraction
identifies the entities from raw texts and the relations among them. An
efficient and accurate CRE system is essential for creating domain knowledge in
the biomedical industry. Existing Machine Learning and Natural Language
Processing (NLP) techniques are not suitable to predict complex relations from
sentences that consist of more than two relations and unspecified entities
efficiently. In this work, deep learning techniques have been used to identify
the appropriate semantic relation based on the context from multiple sentences.
Even though various machine learning models have been used for relation
extraction, they provide better results only for binary relations, i.e.,
relations occurred exactly between the two entities in a sentence. Machine
learning models are not suited for complex sentences that consist of the words
that have various meanings. To address these issues, hybrid deep learning
models have been used to extract the relations from complex sentence
effectively. This paper explores the analysis of various deep learning models
that are used for relation extraction.},
Year          = {2023},
Month         = {Sep},
Note          = {An International Journal of Engineering Science, March 2023},
Url           = {http://arxiv.org/abs/2309.06814v1},
File          = {2309.06814v1.pdf}
}
@article{2309.07172v1,
Author        = {Yuan He and Jiaoyan Chen and Hang Dong and Ian Horrocks},
Title         = {Exploring Large Language Models for Ontology Alignment},
Eprint        = {2309.07172v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This work investigates the applicability of recent generative Large Language
Models (LLMs), such as the GPT series and Flan-T5, to ontology alignment for
identifying concept equivalence mappings across ontologies. To test the
zero-shot performance of Flan-T5-XXL and GPT-3.5-turbo, we leverage challenging
subsets from two equivalence matching datasets of the OAEI Bio-ML track, taking
into account concept labels and structural contexts. Preliminary findings
suggest that LLMs have the potential to outperform existing ontology alignment
systems like BERTMap, given careful framework and prompt design.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.07172v1},
File          = {2309.07172v1.pdf}
}
@article{2309.05936v1,
Author        = {Weiqi Wu and Chengyue Jiang and Yong Jiang and Pengjun Xie and Kewei Tu},
Title         = {Do PLMs Know and Understand Ontological Knowledge?},
Eprint        = {2309.05936v1},
DOI           = {10.18653/v1/2023.acl-long.173},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Ontological knowledge, which comprises classes and properties and their
relationships, is integral to world knowledge. It is significant to explore
whether Pretrained Language Models (PLMs) know and understand such knowledge.
However, existing PLM-probing studies focus mainly on factual knowledge,
lacking a systematic probing of ontological knowledge. In this paper, we focus
on probing whether PLMs store ontological knowledge and have a semantic
understanding of the knowledge rather than rote memorization of the surface
form. To probe whether PLMs know ontological knowledge, we investigate how well
PLMs memorize: (1) types of entities; (2) hierarchical relationships among
classes and properties, e.g., Person is a subclass of Animal and Member of
Sports Team is a subproperty of Member of ; (3) domain and range constraints of
properties, e.g., the subject of Member of Sports Team should be a Person and
the object should be a Sports Team. To further probe whether PLMs truly
understand ontological knowledge beyond memorization, we comprehensively study
whether they can reliably perform logical reasoning with given knowledge
according to ontological entailment rules. Our probing results show that PLMs
can memorize certain ontological knowledge and utilize implicit knowledge in
reasoning. However, both the memorizing and reasoning performances are less
than perfect, indicating incomplete knowledge and understanding.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.05936v1},
File          = {2309.05936v1.pdf}
}
@article{2309.05918v3,
Author        = {Walid S. Saba},
Title         = {Stochastic LLMs do not Understand Language: Towards Symbolic,
  Explainable and Ontologically Based LLMs},
Eprint        = {2309.05918v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In our opinion the exuberance surrounding the relative success of data-driven
large language models (LLMs) is slightly misguided and for several reasons (i)
LLMs cannot be relied upon for factual information since for LLMs all ingested
text (factual or non-factual) was created equal; (ii) due to their subsymbolic
na-ture, whatever 'knowledge' these models acquire about language will always
be buried in billions of microfeatures (weights), none of which is meaningful
on its own; and (iii) LLMs will often fail to make the correct inferences in
several linguistic contexts (e.g., nominal compounds, copredication, quantifier
scope ambi-guities, intensional contexts. Since we believe the relative success
of data-driven large language models (LLMs) is not a reflection on the symbolic
vs. subsymbol-ic debate but a reflection on applying the successful strategy of
a bottom-up reverse engineering of language at scale, we suggest in this paper
applying the effective bottom-up strategy in a symbolic setting resulting in
symbolic, explainable, and ontologically grounded language models.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.05918v3},
File          = {2309.05918v3.pdf}
}
@article{2309.13061v3,
Author        = {Armando D. Diaz Gonzalez and Kevin S. Hughes and Songhui Yue and Sean T. Hayes},
Title         = {Applying BioBERT to Extract Germline Gene-Disease Associations for
  Building a Knowledge Graph from the Biomedical Literature},
Eprint        = {2309.13061v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Published biomedical information has and continues to rapidly increase. The
recent advancements in Natural Language Processing (NLP), have generated
considerable interest in automating the extraction, normalization, and
representation of biomedical knowledge about entities such as genes and
diseases. Our study analyzes germline abstracts in the construction of
knowledge graphs of the of the immense work that has been done in this area for
genes and diseases. This paper presents SimpleGermKG, an automatic knowledge
graph construction approach that connects germline genes and diseases. For the
extraction of genes and diseases, we employ BioBERT, a pre-trained BERT model
on biomedical corpora. We propose an ontology-based and rule-based algorithm to
standardize and disambiguate medical terms. For semantic relationships between
articles, genes, and diseases, we implemented a part-whole relation approach to
connect each entity with its data source and visualize them in a graph-based
knowledge representation. Lastly, we discuss the knowledge graph applications,
limitations, and challenges to inspire the future research of germline corpora.
Our knowledge graph contains 297 genes, 130 diseases, and 46,747 triples.
Graph-based visualizations are used to show the results.},
Year          = {2023},
Month         = {Sep},
Note          = {The 7th International Conference on Information System and Data
  Mining (ICISDM2023-ACM), Atlanta, USA, May 2023},
Url           = {http://arxiv.org/abs/2309.13061v3},
File          = {2309.13061v3.pdf}
}
@article{2309.04019v2,
Author        = {Mengzhou Hu and Sahar Alkhairy and Ingoo Lee and Rudolf T. Pillich and Dylan Fong and Kevin Smith and Robin Bachelder and Trey Ideker and Dexter Pratt},
Title         = {Evaluation of large language models for discovery of gene set function},
Eprint        = {2309.04019v2},
DOI           = {10.1038/s41592-024-02525-x},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.GN},
Abstract      = {Gene set analysis is a mainstay of functional genomics, but it relies on
curated databases of gene functions that are incomplete. Here we evaluate five
Large Language Models (LLMs) for their ability to discover the common
biological functions represented by a gene set, substantiated by supporting
rationale, citations and a confidence assessment. Benchmarking against
canonical gene sets from the Gene Ontology, GPT-4 confidently recovered the
curated name or a more general concept (73% of cases), while benchmarking
against random gene sets correctly yielded zero confidence. Gemini-Pro and
Mixtral-Instruct showed ability in naming but were falsely confident for random
sets, whereas Llama2-70b had poor performance overall. In gene sets derived
from 'omics data, GPT-4 identified novel functions not reported by classical
functional enrichment (32% of cases), which independent review indicated were
largely verifiable and not hallucinations. The ability to rapidly synthesize
common gene functions positions LLMs as valuable 'omics assistants.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.04019v2},
File          = {2309.04019v2.pdf}
}
@article{2309.03723v2,
Author        = {Hemant K. Mishra and Michael Nussbaum and Mark M. Wilde},
Title         = {On the optimal error exponents for classical and quantum
  antidistinguishability},
Eprint        = {2309.03723v2},
DOI           = {10.1007/s11005-024-01821-z},
ArchivePrefix = {arXiv},
PrimaryClass  = {quant-ph},
Abstract      = {The concept of antidistinguishability of quantum states has been studied to
investigate foundational questions in quantum mechanics. It is also called
quantum state elimination, because the goal of such a protocol is to guess
which state, among finitely many chosen at random, the system is not prepared
in (that is, it can be thought of as the first step in a process of
elimination). Antidistinguishability has been used to investigate the reality
of quantum states, ruling out $\psi$-epistemic ontological models of quantum
mechanics [Pusey et al., Nat. Phys., 8(6):475-478, 2012]. Thus, due to the
established importance of antidistinguishability in quantum mechanics,
exploring it further is warranted.
  In this paper, we provide a comprehensive study of the optimal error exponent
-- the rate at which the optimal error probability vanishes to zero
asymptotically -- for classical and quantum antidistinguishability. We derive
an exact expression for the optimal error exponent in the classical case and
show that it is given by the multivariate classical Chernoff divergence. Our
work thus provides this divergence with a meaningful operational interpretation
as the optimal error exponent for antidistinguishing a set of probability
measures. For the quantum case, we provide several bounds on the optimal error
exponent: a lower bound given by the best pairwise Chernoff divergence of the
states, a single-letter semi-definite programming upper bound, and lower and
upper bounds in terms of minimal and maximal multivariate quantum Chernoff
divergences. It remains an open problem to obtain an explicit expression for
the optimal error exponent for quantum antidistinguishability.},
Year          = {2023},
Month         = {Sep},
Note          = {Letters in Mathematical Physics, Volume 114, Article Number 76,
  June 2024},
Url           = {http://arxiv.org/abs/2309.03723v2},
File          = {2309.03723v2.pdf}
}
@article{2309.03685v2,
Author        = {Nicolas Hubert and Pierre Monnin and Mathieu d'Aquin and Davy Monticolo and Armelle Brun},
Title         = {PyGraft: Configurable Generation of Synthetic Schemas and Knowledge
  Graphs at Your Fingertips},
Eprint        = {2309.03685v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge graphs (KGs) have emerged as a prominent data representation and
management paradigm. Being usually underpinned by a schema (e.g., an ontology),
KGs capture not only factual information but also contextual knowledge. In some
tasks, a few KGs established themselves as standard benchmarks. However, recent
works outline that relying on a limited collection of datasets is not
sufficient to assess the generalization capability of an approach. In some
data-sensitive fields such as education or medicine, access to public datasets
is even more limited. To remedy the aforementioned issues, we release PyGraft,
a Python-based tool that generates highly customized, domain-agnostic schemas
and KGs. The synthesized schemas encompass various RDFS and OWL constructs,
while the synthesized KGs emulate the characteristics and scale of real-world
KGs. Logical consistency of the generated resources is ultimately ensured by
running a description logic (DL) reasoner. By providing a way of generating
both a schema and KG in a single pipeline, PyGraft's aim is to empower the
generation of a more diverse array of KGs for benchmarking novel approaches in
areas such as graph-based machine learning (ML), or more generally KG
processing. In graph-based ML in particular, this should foster a more holistic
evaluation of model performance and generalization capability, thereby going
beyond the limited collection of available benchmarks. PyGraft is available at:
https://github.com/nicolas-hbt/pygraft.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.03685v2},
File          = {2309.03685v2.pdf}
}
@article{2309.03631v2,
Author        = {Markus Wenzel and Erik GrÃ¼ner and Nils Strodthoff},
Title         = {Insights Into the Inner Workings of Transformer Models for Protein
  Function Prediction},
Eprint        = {2309.03631v2},
DOI           = {10.1093/bioinformatics/btae031},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Motivation: We explored how explainable artificial intelligence (XAI) can
help to shed light into the inner workings of neural networks for protein
function prediction, by extending the widely used XAI method of integrated
gradients such that latent representations inside of transformer models, which
were finetuned to Gene Ontology term and Enzyme Commission number prediction,
can be inspected too. Results: The approach enabled us to identify amino acids
in the sequences that the transformers pay particular attention to, and to show
that these relevant sequence parts reflect expectations from biology and
chemistry, both in the embedding layer and inside of the model, where we
identified transformer heads with a statistically significant correspondence of
attribution maps with ground truth sequence annotations (e.g. transmembrane
regions, active sites) across many proteins. Availability and Implementation:
Source code can be accessed at https://github.com/markuswenzel/xai-proteins .},
Year          = {2023},
Month         = {Sep},
Note          = {Bioinformatics (2024) btae031},
Url           = {http://arxiv.org/abs/2309.03631v2},
File          = {2309.03631v2.pdf}
}
@article{2309.02723v1,
Author        = {Veronika Heimsbakk and Kristian Torkelsen},
Title         = {Using the Shapes Constraint Language for modelling regulatory
  requirements},
Eprint        = {2309.02723v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IT},
Abstract      = {Ontologies are traditionally expressed in the Web Ontology Language (OWL),
that provides a syntax for expressing taxonomies with axioms regulating class
membership. The semantics of OWL, based on Description Logic (DL), allows for
the use of automated reasoning to check the consistency of ontologies, perform
classification, and to answer DL queries. However, the open world assumption of
OWL, along with limitations in its expressiveness, makes OWL less suitable for
modelling rules and regulations, used in public administration. In such cases,
it is desirable to have closed world semantics and a rule-based engine to check
compliance with regulations. In this paper we describe and discuss data model
management using the Shapes Constraint Language (SHACL), for concept modelling
of concrete requirements in regulation documents within the public sector. We
show how complex regulations, often containing a number of alternative
requirements, can be expressed as constraints, and the utility of SHACL engines
in verification of instance data against the SHACL model. We discuss benefits
of modelling with SHACL, compared to OWL, and demonstrate the maintainability
of the SHACL model by domain experts without prior knowledge of ontology
management.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.02723v1},
File          = {2309.02723v1.pdf}
}
@article{2309.01370v1,
Author        = {Monika Jain and Kuldeep Singh and Raghava Mutharaju},
Title         = {ReOnto: A Neuro-Symbolic Approach for Biomedical Relation Extraction},
Eprint        = {2309.01370v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Relation Extraction (RE) is the task of extracting semantic relationships
between entities in a sentence and aligning them to relations defined in a
vocabulary, which is generally in the form of a Knowledge Graph (KG) or an
ontology. Various approaches have been proposed so far to address this task.
However, applying these techniques to biomedical text often yields
unsatisfactory results because it is hard to infer relations directly from
sentences due to the nature of the biomedical relations. To address these
issues, we present a novel technique called ReOnto, that makes use of neuro
symbolic knowledge for the RE task. ReOnto employs a graph neural network to
acquire the sentence representation and leverages publicly accessible
ontologies as prior knowledge to identify the sentential relation between two
entities. The approach involves extracting the relation path between the two
entities from the ontology. We evaluate the effect of using symbolic knowledge
from ontologies with graph neural networks. Experimental results on two public
biomedical datasets, BioRel and ADE, show that our method outperforms all the
baselines (approximately by 3\%).},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.01370v1},
File          = {2309.01370v1.pdf}
}
@article{2308.15897v1,
Author        = {Alex Ivliev and Stefan Ellmauthaler and Lukas Gerlach and Maximilian Marx and Matthias MeiÃner and Simon Meusel and Markus KrÃ¶tzsch},
Title         = {Nemo: First Glimpse of a New Rule Engine},
Eprint        = {2308.15897v1},
DOI           = {10.4204/EPTCS.385.35},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This system demonstration presents Nemo, a new logic programming engine with
a focus on reliability and performance. Nemo is built for data-centric analytic
computations, modelled in a fully declarative Datalog dialect. Its scalability
for these tasks matches or exceeds that of leading Datalog systems. We
demonstrate uses in reasoning with knowledge graphs and ontologies with 10^5 to
10^8 input facts, all on a laptop. Nemo is written in Rust and available as a
free and open source tool.},
Year          = {2023},
Month         = {Aug},
Note          = {EPTCS 385, 2023, pp. 333-335},
Url           = {http://arxiv.org/abs/2308.15897v1},
File          = {2308.15897v1.pdf}
}
@article{2308.14326v1,
Author        = {Maximilian Staebler and Frank Koester and Christoph Schlueter-Langdon},
Title         = {Towards solving ontological dissonance using network graphs},
Eprint        = {2308.14326v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Data Spaces are an emerging concept for the trusted implementation of
data-based applications and business models, offering a high degree of
flexibility and sovereignty to all stakeholders. As Data Spaces are currently
emerging in different domains such as mobility, health or food, semantic
interfaces need to be identified and implemented to ensure the technical
interoperability of these Data Spaces. This paper consolidates data models from
13 different domains and analyzes the ontological dissonance of these domains.
Using a network graph, central data models and ontology attributes are
identified, while the semantic heterogeneity of these domains is described
qualitatively. The research outlook describes how these results help to connect
different Data Spaces across domains.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.14326v1},
File          = {2308.14326v1.pdf}
}
@article{2308.14199v1,
Author        = {Walid S. Saba},
Title         = {Symbolic and Language Agnostic Large Language Models},
Eprint        = {2308.14199v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We argue that the relative success of large language models (LLMs) is not a
reflection on the symbolic vs. subsymbolic debate but a reflection on employing
an appropriate strategy of bottom-up reverse engineering of language at scale.
However, due to the subsymbolic nature of these models whatever knowledge these
systems acquire about language will always be buried in millions of
microfeatures (weights) none of which is meaningful on its own. Moreover, and
due to their stochastic nature, these models will often fail in capturing
various inferential aspects that are prevalent in natural language. What we
suggest here is employing the successful bottom-up strategy in a symbolic
setting, producing symbolic, language agnostic and ontologically grounded large
language models.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.14199v1},
File          = {2308.14199v1.pdf}
}
@article{2308.13433v1,
Author        = {Tom Westermann and Milapji Singh Gill and Alexander Fay},
Title         = {Representing Timed Automata and Timing Anomalies of Cyber-Physical
  Production Systems in Knowledge Graphs},
Eprint        = {2308.13433v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Model-Based Anomaly Detection has been a successful approach to identify
deviations from the expected behavior of Cyber-Physical Production Systems.
Since manual creation of these models is a time-consuming process, it is
advantageous to learn them from data and represent them in a generic formalism
like timed automata. However, these models - and by extension, the detected
anomalies - can be challenging to interpret due to a lack of additional
information about the system. This paper aims to improve model-based anomaly
detection in CPPS by combining the learned timed automaton with a formal
knowledge graph about the system. Both the model and the detected anomalies are
described in the knowledge graph in order to allow operators an easier
interpretation of the model and the detected anomalies. The authors
additionally propose an ontology of the necessary concepts. The approach was
validated on a five-tank mixing CPPS and was able to formally define both
automata model as well as timing anomalies in automata execution.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.13433v1},
File          = {2308.13433v1.pdf}
}
@article{2308.13104v2,
Author        = {Mohsen Nayebi Kerdabadi and Arya Hadizadeh Moghaddam and Bin Liu and Mei Liu and Zijun Yao},
Title         = {Contrastive Learning of Temporal Distinctiveness for Survival Analysis
  in Electronic Health Records},
Eprint        = {2308.13104v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Survival analysis plays a crucial role in many healthcare decisions, where
the risk prediction for the events of interest can support an informative
outlook for a patient's medical journey. Given the existence of data censoring,
an effective way of survival analysis is to enforce the pairwise temporal
concordance between censored and observed data, aiming to utilize the time
interval before censoring as partially observed time-to-event labels for
supervised learning. Although existing studies mostly employed ranking methods
to pursue an ordering objective, contrastive methods which learn a
discriminative embedding by having data contrast against each other, have not
been explored thoroughly for survival analysis. Therefore, in this paper, we
propose a novel Ontology-aware Temporality-based Contrastive Survival (OTCSurv)
analysis framework that utilizes survival durations from both censored and
observed data to define temporal distinctiveness and construct negative sample
pairs with adjustable hardness for contrastive learning. Specifically, we first
use an ontological encoder and a sequential self-attention encoder to represent
the longitudinal EHR data with rich contexts. Second, we design a temporal
contrastive loss to capture varying survival durations in a supervised setting
through a hardness-aware negative sampling mechanism. Last, we incorporate the
contrastive task into the time-to-event predictive task with multiple loss
components. We conduct extensive experiments using a large EHR dataset to
forecast the risk of hospitalized patients who are in danger of developing
acute kidney injury (AKI), a critical and urgent medical condition. The
effectiveness and explainability of the proposed model are validated through
comprehensive quantitative and qualitative studies.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.13104v2},
File          = {2308.13104v2.pdf}
}
@article{2308.11066v3,
Author        = {Songhui Yue and Xiaoyan Hong and Randy K. Smith},
Title         = {CSM-H-R: A Context Modeling Framework in Supporting Reasoning Automation
  for Interoperable Intelligent Systems and Privacy Protection},
Eprint        = {2308.11066v3},
DOI           = {10.1109/ACCESS.2024.3446274},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The automation of High-Level Context (HLC) reasoning across intelligent
systems at scale is imperative because of the unceasing accumulation of
contextual data, the trend of the fusion of data from multiple sources (e.g.,
sensors, intelligent systems), and the intrinsic complexity and dynamism of
context-based decision-making processes. To mitigate the challenges posed by
these issues, we propose a novel Hierarchical Ontology-State Modeling (HOSM)
framework CSM-H-R, which programmatically combines ontologies and states at the
modeling phase and runtime phase for attaining the ability to recognize
meaningful HLC. It builds on the model of our prior work on the Context State
Machine (CSM) engine by incorporating the H (Hierarchy) and R (Relationship and
tRansition) dimensions to take care of the dynamic aspects of context. The
design of the framework supports the sharing and interoperation of context
among intelligent systems and the components for handling CSMs and the
management of hierarchy, relationship, and transition. Case studies are
developed for IntellElevator and IntellRestaurant, two intelligent applications
in a smart campus setting. The prototype implementation of the framework
experiments on translating the HLC reasoning into vector and matrix computing
and presents the potential of using advanced probabilistic models to reach the
next level of automation in integrating intelligent systems; meanwhile, privacy
protection support is achieved in the application domain by anonymization
through indexing and reducing information correlation. An implementation of the
framework is available at https://github.com/songhui01/CSM-H-R.},
Year          = {2023},
Month         = {Aug},
Note          = {IEEE ACCESS, 2024},
Url           = {http://arxiv.org/abs/2308.11066v3},
File          = {2308.11066v3.pdf}
}
@article{2308.09217v1,
Author        = {Sanaz Saki Norouzi and Mohammad Saeid Mahdavinejad and Pascal Hitzler},
Title         = {Conversational Ontology Alignment with ChatGPT},
Eprint        = {2308.09217v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This study evaluates the applicability and efficiency of ChatGPT for ontology
alignment using a naive approach. ChatGPT's output is compared to the results
of the Ontology Alignment Evaluation Initiative 2022 campaign using conference
track ontologies. This comparison is intended to provide insights into the
capabilities of a conversational large language model when used in a naive way
for ontology matching, and to investigate the potential advantages and
disadvantages of this approach.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.09217v1},
File          = {2308.09217v1.pdf}
}
@article{2308.09729v5,
Author        = {Yilin Wen and Zifeng Wang and Jimeng Sun},
Title         = {MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large
  Language Models},
Eprint        = {2308.09729v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large language models (LLMs) have achieved remarkable performance in natural
language understanding and generation tasks. However, they often suffer from
limitations such as difficulty in incorporating new knowledge, generating
hallucinations, and explaining their reasoning process. To address these
challenges, we propose a novel prompting pipeline, named \method, that
leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency.
Our method enables LLMs to comprehend KG inputs and infer with a combination of
implicit and external knowledge. Moreover, our method elicits the mind map of
LLMs, which reveals their reasoning pathways based on the ontology of
knowledge. We evaluate our method on diverse question \& answering tasks,
especially in medical domains, and show significant improvements over
baselines. We also introduce a new hallucination evaluation benchmark and
analyze the effects of different components of our method. Our results
demonstrate the effectiveness and robustness of our method in merging knowledge
from LLMs and KGs for combined inference. To reproduce our results and extend
the framework further, we make our codebase available at
https://github.com/wyl-willing/MindMap.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.09729v5},
File          = {2308.09729v5.pdf}
}
@article{2308.08200v1,
Author        = {Tobias John and Patrick Koopmann},
Title         = {Towards Ontology-Mediated Planning with OWL DL Ontologies (Extended
  Version)},
Eprint        = {2308.08200v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {While classical planning languages make the closed-domain and closed-world
assumption, there have been various approaches to extend those with DL
reasoning, which is then interpreted under the usual open-world semantics.
Current approaches for planning with DL ontologies integrate the DL directly
into the planning language, and practical approaches have been developed based
on first-order rewritings or rewritings into datalog. We present here a new
approach in which the planning specification and ontology are kept separate,
and are linked together using an interface. This allows planning experts to
work in a familiar formalism, while existing ontologies can be easily
integrated and extended by ontology experts. Our approach for planning with
those ontology-mediated planning problems is optimized for cases with
comparatively small domains, and supports the whole OWL DL fragment. The idea
is to rewrite the ontology-mediated planning problem into a classical planning
problem to be processed by existing planning tools. Different to other
approaches, our rewriting is data-dependent. A first experimental evaluation of
our approach shows the potential and limitations of this approach.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.08200v1},
File          = {2308.08200v1.pdf}
}
@article{2308.09483v1,
Author        = {Sabah Al-Fedaghi},
Title         = {Ontology for Conceptual Modeling: Reality of What Thinging Machines Talk
  About, e.g., Information},
Eprint        = {2308.09483v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {In conceptual modeling (CM) as a subdiscipline of software engineering,
current proposed ontologies (categorical analysis of entities) are typically
established through whole adoption of philosophical theories (e.g. Bunge s). In
this paper, we pursue an interdisciplinary research approach to develop a
diagrammatic-based ontological foundation for CM using philosophical ontology
as a secondary source. It is an endeavor to escape an offshore procurement of
ontology from philosophy and implant it in CM. In such an effort, the CM
diagrammatic language plays an important role in contrast to dogmatic
philosophical languages obsession with abstract entities. Specifically, this
paper is about developing a descriptive (in contrast to formal) ontology that a
modeler accepts as a supplementary account of reality when using thinging
machines (TMs; i.e. a reality that uncovers the ontology of things that TM
modeling discusses or talks about, akin to the ontology of natural language).
The aim here is aligned toward developing CM notions and processes that are
firm enough. Classical analysis of being per se (e.g. identity, substance) is
de-emphasized in this work; nevertheless, philosophical concepts form an
acknowledged authority to compare to. As a case study, such a methodology is
applied to the notion of information. This application would enhance
understanding of the TM methodology and clarify some of the issues that shed
light on the question of the nature of information as an important concept in
software engineering. Information is defined as about events; that is, it is
about existing things. It is viewed as having a subsisting nature that exists
only through being carried on by other things. The results seem to indicate a
promising approach to define information and understand its nature.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.09483v1},
File          = {2308.09483v1.pdf}
}
@article{2308.07876v3,
Author        = {Yibo Hu and Erick Skorupa Parolin and Latifur Khan and Patrick T. Brandt and Javier Osorio and Vito J. D'Orazio},
Title         = {Leveraging Codebook Knowledge with NLI and ChatGPT for Zero-Shot
  Political Relation Classification},
Eprint        = {2308.07876v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Is it possible accurately classify political relations within evolving event
ontologies without extensive annotations? This study investigates zero-shot
learning methods that use expert knowledge from existing annotation codebook,
and evaluates the performance of advanced ChatGPT (GPT-3.5/4) and a natural
language inference (NLI)-based model called ZSP. ChatGPT uses codebook's
labeled summaries as prompts, whereas ZSP breaks down the classification task
into context, event mode, and class disambiguation to refine task-specific
hypotheses. This decomposition enhances interpretability, efficiency, and
adaptability to schema changes. The experiments reveal ChatGPT's strengths and
limitations, and crucially show ZSP's outperformance of dictionary-based
methods and its competitive edge over some supervised models. These findings
affirm the value of ZSP for validating event records and advancing ontology
development. Our study underscores the efficacy of leveraging transfer learning
and existing domain expertise to enhance research efficiency and scalability.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.07876v3},
File          = {2308.07876v3.pdf}
}
@article{2308.07096v1,
Author        = {Mohammed Kharma and Ahmed Sabbah and Mustafa Jarrar},
Title         = {Towards a Cloud-Based Ontology for Service Model Security -- Technical
  Report},
Eprint        = {2308.07096v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {The adoption of cloud computing has brought significant advancements in the
operational models of businesses. However, this shift also brings new security
challenges by expanding the attack surface. The offered services in cloud
computing have various service models. Each cloud service model has a defined
responsibility divided based on the stack layers between the service user and
their cloud provider. Regardless of its service model, each service is
constructed from sub-components and services running on the underlying layers.
In this paper, we aim to enable more transparency and visibility by designing
an ontology that links the provider's services with the sub-components used to
deliver the service. Such breakdown for each cloud service sub-components
enables the end user to track the vulnerabilities on the service level or one
of its sub-components. Such information can result in a better understanding
and management of reported vulnerabilities on the sub-components level and
their impact on the offered services by the cloud provider. Our ontology and
source code are published as an open-source and accessible via GitHub:
\href{https://github.com/mohkharma/cc-ontology}{mohkharma/cc-ontology}},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.07096v1},
File          = {2308.07096v1.pdf}
}
@article{2308.06480v1,
Author        = {Yunshan Ma and Chenchen Ye and Zijian Wu and Xiang Wang and Yixin Cao and Tat-Seng Chua},
Title         = {Context-aware Event Forecasting via Graph Disentanglement},
Eprint        = {2308.06480v1},
DOI           = {10.1145/3580305.3599285},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Event forecasting has been a demanding and challenging task throughout the
entire human history. It plays a pivotal role in crisis alarming and disaster
prevention in various aspects of the whole society. The task of event
forecasting aims to model the relational and temporal patterns based on
historical events and makes forecasting to what will happen in the future. Most
existing studies on event forecasting formulate it as a problem of link
prediction on temporal event graphs. However, such pure structured formulation
suffers from two main limitations: 1) most events fall into general and
high-level types in the event ontology, and therefore they tend to be
coarse-grained and offers little utility which inevitably harms the forecasting
accuracy; and 2) the events defined by a fixed ontology are unable to retain
the out-of-ontology contextual information. To address these limitations, we
propose a novel task of context-aware event forecasting which incorporates
auxiliary contextual information. First, the categorical context provides
supplementary fine-grained information to the coarse-grained events. Second and
more importantly, the context provides additional information towards specific
situation and condition, which is crucial or even determinant to what will
happen next. However, it is challenging to properly integrate context into the
event forecasting framework, considering the complex patterns in the
multi-context scenario. Towards this end, we design a novel framework named
Separation and Collaboration Graph Disentanglement (short as SeCoGD) for
context-aware event forecasting. Since there is no available dataset for this
novel task, we construct three large-scale datasets based on GDELT.
Experimental results demonstrate that our model outperforms a list of SOTA
methods.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.06480v1},
File          = {2308.06480v1.pdf}
}
@article{2308.05609v1,
Author        = {Pedro Ruas and Diana F. Sousa and AndrÃ© Neves and Carlos Cruz and Francisco M. Couto},
Title         = {LASIGE and UNICAGE solution to the NASA LitCoin NLP Competition},
Eprint        = {2308.05609v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Biomedical Natural Language Processing (NLP) tends to become cumbersome for
most researchers, frequently due to the amount and heterogeneity of text to be
processed. To address this challenge, the industry is continuously developing
highly efficient tools and creating more flexible engineering solutions. This
work presents the integration between industry data engineering solutions for
efficient data processing and academic systems developed for Named Entity
Recognition (LasigeUnicage\_NER) and Relation Extraction (BiOnt). Our design
reflects an integration of those components with external knowledge in the form
of additional training data from other datasets and biomedical ontologies. We
used this pipeline in the 2022 LitCoin NLP Challenge, where our team
LasigeUnicage was awarded the 7th Prize out of approximately 200 participating
teams, reflecting a successful collaboration between the academia (LASIGE) and
the industry (Unicage). The software supporting this work is available at
\url{https://github.com/lasigeBioTM/Litcoin-Lasige_Unicage}.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.05609v1},
File          = {2308.05609v1.pdf}
}
@article{2308.03929v4,
Author        = {Ahmed Abdeen Hamed and Byung Suk Lee and Alessandro Crimi and Magdalena M. Misiak},
Title         = {Fact-Checking Generative AI: Ontology-Driven Biological Graphs for
  Disease-Gene Link Verification},
Eprint        = {2308.03929v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Since the launch of various generative AI tools, scientists have been
striving to evaluate their capabilities and contents, in the hope of
establishing trust in their generative abilities. Regulations and guidelines
are emerging to verify generated contents and identify novel uses. we aspire to
demonstrate how ChatGPT claims are checked computationally using the rigor of
network models. We aim to achieve fact-checking of the knowledge embedded in
biological graphs that were contrived from ChatGPT contents at the aggregate
level. We adopted a biological networks approach that enables the systematic
interrogation of ChatGPT's linked entities. We designed an ontology-driven
fact-checking algorithm that compares biological graphs constructed from
approximately 200,000 PubMed abstracts with counterparts constructed from a
dataset generated using the ChatGPT-3.5 Turbo model. In 10-samples of 250
randomly selected records a ChatGPT dataset of 1000 "simulated" articles , the
fact-checking link accuracy ranged from 70% to 86%. This study demonstrated
high accuracy of aggregate disease-gene links relationships found in
ChatGPT-generated texts.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.03929v4},
File          = {2308.03929v4.pdf}
}
@article{2308.07325v1,
Author        = {Mehrdad Jalali and Matthias Mail and Rossella Aversa and Christian KÃ¼bel},
Title         = {MSLE: An ontology for Materials Science Laboratory Equipment.
  Large-Scale Devices for Materials Characterization},
Eprint        = {2308.07325v1},
DOI           = {10.1016/j.mtcomm.2023.105532},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper introduces a new ontology for Materials Science Laboratory
Equipment, termed MSLE. A fundamental issue with materials science laboratory
(hereafter lab) equipment in the real world is that scientists work with
various types of equipment with multiple specifications. For example, there are
many electron microscopes with different parameters in chemical and physical
labs. A critical development to unify the description is to build an equipment
domain ontology as basic semantic knowledge and to guide the user to work with
the equipment appropriately. Here, we propose to develop a consistent ontology
for equipment, the MSLE ontology. In the MSLE, two main existing ontologies,
the Semantic Sensor Network (SSN) and the Material Vocabulary (MatVoc), have
been integrated into the MSLE core to build a coherent ontology. Since various
acronyms and terms have been used for equipment, this paper proposes an
approach to use a Simple Knowledge Organization System (SKOS) to represent the
hierarchical structure of equipment terms. Equipment terms were collected in
various languages and abbreviations and coded into the MSLE using the SKOS
model. The ontology development was conducted in close collaboration with
domain experts and focused on the large-scale devices for materials
characterization available in our research group. Competency questions are
expected to be addressed through the MSLE ontology. Constraints are modeled in
the Shapes Query Language (SHACL); a prototype is shown and validated to show
the value of the modeling constraints.},
Year          = {2023},
Month         = {Aug},
Note          = {Mater. Today Commun. 35 (2023) 105532},
Url           = {http://arxiv.org/abs/2308.07325v1},
File          = {2308.07325v1.pdf}
}
@article{2308.09719v1,
Author        = {Shusaku Egami and Yasunori Yamamoto and Ikki Ohmukai and Takashi Okumura},
Title         = {CIRO: COVID-19 infection risk ontology},
Eprint        = {2308.09719v1},
DOI           = {10.1371/journal.pone.0282291},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Public health authorities perform contact tracing for highly contagious
agents to identify close contacts with the infected cases. However, during the
pandemic caused by coronavirus disease 2019 (COVID-19), this operation was not
employed in countries with high patient volumes. Meanwhile, the Japanese
government conducted this operation, thereby contributing to the control of
infections, at the cost of arduous manual labor by public health officials. To
ease the burden of the officials, this study attempted to automate the
assessment of each person's infection risk through an ontology, called COVID-19
Infection Risk Ontology (CIRO). This ontology expresses infection risks of
COVID-19 formulated by the Japanese government, toward automated assessment of
infection risks of individuals, using Resource Description Framework (RDF) and
SPARQL (SPARQL Protocol and RDF Query Language) queries. For evaluation, we
demonstrated that the knowledge graph built could infer the risks, formulated
by the government. Moreover, we conducted reasoning experiments to analyze the
computational efficiency. The experiments demonstrated usefulness of the
knowledge processing, and identified issues left for deployment.},
Year          = {2023},
Month         = {Aug},
Note          = {PLoS One, 18(3), e0282291, 2023},
Url           = {http://arxiv.org/abs/2308.09719v1},
File          = {2308.09719v1.pdf}
}
@article{2308.02357v1,
Author        = {Nandana Mihindukulasooriya and Sanju Tiwari and Carlos F. Enguix and Kusum Lata},
Title         = {Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation
  from Text},
Eprint        = {2308.02357v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The recent advances in large language models (LLM) and foundation models with
emergent capabilities have been shown to improve the performance of many NLP
tasks. LLMs and Knowledge Graphs (KG) can complement each other such that LLMs
can be used for KG construction or completion while existing KGs can be used
for different tasks such as making LLM outputs explainable or fact-checking in
Neuro-Symbolic manner. In this paper, we present Text2KGBench, a benchmark to
evaluate the capabilities of language models to generate KGs from natural
language text guided by an ontology. Given an input ontology and a set of
sentences, the task is to extract facts from the text while complying with the
given ontology (concepts, relations, domain/range constraints) and being
faithful to the input sentences. We provide two datasets (i) Wikidata-TekGen
with 10 ontologies and 13,474 sentences and (ii) DBpedia-WebNLG with 19
ontologies and 4,860 sentences. We define seven evaluation metrics to measure
fact extraction performance, ontology conformance, and hallucinations by LLMs.
Furthermore, we provide results for two baseline models, Vicuna-13B and
Alpaca-LoRA-13B using automatic prompt generation from test cases. The baseline
results show that there is room for improvement using both Semantic Web and
Natural Language Processing techniques.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.02357v1},
File          = {2308.02357v1.pdf}
}
@article{2308.01597v1,
Author        = {Stefano Borgo and Roberta Ferrario and Aldo Gangemi and Nicola Guarino and Claudio Masolo and Daniele Porello and Emilio M. Sanfilippo and Laure Vieu},
Title         = {DOLCE: A Descriptive Ontology for Linguistic and Cognitive Engineering},
Eprint        = {2308.01597v1},
DOI           = {10.3233/AO-210259},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {DOLCE, the first top-level (foundational) ontology to be axiomatized, has
remained stable for twenty years and today is broadly used in a variety of
domains. DOLCE is inspired by cognitive and linguistic considerations and aims
to model a commonsense view of reality, like the one human beings exploit in
everyday life in areas as diverse as socio-technical systems, manufacturing,
financial transactions and cultural heritage. DOLCE clearly lists the
ontological choices it is based upon, relies on philosophical principles, is
richly formalized, and is built according to well-established ontological
methodologies, e.g. OntoClean. Because of these features, it has inspired most
of the existing top-level ontologies and has been used to develop or improve
standards and public domain resources (e.g. CIDOC CRM, DBpedia and WordNet).
Being a foundational ontology, DOLCE is not directly concerned with domain
knowledge. Its purpose is to provide the general categories and relations
needed to give a coherent view of reality, to integrate domain knowledge, and
to mediate across domains. In these 20 years DOLCE has shown that applied
ontologies can be stable and that interoperability across reference and domain
ontologies is a reality. This paper briefly introduces the ontology and shows
how to use it on a few modeling cases.},
Year          = {2023},
Month         = {Aug},
Note          = {Applied Ontology 17 (2022):45-69},
Url           = {http://arxiv.org/abs/2308.01597v1},
File          = {2308.01597v1.pdf}
}
@article{2308.00735v1,
Author        = {Marcio Ferreira Moreno and Rafael Rossi de Mello BrandÃ£o},
Title         = {A Knowledge-Oriented Approach to Enhance Integration and Communicability
  in the Polkadot Ecosystem},
Eprint        = {2308.00735v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The Polkadot ecosystem is a disruptive and highly complex multi-chain
architecture that poses challenges in terms of data analysis and
communicability. Currently, there is a lack of standardized and holistic
approaches to retrieve and analyze data across parachains and applications,
making it difficult for general users and developers to access ecosystem data
consistently. This paper proposes a conceptual framework that includes a domain
ontology called POnto (a Polkadot Ontology) to address these challenges. POnto
provides a structured representation of the ecosystem's concepts and
relationships, enabling a formal understanding of the platform. The proposed
knowledge-oriented approach enhances integration and communicability, enabling
a wider range of users to participate in the ecosystem and facilitating the
development of AI-based applications. The paper presents a case study
methodology to validate the proposed framework, which includes expert feedback
and insights from the Polkadot community. The POnto ontology and the roadmap
for a query engine based on a Controlled Natural Language using the ontology,
provide valuable contributions to the growth and adoption of the Polkadot
ecosystem in heterogeneous socio-technical environments.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.00735v1},
File          = {2308.00735v1.pdf}
}
@article{2308.00607v1,
Author        = {Alan Perotti and Simone Bertolotto and Eliana Pastor and AndrÃ© Panisson},
Title         = {Beyond One-Hot-Encoding: Injecting Semantics to Drive Image Classifiers},
Eprint        = {2308.00607v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Images are loaded with semantic information that pertains to real-world
ontologies: dog breeds share mammalian similarities, food pictures are often
depicted in domestic environments, and so on. However, when training machine
learning models for image classification, the relative similarities amongst
object classes are commonly paired with one-hot-encoded labels. According to
this logic, if an image is labelled as 'spoon', then 'tea-spoon' and 'shark'
are equally wrong in terms of training loss. To overcome this limitation, we
explore the integration of additional goals that reflect ontological and
semantic knowledge, improving model interpretability and trustworthiness. We
suggest a generic approach that allows to derive an additional loss term
starting from any kind of semantic information about the classification label.
First, we show how to apply our approach to ontologies and word embeddings, and
discuss how the resulting information can drive a supervised learning process.
Second, we use our semantically enriched loss to train image classifiers, and
analyse the trade-offs between accuracy, mistake severity, and learned internal
representations. Finally, we discuss how this approach can be further exploited
in terms of explainability and adversarial robustness. Code repository:
https://github.com/S1M0N38/semantic-encodings},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.00607v1},
File          = {2308.00607v1.pdf}
}
@article{2308.00447v1,
Author        = {Eren Unlu},
Title         = {Structural Embeddings of Tools for Large Language Models},
Eprint        = {2308.00447v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {It is evident that the current state of Large Language Models (LLMs)
necessitates the incorporation of external tools. The lack of straightforward
algebraic and logical reasoning is well documented and prompted researchers to
develop frameworks which allow LLMs to operate via external tools. The
ontological nature of tool utilization for a specific task can be well
formulated with a Directed Acyclic Graph (DAG). The central aim of the paper is
to highlight the importance of graph based approaches to LLM-tool interaction
in near future. We propose an exemplary framework to guide the orchestration of
exponentially increasing numbers of external tools with LLMs,where objectives
and functionalities of tools are graph encoded hierarchically. Assuming that
textual segments of a Chain-of-Thought (CoT) can be imagined as a tool as
defined here, the graph based framework can pave new avenues in that particular
direction as well.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.00447v1},
File          = {2308.00447v1.pdf}
}
@article{2308.00116v1,
Author        = {Rushrukh Rayan and Cogan Shimizu and Heidi Sieverding and Pascal Hitzler},
Title         = {A Modular Ontology for MODS -- Metadata Object Description Schema},
Eprint        = {2308.00116v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The Metadata Object Description Schema (MODS) was developed to describe
bibliographic concepts and metadata and is maintained by the Library of
Congress. Its authoritative version is given as an XML schema based on an XML
mindset which means that it has significant limitations for use in a knowledge
graphs context. We have therefore developed the Modular MODS Ontology (MMODS-O)
which incorporates all elements and attributes of the MODS XML schema. In
designing the ontology, we adopt the recent Modular Ontology Design Methodology
(MOMo) with the intention to strike a balance between modularity and quality
ontology design on the one hand, and conservative backward compatibility with
MODS on the other.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2308.00116v1},
File          = {2308.00116v1.pdf}
}
@article{2307.16773v2,
Author        = {Tianxing Wu and Xudong Cao and Yipeng Zhu and Feiyue Wu and Tianling Gong and Yuxiang Wang and Shenqi Jing},
Title         = {AsdKB: A Chinese Knowledge Base for the Early Screening and Diagnosis of
  Autism Spectrum Disorder},
Eprint        = {2307.16773v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {To easily obtain the knowledge about autism spectrum disorder and help its
early screening and diagnosis, we create AsdKB, a Chinese knowledge base on
autism spectrum disorder. The knowledge base is built on top of various
sources, including 1) the disease knowledge from SNOMED CT and ICD-10 clinical
descriptions on mental and behavioural disorders, 2) the diagnostic knowledge
from DSM-5 and different screening tools recommended by social organizations
and medical institutes, and 3) the expert knowledge on professional physicians
and hospitals from the Web. AsdKB contains both ontological and factual
knowledge, and is accessible as Linked Data at https://w3id.org/asdkb/. The
potential applications of AsdKB are question answering, auxiliary diagnosis,
and expert recommendation, and we illustrate them with a prototype which can be
accessed at http://asdkb.org.cn/.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.16773v2},
File          = {2307.16773v2.pdf}
}
@article{2307.16699v1,
Author        = {Patricia Mateiu and Adrian Groza},
Title         = {Ontology engineering with Large Language Models},
Eprint        = {2307.16699v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We tackle the task of enriching ontologies by automatically translating
natural language sentences into Description Logic. Since Large Language Models
(LLMs) are the best tools for translations, we fine-tuned a GPT-3 model to
convert Natural Language sentences into OWL Functional Syntax. We employ
objective and concise examples to fine-tune the model regarding: instances,
class subsumption, domain and range of relations, object properties
relationships, disjoint classes, complements, cardinality restrictions. The
resulted axioms are used to enrich an ontology, in a human supervised manner.
The developed tool is publicly provided as a Protge plugin.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.16699v1},
File          = {2307.16699v1.pdf}
}
@article{2307.16648v2,
Author        = {Hamed Babaei Giglou and Jennifer D'Souza and SÃ¶ren Auer},
Title         = {LLMs4OL: Large Language Models for Ontology Learning},
Eprint        = {2307.16648v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs)
for Ontology Learning (OL). LLMs have shown significant advancements in natural
language processing, demonstrating their ability to capture complex language
patterns in different knowledge domains. Our LLMs4OL paradigm investigates the
following hypothesis: \textit{Can LLMs effectively apply their language pattern
capturing capability to OL, which involves automatically extracting and
structuring knowledge from natural language text?} To test this hypothesis, we
conduct a comprehensive evaluation using the zero-shot prompting method. We
evaluate nine different LLM model families for three main OL tasks: term
typing, taxonomy discovery, and extraction of non-taxonomic relations.
Additionally, the evaluations encompass diverse genres of ontological
knowledge, including lexicosemantic knowledge in WordNet, geographical
knowledge in GeoNames, and medical knowledge in UMLS.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.16648v2},
File          = {2307.16648v2.pdf}
}
@article{2308.00814v1,
Author        = {Govind Krishnan. V},
Title         = {How Bohr's Copenhagen interpretation is realist and solves the
  measurement problem},
Eprint        = {2308.00814v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {physics.hist-ph},
Abstract      = {The field of interpretation of quantum mechanics emerged in an attempt to
solve the measurement problem. This turned on the perception that Niels Bohr
avoided addressing the measurement problem by taking an instrumentalist view of
quantum mechanics. I argue that this view is mistaken and Bohr's interpretation
of quantum mechanics is realist. Moreover, Bohr's interpretation, which is
different from textbook quantum mechanics (which is due more to Von Neumann and
Paul Dirac), succeeds in solving the measurement problem. While the claim that
Bohr dissolves the measurement problem within the limits of the epistemological
framework he assumes has been made by a few authors, rarely has the case been
made that Bohr's project unambiguously and completely overcomes the measurement
problem. I make the strong case that Bohr eliminated the measurement problem
altogether. For this, I put forward two new postulates through which to make
sense of Bohr's interpretation. The article thus seeks to single out Bohr's
interpretation from the various views that go together under the umbrella of
orthodox quantum mechanics, and which have been traditionally considered
susceptible to the measurement problem. It shows that Bohr's interpretation
should be classified along with those like hidden variable theories, collapse
models, modal interpretations etc., which offer a solution to the measurement
problem and are committed to a realist ontology.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2308.00814v1},
File          = {2308.00814v1.pdf}
}
@article{2307.13004v1,
Author        = {Zihao Li and Changkun Jiang and Jianqiang Li},
Title         = {DeepGATGO: A Hierarchical Pretraining-Based Graph-Attention Model for
  Automatic Protein Function Prediction},
Eprint        = {2307.13004v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.QM},
Abstract      = {Automatic protein function prediction (AFP) is classified as a large-scale
multi-label classification problem aimed at automating protein enrichment
analysis to eliminate the current reliance on labor-intensive wet-lab methods.
Currently, popular methods primarily combine protein-related information and
Gene Ontology (GO) terms to generate final functional predictions. For example,
protein sequences, structural information, and protein-protein interaction
networks are integrated as prior knowledge to fuse with GO term embeddings and
generate the ultimate prediction results. However, these methods are limited by
the difficulty in obtaining structural information or network topology
information, as well as the accuracy of such data. Therefore, more and more
methods that only use protein sequences for protein function prediction have
been proposed, which is a more reliable and computationally cheaper approach.
However, the existing methods fail to fully extract feature information from
protein sequences or label data because they do not adequately consider the
intrinsic characteristics of the data itself. Therefore, we propose a
sequence-based hierarchical prediction method, DeepGATGO, which processes
protein sequences and GO term labels hierarchically, and utilizes graph
attention networks (GATs) and contrastive learning for protein function
prediction. Specifically, we compute embeddings of the sequence and label data
using pre-trained models to reduce computational costs and improve the
embedding accuracy. Then, we use GATs to dynamically extract the structural
information of non-Euclidean data, and learn general features of the label
dataset with contrastive learning by constructing positive and negative example
samples. Experimental results demonstrate that our proposed model exhibits
better scalability in GO term enrichment analysis on large-scale datasets.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.13004v1},
File          = {2307.13004v1.pdf}
}
@article{2307.12265v1,
Author        = {Tiziano Dalmonte and Andrea Mazzullo and Ana Ozaki and Nicolas Troquard},
Title         = {Non-Normal Modal Description Logics (Extended Version)},
Eprint        = {2307.12265v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LO},
Abstract      = {Modal logics are widely used in multi-agent systems to reason about actions,
abilities, norms, or epistemic states. Combined with description logic
languages, they are also a powerful tool to formalise modal aspects of
ontology-based reasoning over an object domain. However, the standard
relational semantics for modalities is known to validate principles deemed
problematic in agency, deontic, or epistemic applications. To overcome these
difficulties, weaker systems of so-called non-normal modal logics, equipped
with neighbourhood semantics that generalise the relational one, have been
investigated both at the propositional and at the description logic level. We
present here a family of non-normal modal description logics, obtained by
extending ALC-based languages with non-normal modal operators. For formulas
interpreted on neighbourhood models over varying domains, we provide a modular
framework of terminating, correct, and complete tableau-based satisfiability
checking algorithms in NExpTime. For a subset of these systems, we also
consider a reduction to satisfiability on constant domain relational models.
Moreover, we investigate the satisfiability problem in fragments obtained by
disallowing the application of modal operators to description logic concepts,
providing tight ExpTime complexity results.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.12265v1},
File          = {2307.12265v1.pdf}
}
@article{2307.12148v1,
Author        = {Avijit Lahiri},
Title         = {Quantum Mechanical Reality: Entanglement and Decoherence},
Eprint        = {2307.12148v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {quant-ph},
Abstract      = {We look into the ontology of quantum theory as distinct from that of the
classical theory in the sciences, following a broadly Kantian tradition and
distinguishing between the noumenal and phenomenal realities where the former
is independent of our perception while the latter is assembled from the former
by means of fragmentary bits of interpretation. Within this framework, theories
are conceptual constructs applying to models generated in the phenomenal world
within limited contexts.The ontology of quantum theory principally rests on the
view that entities in the world are pervasively correlated with one another not
by means of probabilities as in the case of the classical theory, but by means
of probability amplitudes involving finely tuned phases of quantum mechanical
states (entanglement). The quantum correlations are shared globally in the
process of environment-induced decoherence whereby locally generated
correlations are removed, the removal being especially manifest in the case of
systems that appear as classical ones, in which case the process is almost
instantaneous, being, in all likelihood, driven by field fluctuations in the
Planck regime. This points to factors of an unknown nature determining its
finest details, since Planck scale physics remains an obscure terrain. In other
words, the present day quantum theory holds within a limited context set by the
Planck scale.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.12148v1},
File          = {2307.12148v1.pdf}
}
@article{2307.12051v1,
Author        = {Georg Gottlob and Marco Manna and Cinzia Marte},
Title         = {Dyadic Existential Rules},
Eprint        = {2307.12051v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CC},
Abstract      = {Existential rules form an expressive Datalog-based language to specify
ontological knowledge. The presence of existential quantification in
rule-heads, however, makes the main reasoning tasks undecidable. To overcome
this limitation, in the last two decades, a number of classes of existential
rules guaranteeing the decidability of query answering have been proposed.
Unfortunately, only some of these classes fully encompass Datalog and, often,
this comes at the price of higher computational complexity. Moreover,
expressive classes are typically unable to exploit tools developed for classes
exhibiting lower expressiveness. To mitigate these shortcomings, this paper
introduces a novel general syntactic condition that allows us to define,
systematically and in a uniform way, from any decidable class $\mathcal{C}$ of
existential rules, a new class called Dyadic-$\mathcal{C}$ enjoying the
following properties: $(i)$ it is decidable; $(ii)$ it generalises Datalog;
$(iii)$ it generalises $\mathcal{C}$; $(iv)$ it can effectively exploit any
reasoner for query answering over $\mathcal{C}$; and $(v)$ its computational
complexity does not exceed the highest between the one of $\mathcal{C}$ and the
one of Datalog. Under consideration in Theory and Practice of Logic Programming
(TPLP).},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.12051v1},
File          = {2307.12051v1.pdf}
}
@article{2307.11282v1,
Author        = {Mordecai Waegell and Kelvin J. McQueen and Emily C. Adlam},
Title         = {The Generative Programs Framework},
Eprint        = {2307.11282v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {physics.hist-ph},
Abstract      = {Recently there has been significant interest in using causal modelling
techniques to understand the structure of physical theories. However, the
notion of `causation' is limiting - insisting that a physical theory must
involve causal structure already places significant constraints on the form
that theory may take. Thus in this paper, we aim to set out a more general
structural framework. We argue that any quantitative physical theory can be
represented in the form of a generative program, i.e. a list of instructions
showing how to generate the empirical data; the information-processing
structure associated with this program can be represented by a directed acyclic
graph (DAG). We suggest that these graphs can be interpreted as encoding
relations of `ontological priority,' and that ontological priority is a
suitable generalisation of causation which applies even to theories that don't
have a natural causal structure. We discuss some applications of our framework
to philosophical questions about realism, operationalism, free will, locality
and fine-tuning.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.11282v1},
File          = {2307.11282v1.pdf}
}
@article{2307.11206v1,
Author        = {Walid S. Saba},
Title         = {Towards Ontologically Grounded and Language-Agnostic Knowledge Graphs},
Eprint        = {2307.11206v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge graphs (KGs) have become the standard technology for the
representation of factual information in applications such as recommendation
engines, search, and question-answering systems. However, the continual
updating of KGs, as well as the integration of KGs from different domains and
KGs in different languages, remains to be a major challenge. What we suggest
here is that by a reification of abstract objects and by acknowledging the
ontological distinction between concepts and types, we arrive at an
ontologically grounded and language-agnostic representation that can alleviate
the difficulties in KG integration.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.11206v1},
File          = {2307.11206v1.pdf}
}
@article{2307.11669v1,
Author        = {Theodorus Maria Nieuwenhuizen},
Title         = {Contra multos verbos: On scandals of quantum mechanics},
Eprint        = {2307.11669v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {quant-ph},
Abstract      = {In 2008 Nico van Kampen wrote in his letter {\it The scandal of quantum
mechanics}: ``The scandal is that there are still many articles, discussions
and textbooks, which advertise various interpretations and philosophical
profundities." Not much has changed since then, while social media have given a
platform for more of what Nico would term ``a scandal''. A detailed viewpoint
is presented on the status of quantum mechanics, distilled from two decades of
work with Armen Allahverdyan and Roger Balian on the dynamical solution of
Curie-Weiss models for quantum measurement. It embodies a certain minimal form
of the statistical interpretation and stays clear of ontological connections.
Along the way, comments on various related subjects, terms and interpretations
are given.},
Year          = {2023},
Month         = {Jul},
Note          = {The Quantum-Like Revolution: A Festschrift for Andrei Khrennikov,
  91-124 (2023)},
Url           = {http://arxiv.org/abs/2307.11669v1},
File          = {2307.11669v1.pdf}
}
@article{2307.10778v1,
Author        = {Jens-Joris Decorte and Severine Verlinden and Jeroen Van Hautte and Johannes Deleu and Chris Develder and Thomas Demeester},
Title         = {Extreme Multi-Label Skill Extraction Training using Large Language
  Models},
Eprint        = {2307.10778v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Online job ads serve as a valuable source of information for skill
requirements, playing a crucial role in labor market analysis and e-recruitment
processes. Since such ads are typically formatted in free text, natural
language processing (NLP) technologies are required to automatically process
them. We specifically focus on the task of detecting skills (mentioned
literally, or implicitly described) and linking them to a large skill ontology,
making it a challenging case of extreme multi-label classification (XMLC).
Given that there is no sizable labeled (training) dataset are available for
this specific XMLC task, we propose techniques to leverage general Large
Language Models (LLMs). We describe a cost-effective approach to generate an
accurate, fully synthetic labeled dataset for skill extraction, and present a
contrastive learning strategy that proves effective in the task. Our results
across three skill extraction benchmarks show a consistent increase of between
15 to 25 percentage points in \textit{R-Precision@5} compared to previously
published results that relied solely on distant supervision through literal
matches.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.10778v1},
File          = {2307.10778v1.pdf}
}
@article{2307.10702v1,
Author        = {Ngoc Luyen Le and Marie-HÃ©lÃ¨ne Abel and Philippe Gouspillou},
Title         = {A Constraint-based Recommender System via RDF Knowledge Graphs},
Eprint        = {2307.10702v1},
DOI           = {10.1109/CSCWD57460.2023.10152701},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Knowledge graphs, represented in RDF, are able to model entities and their
relations by means of ontologies. The use of knowledge graphs for information
modeling has attracted interest in recent years. In recommender systems, items
and users can be mapped and integrated into the knowledge graph, which can
represent more links and relationships between users and items.
Constraint-based recommender systems are based on the idea of explicitly
exploiting deep recommendation knowledge through constraints to identify
relevant recommendations. When combined with knowledge graphs, a
constraint-based recommender system gains several benefits in terms of
constraint sets. In this paper, we investigate and propose the construction of
a constraint-based recommender system via RDF knowledge graphs applied to the
vehicle purchase/sale domain. The results of our experiments show that the
proposed approach is able to efficiently identify recommendations in accordance
with user preferences.},
Year          = {2023},
Month         = {Jul},
Note          = {The 26th International Conference on Computer Supported
  Cooperative Work in Design (CSCWD 2023 ), May 2023, Rio de Janeiro, Brazil.
  pp.849-854},
Url           = {http://arxiv.org/abs/2307.10702v1},
File          = {2307.10702v1.pdf}
}
@article{2307.10680v1,
Author        = {Ngoc Luyen Le and Marie-HÃ©lÃ¨ne Abel and Philippe Gouspillou},
Title         = {A Personalized Recommender System Based-on Knowledge Graph Embeddings},
Eprint        = {2307.10680v1},
DOI           = {10.1007/978-3-031-27762-7_35},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge graphs have proven to be effective for modeling entities and their
relationships through the use of ontologies. The recent emergence in interest
for using knowledge graphs as a form of information modeling has led to their
increased adoption in recommender systems. By incorporating users and items
into the knowledge graph, these systems can better capture the implicit
connections between them and provide more accurate recommendations. In this
paper, we investigate and propose the construction of a personalized
recommender system via knowledge graphs embedding applied to the vehicle
purchase/sale domain. The results of our experimentation demonstrate the
efficacy of the proposed method in providing relevant recommendations that are
consistent with individual users.},
Year          = {2023},
Month         = {Jul},
Note          = {The International Conference on Artificial Intelligence and
  Computer Vision (AICV2023), Mar 2023, Marrakesh, Morocco. pp.368-378},
Url           = {http://arxiv.org/abs/2307.10680v1},
File          = {2307.10680v1.pdf}
}
@article{2307.10577v3,
Author        = {Hugo Latapie and Shan Yu and Patrick Hammer and Kristinn R. Thorisson and Vahagn Petrosyan and Brandon Kynoch and Alind Khare and Payman Behnam and Alexey Tumanov and Aksheit Saxena and Anish Aralikatti and Hanning Chen and Mohsen Imani and Mike Archbold and Tangrui Li and Pei Wang and Justin Hart},
Title         = {Ethosight: A Reasoning-Guided Iterative Learning System for Nuanced
  Perception based on Joint-Embedding & Contextual Label Affinity},
Eprint        = {2307.10577v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Traditional computer vision models often necessitate extensive data
acquisition, annotation, and validation. These models frequently struggle in
real-world applications, resulting in high false positive and negative rates,
and exhibit poor adaptability to new scenarios, often requiring costly
retraining. To address these issues, we present Ethosight, a flexible and
adaptable zero-shot video analytics system. Ethosight begins from a clean slate
based on user-defined video analytics, specified through natural language or
keywords, and leverages joint embedding models and reasoning mechanisms
informed by ontologies such as WordNet and ConceptNet. Ethosight operates
effectively on low-cost edge devices and supports enhanced runtime adaptation,
thereby offering a new approach to continuous learning without catastrophic
forgetting. We provide empirical validation of Ethosight's promising
effectiveness across diverse and complex use cases, while highlighting areas
for further improvement. A significant contribution of this work is the release
of all source code and datasets to enable full reproducibility and to foster
further innovation in both the research and commercial domains.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.10577v3},
File          = {2307.10577v3.pdf}
}
@article{2307.09942v1,
Author        = {Brandon Theodorou and Cao Xiao and Jimeng Sun},
Title         = {TREEMENT: Interpretable Patient-Trial Matching via Personalized Dynamic
  Tree-Based Memory Network},
Eprint        = {2307.09942v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Clinical trials are critical for drug development but often suffer from
expensive and inefficient patient recruitment. In recent years, machine
learning models have been proposed for speeding up patient recruitment via
automatically matching patients with clinical trials based on longitudinal
patient electronic health records (EHR) data and eligibility criteria of
clinical trials. However, they either depend on trial-specific expert rules
that cannot expand to other trials or perform matching at a very general level
with a black-box model where the lack of interpretability makes the model
results difficult to be adopted.
  To provide accurate and interpretable patient trial matching, we introduce a
personalized dynamic tree-based memory network model named TREEMENT. It
utilizes hierarchical clinical ontologies to expand the personalized patient
representation learned from sequential EHR data, and then uses an attentional
beam-search query learned from eligibility criteria embedding to offer a
granular level of alignment for improved performance and interpretability. We
evaluated TREEMENT against existing models on real-world datasets and
demonstrated that TREEMENT outperforms the best baseline by 7% in terms of
error reduction in criteria-level matching and achieves state-of-the-art
results in its trial-level matching ability. Furthermore, we also show TREEMENT
can offer good interpretability to make the model results easier for adoption.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.09942v1},
File          = {2307.09942v1.pdf}
}
@article{2307.08958v1,
Author        = {Jennifer L. Clarke and Laurel D. Cooper and Monica F. Poelchau and Tanya Z. Berardini and Justin Elser and Andrew D. Farmer and Stephen Ficklin and Sunita Kumari and Marie-AngÃ©lique Laporte and Rex T. Nelson and Rie Sadohara and Peter Selby and Anne E. Thessen and Brandon Whitehead and Taner Z. Sen},
Title         = {Data sharing and ontology use among agricultural genetics, genomics, and
  breeding databases and resources of the AgBioData Consortium},
Eprint        = {2307.08958v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Over the last several decades, there has been rapid growth in the number and
scope of agricultural genetics, genomics and breeding (GGB) databases and
resources. The AgBioData Consortium (https://www.agbiodata.org/) currently
represents 44 databases and resources covering model or crop plant and animal
GGB data, ontologies, pathways, genetic variation and breeding platforms
(referred to as 'databases' throughout). One of the goals of the Consortium is
to facilitate FAIR (Findable, Accessible, Interoperable, and Reusable) data
management and the integration of datasets which requires data sharing, along
with structured vocabularies and/or ontologies. Two AgBioData working groups,
focused on Data Sharing and Ontologies, conducted a survey to assess the status
and future needs of the members in those areas. A total of 33 researchers
responded to the survey, representing 37 databases. Results suggest that data
sharing practices by AgBioData databases are in a healthy state, but it is not
clear whether this is true for all metadata and data types across all
databases; and that ontology use has not substantially changed since a similar
survey was conducted in 2017. We recommend 1) providing training for database
personnel in specific data sharing techniques, as well as in ontology use; 2)
further study on what metadata is shared, and how well it is shared among
databases; 3) promoting an understanding of data sharing and ontologies in the
stakeholder community; 4) improving data sharing and ontologies for specific
phenotypic data types and formats; and 5) lowering specific barriers to data
sharing and ontology use, by identifying sustainability solutions, and the
identification, promotion, or development of data standards. Combined, these
improvements are likely to help AgBioData databases increase development
efforts towards improved ontology use, and data sharing via programmatic means.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.08958v1},
File          = {2307.08958v1.pdf}
}
@article{2307.11769v1,
Author        = {Yun Tang and Antonio A. Bruto da Costa and Jason Zhang and Irvine Patrick and Siddartha Khastgir and Paul Jennings},
Title         = {Domain Knowledge Distillation from Large Language Model: An Empirical
  Study in the Autonomous Driving Domain},
Eprint        = {2307.11769v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Engineering knowledge-based (or expert) systems require extensive manual
effort and domain knowledge. As Large Language Models (LLMs) are trained using
an enormous amount of cross-domain knowledge, it becomes possible to automate
such engineering processes. This paper presents an empirical automation and
semi-automation framework for domain knowledge distillation using prompt
engineering and the LLM ChatGPT. We assess the framework empirically in the
autonomous driving domain and present our key observations. In our
implementation, we construct the domain knowledge ontology by "chatting" with
ChatGPT. The key finding is that while fully automated domain ontology
construction is possible, human supervision and early intervention typically
improve efficiency and output quality as they lessen the effects of response
randomness and the butterfly effect. We, therefore, also develop a web-based
distillation assistant enabling supervision and flexible intervention at
runtime. We hope our findings and tools could inspire future research toward
revolutionizing the engineering of knowledge-based systems across application
domains.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.11769v1},
File          = {2307.11769v1.pdf}
}
@article{2307.09605v1,
Author        = {Lars Vogt and Marcel Konrad and Manuel Prinz},
Title         = {Towards a Rosetta Stone for (meta)data: Learning from natural language
  to improve semantic and cognitive interoperability},
Eprint        = {2307.09605v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {In order to effectively manage the overwhelming influx of data, it is crucial
to ensure that data is findable, accessible, interoperable, and reusable
(FAIR). While ontologies and knowledge graphs have been employed to enhance
FAIRness, challenges remain regarding semantic and cognitive interoperability.
We explore how English facilitates reliable communication of terms and
statements, and transfer our findings to a framework of ontologies and
knowledge graphs, while treating terms and statements as minimal information
units. We categorize statement types based on their predicates, recognizing the
limitations of modeling non-binary predicates with multiple triples, which
negatively impacts interoperability. Terms are associated with different frames
of reference, and different operations require different schemata. Term
mappings and schema crosswalks are therefore vital for semantic
interoperability. We propose a machine-actionable Rosetta Stone Framework for
(meta)data, which uses reference terms and schemata as an interlingua to
minimize mappings and crosswalks. Modeling statements rather than a
human-independent reality ensures cognitive familiarity and thus better
interoperability of data structures. We extend this Rosetta modeling paradigm
to reference schemata, resulting in simple schemata with a consistent structure
across statement types, empowering domain experts to create their own schemata
using the Rosetta Editor, without requiring knowledge of semantics. The Editor
also allows specifying textual and graphical display templates for each schema,
delivering human-readable data representations alongside machine-actionable
data structures. The Rosetta Query Builder derives queries based on completed
input forms and the information from corresponding reference schemata. This
work sets the conceptual ground for the Rosetta Stone Framework that we plan to
develop in the future.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.09605v1},
File          = {2307.09605v1.pdf}
}
@article{2307.06975v2,
Author        = {Luigi Capogrosso and Alessio Mascolini and Federico Girella and Geri Skenderi and Sebastiano Gaiardelli and Nicola Dall'Ora and Francesco Ponzio and Enrico Fraccaroli and Santa Di Cataldo and Sara Vinco and Enrico Macii and Franco Fummi and Marco Cristani},
Title         = {Neuro-symbolic Empowered Denoising Diffusion Probabilistic Models for
  Real-time Anomaly Detection in Industry 4.0},
Eprint        = {2307.06975v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Industry 4.0 involves the integration of digital technologies, such as IoT,
Big Data, and AI, into manufacturing and industrial processes to increase
efficiency and productivity. As these technologies become more interconnected
and interdependent, Industry 4.0 systems become more complex, which brings the
difficulty of identifying and stopping anomalies that may cause disturbances in
the manufacturing process. This paper aims to propose a diffusion-based model
for real-time anomaly prediction in Industry 4.0 processes. Using a
neuro-symbolic approach, we integrate industrial ontologies in the model,
thereby adding formal knowledge on smart manufacturing. Finally, we propose a
simple yet effective way of distilling diffusion models through Random Fourier
Features for deployment on an embedded system for direct integration into the
manufacturing process. To the best of our knowledge, this approach has never
been explored before.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.06975v2},
File          = {2307.06975v2.pdf}
}
@article{2307.05727v2,
Author        = {Tiffany J. Callahan and Ignacio J. Tripodi and Adrianne L. Stefanski and Luca Cappelletti and Sanya B. Taneja and Jordan M. Wyrwa and Elena Casiraghi and Nicolas A. Matentzoglu and Justin Reese and Jonathan C. Silverstein and Charles Tapley Hoyt and Richard D. Boyce and Scott A. Malec and Deepak R. Unni and Marcin P. Joachimiak and Peter N. Robinson and Christopher J. Mungall and Emanuele Cavalleri and Tommaso Fontana and Giorgio Valentini and Marco Mesiti and Lucas A. Gillenwater and Brook Santangelo and Nicole A. Vasilevsky and Robert Hoehndorf and Tellen D. Bennett and Patrick B. Ryan and George Hripcsak and Michael G. Kahn and Michael Bada and William A. Baumgartner Jr and Lawrence E. Hunter},
Title         = {An Open-Source Knowledge Graph Ecosystem for the Life Sciences},
Eprint        = {2307.05727v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Translational research requires data at multiple scales of biological
organization. Advancements in sequencing and multi-omics technologies have
increased the availability of these data, but researchers face significant
integration challenges. Knowledge graphs (KGs) are used to model complex
phenomena, and methods exist to construct them automatically. However, tackling
complex biomedical integration problems requires flexibility in the way
knowledge is modeled. Moreover, existing KG construction methods provide robust
tooling at the cost of fixed or limited choices among knowledge representation
models. PheKnowLator (Phenotype Knowledge Translator) is a semantic ecosystem
for automating the FAIR (Findable, Accessible, Interoperable, and Reusable)
construction of ontologically grounded KGs with fully customizable knowledge
representation. The ecosystem includes KG construction resources (e.g., data
preparation APIs), analysis tools (e.g., SPARQL endpoints and abstraction
algorithms), and benchmarks (e.g., prebuilt KGs and embeddings). We evaluated
the ecosystem by systematically comparing it to existing open-source KG
construction methods and by analyzing its computational performance when used
to construct 12 large-scale KGs. With flexible knowledge representation,
PheKnowLator enables fully customizable KGs without compromising performance or
usability.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.05727v2},
File          = {2307.05727v2.pdf}
}
@article{2307.05096v1,
Author        = {Konstantia Zarkogianni and Edmund Dervakos and George Filandrianos and Theofanis Ganitidis and Vasiliki Gkatzou and Aikaterini Sakagianni and Raghu Raghavendra and C. L. Max Nikias and Giorgos Stamou and Konstantina S. Nikita},
Title         = {The smarty4covid dataset and knowledge base: a framework enabling
  interpretable analysis of audio signals},
Eprint        = {2307.05096v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SD},
Abstract      = {Harnessing the power of Artificial Intelligence (AI) and m-health towards
detecting new bio-markers indicative of the onset and progress of respiratory
abnormalities/conditions has greatly attracted the scientific and research
interest especially during COVID-19 pandemic. The smarty4covid dataset contains
audio signals of cough (4,676), regular breathing (4,665), deep breathing
(4,695) and voice (4,291) as recorded by means of mobile devices following a
crowd-sourcing approach. Other self reported information is also included (e.g.
COVID-19 virus tests), thus providing a comprehensive dataset for the
development of COVID-19 risk detection models. The smarty4covid dataset is
released in the form of a web-ontology language (OWL) knowledge base enabling
data consolidation from other relevant datasets, complex queries and reasoning.
It has been utilized towards the development of models able to: (i) extract
clinically informative respiratory indicators from regular breathing records,
and (ii) identify cough, breath and voice segments in crowd-sourced audio
recordings. A new framework utilizing the smarty4covid OWL knowledge base
towards generating counterfactual explanations in opaque AI-based COVID-19 risk
detection models is proposed and validated.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.05096v1},
File          = {2307.05096v1.pdf}
}
@article{2307.05082v1,
Author        = {Oleksandr Palagin and Vladislav Kaverinskiy and Anna Litvin and Kyrylo Malakhov},
Title         = {OntoChatGPT Information System: Ontology-Driven Structured Prompts for
  ChatGPT Meta-Learning},
Eprint        = {2307.05082v1},
DOI           = {10.47839/ijc.22.2.3086},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This research presents a comprehensive methodology for utilizing an
ontology-driven structured prompts system in interplay with ChatGPT, a widely
used large language model (LLM). The study develops formal models, both
information and functional, and establishes the methodological foundations for
integrating ontology-driven prompts with ChatGPT's meta-learning capabilities.
The resulting productive triad comprises the methodological foundations,
advanced information technology, and the OntoChatGPT system, which collectively
enhance the effectiveness and performance of chatbot systems. The
implementation of this technology is demonstrated using the Ukrainian language
within the domain of rehabilitation. By applying the proposed methodology, the
OntoChatGPT system effectively extracts entities from contexts, classifies
them, and generates relevant responses. The study highlights the versatility of
the methodology, emphasizing its applicability not only to ChatGPT but also to
other chatbot systems based on LLMs, such as Google's Bard utilizing the PaLM 2
LLM. The underlying principles of meta-learning, structured prompts, and
ontology-driven information retrieval form the core of the proposed
methodology, enabling their adaptation and utilization in various LLM-based
systems. This versatile approach opens up new possibilities for NLP and
dialogue systems, empowering developers to enhance the performance and
functionality of chatbot systems across different domains and languages.},
Year          = {2023},
Month         = {Jul},
Note          = {International Journal of Computing (2023), 22(2), 170-183},
Url           = {http://arxiv.org/abs/2307.05082v1},
File          = {2307.05082v1.pdf}
}
@article{2307.04772v1,
Author        = {Logan Nye},
Title         = {Digital Twins for Patient Care via Knowledge Graphs and Closed-Form
  Continuous-Time Liquid Neural Networks},
Eprint        = {2307.04772v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Digital twin technology has is anticipated to transform healthcare, enabling
personalized medicines and support, earlier diagnoses, simulated treatment
outcomes, and optimized surgical plans. Digital twins are readily gaining
traction in industries like manufacturing, supply chain logistics, and civil
infrastructure. Not in patient care, however. The challenge of modeling complex
diseases with multimodal patient data and the computational complexities of
analyzing it have stifled digital twin adoption in the biomedical vertical.
Yet, these major obstacles can potentially be handled by approaching these
models in a different way. This paper proposes a novel framework for addressing
the barriers to clinical twin modeling created by computational costs and
modeling complexities. We propose structuring patient health data as a
knowledge graph and using closed-form continuous-time liquid neural networks,
for real-time analytics. By synthesizing multimodal patient data and leveraging
the flexibility and efficiency of closed form continuous time networks and
knowledge graph ontologies, our approach enables real time insights,
personalized medicine, early diagnosis and intervention, and optimal surgical
planning. This novel approach provides a comprehensive and adaptable view of
patient health along with real-time analytics, paving the way for digital twin
simulations and other anticipated benefits in healthcare.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.04772v1},
File          = {2307.04772v1.pdf}
}
@article{2307.03067v2,
Author        = {Yuan He and Jiaoyan Chen and Hang Dong and Ian Horrocks and Carlo Allocca and Taehun Kim and Brahmananda Sapkota},
Title         = {DeepOnto: A Python Package for Ontology Engineering with Deep Learning},
Eprint        = {2307.03067v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Integrating deep learning techniques, particularly language models (LMs),
with knowledge representation techniques like ontologies has raised widespread
attention, urging the need of a platform that supports both paradigms. Although
packages such as OWL API and Jena offer robust support for basic ontology
processing features, they lack the capability to transform various types of
information within ontologies into formats suitable for downstream deep
learning-based applications. Moreover, widely-used ontology APIs are primarily
Java-based while deep learning frameworks like PyTorch and Tensorflow are
mainly for Python programming. To address the needs, we present DeepOnto, a
Python package designed for ontology engineering with deep learning. The
package encompasses a core ontology processing module founded on the
widely-recognised and reliable OWL API, encapsulating its fundamental features
in a more "Pythonic" manner and extending its capabilities to incorporate other
essential components including reasoning, verbalisation, normalisation,
taxonomy, projection, and more. Building on this module, DeepOnto offers a
suite of tools, resources, and algorithms that support various ontology
engineering tasks, such as ontology alignment and completion, by harnessing
deep learning methods, primarily pre-trained LMs. In this paper, we also
demonstrate the practical utility of DeepOnto through two use-cases: the
Digital Health Coaching in Samsung Research UK and the Bio-ML track of the
Ontology Alignment Evaluation Initiative (OAEI).},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.03067v2},
File          = {2307.03067v2.pdf}
}
@article{2307.01933v1,
Author        = {Zijie Huang and Daheng Wang and Binxuan Huang and Chenwei Zhang and Jingbo Shang and Yan Liang and Zhengyang Wang and Xian Li and Christos Faloutsos and Yizhou Sun and Wei Wang},
Title         = {Concept2Box: Joint Geometric Embeddings for Learning Two-View Knowledge
  Graphs},
Eprint        = {2307.01933v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge graph embeddings (KGE) have been extensively studied to embed
large-scale relational data for many real-world applications. Existing methods
have long ignored the fact many KGs contain two fundamentally different views:
high-level ontology-view concepts and fine-grained instance-view entities. They
usually embed all nodes as vectors in one latent space. However, a single
geometric representation fails to capture the structural differences between
two views and lacks probabilistic semantics towards concepts' granularity. We
propose Concept2Box, a novel approach that jointly embeds the two views of a KG
using dual geometric representations. We model concepts with box embeddings,
which learn the hierarchy structure and complex relations such as overlap and
disjoint among them. Box volumes can be interpreted as concepts' granularity.
Different from concepts, we model entities as vectors. To bridge the gap
between concept box embeddings and entity vector embeddings, we propose a novel
vector-to-box distance metric and learn both embeddings jointly. Experiments on
both the public DBpedia KG and a newly-created industrial KG showed the
effectiveness of Concept2Box.},
Year          = {2023},
Month         = {Jul},
Note          = {ACL 2023},
Url           = {http://arxiv.org/abs/2307.01933v1},
File          = {2307.01933v1.pdf}
}
@article{2307.00827v2,
Author        = {Luis Miguel Vieira da Silva and Aljosha KÃ¶cher and Milapji Singh Gill and Marco Weiss and Alexander Fay},
Title         = {Toward a Mapping of Capability and Skill Models using Asset
  Administration Shells and Ontologies},
Eprint        = {2307.00827v2},
DOI           = {10.1109/ETFA54631.2023.10275459},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {In order to react efficiently to changes in production, resources and their
functions must be integrated into plants in accordance with the plug and
produce principle. In this context, research on so-called capabilities and
skills has shown promise. However, there are currently two incompatible
approaches to modeling capabilities and skills. On the one hand, formal
descriptions using ontologies have been developed. On the other hand, there are
efforts to standardize submodels of the Asset Administration Shell (AAS) for
this purpose. In this paper, we present ongoing research to connect these two
incompatible modeling approaches. Both models are analyzed to identify
comparable as well as dissimilar model elements. Subsequently, we present a
concept for a bidirectional mapping between AAS submodels and a capability and
skill ontology. For this purpose, two unidirectional, declarative mappings are
applied that implement transformations from one modeling approach to the other
- and vice versa.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.00827v2},
File          = {2307.00827v2.pdf}
}
@article{2307.00793v1,
Author        = {Xiaoyu Jin and Chun Fu and Hussain Kazmi and Atilla Balint and Ada Canaydin and Matias Quintana and Filip Biljecki and Fu Xiao and Clayton Miller},
Title         = {The Building Data Genome Directory -- An open, comprehensive data
  sharing platform for building performance research},
Eprint        = {2307.00793v1},
DOI           = {10.1088/1742-6596/2600/3/032003},
ArchivePrefix = {arXiv},
PrimaryClass  = {stat.AP},
Abstract      = {The building sector plays a crucial role in the worldwide decarbonization
effort, accounting for significant portions of energy consumption and
environmental effects. However, the scarcity of open data sources is a
continuous challenge for built environment researchers and practitioners.
Although several efforts have been made to consolidate existing open datasets,
no database currently offers a comprehensive collection of building data types
with all subcategories and time granularities (e.g., year, month, and
sub-hour). This paper presents the Building Data Genome Directory, an open
data-sharing platform serving as a one-stop shop for the data necessary for
vital categories of building energy research. The data directory is an online
portal (http://buildingdatadirectory.org/) that allows filtering and
discovering valuable datasets. The directory covers meter, building-level, and
aggregated community-level data at the spatial scale and year-to-minute level
at the temporal scale. The datasets were consolidated from a comprehensive
exploration of sources, including governments, research institutes, and online
energy dashboards. The results of this effort include the aggregation of 60
datasets pertaining to building energy ontologies, building energy models,
building energy and water data, electric vehicle data, weather data, building
information data, text-mining-based research data, image data of buildings,
fault detection diagnosis data and occupant data. A crowdsourcing mechanism in
the platform allows users to submit datasets they suggest for inclusion by
filling out an online form. This directory can fuel research and applications
on building energy efficiency, which is an essential step toward addressing the
world's energy and environmental challenges.},
Year          = {2023},
Month         = {Jul},
Note          = {J Phys Conf Ser. 2023;2600: 032003},
Url           = {http://arxiv.org/abs/2307.00793v1},
File          = {2307.00793v1.pdf}
}
@article{2307.07517v1,
Author        = {Riichiro Mizoguchi},
Title         = {Causing is Achieving -- A solution to the problem of causation},
Eprint        = {2307.07517v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {From the standpoint of applied ontology, the problem of understanding and
modeling causation has been recently challenged on the premise that causation
is real. As a consequence, the following three results were obtained: (1)
causation can be understood via the notion of systemic function; (2) any cause
can be decomposed using only four subfunctions, namely Achieves, Prevents,
Allows, and Disallows; and (3) the last three subfunctions can be defined in
terms of Achieves alone. It follows that the essence of causation lies in a
single function, namely Achieves. It remains to elucidate the nature of the
Achieves function, which has been elaborated only partially in the previous
work. In this paper, we first discuss a couple of underlying policies in the
above-mentioned causal theory since these are useful in the discussion, then
summarize the results obtained in the former paper, and finally reveal the
nature of Achieves giving a complete solution to the problem of what causation
is.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.07517v1},
File          = {2307.07517v1.pdf}
}
@article{2306.17514v1,
Author        = {Giampaolo Bella and Gianpietro Castiglione and Daniele Francesco Santamaria},
Title         = {A behaviouristic approach to representing processes and procedures in
  the OASIS 2 ontology},
Eprint        = {2306.17514v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Foundational ontologies devoted to the effective representation of processes
and procedures are not widely investigated at present, thereby limiting the
practical adoption of semantic approaches in real scenarios where the precise
instructions to follow must be considered. Also, the representation ought to
include how agents should carry out the actions associated with the process,
whether or not agents are able to perform those actions, the possible roles
played as well as the related events.
  The OASIS ontology provides an established model to capture agents and their
interactions but lacks means for representing processes and procedures carried
out by agents. This motivates the research presented in this article, which
delivers an extension of the OASIS 2 ontology to combine the capabilities for
representing agents and their behaviours with the full conceptualization of
processes and procedures. The overarching goal is to deliver a foundational OWL
ontology that deals with agent planning, reaching a balance between generality
and applicability, which is known to be an open challenge.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.17514v1},
File          = {2306.17514v1.pdf}
}
@article{2306.17494v2,
Author        = {Gianpietro Castiglione and Daniele Francesco Santamaria and Giampaolo Bella},
Title         = {An Ontological Approach to Compliance Verification of the NIS 2
  Directive},
Eprint        = {2306.17494v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {Cybersecurity, which notoriously concerns both human and technological
aspects, is becoming more and more regulated by a number of textual documents
spanning several pages, such as the European GDPR Regulation and the NIS
Directive. This paper introduces an approach that leverages techniques of
semantic representation and reasoning, hence an ontological approach, towards
the compliance check with the security measures that textual documents
prescribe. We choose the ontology instrument to achieve two fundamental
objectives: domain modelling and resource interrogation. The formalisation of
entities and relations from the directive, and the consequent improved
structuring with respect to sheer prose is dramatically helpful for any
organisation through the hard task of compliance verification. The semantic
approach is demonstrated with two articles of the new European NIS 2 directive.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.17494v2},
File          = {2306.17494v2.pdf}
}
@article{2307.01211v1,
Author        = {Giampaolo Bella and Gianpietro Castiglione and Daniele Francesco Santamaria},
Title         = {An automated method for the ontological representation of security
  directives},
Eprint        = {2307.01211v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large documents written in juridical language are difficult to interpret,
with long sentences leading to intricate and intertwined relations between the
nouns. The present paper frames this problem in the context of recent European
security directives. The complexity of their language is here thwarted by
automating the extraction of the relevant information, namely of the parts of
speech from each clause, through a specific tailoring of Natural Language
Processing (NLP) techniques. These contribute, in combination with ontology
development principles, to the design of our automated method for the
representation of security directives as ontologies. The method is showcased on
a practical problem, namely to derive an ontology representing the NIS 2
directive, which is the peak of cybersecurity prescripts at the European level.
Although the NLP techniques adopted showed some limitations and had to be
complemented by manual analysis, the overall results provide valid support for
directive compliance in general and for ontology development in particular.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2307.01211v1},
File          = {2307.01211v1.pdf}
}
@article{2306.14704v3,
Author        = {Hang Dong and Jiaoyan Chen and Yuan He and Ian Horrocks},
Title         = {Ontology Enrichment from Texts: A Biomedical Dataset for Concept
  Discovery and Placement},
Eprint        = {2306.14704v3},
DOI           = {10.1145/3583780.3615126},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Mentions of new concepts appear regularly in texts and require automated
approaches to harvest and place them into Knowledge Bases (KB), e.g.,
ontologies and taxonomies. Existing datasets suffer from three issues, (i)
mostly assuming that a new concept is pre-discovered and cannot support
out-of-KB mention discovery; (ii) only using the concept label as the input
along with the KB and thus lacking the contexts of a concept label; and (iii)
mostly focusing on concept placement w.r.t a taxonomy of atomic concepts,
instead of complex concepts, i.e., with logical operators. To address these
issues, we propose a new benchmark, adapting MedMentions dataset (PubMed
abstracts) with SNOMED CT versions in 2014 and 2017 under the Diseases
sub-category and the broader categories of Clinical finding, Procedure, and
Pharmaceutical / biologic product. We provide usage on the evaluation with the
dataset for out-of-KB mention discovery and concept placement, adapting recent
Large Language Model based methods.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.14704v3},
File          = {2306.14704v3.pdf}
}
@article{2306.13922v1,
Author        = {Aviv Weinstein and Yoav Goldberg},
Title         = {Unsupervised Mapping of Arguments of Deverbal Nouns to Their
  Corresponding Verbal Labels},
Eprint        = {2306.13922v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Deverbal nouns are nominal forms of verbs commonly used in written English
texts to describe events or actions, as well as their arguments. However, many
NLP systems, and in particular pattern-based ones, neglect to handle such
nominalized constructions. The solutions that do exist for handling arguments
of nominalized constructions are based on semantic annotation and require
semantic ontologies, making their applications restricted to a small set of
nouns. We propose to adopt instead a more syntactic approach, which maps the
arguments of deverbal nouns to the universal-dependency relations of the
corresponding verbal construction. We present an unsupervised mechanism --
based on contextualized word representations -- which allows to enrich
universal-dependency trees with dependency arcs denoting arguments of deverbal
nouns, using the same labels as the corresponding verbal cases. By sharing the
same label set as in the verbal case, patterns that were developed for verbs
can be applied without modification but with high accuracy also to the nominal
constructions.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.13922v1},
File          = {2306.13922v1.pdf}
}
@article{2306.13833v1,
Author        = {Sabah Al-Fedaghi},
Title         = {In Pursuit of Unification of Conceptual Models: Sets as Machines},
Eprint        = {2306.13833v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {Conceptual models as representations of real-world systems are based on
diverse techniques in various disciplines but lack a framework that provides
multidisciplinary ontological understanding of real-world phenomena.
Concurrently, systems complexity has intensified, leading to a rise in
developing models using different formalisms and diverse representations even
within a single domain. Conceptual models have become larger; languages tend to
acquire more features, and it is not unusual to use different modeling
languages for different components. This diversity has caused problems with
consistency between models and incompatibly with designed systems. Two main
solutions have been adopted over the last few years: (1) A currently dominant
technology-based solution tries to harmonize or unify models, e.g., unifies EER
and UML. This solution would solidify modeling achievements, reaping benefits
from huge investments over the last thirty years. (2) A less prevalent solution
is to pursuit deeper roots that reveal unifying modeling principles and
apparatuses. An example of the second method is a category theory-based
approach that utilizes the strengths of the graph and set theory, along with
other topological tools. This manuscript is a sequel in a research venture that
belongs to the second approach and uses a model called thinging machines (TMs)
founded on Stoic ontology and Lupascian logic. TM modeling contests the thesis
that there is no universal approach that covers all aspects of an application,
and the paper demonstrates that pursuing such universality is anything but a
dead-end method. This paper continues in this direction, with emphasis on TM
foundation (e.g., existence and subsistence of things) and exemplifies this
pursuit by proposing an alternative representation of set theory.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.13833v1},
File          = {2306.13833v1.pdf}
}
@article{2306.11892v1,
Author        = {Saed Rezayi and Zhengliang Liu and Zihao Wu and Chandra Dhakal and Bao Ge and Haixing Dai and Gengchen Mai and Ninghao Liu and Chen Zhen and Tianming Liu and Sheng Li},
Title         = {Exploring New Frontiers in Agricultural NLP: Investigating the Potential
  of Large Language Models for Food Applications},
Eprint        = {2306.11892v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper explores new frontiers in agricultural natural language processing
by investigating the effectiveness of using food-related text corpora for
pretraining transformer-based language models. In particular, we focus on the
task of semantic matching, which involves establishing mappings between food
descriptions and nutrition data. To accomplish this, we fine-tune a pre-trained
transformer-based language model, AgriBERT, on this task, utilizing an external
source of knowledge, such as the FoodOn ontology. To advance the field of
agricultural NLP, we propose two new avenues of exploration: (1) utilizing
GPT-based models as a baseline and (2) leveraging ChatGPT as an external source
of knowledge. ChatGPT has shown to be a strong baseline in many NLP tasks, and
we believe it has the potential to improve our model in the task of semantic
matching and enhance our model's understanding of food-related concepts and
relationships. Additionally, we experiment with other applications, such as
cuisine prediction based on food ingredients, and expand the scope of our
research to include other NLP tasks beyond semantic matching. Overall, this
paper provides promising avenues for future research in this field, with
potential implications for improving the performance of agricultural NLP
applications.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.11892v1},
File          = {2306.11892v1.pdf}
}
@article{2306.10723v2,
Author        = {Teodoro Baldazzi and Luigi Bellomarini and Stefano Ceri and Andrea Colombo and Andrea Gentili and Emanuel Sallinger},
Title         = {Fine-tuning Large Enterprise Language Models via Ontological Reasoning},
Eprint        = {2306.10723v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) exploit fine-tuning as a technique to adapt to
diverse goals, thanks to task-specific training data. Task specificity should
go hand in hand with domain orientation, that is, the specialization of an LLM
to accurately address the tasks of a given realm of interest. However, models
are usually fine-tuned over publicly available data or, at most, over ground
data from databases, ignoring business-level definitions and domain experience.
On the other hand, Enterprise Knowledge Graphs (EKGs) are able to capture and
augment such domain knowledge via ontological reasoning. With the goal of
combining LLM flexibility with the domain orientation of EKGs, we propose a
novel neurosymbolic architecture that leverages the power of ontological
reasoning to build task- and domain-specific corpora for LLM fine-tuning.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.10723v2},
File          = {2306.10723v2.pdf}
}
@article{2306.17175v2,
Author        = {Rakhilya Lee Mekhtieva and Brandon Forbes and Dalal Alrajeh and Brendan Delaney and Alessandra Russo},
Title         = {RECAP-KG: Mining Knowledge Graphs from Raw GP Notes for Remote COVID-19
  Assessment in Primary Care},
Eprint        = {2306.17175v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Clinical decision-making is a fundamental stage in delivering appropriate
care to patients. In recent years several decision-making systems designed to
aid the clinician in this process have been developed. However, technical
solutions currently in use are based on simple regression models and are only
able to take into account simple pre-defined multiple-choice features, such as
patient age, pre-existing conditions, smoker status, etc. One particular source
of patient data, that available decision-making systems are incapable of
processing is the collection of patient consultation GP notes. These contain
crucial signs and symptoms - the information used by clinicians in order to
make a final decision and direct the patient to the appropriate care.
Extracting information from GP notes is a technically challenging problem, as
they tend to include abbreviations, typos, and incomplete sentences.
  This paper addresses this open challenge. We present a framework that
performs knowledge graph construction from raw GP medical notes written during
or after patient consultations. By relying on support phrases mined from the
SNOMED ontology, as well as predefined supported facts from values used in the
RECAP (REmote COVID-19 Assessment in Primary Care) patient risk prediction
tool, our graph generative framework is able to extract structured knowledge
graphs from the highly unstructured and inconsistent format that consultation
notes are written in. Our knowledge graphs include information about existing
patient symptoms, their duration, and their severity.
  We apply our framework to consultation notes of COVID-19 patients in the UK
COVID-19 Clinical Assesment Servcie (CCAS) patient dataset. We provide a
quantitative evaluation of the performance of our framework, demonstrating that
our approach has better accuracy than traditional NLP methods when answering
questions about patients.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.17175v2},
File          = {2306.17175v2.pdf}
}
@article{2306.10300v1,
Author        = {Subhashis Das and Debashis Naskar and Sayon Roy},
Title         = {Reorganizing Educational Institutional Domain using Faceted Ontological
  Principles},
Eprint        = {2306.10300v1},
DOI           = {10.5771/0943-7444-2022-1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The purpose of this work is to find out how different library classification
systems and linguistic ontologies arrange a particular domain of interest and
what are the limitations for information retrieval. We use knowledge
representation techniques and languages for construction of a domain specific
ontology. This ontology would help not only in problem solving, but it would
demonstrate the ease with which complex queries can be handled using principles
of domain ontology, thereby facilitating better information retrieval.},
Year          = {2023},
Month         = {Jun},
Note          = {KO KNOWLEDGE ORGANIZATION, 49(1), 6-21 (2022)},
Url           = {http://arxiv.org/abs/2306.10300v1},
File          = {2306.10300v1.pdf}
}
@article{2306.09885v1,
Author        = {Gerard t Hooft},
Title         = {An ontological description for relativistic, massive bosons},
Eprint        = {2306.09885v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {quant-ph},
Abstract      = {Relativistic, scalar particles are considered, contained in a box with
periodic boundary conditions. Although interactions are not expected to be a
fundamental problem, we concentrate on free particles. By considering them to
be harmonic oscillators, it is found that their dynamical variables can be
replaced by a completely ontological set, which means that, here, quantum
mechanics does not deviate from a purely geometric, ontological particle
system. The effects of the mass terms are included. Locality holds for the
quantum theory, and seems to be fully obeyed also by the classical treatment,
although further discussion will be needed. Quantised interactions are briefly
speculated on, but mostly postponed to later. We do discuss extensively the
distinction between the quantum treatment and the classical one, even though
they produce exactly the same equations mathematically. We briefly explain how
this result can be squared with the usual quantum no-go theorems. It is
suggested to apply this theory for real time quantum model simulations.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.09885v1},
File          = {2306.09885v1.pdf}
}
@article{2306.09753v1,
Author        = {Matteo Busso and Xiaoyue Li},
Title         = {A context model for collecting diversity-aware data},
Eprint        = {2306.09753v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CY},
Abstract      = {Diversity-aware data are essential for a robust modeling of human behavior in
context. In addition, being the human behavior of interest for numerous
applications, data must also be reusable across domain, to ensure diversity of
interpretations. Current data collection techniques allow only a partial
representation of the diversity of people and often generate data that is
difficult to reuse. To fill this gap, we propose a data collection methodology,
within a hybrid machine-artificial intelligence approach, and its related
dataset, based on a comprehensive ontological notion of context which enables
data reusability. The dataset has a sample of 158 participants and is collected
via the iLog smartphone application. It contains more than 170 GB of subjective
and objective data, which comes from 27 smartphone sensors that are associated
with 168,095 self-reported annotations on the participants context. The dataset
is highly reusable, as demonstrated by its diverse applications.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.09753v1},
File          = {2306.09753v1.pdf}
}
@article{2306.08937v3,
Author        = {Lijun Yu and Jin Miao and Xiaoyu Sun and Jiayi Chen and Alexander G. Hauptmann and Hanjun Dai and Wei Wei},
Title         = {DocumentNet: Bridging the Data Gap in Document Pre-Training},
Eprint        = {2306.08937v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Document understanding tasks, in particular, Visually-rich Document Entity
Retrieval (VDER), have gained significant attention in recent years thanks to
their broad applications in enterprise AI. However, publicly available data
have been scarce for these tasks due to strict privacy constraints and high
annotation costs. To make things worse, the non-overlapping entity spaces from
different datasets hinder the knowledge transfer between document types. In
this paper, we propose a method to collect massive-scale and weakly labeled
data from the web to benefit the training of VDER models. The collected
dataset, named DocumentNet, does not depend on specific document types or
entity sets, making it universally applicable to all VDER tasks. The current
DocumentNet consists of 30M documents spanning nearly 400 document types
organized in a four-level ontology. Experiments on a set of broadly adopted
VDER tasks show significant improvements when DocumentNet is incorporated into
the pre-training for both classic and few-shot learning settings. With the
recent emergence of large language models (LLMs), DocumentNet provides a large
data source to extend their multi-modal capabilities for VDER.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.08937v3},
File          = {2306.08937v3.pdf}
}
@article{2306.10061v2,
Author        = {Giampaolo Bella and Domenico Cantone and Carmelo Fabio Longo and Marianna Nicolosi-Asmundo and Daniele Francesco Santamaria},
Title         = {The Ontology for Agents, Systems and Integration of Services: OASIS
  version 2},
Eprint        = {2306.10061v2},
DOI           = {10.3233/IA-230002},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Semantic representation is a key enabler for several application domains, and
the multi-agent systems realm makes no exception. Among the methods for
semantically representing agents, one has been essentially achieved by taking a
behaviouristic vision, through which one can describe how they operate and
engage with their peers. The approach essentially aims at defining the
operational capabilities of agents through the mental states related with the
achievement of tasks. The OASIS ontology -- An Ontology for Agent, Systems, and
Integration of Services, presented in 2019 -- pursues the behaviouristic
approach to deliver a semantic representation system and a communication
protocol for agents and their commitments. This paper reports on the main
modeling choices concerning the representation of agents in OASIS 2, the latest
major upgrade of OASIS, and the achievement reached by the ontology since it
was first introduced, in particular in the context of ontologies for
blockchains.},
Year          = {2023},
Month         = {Jun},
Note          = {Intelligenza Artificiale, Vol. 17, no 1, pp. 51-62, 2023},
Url           = {http://arxiv.org/abs/2306.10061v2},
File          = {2306.10061v2.pdf}
}
@article{2306.04553v1,
Author        = {Ngoc Luyen Le and Jinfeng Zhong and Elsa Negre and Marie-HÃ©lÃ¨ne Abel},
Title         = {Constraint-based recommender system for crisis management simulations},
Eprint        = {2306.04553v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {In the context of the evacuation of populations, some citizens/volunteers may
want and be able to participate in the evacuation of populations in difficulty
by coming to lend a hand to emergency/evacuation vehicles with their own
vehicles. One way of framing these impulses of solidarity would be to be able
to list in real-time the citizens/volunteers available with their vehicles
(land, sea, air, etc.), to be able to geolocate them according to the risk
areas to be evacuated, and adding them to the evacuation/rescue vehicles.
Because it is difficult to propose an effective real-time operational system on
the field in a real crisis situation, in this work, we propose to add a module
for recommending driver/vehicle pairs (with their specificities) to a system of
crisis management simulation. To do that, we chose to model and develop an
ontology-supported constraint-based recommender system for crisis management
simulations.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.04553v1},
File          = {2306.04553v1.pdf}
}
@article{2306.03247v1,
Author        = {Ngoc Luyen Le and Marie-HÃ©lÃ¨ne Abel and Philippe Gouspillou},
Title         = {Construction d'un systÃ¨me de recommandation basÃ© sur des contraintes
  via des graphes de connaissances},
Eprint        = {2306.03247v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Knowledge graphs in RDF model entities and their relations using ontologies,
and have gained popularity for information modeling. In recommender systems,
knowledge graphs help represent more links and relationships between users and
items. Constraint-based recommender systems leverage deep recommendation
knowledge to identify relevant suggestions. When combined with knowledge
graphs, they offer benefits in constraint sets. This paper explores a
constraint-based recommender system using RDF knowledge graphs for the vehicle
purchase/sale domain. Our experiments demonstrate that the proposed approach
efficiently identifies recommendations based on user preferences.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.03247v1},
File          = {2306.03247v1.pdf}
}
@article{2306.02521v1,
Author        = {Tim S. Lyon and Piotr Ostropolski-Nalewaja},
Title         = {Connecting Proof Theory and Knowledge Representation: Sequent Calculi
  and the Chase with Existential Rules},
Eprint        = {2306.02521v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LO},
Abstract      = {Chase algorithms are indispensable in the domain of knowledge base querying,
which enable the extraction of implicit knowledge from a given database via
applications of rules from a given ontology. Such algorithms have proved
beneficial in identifying logical languages which admit decidable query
entailment. Within the discipline of proof theory, sequent calculi have been
used to write and design proof-search algorithms to identify decidable classes
of logics. In this paper, we show that the chase mechanism in the context of
existential rules is in essence the same as proof-search in an extension of
Gentzen's sequent calculus for first-order logic. Moreover, we show that
proof-search generates universal models of knowledge bases, a feature also
exhibited by the chase. Thus, we formally connect a central tool for
establishing decidability proof-theoretically with a central decidability tool
in the context of knowledge representation.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.02521v1},
File          = {2306.02521v1.pdf}
}
@article{2306.02130v2,
Author        = {Jana StrakovÃ¡ and Eva FuÄÃ­kovÃ¡ and Jan HajiÄ and ZdeÅka UreÅ¡ovÃ¡},
Title         = {Extending an Event-type Ontology: Adding Verbs and Classes Using
  Fine-tuned LLMs Suggestions},
Eprint        = {2306.02130v2},
DOI           = {10.18653/v1/2023.law-1.9},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this project, we have investigated the use of advanced machine learning
methods, specifically fine-tuned large language models, for pre-annotating data
for a lexical extension task, namely adding descriptive words (verbs) to an
existing (but incomplete, as of yet) ontology of event types. Several research
questions have been focused on, from the investigation of a possible heuristics
to provide at least hints to annotators which verbs to include and which are
outside the current version of the ontology, to the possible use of the
automatic scores to help the annotators to be more efficient in finding a
threshold for identifying verbs that cannot be assigned to any existing class
and therefore they are to be used as seeds for a new class. We have also
carefully examined the correlation of the automatic scores with the human
annotation. While the correlation turned out to be strong, its influence on the
annotation proper is modest due to its near linearity, even though the mere
fact of such pre-annotation leads to relatively short annotation times.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.02130v2},
File          = {2306.02130v2.pdf}
}
@article{2306.02123v1,
Author        = {Ali Turfah and Xiaoquan Wen and Lili Zhao},
Title         = {Non-parametric Bayesian mixture model to study adverse events of
  COVID-19 vaccines},
Eprint        = {2306.02123v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {stat.ME},
Abstract      = {The vaccine adverse event reporting system (VAERS) is a vital resource for
post-licensure vaccine safety monitoring and has played a key role in assessing
the safety of COVID-19 vaccines. However it is difficult to properly identify
rare adverse events (AEs) associated with vaccines due to small or zero counts.
We propose a Bayesian model with a Dirichlet Process Mixture prior to improve
accuracy of the AE estimates with small counts by allowing data-guided
information sharing between AE estimates. We also propose a negative control
procedure embedded in our Bayesian model to mitigate the reporting bias due to
the heightened awareness of COVID-19 vaccines, and use it to identify
associated AEs as well as associated AE groups defined by the organ system in
the Medical Dictionary for Regulatory Activities (MedDRA) ontology. The
proposed model is evaluated using simulation studies, in which it outperforms
baseline models without information sharing and is applied to study the safety
of COVID-19 vaccines using VAERS data.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.02123v1},
File          = {2306.02123v1.pdf}
}
@article{2306.01504v1,
Author        = {Ngoc Luyen Le and Jinfeng Zhong and Elsa Negre and Marie-HÃ©lÃ¨ne Abel},
Title         = {SystÃ¨me de recommandations basÃ© sur les contraintes pour les
  simulations de gestion de crise},
Eprint        = {2306.01504v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {In the context of the evacuation of populations, some citizens/volunteers may
want and be able to participate in the evacuation of populations in difficulty
by coming to lend a hand to emergency/evacuation vehicles with their own
vehicles. One way of framing these impulses of solidarity would be to be able
to list in real-time the citizens/volunteers available with their vehicles
(land, sea, air, etc.), to be able to geolocate them according to the risk
areas to be evacuated, and adding them to the evacuation/rescue vehicles.
Because it is difficult to propose an effective real-time operational system on
the field in a real crisis situation, in this work, we propose to add a module
for recommending driver/vehicle pairs (with their specificities) to a system of
crisis management simulation. To do that, we chose to model and develop an
ontology-supported constraint-based recommender system for crisis management
simulations.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.01504v1},
File          = {2306.01504v1.pdf}
}
@article{2306.00665v1,
Author        = {FranÃ§ois Remy and Thomas Demeester},
Title         = {Automatic Glossary of Clinical Terminology: a Large-Scale Dictionary of
  Biomedical Definitions Generated from Ontological Knowledge},
Eprint        = {2306.00665v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Background: More than 400,000 biomedical concepts and some of their
relationships are contained in SnomedCT, a comprehensive biomedical ontology.
However, their concept names are not always readily interpretable by
non-experts, or patients looking at their own electronic health records (EHR).
Clear definitions or descriptions in understandable language are often not
available. Therefore, generating human-readable definitions for biomedical
concepts might help make the information they encode more accessible and
understandable to a wider public.
  Objective: In this article, we introduce the Automatic Glossary of Clinical
Terminology (AGCT), a large-scale biomedical dictionary of clinical concepts
generated using high-quality information extracted from the biomedical
knowledge contained in SnomedCT.
  Methods: We generate a novel definition for every SnomedCT concept, after
prompting the OpenAI Turbo model, a variant of GPT 3.5, using a high-quality
verbalization of the SnomedCT relationships of the to-be-defined concept. A
significant subset of the generated definitions was subsequently judged by NLP
researchers with biomedical expertise on 5-point scales along the following
three axes: factuality, insight, and fluency.
  Results: AGCT contains 422,070 computer-generated definitions for SnomedCT
concepts, covering various domains such as diseases, procedures, drugs, and
anatomy. The average length of the definitions is 49 words. The definitions
were assigned average scores of over 4.5 out of 5 on all three axes, indicating
a majority of factual, insightful, and fluent definitions.
  Conclusion: AGCT is a novel and valuable resource for biomedical tasks that
require human-readable definitions for SnomedCT concepts. It can also serve as
a base for developing robust biomedical retrieval models or other applications
that leverage natural language understanding of biomedical knowledge.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.00665v1},
File          = {2306.00665v1.pdf}
}
@article{2306.00377v1,
Author        = {Muhammad Shoaib Farooq and Muhammad Talha Waseem},
Title         = {Developing and Building Ontologies in Cyber Security},
Eprint        = {2306.00377v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {Cyber Security is one of the most arising disciplines in our modern society.
We work on Cybersecurity domain and in this the topic we chose is Cyber
Security Ontologies. In this we gather all latest and previous ontologies and
compare them on the basis of different analyzing factors to get best of them.
Reason to select this topic is to assemble different ontologies from different
era of time. Because, researches that included in this SLR is mostly studied
single ontology. If any researcher wants to study ontologies, he has to study
every single ontology and select which one is best for his research. So, we
assemble different types of ontology and compare them against each other to get
best of them. A total 24 papers between years 2010-2020 are carefully selected
through systematic process and classified accordingly. Lastly, this SLR have
been presented to provide the researchers promising future directions in the
domain of cybersecurity ontologies.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.00377v1},
File          = {2306.00377v1.pdf}
}
@article{2305.19997v1,
Author        = {Junwei Lu and Jin Yin and Tianxi Cai},
Title         = {Knowledge Graph Embedding with Electronic Health Records Data via Latent
  Graphical Block Model},
Eprint        = {2305.19997v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {stat.ML},
Abstract      = {Due to the increasing adoption of electronic health records (EHR), large
scale EHRs have become another rich data source for translational clinical
research. Despite its potential, deriving generalizable knowledge from EHR data
remains challenging. First, EHR data are generated as part of clinical care
with data elements too detailed and fragmented for research. Despite recent
progress in mapping EHR data to common ontology with hierarchical structures,
much development is still needed to enable automatic grouping of local EHR
codes to meaningful clinical concepts at a large scale. Second, the total
number of unique EHR features is large, imposing methodological challenges to
derive reproducible knowledge graph, especially when interest lies in
conditional dependency structure. Third, the detailed EHR data on a very large
patient cohort imposes additional computational challenge to deriving a
knowledge network. To overcome these challenges, we propose to infer the
conditional dependency structure among EHR features via a latent graphical
block model (LGBM). The LGBM has a two layer structure with the first providing
semantic embedding vector (SEV) representation for the EHR features and the
second overlaying a graphical block model on the latent SEVs. The block
structures on the graphical model also allows us to cluster synonymous features
in EHR. We propose to learn the LGBM efficiently, in both statistical and
computational sense, based on the empirical point mutual information matrix. We
establish the statistical rates of the proposed estimators and show the perfect
recovery of the block structure. Numerical results from simulation studies and
real EHR data analyses suggest that the proposed LGBM estimator performs well
in finite sample.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.19997v1},
File          = {2305.19997v1.pdf}
}
@article{2305.17208v2,
Author        = {Angeline Aguinaldo and Evan Patterson and James Fairbanks and William Regli and Jaime Ruiz},
Title         = {A Categorical Representation Language and Computational System for
  Knowledge-Based Planning},
Eprint        = {2305.17208v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Classical planning representation languages based on first-order logic have
preliminarily been used to model and solve robotic task planning problems.
Wider adoption of these representation languages, however, is hindered by the
limitations present when managing implicit world changes with concise action
models. To address this problem, we propose an alternative approach to
representing and managing updates to world states during planning. Based on the
category-theoretic concepts of $\mathsf{C}$-sets and double-pushout rewriting
(DPO), our proposed representation can effectively handle structured knowledge
about world states that support domain abstractions at all levels. It
formalizes the semantics of predicates according to a user-provided ontology
and preserves the semantics when transitioning between world states. This
method provides a formal semantics for using knowledge graphs and relational
databases to model world states and updates in planning. In this paper, we
conceptually compare our category-theoretic representation with the classical
planning representation. We show that our proposed representation has
advantages over the classical representation in terms of handling implicit
preconditions and effects, and provides a more structured framework in which to
model and solve planning problems.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.17208v2},
File          = {2305.17208v2.pdf}
}
@article{2305.16756v2,
Author        = {NicolÃ² Tamagnone and Selim Fekih and Ximena Contla and Nayid Orozco and Navid Rekabsaz},
Title         = {Leveraging Domain Knowledge for Inclusive and Bias-aware Humanitarian
  Response Entry Classification},
Eprint        = {2305.16756v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Accurate and rapid situation analysis during humanitarian crises is critical
to delivering humanitarian aid efficiently and is fundamental to humanitarian
imperatives and the Leave No One Behind (LNOB) principle. This data analysis
can highly benefit from language processing systems, e.g., by classifying the
text data according to a humanitarian ontology. However, approaching this by
simply fine-tuning a generic large language model (LLM) involves considerable
practical and ethical issues, particularly the lack of effectiveness on
data-sparse and complex subdomains, and the encoding of societal biases and
unwanted associations. In this work, we aim to provide an effective and
ethically-aware system for humanitarian data analysis. We approach this by (1)
introducing a novel architecture adjusted to the humanitarian analysis
framework, (2) creating and releasing a novel humanitarian-specific LLM called
HumBert, and (3) proposing a systematic way to measure and mitigate biases. Our
experiments' results show the better performance of our approach on zero-shot
and full-training settings in comparison with strong baseline models, while
also revealing the existence of biases in the resulting LLMs. Utilizing a
targeted counterfactual data augmentation approach, we significantly reduce
these biases without compromising performance.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.16756v2},
File          = {2305.16756v2.pdf}
}
@article{2305.14898v1,
Author        = {Keming Lu and Xiaoman Pan and Kaiqiang Song and Hongming Zhang and Dong Yu and Jianshu Chen},
Title         = {PIVOINE: Instruction Tuning for Open-world Information Extraction},
Eprint        = {2305.14898v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We consider the problem of Open-world Information Extraction (Open-world IE),
which extracts comprehensive entity profiles from unstructured texts. Different
from the conventional closed-world setting of Information Extraction (IE),
Open-world IE considers a more general situation where entities and relations
could be beyond a predefined ontology. More importantly, we seek to develop a
large language model (LLM) that is able to perform Open-world IE to extract
desirable entity profiles characterized by (possibly fine-grained) natural
language instructions. We achieve this by finetuning LLMs using instruction
tuning. In particular, we construct INSTRUCTOPENWIKI, a substantial instruction
tuning dataset for Open-world IE enriched with a comprehensive corpus,
extensive annotations, and diverse instructions. We finetune the pretrained
BLOOM models on INSTRUCTOPENWIKI and obtain PIVOINE, an LLM for Open-world IE
with strong instruction-following capabilities. Our experiments demonstrate
that PIVOINE significantly outperforms traditional closed-world methods and
other LLM baselines, displaying impressive generalization capabilities on both
unseen instructions and out-of-ontology cases. Consequently, PIVOINE emerges as
a promising solution to tackle the open-world challenge in IE effectively.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.14898v1},
File          = {2305.14898v1.pdf}
}
@article{2305.13521v2,
Author        = {Nan Xu and Hongming Zhang and Jianshu Chen},
Title         = {CEO: Corpus-based Open-Domain Event Ontology Induction},
Eprint        = {2305.13521v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Existing event-centric NLP models often only apply to the pre-defined
ontology, which significantly restricts their generalization capabilities. This
paper presents CEO, a novel Corpus-based Event Ontology induction model to
relax the restriction imposed by pre-defined event ontologies. Without direct
supervision, CEO leverages distant supervision from available summary datasets
to detect corpus-wise salient events and exploits external event knowledge to
force events within a short distance to have close embeddings. Experiments on
three popular event datasets show that the schema induced by CEO has better
coverage and higher accuracy than previous methods. Moreover, CEO is the first
event ontology induction model that can induce a hierarchical event ontology
with meaningful names on eleven open-domain corpora, making the induced schema
more trustworthy and easier to be further curated.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.13521v2},
File          = {2305.13521v2.pdf}
}
@article{2305.12363v3,
Author        = {Laksh Nanwani and Anmol Agarwal and Kanishk Jain and Raghav Prabhakar and Aaron Monis and Aditya Mathur and Krishna Murthy and Abdul Hafez and Vineet Gandhi and K. Madhava Krishna},
Title         = {Instance-Level Semantic Maps for Vision Language Navigation},
Eprint        = {2305.12363v3},
DOI           = {10.1109/RO-MAN57019.2023.10309534},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Humans have a natural ability to perform semantic associations with the
surrounding objects in the environment. This allows them to create a mental map
of the environment, allowing them to navigate on-demand when given linguistic
instructions. A natural goal in Vision Language Navigation (VLN) research is to
impart autonomous agents with similar capabilities. Recent works take a step
towards this goal by creating a semantic spatial map representation of the
environment without any labeled data. However, their representations are
limited for practical applicability as they do not distinguish between
different instances of the same object. In this work, we address this
limitation by integrating instance-level information into spatial map
representation using a community detection algorithm and utilizing word
ontology learned by large language models (LLMs) to perform open-set semantic
associations in the mapping representation. The resulting map representation
improves the navigation performance by two-fold (233%) on realistic language
commands with instance-specific descriptions compared to the baseline. We
validate the practicality and effectiveness of our approach through extensive
qualitative and quantitative experiments.},
Year          = {2023},
Month         = {May},
Note          = {IEEE RO-MAN 2023},
Url           = {http://arxiv.org/abs/2305.12363v3},
File          = {2305.12363v3.pdf}
}
@article{2305.13338v3,
Author        = {Marcin P. Joachimiak and J. Harry Caufield and Nomi L. Harris and Hyeongsik Kim and Christopher J. Mungall},
Title         = {Gene Set Summarization using Large Language Models},
Eprint        = {2305.13338v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.GN},
Abstract      = {Molecular biologists frequently interpret gene lists derived from
high-throughput experiments and computational analysis. This is typically done
as a statistical enrichment analysis that measures the over- or
under-representation of biological function terms associated with genes or
their properties, based on curated assertions from a knowledge base (KB) such
as the Gene Ontology (GO). Interpreting gene lists can also be framed as a
textual summarization task, enabling the use of Large Language Models (LLMs),
potentially utilizing scientific texts directly and avoiding reliance on a KB.
  We developed SPINDOCTOR (Structured Prompt Interpolation of Natural Language
Descriptions of Controlled Terms for Ontology Reporting), a method that uses
GPT models to perform gene set function summarization as a complement to
standard enrichment analysis. This method can use different sources of gene
functional information: (1) structured text derived from curated ontological KB
annotations, (2) ontology-free narrative gene summaries, or (3) direct model
retrieval.
  We demonstrate that these methods are able to generate plausible and
biologically valid summary GO term lists for gene sets. However, GPT-based
approaches are unable to deliver reliable scores or p-values and often return
terms that are not statistically significant. Crucially, these methods were
rarely able to recapitulate the most precise and informative term from standard
enrichment, likely due to an inability to generalize and reason using an
ontology. Results are highly nondeterministic, with minor variations in prompt
resulting in radically different term lists. Our results show that at this
point, LLM-based methods are unsuitable as a replacement for standard term
enrichment analysis and that manual curation of ontological assertions remains
necessary.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.13338v3},
File          = {2305.13338v3.pdf}
}
@article{2305.12307v3,
Author        = {Tanay Komarlu and Minhao Jiang and Xuan Wang and Jiawei Han},
Title         = {OntoType: Ontology-Guided and Pre-Trained Language Model Assisted
  Fine-Grained Entity Typing},
Eprint        = {2305.12307v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Fine-grained entity typing (FET), which assigns entities in text with
context-sensitive, fine-grained semantic types, is a basic but important task
for knowledge extraction from unstructured text. FET has been studied
extensively in natural language processing and typically relies on
human-annotated corpora for training, which is costly and difficult to scale.
Recent studies explore the utilization of pre-trained language models (PLMs) as
a knowledge base to generate rich and context-aware weak supervision for FET.
However, a PLM still requires direction and guidance to serve as a knowledge
base as they often generate a mixture of rough and fine-grained types, or
tokens unsuitable for typing. In this study, we vision that an ontology
provides a semantics-rich, hierarchical structure, which will help select the
best results generated by multiple PLM models and head words. Specifically, we
propose a novel annotation-free, ontology-guided FET method, OntoType, which
follows a type ontological structure, from coarse to fine, ensembles multiple
PLM prompting results to generate a set of type candidates, and refines its
type resolution, under the local context with a natural language inference
model. Our experiments on the Ontonotes, FIGER, and NYT datasets using their
associated ontological structures demonstrate that our method outperforms the
state-of-the-art zero-shot fine-grained entity typing methods as well as a
typical LLM method, ChatGPT. Our error analysis shows that refinement of the
existing ontology structures will further improve fine-grained entity typing.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.12307v3},
File          = {2305.12307v3.pdf}
}
@article{2305.11592v1,
Author        = {Piyush Kumar Garg and Roshni Chakraborty and Srishti Gupta and Sourav Kumar Dandapat},
Title         = {IKDSumm: Incorporating Key-phrases into BERT for extractive Disaster
  Tweet Summarization},
Eprint        = {2305.11592v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Online social media platforms, such as Twitter, are one of the most valuable
sources of information during disaster events. Therefore, humanitarian
organizations, government agencies, and volunteers rely on a summary of this
information, i.e., tweets, for effective disaster management. Although there
are several existing supervised and unsupervised approaches for automated tweet
summary approaches, these approaches either require extensive labeled
information or do not incorporate specific domain knowledge of disasters.
Additionally, the most recent approaches to disaster summarization have
proposed BERT-based models to enhance the summary quality. However, for further
improved performance, we introduce the utilization of domain-specific knowledge
without any human efforts to understand the importance (salience) of a tweet
which further aids in summary creation and improves summary quality. In this
paper, we propose a disaster-specific tweet summarization framework, IKDSumm,
which initially identifies the crucial and important information from each
tweet related to a disaster through key-phrases of that tweet. We identify
these key-phrases by utilizing the domain knowledge (using existing ontology)
of disasters without any human intervention. Further, we utilize these
key-phrases to automatically generate a summary of the tweets. Therefore, given
tweets related to a disaster, IKDSumm ensures fulfillment of the summarization
key objectives, such as information coverage, relevance, and diversity in
summary without any human intervention. We evaluate the performance of IKDSumm
with 8 state-of-the-art techniques on 12 disaster datasets. The evaluation
results show that IKDSumm outperforms existing techniques by approximately
2-79% in terms of ROUGE-N F1-score.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.11592v1},
File          = {2305.11592v1.pdf}
}
@article{2305.11315v1,
Author        = {Zeyu Zhang and Steven Bethard},
Title         = {Improving Toponym Resolution with Better Candidate Generation,
  Transformer-based Reranking, and Two-Stage Resolution},
Eprint        = {2305.11315v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Geocoding is the task of converting location mentions in text into structured
data that encodes the geospatial semantics. We propose a new architecture for
geocoding, GeoNorm. GeoNorm first uses information retrieval techniques to
generate a list of candidate entries from the geospatial ontology. Then it
reranks the candidate entries using a transformer-based neural network that
incorporates information from the ontology such as the entry's population. This
generate-and-rerank process is applied twice: first to resolve the less
ambiguous countries, states, and counties, and second to resolve the remaining
location mentions, using the identified countries, states, and counties as
context. Our proposed toponym resolution framework achieves state-of-the-art
performance on multiple datasets. Code and models are available at
\url{https://github.com/clulab/geonorm}.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.11315v1},
File          = {2305.11315v1.pdf}
}
@article{2305.11051v1,
Author        = {Gianluca Carletti and Elio Giulianelli and Anna Sofia Lippolis and Giorgia Lodi and Andrea Giovanni Nuzzolese and Marco Picone and Giulio Settanta},
Title         = {The Water Health Open Knowledge Graph},
Eprint        = {2305.11051v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Recently, an increasing interest in the management of water and health
resources has been recorded. This interest is fed by the global sustainability
challenges posed to the humanity that have water scarcity and quality at their
core. Thus, the availability of effective, meaningful and open data is crucial
to address those issues in the broader context of the Sustainable Development
Goals of clean water and sanitation as targeted by the United Nations. In this
paper, we present the Water Health Open Knowledge Graph (WHOW-KG) along with
its design methodology and analysis on impact. WHOW-KG is a semantic knowledge
graph that models data on water consumption, pollution, infectious disease
rates and drug distribution. The WHOW-KG is developed in the context of the
EU-funded WHOW (Water Health Open Knowledge) project and aims at supporting a
wide range of applications: from knowledge discovery to decision-making, making
it a valuable resource for researchers, policymakers, and practitioners in the
water and health domains. The WHOW-KG consists of a network of five ontologies
and related linked open data, modelled according to those ontologies.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.11051v1},
File          = {2305.11051v1.pdf}
}
@article{2305.09817v1,
Author        = {Tianyu Chen},
Title         = {A Method for Training-free Person Image Picture Generation},
Eprint        = {2305.09817v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {The current state-of-the-art Diffusion model has demonstrated excellent
results in generating images. However, the images are monotonous and are mostly
the result of the distribution of images of people in the training set, making
it challenging to generate multiple images for a fixed number of individuals.
This problem can often only be solved by fine-tuning the training of the model.
This means that each individual/animated character image must be trained if it
is to be drawn, and the hardware and cost of this training is often beyond the
reach of the average user, who accounts for the largest number of people. To
solve this problem, the Character Image Feature Encoder model proposed in this
paper enables the user to use the process by simply providing a picture of the
character to make the image of the character in the generated image match the
expectation. In addition, various details can be adjusted during the process
using prompts. Unlike traditional Image-to-Image models, the Character Image
Feature Encoder extracts only the relevant image features, rather than
information about the model's composition or movements. In addition, the
Character Image Feature Encoder can be adapted to different models after
training. The proposed model can be conveniently incorporated into the Stable
Diffusion generation process without modifying the model's ontology or used in
combination with Stable Diffusion as a joint model.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.09817v1},
File          = {2305.09817v1.pdf}
}
@article{2305.09785v1,
Author        = {Na Li and Hanane Kteich and Zied Bouraoui and Steven Schockaert},
Title         = {Distilling Semantic Concept Embeddings from Contrastively Fine-Tuned
  Language Models},
Eprint        = {2305.09785v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Learning vectors that capture the meaning of concepts remains a fundamental
challenge. Somewhat surprisingly, perhaps, pre-trained language models have
thus far only enabled modest improvements to the quality of such concept
embeddings. Current strategies for using language models typically represent a
concept by averaging the contextualised representations of its mentions in some
corpus. This is potentially sub-optimal for at least two reasons. First,
contextualised word vectors have an unusual geometry, which hampers downstream
tasks. Second, concept embeddings should capture the semantic properties of
concepts, whereas contextualised word vectors are also affected by other
factors. To address these issues, we propose two contrastive learning
strategies, based on the view that whenever two sentences reveal similar
properties, the corresponding contextualised vectors should also be similar.
One strategy is fully unsupervised, estimating the properties which are
expressed in a sentence from the neighbourhood structure of the contextualised
word embeddings. The second strategy instead relies on a distant supervision
signal from ConceptNet. Our experimental results show that the resulting
vectors substantially outperform existing concept embeddings in predicting the
semantic properties of concepts, with the ConceptNet-based strategy achieving
the best results. These findings are furthermore confirmed in a clustering task
and in the downstream task of ontology completion.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.09785v1},
File          = {2305.09785v1.pdf}
}
@article{2305.09335v1,
Author        = {Siyuan Wang and Jianming Zheng and Xuejun Hu and Fei Cai and Chengyu Song and Xueshan Luo},
Title         = {MsPrompt: Multi-step Prompt Learning for Debiasing Few-shot Event
  Detection},
Eprint        = {2305.09335v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Event detection (ED) is aimed to identify the key trigger words in
unstructured text and predict the event types accordingly. Traditional ED
models are too data-hungry to accommodate real applications with scarce labeled
data. Besides, typical ED models are facing the context-bypassing and disabled
generalization issues caused by the trigger bias stemming from ED datasets.
Therefore, we focus on the true few-shot paradigm to satisfy the low-resource
scenarios. In particular, we propose a multi-step prompt learning model
(MsPrompt) for debiasing few-shot event detection, that consists of the
following three components: an under-sampling module targeting to construct a
novel training set that accommodates the true few-shot setting, a multi-step
prompt module equipped with a knowledge-enhanced ontology to leverage the event
semantics and latent prior knowledge in the PLMs sufficiently for tackling the
context-bypassing problem, and a prototypical module compensating for the
weakness of classifying events with sparse data and boost the generalization
performance. Experiments on two public datasets ACE-2005 and FewEvent show that
MsPrompt can outperform the state-of-the-art models, especially in the strict
low-resource scenarios reporting 11.43% improvement in terms of weighted
F1-score against the best-performing baseline and achieving an outstanding
debiasing performance.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.09335v1},
File          = {2305.09335v1.pdf}
}
@article{2305.08477v3,
Author        = {Arcangelo Massari and Silvio Peroni and Francesca Tomasi and Ivan Heibi},
Title         = {Representing provenance and track changes of cultural heritage metadata
  in RDF: a survey of existing approaches},
Eprint        = {2305.08477v3},
DOI           = {10.1093/llc/fqaf076},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {In the realm of Digital Humanities, the management of cultural heritage
metadata is pivotal for ensuring data trustworthiness. Provenance information -
contextual metadata detailing the origin and history of data - plays a crucial
role in this process. However, tracking provenance and changes in metadata
using the Resource Description Framework (RDF) presents significant challenges
due to the limitations of foundational Semantic Web technologies. This article
offers a comprehensive review of existing models and approaches for
representing provenance and tracking changes in RDF, with a specific focus on
cultural heritage metadata. It examines W3C standard proposals such as RDF
Reification and n-ary relations, along with various alternative systems.
Through an in-depth analysis, the study identifies Named Graphs, RDF*, the
Provenance Ontology (PROV-O), Dublin Core (DC), Conjectural Graphs, and the
OpenCitations Data Model (OCDM) as the most effective solutions. These models
are evaluated based on their compliance with RDF standards, scalability, and
applicability across different domains. The findings underscore the importance
of selecting the appropriate model to ensure robust and reliable management of
provenance in RDF datasets, thereby contributing to the ongoing discourse on
provenance representation in the Digital Humanities.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.08477v3},
File          = {2305.08477v3.pdf}
}
@article{2305.07163v3,
Author        = {Fernando Zhapa-Camacho and Robert Hoehndorf},
Title         = {Lattice-preserving $\mathcal{ALC}$ ontology embeddings with saturation},
Eprint        = {2305.07163v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LO},
Abstract      = {Generating vector representations (embeddings) of OWL ontologies is a growing
task due to its applications in predicting missing facts and knowledge-enhanced
learning in fields such as bioinformatics. The underlying semantics of OWL
ontologies are expressed using Description Logics (DLs). Initial approaches to
generate embeddings relied on constructing a graph out of ontologies,
neglecting the semantics of the logic therein. Recent semantic-preserving
embedding methods often target lightweight DL languages like
$\mathcal{EL}^{++}$, ignoring more expressive information in ontologies.
Although some approaches aim to embed more descriptive DLs like
$\mathcal{ALC}$, those methods require the existence of individuals, while many
real-world ontologies are devoid of them. We propose an ontology embedding
method for the $\mathcal{ALC}$ DL language that considers the lattice structure
of concept descriptions. We use connections between DL and Category Theory to
materialize the lattice structure and embed it using an order-preserving
embedding method. We show that our method outperforms state-of-the-art methods
in several knowledge base completion tasks. Furthermore, we incoporate
saturation procedures that increase the information within the constructed
lattices. We make our code and data available at
\url{https://github.com/bio-ontology-research-group/catE}.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.07163v3},
File          = {2305.07163v3.pdf}
}
@article{2305.07447v1,
Author        = {Qiuqiang Kong and Ke Chen and Haohe Liu and Xingjian Du and Taylor Berg-Kirkpatrick and Shlomo Dubnov and Mark D. Plumbley},
Title         = {Universal Source Separation with Weakly Labelled Data},
Eprint        = {2305.07447v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SD},
Abstract      = {Universal source separation (USS) is a fundamental research task for
computational auditory scene analysis, which aims to separate mono recordings
into individual source tracks. There are three potential challenges awaiting
the solution to the audio source separation task. First, previous audio source
separation systems mainly focus on separating one or a limited number of
specific sources. There is a lack of research on building a unified system that
can separate arbitrary sources via a single model. Second, most previous
systems require clean source data to train a separator, while clean source data
are scarce. Third, there is a lack of USS system that can automatically detect
and separate active sound classes in a hierarchical level. To use large-scale
weakly labeled/unlabeled audio data for audio source separation, we propose a
universal audio source separation framework containing: 1) an audio tagging
model trained on weakly labeled data as a query net; and 2) a conditional
source separation model that takes query net outputs as conditions to separate
arbitrary sound sources. We investigate various query nets, source separation
models, and training strategies and propose a hierarchical USS strategy to
automatically detect and separate sound classes from the AudioSet ontology. By
solely leveraging the weakly labelled AudioSet, our USS system is successful in
separating a wide variety of sound classes, including sound event separation,
music source separation, and speech enhancement. The USS system achieves an
average signal-to-distortion ratio improvement (SDRi) of 5.57 dB over 527 sound
classes of AudioSet; 10.57 dB on the DCASE 2018 Task 2 dataset; 8.12 dB on the
MUSDB18 dataset; an SDRi of 7.28 dB on the Slakh2100 dataset; and an SSNR of
9.00 dB on the voicebank-demand dataset. We release the source code at
https://github.com/bytedance/uss},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.07447v1},
File          = {2305.07447v1.pdf}
}
@article{2305.06801v1,
Author        = {FranÃ§ois Remy and Alfiya Khabibullina and Thomas Demeester},
Title         = {Detecting Idiomatic Multiword Expressions in Clinical Terminology using
  Definition-Based Representation Learning},
Eprint        = {2305.06801v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper shines a light on the potential of definition-based semantic
models for detecting idiomatic and semi-idiomatic multiword expressions (MWEs)
in clinical terminology. Our study focuses on biomedical entities defined in
the UMLS ontology and aims to help prioritize the translation efforts of these
entities. In particular, we develop an effective tool for scoring the
idiomaticity of biomedical MWEs based on the degree of similarity between the
semantic representations of those MWEs and a weighted average of the
representation of their constituents. We achieve this using a biomedical
language model trained to produce similar representations for entity names and
their definitions, called BioLORD. The importance of this definition-based
approach is highlighted by comparing the BioLORD model to two other
state-of-the-art biomedical language models based on Transformer: SapBERT and
CODER. Our results show that the BioLORD model has a strong ability to identify
idiomatic MWEs, not replicated in other models. Our corpus-free idiomaticity
estimation helps ontology translators to focus on more challenging MWEs.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.06801v1},
File          = {2305.06801v1.pdf}
}
@article{2305.05640v3,
Author        = {Christos Theodoropoulos and Natasha Mulligan and Thaddeus Stappenbeck and Joao Bettencourt-Silva},
Title         = {Representation Learning for Person or Entity-centric Knowledge Graphs:
  An Application in Healthcare},
Eprint        = {2305.05640v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge graphs (KGs) are a popular way to organise information based on
ontologies or schemas and have been used across a variety of scenarios from
search to recommendation. Despite advances in KGs, representing knowledge
remains a non-trivial task across industries and it is especially challenging
in the biomedical and healthcare domains due to complex interdependent
relations between entities, heterogeneity, lack of standardization, and
sparseness of data. KGs are used to discover diagnoses or prioritize genes
relevant to disease, but they often rely on schemas that are not centred around
a node or entity of interest, such as a person. Entity-centric KGs are
relatively unexplored but hold promise in representing important facets
connected to a central node and unlocking downstream tasks beyond graph
traversal and reasoning, such as generating graph embeddings and training graph
neural networks for a wide range of predictive tasks. This paper presents an
end-to-end representation learning framework to extract entity-centric KGs from
structured and unstructured data. We introduce a star-shaped ontology to
represent the multiple facets of a person and use it to guide KG creation.
Compact representations of the graphs are created leveraging graph neural
networks and experiments are conducted using different levels of heterogeneity
or explicitness. A readmission prediction task is used to evaluate the results
of the proposed framework, showing a stable system, robust to missing data,
that outperforms a range of baseline machine learning classifiers. We highlight
that this approach has several potential applications across domains and is
open-sourced. Lastly, we discuss lessons learned, challenges, and next steps
for the adoption of the framework in practice.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.05640v3},
File          = {2305.05640v3.pdf}
}
@article{2305.05302v1,
Author        = {Eliya Habba and Renana Keydar and Dan Bareket and Gabriel Stanovsky},
Title         = {The Perfect Victim: Computational Analysis of Judicial Attitudes towards
  Victims of Sexual Violence},
Eprint        = {2305.05302v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We develop computational models to analyze court statements in order to
assess judicial attitudes toward victims of sexual violence in the Israeli
court system. The study examines the resonance of "rape myths" in the criminal
justice system's response to sex crimes, in particular in judicial assessment
of victim's credibility. We begin by formulating an ontology for evaluating
judicial attitudes toward victim's credibility, with eight ordinal labels and
binary categorizations. Second, we curate a manually annotated dataset for
judicial assessments of victim's credibility in the Hebrew language, as well as
a model that can extract credibility labels from court cases. The dataset
consists of 855 verdict decision documents in sexual assault cases from
1990-2021, annotated with the help of legal experts and trained law students.
The model uses a combined approach of syntactic and latent structures to find
sentences that convey the judge's attitude towards the victim and classify them
according to the credibility label set. Our ontology, data, and models will be
made available upon request, in the hope they spur future progress in this
judicial important task.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.05302v1},
File          = {2305.05302v1.pdf}
}
@article{2305.04676v1,
Author        = {Milena Trajanoska and Riste Stojanov and Dimitar Trajanov},
Title         = {Enhancing Knowledge Graph Construction Using Large Language Models},
Eprint        = {2305.04676v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The growing trend of Large Language Models (LLM) development has attracted
significant attention, with models for various applications emerging
consistently. However, the combined application of Large Language Models with
semantic technologies for reasoning and inference is still a challenging task.
This paper analyzes how the current advances in foundational LLM, like ChatGPT,
can be compared with the specialized pretrained models, like REBEL, for joint
entity and relation extraction. To evaluate this approach, we conducted several
experiments using sustainability-related text as our use case. We created
pipelines for the automatic creation of Knowledge Graphs from raw texts, and
our findings indicate that using advanced LLM models can improve the accuracy
of the process of creating these graphs from unstructured text. Furthermore, we
explored the potential of automatic ontology creation using foundation LLM
models, which resulted in even more relevant and accurate knowledge graphs.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.04676v1},
File          = {2305.04676v1.pdf}
}
@article{2305.04055v1,
Author        = {Mahender Kumar and Ruby Rani and Mirko Botarelli and Gregory Epiophaniou and Carsten Maple},
Title         = {Science and Technology Ontology: A Taxonomy of Emerging Topics},
Eprint        = {2305.04055v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {Ontologies play a critical role in Semantic Web technologies by providing a
structured and standardized way to represent knowledge and enabling machines to
understand the meaning of data. Several taxonomies and ontologies have been
generated, but individuals target one domain, and only some of those have been
found expensive in time and manual effort. Also, they need more coverage of
unconventional topics representing a more holistic and comprehensive view of
the knowledge landscape and interdisciplinary collaborations. Thus, there needs
to be an ontology covering Science and Technology and facilitate
multidisciplinary research by connecting topics from different fields and
domains that may be related or have commonalities. To address these issues, we
present an automatic Science and Technology Ontology (S&TO) that covers
unconventional topics in different science and technology domains. The proposed
S&TO can promote the discovery of new research areas and collaborations across
disciplines. The ontology is constructed by applying BERTopic to a dataset of
393,991 scientific articles collected from Semantic Scholar from October 2021
to August 2022, covering four fields of science. Currently, S&TO includes 5,153
topics and 13,155 semantic relations. S&TO model can be updated by running
BERTopic on more recent datasets},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.04055v1},
File          = {2305.04055v1.pdf}
}
@article{2305.04049v1,
Author        = {Yuxia Wu and Tianhao Dai and Zhedong Zheng and Lizi Liao},
Title         = {Actively Discovering New Slots for Task-oriented Conversation},
Eprint        = {2305.04049v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Existing task-oriented conversational search systems heavily rely on domain
ontologies with pre-defined slots and candidate value sets. In practical
applications, these prerequisites are hard to meet, due to the emerging new
user requirements and ever-changing scenarios. To mitigate these issues for
better interaction performance, there are efforts working towards detecting
out-of-vocabulary values or discovering new slots under unsupervised or
semi-supervised learning paradigm. However, overemphasizing on the conversation
data patterns alone induces these methods to yield noisy and arbitrary slot
results. To facilitate the pragmatic utility, real-world systems tend to
provide a stringent amount of human labelling quota, which offers an
authoritative way to obtain accurate and meaningful slot assignments.
Nonetheless, it also brings forward the high requirement of utilizing such
quota efficiently. Hence, we formulate a general new slot discovery task in an
information extraction fashion and incorporate it into an active learning
framework to realize human-in-the-loop learning. Specifically, we leverage
existing language tools to extract value candidates where the corresponding
labels are further leveraged as weak supervision signals. Based on these, we
propose a bi-criteria selection scheme which incorporates two major strategies,
namely, uncertainty-based sampling and diversity-based sampling to efficiently
identify terms of interest. We conduct extensive experiments on several public
datasets and compare with a bunch of competitive baselines to demonstrate the
effectiveness of our method. We have made the code and data used in this paper
publicly available.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.04049v1},
File          = {2305.04049v1.pdf}
}
@article{2305.03793v1,
Author        = {Danilo Ribeiro and Omid Abdar and Jack Goetz and Mike Ross and Annie Dong and Kenneth Forbus and Ahmed Mohamed},
Title         = {Towards Zero-Shot Frame Semantic Parsing with Task Agnostic Ontologies
  and Simple Labels},
Eprint        = {2305.03793v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Frame semantic parsing is an important component of task-oriented dialogue
systems. Current models rely on a significant amount training data to
successfully identify the intent and slots in the user's input utterance. This
creates a significant barrier for adding new domains to virtual assistant
capabilities, as creation of this data requires highly specialized NLP
expertise. In this work we propose OpenFSP, a framework that allows for easy
creation of new domains from a handful of simple labels that can be generated
without specific NLP knowledge. Our approach relies on creating a small, but
expressive, set of domain agnostic slot types that enables easy annotation of
new domains. Given such annotation, a matching algorithm relying on sentence
encoders predicts the intent and slots for domains defined by end-users.
Extensive experiments on the TopV2 dataset shows that our model outperforms
strong baselines in this simple labels setting.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.03793v1},
File          = {2305.03793v1.pdf}
}

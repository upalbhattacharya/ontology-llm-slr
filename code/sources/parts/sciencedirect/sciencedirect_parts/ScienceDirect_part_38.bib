@article{BASKARADA20182,
title = {A philosophical discussion of qualitative, quantitative, and mixed methods research in social science},
journal = {Qualitative Research Journal},
volume = {18},
number = {1},
pages = {2-21},
year = {2018},
issn = {1443-9883},
doi = {https://doi.org/10.1108/QRJ-D-17-00042},
url = {https://www.sciencedirect.com/science/article/pii/S1443988318000334},
author = {Saša Baškarada and Andy Koronios},
keywords = {Quantitative, Qualitative, Positivism, Paradigm, Mixed methods research, Interpretivism},
abstract = {Purpose
Much of the contemporary methodological literature tends to be self-referential and frequently ignorant of the breadth and depth of philosophical assumptions underpinning various methodological positions. Without a clear understanding of the philosophical underpinnings, logically deriving applicable validity criteria becomes very difficult (if not impossible). As a result, the purpose of this paper is to present a critical review of historical and more recent philosophical arguments for qualitative, quantitative, and mixed methods research in social science.
Design/methodology/approach
A targeted review of seminal philosophy of science papers dealing with ontological and epistemological assumptions of, and relation between, natural and social science.
Findings
The paper highlights the link between ontological/epistemological assumptions and methodological choices in social science. Key differences between the natural and social science are discussed and situated within the main paradigms.
Originality/value
The paper draws attention to a range of difficulties associated with the adoption of the natural sciences and the related positivist approaches as a role model for work in the social sciences. Unique contributions of interpretive and critical approaches are highlighted. The paper may be of value to scholars who are interested in the historical context of the still-ongoing qualitative-quantitative debate.}
}
@article{BAAZOUZI20233839,
title = {An Interactive Tool to Bootstrap Semantic Table Interpretation},
journal = {Procedia Computer Science},
volume = {225},
pages = {3839-3855},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.380},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923015387},
author = {Wiem Baazouzi and Marouen Kachroudi and Sami Faiz},
keywords = {Tabular Data, Semantic Table Interpretation, Knowledge Graph},
abstract = {Since it is widely conveyed on the Web, tabular data is mostly organized in a table structure. Indeed, this data format is widely used in multiple scenarios on the web and even within data storage entities. In addition, tabular data is a source of information that deserves to be interpreted and exploited. In this framework, efforts to extract meaningful information from tabular data based on semantic approaches, such as an ontology or a knowledge graph, are commonly referred to as Semantic Table Interpretation (STI). In this paper, we present an interactive tool that deploys a mapping approach to overcome possible semantic gaps in tabular data against a Knowledge Graph. Indeed, the ultimate goal of this tool is to provide an application and adaptable framework that combines search and filtering services associated with text preprocessing techniques. The experimental evaluation was conducted under the SemTab challenge and yielded encouraging and promising results regarding its performance and rankings.}
}
@article{VANWEERDENBURG2019143,
title = {Where to go and what to do: Extracting leisure activity potentials from Web data on urban space},
journal = {Computers, Environment and Urban Systems},
volume = {73},
pages = {143-156},
year = {2019},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2018.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0198971518302138},
author = {Demi {van Weerdenburg} and Simon Scheider and Benjamin Adams and Bas Spierings and Egbert {van der Zee}},
keywords = {Place affordance, Urban space, Knowledge extraction, City planning, Latent semantics, Multi-label classification},
abstract = {Web data is the most prominent source of information for deciding where to go and what to do. Exploiting this source for geographic analysis, however, does not come without difficulties. First, in recent years, the amount and diversity of available Web information about urban space have exploded, and it is therefore increasingly difficult to overview and exploit. Second, the bulk of information is in an unstructured form which is difficult to process and interpret by computers. Third, semi-structured sources, such as Web rankings, geolocated tags, check-ins, or mobile sensor data, do not fully reflect the more subtle qualities of a place, including the particular functions that make it attractive. In this article, we explore a method to capture leisure activity potentials from Web data on urban space using semantic topic models. We test three supervised multi-label machine learning strategies exploiting geolocated webtexts and place tags to estimate whether a given type of leisure activity is afforded or not. We train and validate these models on a manually curated dataset labeled with leisure ontology classes for the city of Zwolle, and discuss their potential for urban leisure and tourism research and related city policies and planning. We found that multi-label affordance estimation is not straightforward but can be made to work using both official webtexts and user-generated content on a medium semantic level. This opens up new opportunities for data-driven approaches to urban leisure and tourism studies.}
}
@article{LI2021102144,
title = {A semantic-level component-based scheduling method for customized manufacturing},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {71},
pages = {102144},
year = {2021},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2021.102144},
url = {https://www.sciencedirect.com/science/article/pii/S0736584521000296},
author = {Di Li and Hao Tang},
keywords = {CPPS, Ontology, Markov decision process, Customized manufacturing},
abstract = {To meet the increasingly complex needs of customers, scheduling faces challenges of the high uncertainty of product arrival in customized manufacturing (CM). This paper proposes a semantic-level component-based scheduling method to solve the uncertainty via the integration of the information model and the computation model. In our proposal, we first construct a component-based framework to illustrate the composition and execution mechanism of a component. Then we present a semantic-enriched information model to obtain the state of the shop floor through automatic semantic reasoning. Additionally, we build a computation model to abstract the stochastic scheduling process of CM. Finally, we design an iteration algorithm to solve the computation model through the interaction between the information model and computation model. In experiments, we show that for random arrivals of products, our proposal can ensure the timeliness of the learning and decision-making, and the task assignment performance is the best compared with the other two methods.}
}
@article{MCLEISH2019155,
title = {Emergence and topological order in classical and quantum systems},
journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
volume = {66},
pages = {155-169},
year = {2019},
issn = {1355-2198},
doi = {https://doi.org/10.1016/j.shpsb.2019.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S1355219817302034},
author = {Tom McLeish and Mark Pexton and Tom Lancaster},
abstract = {There has been growing interest in systems in condensed matter physics as a potential source of examples of both epistemic and ontological emergence. One of these case studies is the fractional quantum Hall state (FQHS). In the FQHS a system of electrons displays a type of holism due to a pattern of long-range quantum entanglement that some argue is emergent. Indeed, in general, quantum entanglement is sometimes cited as the best candidate for one form of ontological emergence. In this paper we argue that there are significant formal and physical parallels between the quantum FQHS and classical polymer systems. Both types of system cannot be explained simply by considering an aggregation of local microphysical properties alone, since important features of each are globally determined by topological features. As such, we argue that if the FQHS is a case of ontological emergence then it is not due to the quantum nature of the system and classical polymer systems are ontologically emergent as well.}
}
@article{MUIR2023S1,
title = {Sharing across the space: Introduction to a special issue on bridging Indigenous and non-Indigenous knowledge systems},
journal = {Journal of Great Lakes Research},
volume = {49},
pages = {S1-S11},
year = {2023},
note = {Supplement on Bridging Indigenous and non-Indigenous Knowledge Systems},
issn = {0380-1330},
doi = {https://doi.org/10.1016/j.jglr.2023.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0380133023000898},
author = {A.M. Muir and A.T. Duncan and K. Almack and N. Boucher and E.S. Dunlop and C. Febria and J.T. Ives and R. Lauzon and H. Lickers and W.P. Mattes and D. McGregor and H. McGregor and A.J. Reid},
keywords = {Water, Partnership, Reconciliation, Language, Wise practices, Two-eyed seeing},
abstract = {This special issue contains 16 articles inspired from a session at the 2021 64th International Association for Great Lakes Research Annual Meeting entitled: “Bridging Knowledge Systems between Indigenous and non-Indigenous communities.” Four common themes associated with bridging knowledge systems emerged from the collection of articles herein. First, wise practices should form the foundation of ethical, responsive, and productive collaborations. Second, inclusive, and accessible practices can improve our ability to bridge knowledge systems. Third, celebrating and embracing diverse languages and cultures enriches our connection to and understanding of the world around us; languages and cultures are a critical aspect of ontology and expression of knowledge that cut across all articles contained in this issue. Fourth, constructs, such as Etuaptmumk or Two-Eyed Seeing, can help build mutual and equitable relationships drawing on strengths of both Indigenous and non-Indigenous knowledge bases. Lessons in applying knowledge-bridging constructs are contained throughout the collection of articles. Indigenous knowledges are a rich source of experiential learning that can no longer be ignored. Creating ethical spaces for co-production of knowledge, co-learning, and joint stewardship is critical to our future and our ability to uphold Indigenous rights today. Throughout this issue, many elements of guidance are offered as ways to begin building the relationships required to bridge knowledge systems in a good way. We intend this collection to further relationship-building and ultimately trust-building among Indigenous and non-Indigenous Peoples and communities.}
}
@article{REY2023e20505,
title = {Transformer-based approach to variable typing},
journal = {Heliyon},
volume = {9},
number = {10},
pages = {e20505},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e20505},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023077137},
author = {Charles Arthel Rey and Jose Lorenzo Danguilan and Karl Patrick Mendoza and Miguel Francisco Remolona},
keywords = {Natural language processing, Transformers, Entity recognition, Relation extraction, Variable typing, Machine learning, Mathematical knowledge},
abstract = {The upsurge of multifarious endeavors across scientific fields propelled Big Data in the scientific domain. Despite the advancements in management systems, researchers find that mathematical knowledge remains one of the most challenging to manage due to the latter's inherent heterogeneity. One novel recourse being explored is variable typing where current works remain preliminary and, thus, provide a wide room for contribution. In this study, a primordial attempt to implement the end-to-end Entity Recognition (ER) and Relation Extraction (RE) approach to variable typing was made using the BERT (Bidirectional Encoder Representations from Transformers) model. A micro-dataset was developed for this process. According to our findings, the ER model and RE model, respectively, have Precision of 0.8142 and 0.4919, Recall of 0.7816 and 0.6030, and F1-Scores of 0.7975 and 0.5418. Despite the limited dataset, the models performed at par with values in the literature. This work also discusses the factors affecting this BERT-based approach, giving rise to suggestions for future implementations.}
}
@article{SOMMERFELDT2023102369,
title = {OPR and its corruption of “publics”: A critique},
journal = {Public Relations Review},
volume = {49},
number = {4},
pages = {102369},
year = {2023},
issn = {0363-8111},
doi = {https://doi.org/10.1016/j.pubrev.2023.102369},
url = {https://www.sciencedirect.com/science/article/pii/S036381112300084X},
author = {Erich J. Sommerfeldt and Robert L. Heath},
keywords = {Publics, OPR, Organization-public relationships, Stakeholders, Critical theory},
abstract = {This paper advances existing critiques of the organization-public relationship (OPR) research tradition by explicating the poor ontological conceptualization of publics in OPR research as well as related methodological concerns. The authors critically analyze the publics and OPR literature, furthering past criticisms and raising new questions about the heuristic value of OPR research. The paper points out three major weaknesses of the OPR tradition in its use of publics: (1) serious vacillation in terms resulting in conceptual inconsistencies, (2) inappropriate operationalization of variables and the use of probability sampling, and (3) a lack of insightful context and inclusion of relationship factors in OPR studies, to which publics are inextricably bound. The critique cautions scholars engaged in OPR research to improve their literature reviews and to enlarge their methodologies beyond a narrow scope of relational variables, which are theoretically fragile if not devoid of research value. Finally, the critique suggests the importance of conceptualizing publics in ways that acknowledge their agency.}
}
@incollection{KUILER2020127,
title = {7 - Knowledge formulation in the health domain: a semiotics-powered approach to data analytics and democratization},
editor = {Feras A. Batarseh and Ruixin Yang},
booktitle = {Data Democracy},
publisher = {Academic Press},
pages = {127-146},
year = {2020},
isbn = {978-0-12-818366-3},
doi = {https://doi.org/10.1016/B978-0-12-818366-3.00007-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128183663000071},
author = {Erik W. Kuiler and Connie L. McNeely},
keywords = {Big data, Data analytics, Health, Knowledge},
abstract = {1Something stands for something else.2[1, p. 240].Developments in knowledge-based systems and information and communications technology (ICT) occupy increasingly important positions in the health domain. The advent of ICT has enabled “big data” analytics that emphasize leveraging large, complex datasets to manage population health, drive down disease rates, and control costs. Moreover, knowledge formulation, as well as the growth and application of ICT-supported knowledge-based systems, has important implications for the practice of medicine, healthcare distribution, evidence-based health policy making, and professional research agendas. A conceptual analytical framework is presented that encompasses different aspects of knowledge formulation based on semiotics-focused interdependencies. Semiotics is normative, bounded by epistemic, and cultural contexts and provides the foundation for ontology development. Knowledge formulation depends on ontologies to provide repositories for formal specifications of the meanings of symbols delineated by semiotics. Accordingly, the framework posits and addresses semiotics-related pragmatics through two kinds of paradigms: (1) a model-based analytics paradigm that reflects the needs of health researchers and (2) a heuristics-based analytics paradigm that is appropriate for medical staff, patients, and other nonresearch-oriented end-user communities. The framework supports data democratization through the operationalization of these paradigms.}
}
@article{LEPSKIY2021622,
title = {Systems Analysis of the Foundations for the Formation of new Paradigms of Control},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {13},
pages = {622-626},
year = {2021},
note = {20th IFAC Conference on Technology, Culture, and International Stability TECIS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.10.519},
url = {https://www.sciencedirect.com/science/article/pii/S240589632101956X},
author = {Vladimir Lepskiy},
keywords = {control, cybernetics, philosophy of science, scientific rationality, paradigm},
abstract = {At the beginning of the 21st century, an unsystematic variety of paradigms and ontologies of cybernetics is growing, which complicates the improvement of control mechanisms and the implementation of digital technologies and artificial intelligence. We propose systemic foundations that will allow the systematization of the developed and future paradigms and ontologies of cybernetics. The analysis of separate paradigms of cybernetics of the second and third order is carried out. The evolution of paradigms of cybernetics is considered on the basis of modern concepts of scientific rationality, and an adequate form of third-order cybernetics of self-developing poly-subject (reflexive-active) environments is substantiated. The system of principles and trends in the control of social systems is proposed as an additional tool for analyzing new paradigms of cybernetics.}
}
@article{MILLECAM20211922,
title = {Coming of age of Allotrope: Proceedings from the Fall 2020 Allotrope Connect},
journal = {Drug Discovery Today},
volume = {26},
number = {8},
pages = {1922-1928},
year = {2021},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2021.03.028},
url = {https://www.sciencedirect.com/science/article/pii/S1359644621001653},
author = {Todd Millecam and Austin J. Jarrett and Naomi Young and Dana E. Vanderwall and Dennis {Della Corte}},
keywords = {Precompetitive consortium, Semantics, Metadata, Standard, Harmonization, Laboratory IT, Allotrope, Digital Lab},
abstract = {The Allotrope Foundation (AF) is a group of pharmaceutical, device vendor, and software companies that develops and releases technologies [the Allotrope Data Format (ADF), the Allotrope Foundation Ontology (AFO), and the Allotrope Data Models (ADM)] to simplify the exchange of electronic data. We present here the first comprehensive history of the AF, its structure, a list of members and partners, and an introduction to the technologies. Finally, we provide current insights into the adoption and development of the technologies by summarizing the Fall 2020 Allotrope Connect virtual conference. This overview provides an easy access to the AF and highlights opportunities for collaboration.}
}
@article{CAO2023102201,
title = {A hybrid approach to system verification in early design for complex mechatronic systems based on formal functional semantics},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102201},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102201},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623003294},
author = {Yue Cao and Yusheng Liu and Xujia Qin},
keywords = {System verification, System design, Mechatronic systems, Formal verification},
abstract = {Model-based systems engineering (MBSE) has been adopted as a mainstream design methodology for complex mechatronic systems. According to this paradigm, structured models, typically represented in SysML are used to describe the system of interest from various aspects. Among these models, the system behavior model is fundamental for the subsequent design and hence should be verified against system functions in the early design stage. However, due to the lack of formal semantics of the involved SysML models and software-physical hybrid characteristics of the mechatronic systems, it is not a trivial task to apply existing formal verification methods to solve this problem. In this study, a novel approach relying on hybrid functional semantics is proposed. First, this verification problem is formally defined based on an in-depth analysis of the architectures and traceability between the involved SysML models. Then, a graphical modeling language to represent the hybrid functional semantics in SysML is defined so that the function and behavior models can be enhanced and explicitly correlated by the formal semantics. Finally, based on the semantically enhanced models, the model verification problem is formally re-defined as a semantics verification problem and further divided into two sub-problems, i.e., continuous and discrete semantics verification problems, which are automatically solved by leveraging existing verification techniques including functional-effect compatibility check and symbolic model checking. A mobile robot is illustrated to show the effectiveness of the proposed approach.}
}
@article{VIDAL2025100856,
title = {Integrating Knowledge Graphs with Symbolic AI: The Path to Interpretable Hybrid AI Systems in Medicine},
journal = {Journal of Web Semantics},
volume = {84},
pages = {100856},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100856},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000428},
author = {Maria-Esther Vidal and Yashrajsinh Chudasama and Hao Huang and Disha Purohit and Maria Torrente},
keywords = {Knowledge Graphs, Neuro-symbolic systems, Semantic Data Management, Valid link prediction, Counterfactual prediction, KG-based applications},
abstract = {Knowledge Graphs (KGs) are graph-based structures that integrate heterogeneous data, capture domain knowledge, and enable explainable AI through symbolic reasoning. This position paper examines the challenges and research opportunities in integrating KGs with neuro-symbolic AI, highlighting their potential to enhance explainability, scalability, and context-aware reasoning in hybrid AI systems. Using a lung cancer use case, we illustrate how hybrid approaches address tasks such as link prediction—uncovering hidden relationships in medical data—and counterfactual reasoning—analyzing alternative scenarios to understand causal factors. The discussion is framed around TrustKG, which demonstrates how constraint validation, causal reasoning, and user-centric communication can support transparent and reliable decision-making. Additionally, we identify current limitations of KGs, including gaps in knowledge coverage, evolving data integration challenges, and the need for improved usability and impact assessment. These insights are not limited to healthcare but extend to other domains like energy, manufacturing, and mobility, showcasing the broad applicability of KGs. Finally, we propose research directions to unlock their full potential in building robust, transparent, and widely adopted real-world applications.}
}
@article{NASRALAH2020,
title = {Social Media Text Mining Framework for Drug Abuse: Development and Validation Study With an Opioid Crisis Case Analysis},
journal = {Journal of Medical Internet Research},
volume = {22},
number = {8},
year = {2020},
issn = {1438-8871},
doi = {https://doi.org/10.2196/18350},
url = {https://www.sciencedirect.com/science/article/pii/S1438887120012777},
author = {Tareq Nasralah and Omar El-Gayar and Yong Wang},
keywords = {drug abuse, social media, infodemiology, infoveillance, text mining, opioid crisis},
abstract = {Background
Social media are considered promising and viable sources of data for gaining insights into various disease conditions and patients’ attitudes, behaviors, and medications. They can be used to recognize communication and behavioral themes of problematic use of prescription drugs. However, mining and analyzing social media data have challenges and limitations related to topic deduction and data quality. As a result, we need a structured approach to analyze social media content related to drug abuse in a manner that can mitigate the challenges and limitations surrounding the use of such data.
Objective
This study aimed to develop and evaluate a framework for mining and analyzing social media content related to drug abuse. The framework is designed to mitigate challenges and limitations related to topic deduction and data quality in social media data analytics for drug abuse.
Methods
The proposed framework started with defining different terms related to the keywords, categories, and characteristics of the topic of interest. We then used the Crimson Hexagon platform to collect data based on a search query informed by a drug abuse ontology developed using the identified terms. We subsequently preprocessed the data and examined the quality using an evaluation matrix. Finally, a suitable data analysis approach could be used to analyze the collected data.
Results
The framework was evaluated using the opioid epidemic as a drug abuse case analysis. We demonstrated the applicability of the proposed framework to identify public concerns toward the opioid epidemic and the most discussed topics on social media related to opioids. The results from the case analysis showed that the framework could improve the discovery and identification of topics in social media domains characterized by a plethora of highly diverse terms and lack of a commonly available dictionary or language by the community, such as in the case of opioid and drug abuse.
Conclusions
The proposed framework addressed the challenges related to topic detection and data quality. We demonstrated the applicability of the proposed framework to identify the common concerns toward the opioid epidemic and the most discussed topics on social media related to opioids.}
}
@article{SAMOURKASIDIS2020105171,
title = {A semantic approach for timeseries data fusion},
journal = {Computers and Electronics in Agriculture},
volume = {169},
pages = {105171},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2019.105171},
url = {https://www.sciencedirect.com/science/article/pii/S0168169919318514},
author = {Argyrios Samourkasidis and Ioannis N. Athanasiadis},
keywords = {Environmental timeseries, Internet of Things, Legacy data, Semantic heterogeneity, Templates, FAIR data, Reasoning, Interoperability, Data reuse, APSIM, AgMIP, DSSAT, WOFOST},
abstract = {The data deluge following the rise of Internet of Things contributes towards the creation of non-reusable data silos. Especially in the environmental sciences domain, syntactic and semantic heterogeneity hinders data re-usability as most times manual labour and domain expertise is required. Both the different syntaxes under which environmental timeseries are formatted and the implicit semantics which are used to describe them contribute to this end. Usually, the real meaning of data is obscured in a combination of short data labels, titles and various value codes, that require domain or institutional knowledge to decipher. The FAIR data principles for scientific data sharing are stewardship offer a framework based on community-adopted metadata. In this work, we present the Environmental Data Acquisition Module (EDAM) which focuses on data interoperability and reuse, and deals with syntactic and semantic heterogeneity using a template approach. Data curators draft templates to describe in an abstract fashion the syntax of the timeseries datasets they want to acquire or disseminate. They complement each template with a metadata file, which is used to annotate observables and their properties (including physical quantities and units of measurement) with terms from an ontology. EDAM employs a reasoner to infer compatibility among syntactically and semantically heterogeneous datasets, and enables timeseries, format and units of measurement transformation on-the-fly. Our approach utilizes a local ontology to store metadata about datasets, which enables EDAM to acquire and transform datasets which were originally stored with different semantics and syntaxes. We demonstrate EDAM in a case study where we transform meteorological input files of four agricultural models. Our approach, allows to cut across environmental data silos and facilitate timeseries reusability, as it enables users to (a) discover datasets in other formats, (b) transform them and (c) reuse them in their scientific workflows. This directly contributes to the toolshed for FAIR data management in environmental sciences. EDAM implementation has been released under an open-source license.}
}
@incollection{BORELLI2025,
title = {Unveiling Pain Semantics Through Lexical Norms},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00835-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041008358},
author = {Eleonora Borelli},
keywords = {Language, Lexical norms, Pain, Physical pain, Semantics, Social pain, Words},
abstract = {Language is the primary medium through which individuals express their experiences of physical and social pain. Research highlights a bidirectional relationship between pain-related language and pain perception, as well as a distinctive status of pain-related words compared to general negative words, emphasizing the need for the development of lexical norms to deepen our understanding. The Words of Pain (WOP) database addresses this need in Italian, providing comprehensive normative data that enable the creation of controlled stimuli and the exploration of pain semantics. Analyses reveal distinct processing patterns for pain-related versus non-pain-related words and unique cognitive and emotional profiles for physical and social pain. Developing comparable lexical norms in other languages is essential for achieving a globally comprehensive understanding of pain language and its cognitive and emotional underpinnings.}
}
@article{SHANG2024,
title = {Electronic Health Record–Oriented Knowledge Graph System for Collaborative Clinical Decision Support Using Multicenter Fragmented Medical Data: Design and Application Study},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/54263},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124003595},
author = {Yong Shang and Yu Tian and Kewei Lyu and Tianshu Zhou and Ping Zhang and Jianghua Chen and Jingsong Li},
keywords = {knowledge graph, electronic health record, ontology, data fragmentation, data privacy, knowledge graphs, visualization, ontologies, data science, privacy, security, collaborative, collaboration, kidney, CKD, nephrology, EHR, health record, hypernym, encryption, encrypt, encrypted, decision support, semantic, vocabulary, blockchain},
abstract = {Background
The medical knowledge graph provides explainable decision support, helping clinicians with prompt diagnosis and treatment suggestions. However, in real-world clinical practice, patients visit different hospitals seeking various medical services, resulting in fragmented patient data across hospitals. With data security issues, data fragmentation limits the application of knowledge graphs because single-hospital data cannot provide complete evidence for generating precise decision support and comprehensive explanations. It is important to study new methods for knowledge graph systems to integrate into multicenter, information-sensitive medical environments, using fragmented patient records for decision support while maintaining data privacy and security.
Objective
This study aims to propose an electronic health record (EHR)–oriented knowledge graph system for collaborative reasoning with multicenter fragmented patient medical data, all the while preserving data privacy.
Methods
The study introduced an EHR knowledge graph framework and a novel collaborative reasoning process for utilizing multicenter fragmented information. The system was deployed in each hospital and used a unified semantic structure and Observational Medical Outcomes Partnership (OMOP) vocabulary to standardize the local EHR data set. The system transforms local EHR data into semantic formats and performs semantic reasoning to generate intermediate reasoning findings. The generated intermediate findings used hypernym concepts to isolate original medical data. The intermediate findings and hash-encrypted patient identities were synchronized through a blockchain network. The multicenter intermediate findings were collaborated for final reasoning and clinical decision support without gathering original EHR data.
Results
The system underwent evaluation through an application study involving the utilization of multicenter fragmented EHR data to alert non-nephrology clinicians about overlooked patients with chronic kidney disease (CKD). The study covered 1185 patients in nonnephrology departments from 3 hospitals. The patients visited at least two of the hospitals. Of these, 124 patients were identified as meeting CKD diagnosis criteria through collaborative reasoning using multicenter EHR data, whereas the data from individual hospitals alone could not facilitate the identification of CKD in these patients. The assessment by clinicians indicated that 78/91 (86%) patients were CKD positive.
Conclusions
The proposed system was able to effectively utilize multicenter fragmented EHR data for clinical application. The application study showed the clinical benefits of the system with prompt and comprehensive decision support.}
}
@article{SKOBELEV2019154,
title = {Development of a Knowledge Base in the “Smart Farming” System for Agricultural Enterprise Management},
journal = {Procedia Computer Science},
volume = {150},
pages = {154-161},
year = {2019},
note = {Proceedings of the 13th International Symposium “Intelligent Systems 2018” (INTELS’18), 22-24 October, 2018, St. Petersburg, Russia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.02.029},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919303734},
author = {P.O. Skobelev and E.V. Simonova and S.V. Smirnov and D.S. Budaev and G.Yu. Voshchuk and A.L. Morokov},
keywords = {knowledge base, Aristotle metaontology, ontology of plant growing, semantic network, multi-agent technology, smart decision support system},
abstract = {Increasing the efficiency of agricultural production is a very important task. To solve this problem, it is proposed to use the “Smart Farming” cloud system for precision farming management. The novelty of the proposed approach lies in using the knowledge base and multi-agent technology to develop coordinated decisions on management of agricultural enterprises. The paper focuses on development of the knowledge base in the "Smart Farming" system on precision agriculture. Information storage is organized in the form of a semantic network of concepts and relations on the "All about the concept" principle in a single repository, which facilitates the work of farmers with this resource. The paper covers storage, editing, verification, and visualization of knowledge representation about the domain of crop production, production resources, agricultural machinery, equipment and other material resources, as well as peculiarities of the tasks of precision farming. The knowledge base of plant production, built on ontological principles, will be useful to enterprise managers, agronomists, machine operators, planning services and other specialists of large, medium and small farms, as well as to individual farmers.}
}
@article{SUN201968,
title = {Exploring eWOM in online customer reviews: Sentiment analysis at a fine-grained level},
journal = {Engineering Applications of Artificial Intelligence},
volume = {81},
pages = {68-78},
year = {2019},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2019.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0952197619300272},
author = {Qing Sun and Jianwei Niu and Zhong Yao and Hao Yan},
keywords = {Sentiment analysis, eWOM, Fuzzy product ontology},
abstract = {Customer reviews in social media and electronic commerce Web sites contain valuable electronic word-of-mouth (eWOM) information of products, which facilitates firms’ business strategy and individual consumers’ comparison shopping. Exploring eWOM of products embedded in customer reviews has attracted interest from researchers in various fields. Coarse-grained and context-free sentiment analysis approaches have been used in existing researches, which however often fail to satisfy the firms’ demands of fine-grained extraction of market intelligence from social media. In this study, we propose an original method to explore eWOM of products based on sentiment analysis at fine-grained level from a large volume of online customer reviews. We illustrate a feature-based and context-sensitive sentiment analysis mechanism that can leverage the sheer volume of customer reviews in social media sites. A novel semi-supervised fuzzy product ontology mining algorithm is proposed to extract semantic knowledge from online customer reviews with positive or negative labels. Based on real-world online customer review data set, the proposed method shows remarkable performance improvement over baseline methods at exploring eWOM of product a fine-grained level. With the novel eWOM exploring method, firms can improve their product design and marketing strategies, and potential consumers can make better online purchase decisions.}
}
@article{XI2025301846,
title = {Towards a joint semantic analysis in mobile forensics environments},
journal = {Forensic Science International: Digital Investigation},
volume = {52},
pages = {301846},
year = {2025},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2024.301846},
url = {https://www.sciencedirect.com/science/article/pii/S2666281724001732},
author = {Jian Xi and Melanie Siegel and Dirk Labudde and Michael Spranger},
keywords = {Semantic analysis, Mobile forensics, Topic modeling, Natural language processing, Multimodal machine learning, Communication analysis, Text mining, Semantic network},
abstract = {In recent years, mobile devices have become the dominant communication medium in our daily lives. This trend is also evident in the planning, arranging, and committing of criminal activities, particularly in organized crime. Accordingly, mobile devices have become an essential source of evidence for data analysts or investigators, especially in Law Enforcement Agencies (LEAs). However, communication via mobile devices generates vast amounts of data, rendering manual analysis impractical and resulting in growing backlogs of evidence awaiting analysis process, which can take months to years, thereby hindering investigations and trials. The automatic analysis of textual chat messages falls short because communication is not limited to the single modality, such as text, but instead spans multiple modalities, including voice messages, pictures, videos, and sometimes various messengers (channels). These modalities frequently overlap or interchange within the same communication, further complicating the analysis process. To achieve a correct and comprehensive understanding of such communication, it is essential to consider all modalities and channels through a consistent joint semantic analysis. This paper introduces a novel mobile forensics approach that enables efficient assessment of mobile data without losing semantic consistency by unifying semantic concepts across different modalities and channels. Additionally, a knowledge-guided topic modeling approach is proposed, integrating expertise into the investigation process to effectively examine large volumes of noisy mobile data. In this way, investigators can quickly identify evidentiary parts of the communication and completely facilitate reconstructing the course of events.}
}
@article{XU2020103006,
title = {Semantic approach to compliance checking of underground utilities},
journal = {Automation in Construction},
volume = {109},
pages = {103006},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.103006},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519305606},
author = {Xin Xu and Hubo Cai},
keywords = {Ontology, Data integration, Semantic reasoning, Utility compliance checking},
abstract = {Utility regulations stipulate the spatial configurations between underground utilities and their surroundings to avoid interferences and disruptions of utility services. Utility compliance checking aims to detect spatial non-compliances in underground utilities by examining geospatial data of utilities and their surroundings against textual data of utility regulations. However, the integration of heterogeneous utility geospatial and textual data for compliance checking remains a big challenge. This paper presents a semantic approach to integrate heterogeneous data and enable automated compliance checking of underground utilities through logic and spatial reasoning. The approach consists of the following key components: (1) four interlinked ontologies that provide the semantic schema for heterogeneous data relevant to utility compliance checking, (2) two data convertors for the conversion of heterogeneous data from proprietary formats into a common and interoperable format following the semantic schema, and (3) a query mechanism with spatial extensions for the detection of non-compliant utility instances. The approach was tested on a sample utility database, and the results demonstrate the success of the proposed approach in the integration of heterogeneous data from multiple sources and automated detection of spatial non-compliances in underground utilities. In addition to utility compliance checking, the approach can be extended to other application cases where both data integration from multiple sources and spatial reasoning are required.}
}
@article{MIKELS2025100686,
title = {Defining community: A pragmatic political ecology approach},
journal = {Environmental and Sustainability Indicators},
volume = {26},
pages = {100686},
year = {2025},
issn = {2665-9727},
doi = {https://doi.org/10.1016/j.indic.2025.100686},
url = {https://www.sciencedirect.com/science/article/pii/S2665972725001072},
author = {Jessica Mikels and Rachel Menale},
keywords = {American and French pragmatism, Political ecology, Redefining community, Human/nature divide},
abstract = {There is a growing recognition of the need to bridge the ecological and the social to best respond to the myriad of socio-environmental challenges of the Anthropocene. While this recognition has grown, we often lack the language for non-dualistic thinking of all species’ needs. In this brief, theory-focused piece, we propose a non-dualistic definition of community that may provide an orienting framework toward a more inclusive and holistic idea of community in community conservation. This concept aims to simultaneously consider both human and more-than-human species, linking human environmental justice with the conservation of more-than-human species through the lens of Pragmatic Political Ecology. We outline the philosophical foundations of this idea, develop indicators for assessing if projects meet the definition proposed, and provide examples of existing work that utilize this approach in community conservation.}
}
@article{LI2025106005,
title = {Towards worker-centric construction scene understanding: Status quo and future directions},
journal = {Automation in Construction},
volume = {171},
pages = {106005},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106005},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525000457},
author = {Huimin Li and Hui Deng and Yichuan Deng},
keywords = {Scene understanding, Construction site, Computer vision, Image captioning, Construction worker},
abstract = {Construction scene understanding is the process of perceiving, analyzing, and interpreting three-dimensional dynamic scenes observed through sensor networks, which is usually real-time. The purpose is to understand the construction scene by analyzing the geometric and semantic features of the objects and their relationships. Construction scene understanding is a basic technology for construction automation and Human-Robot Interaction. Although there are several reviews on this topic, existing reviews do not cover the latest technological advances and lack systematic elaboration for construction scene understanding. Therefore, this paper reviews research on construction scene understanding conducted over the past fifteen years, summarizing five key scene elements: signals, pixels, objects, relationships, and events, along with the richness of semantic information. It also outlines advancements in perception and reasoning abilities in this domain. In addition, this review proposes five research trends and seven future directions to provide some inspiration for researchers.}
}
@article{BOSHKOSKA201964,
title = {A decision support system for evaluation of the knowledge sharing crossing boundaries in agri-food value chains},
journal = {Computers in Industry},
volume = {110},
pages = {64-80},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S0166361518305773},
author = {Biljana Mileva Boshkoska and Shaofeng Liu and Guoqing Zhao and Alejandro Fernandez and Susana Gamboa and Mariana {del Pino} and Pascale Zarate and Jorge Hernandez and Huilan Chen},
keywords = {Knowledge sharing, Knowledge boundaries, Decision support system, Agricultural value chain},
abstract = {An agri-food value chain (VC) represents a set of activities aimed at delivering highly valuable products to the market. Due to the diversity of actors in the agri-food VCs´ accumulated knowledge is typically situated within the boundaries of each entity of the VC. Hence, the question is how to improve knowledge sharing in agri-food VC, or more specifically how can knowledge flow and mobilize among different actors in the VC. To answer this question, we present a decision support system (DSS) for evaluation of knowledge sharing crossing boundaries in agri-food VC. The proposed DSS is developed through two phases: (i) identification of the most common knowledge boundaries by using machine learning and ontology technologies; (ii) transformation of the obtained ontology into a DSS for the evaluation of existing knowledge boundaries. In particular, the developed DSS helps in identifying, evaluating and providing directions for improvement of the knowledge sharing crossing boundaries in agri-food VC. We apply the DSS to evaluate three real VCs: a tomato VC in Argentina, a Chinese leaf VC in China and a brassica VC in the UK. The comparative analysis across the three varied case studies and their evaluation with the proposed DSS lead to more insights into knowledge-based decisions that a particular VC needs to address to improve its knowledge flow, in particular, to obtain insights in the transparency and interoperability of data and knowledge crossing boundaries in agri-food VCs.}
}
@article{ABBADANDALOUSSI2020101505,
title = {On the declarative paradigm in hybrid business process representations: A conceptual framework and a systematic literature study},
journal = {Information Systems},
volume = {91},
pages = {101505},
year = {2020},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2020.101505},
url = {https://www.sciencedirect.com/science/article/pii/S0306437920300168},
author = {Amine {Abbad Andaloussi} and Andrea Burattin and Tijs Slaats and Ekkart Kindler and Barbara Weber},
keywords = {Hybrid process model, Understandability of process models, Process flexibility, Declarative process modeling, Business process modeling},
abstract = {Process modeling plays a central role in the development of today’s process-aware information systems both on the management level (e.g., providing input for requirements elicitation and fostering communication) and on the enactment level (providing a blue-print for process execution and enabling simulation). The literature comprises a variety of process modeling approaches proposing different modeling languages (i.e., imperative and declarative languages) and different types of process artifact support (i.e., process models, textual process descriptions, and guided simulations). However, the use of an individual modeling language or a single type of process artifact is usually not enough to provide a clear and concise understanding of the process. To overcome this limitation, a set of so-called “hybrid” approaches combining languages and artifacts have been proposed, but no common grounds have been set to define and categorize them. This work aims at providing a fundamental understanding of these hybrid approaches by defining a unified terminology, providing a conceptual framework and proposing an overarching overview to identify and analyze them. Since no common terminology has been used in the literature, we combined existing concepts and ontologies to define a “Hybrid Business Process Representation” (HBPR). Afterwards, we conducted a Systematic Literature Review (SLR) to identify and investigate the characteristics of HBPRs combining imperative and declarative languages or artifacts. The SLR resulted in 30 articles which were analyzed. The results indicate the presence of two distinct research lines and show common motivations driving the emergence of HBPRs, a limited maturity of existing approaches, and diverse application domains. Moreover, the results are synthesized into a taxonomy classifying different types of representations. Finally, the outcome of the study is used to provide a research agenda delineating the directions for future work.}
}
@article{TOTH2023102260,
title = {The human-centric Industry 5.0 collaboration architecture},
journal = {MethodsX},
volume = {11},
pages = {102260},
year = {2023},
issn = {2215-0161},
doi = {https://doi.org/10.1016/j.mex.2023.102260},
url = {https://www.sciencedirect.com/science/article/pii/S2215016123002571},
author = {Attila Tóth and László Nagy and Roderick Kennedy and Belej Bohuš and János Abonyi and Tamás Ruppert},
keywords = {Collaboration, Industry 5.0, Artificial intelligence, Manufacturing, Industry 4.0, Human-centric, Ontology, Knowledge graph},
abstract = {While the primary focus of Industry 4.0 revolves around extensive digitalization, Industry 5.0, on the other hand, seeks to integrate innovative technologies with human actors, signifying an approach that is more value-driven than technology-centric. The key objectives of the Industry 5.0 paradigm, which were not central to Industry 4.0, underscore that production should not only be digitalized but also resilient, sustainable, and human-centric. This paper is focusing on the human-centric pillar of Industry 5.0. The proposed methodology addresses the need for a human-AI collaborative process design and innovation approach to support the development and deployment of advanced AI-driven co-creation and collaboration tools. The method aims to solve the problem of integrating various innovative agents (human, AI, IoT, robot) in a plant-level collaboration process through a generic semantic definition, utilizing a time event-driven process. It also encourages the development of AI techniques for human-in-the-loop optimization, incorporating cross-checking with alternative feedback loop models. Benefits of this methodology include the Industry 5.0 collaboration architecture (I5arc), which provides new adaptable, generic frameworks, concepts, and methodologies for modern knowledge creation and sharing to enhance plant collaboration processes. •The I5arc aims to investigate and establish a truly integrated human-AI collaboration model, equipped with methods and tools for human-AI driven co-creation.•Provide a framework for the co-execution of processes and activities, with humans remaining empowered and in control.•The framework primarily targets human-AI collaboration processes and activities in industrial plants, with potential applicability to other societal contexts.}
}
@article{LI2025154129,
title = {Bioinformatics identification of shared signaling pathways and core targets linking Benzo[a]pyrene exposure to HCC progression},
journal = {Toxicology},
volume = {514},
pages = {154129},
year = {2025},
issn = {0300-483X},
doi = {https://doi.org/10.1016/j.tox.2025.154129},
url = {https://www.sciencedirect.com/science/article/pii/S0300483X2500085X},
author = {Yong-Le Li and Rong He and Meng Tang and Jing-Yi Lan and Guo-Yang Liu and Li-He Jiang},
keywords = {BaP, HCC, Single-cell sequencing, Molecular docking, PPI},
abstract = {With the increasing prevalence of environmental pollutants, there is growing concern about the potential effects of these substances in major diseases such as liver cancer. Previous studies have suggested that various chemicals, such as benzo[a]pyrene(BaP), produced by burning carbon containing fuels, may negatively affect liver health, but the exact mechanisms remain unclear. This study aimed to explore the potential molecular mechanisms of BaP in the progression of liver cancer. Through an exhaustive study of databases such as ChEMBL, SwissTargetPrediction, STITCH and TCGA, we identified 169 potential targets that are closely related to BaP and liver cancer. Next, we conducted Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment analyses using the clusterProfiler package to study the biological functions and important pathways of potential targets induced by BaP, which showed that these targets were associated with mitochondrial function, cellular energy metabolism and REDOX reactions. The protein interaction (PPI) network was constructed using the STRING database and Cytoscape software to identify the core targets UBA52, NDUFS8, CYP1A2, NDUFS1 and CYP3A4. The interaction between BaP and these core proteins was further analyzed via molecular docking using the CB-Dock2 database, demonstrating high binding stability, which suggests their critical role in BaP-induced hepatocellular carcinoma (HCC) toxicity. Subsequently, we found significant differences in the expression of five core genes (UBA52, NDUFS8, CYP1A2, NDUFS1, CYP3A4) in HCC, and significant correlation between UBA52, NDUFS8 and CYP3A4 and survival of HCC patients. Single-cell sequencing analysis showed that the expression of UBA52 gene was particularly pronounced in the three immune cells.}
}
@article{N2025504,
title = {Retrieval-Augmented Generation for Multiple-Choice Questions and Answers Generation},
journal = {Procedia Computer Science},
volume = {259},
pages = {504-511},
year = {2025},
note = {Sixth International Conference on Futuristic Trends in Networks and Computing Technologies (FTNCT06), held in Uttarakhand, India},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.03.352},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925010968},
author = {Pradeesh N and Remya T and MG Thushara and K Arun Krishna and Pranav V},
keywords = {RAG, Education, Question Generation, LLM},
abstract = {Generating a large volume of diverse questions for educational purposes can be a challenging and time-consuming task for educators. Traditional methods like manually writing questions or using templates often fall short when it comes to variety and relevance. In order to cope with these difficulties, we have examined a different approach adopting Retrieval-Augmented Generation ( RAG). RAG is an enhancement technique that allows retrieving independent documents and combines it with artificial intelligence that is able to qualitatively generate contextually adequate questions. In this research, RAG was employed through the Ample LMS platform in which PDF documents were the source of generating MCQ questions. The results of the research indicate that with this method, the process of formulating the questions is relatively faster and the questions generated are of better and diverse in nature. With RAG, the burden of constructing questions is lessened for teachers and the students are provided with better interactive learning opportunities. This approach presents a better alternative that is flexible and suitable for various educational requirements, demonstrating the effectiveness of AI in the improvement of the learning and teaching processes.}
}
@article{RANDEZ2023100057,
title = {Using the listening guide method for short story construction: Redefining researcher and participant through immanence},
journal = {Research Methods in Applied Linguistics},
volume = {2},
number = {2},
pages = {100057},
year = {2023},
issn = {2772-7661},
doi = {https://doi.org/10.1016/j.rmal.2023.100057},
url = {https://www.sciencedirect.com/science/article/pii/S2772766123000174},
author = {Robert A. Randez},
abstract = {Narrative inquiry in applied linguistics is rich with different foci, scholarly lineage, and practices (Barkhuizen, 2020). However, a consequence of this variety concerns the analytical rigor and ethical trustworthiness due to a lack of method standardization (Barkhuizen & Consoli, 2021). This article addresses this concern through the onto-epistemological construct of the plane of immanence (Deleuze & Guattari, 1987; St. Pierre, 2019) by expanding the analytical process of short story narrative inquiry (Barkhuizen, 2016, 2017) using the listening guide (Brown & Gilligan, 1992; Woodcock, 2016). Throughout this article, you will see how this ontological, methodological, and method combination recognizes the researcher/participant dynamic in a manner that is more conducive to the reality of narrative inquiry scholarship. Additionally, this article hopes to serve as a methodological guide making short story narrative inquiry more approachable to unfamiliar scholars.}
}
@article{ARRIAGA2025100561,
title = {A Peruvian Machine learning model for E-commerce product matching in South America, Spain and Portugal},
journal = {Journal of Open Innovation: Technology, Market, and Complexity},
volume = {11},
number = {3},
pages = {100561},
year = {2025},
issn = {2199-8531},
doi = {https://doi.org/10.1016/j.joitmc.2025.100561},
url = {https://www.sciencedirect.com/science/article/pii/S2199853125000964},
author = {B. Arriaga and A. Gómez and A. Palacios and W. Aliaga},
keywords = {E-commerce, Intelligent systems, Machine learning, Product matching},
abstract = {The rapid growth of e-Commerce in Latin America, driven by the increase in digital adoption among younger generations and accelerated by the COVID-19 pandemic, has reshaped how businesses engage with consumers. In Peru alone, the number of online shoppers increased by 131% between 2019 and 2021. However, the lack of a standardized global product identifier continues to hinder product comparison across platforms, weakening the Zero Moment of Truth (ZMOT) and reducing consumers’ ability to make informed purchasing decisions. To address this challenge, this study proposes a multimodal product classification model that combines natural language processing and image analysis to identify and match similar products in online retail stores. The model leverages textual embeddings and visual features to overcome inconsistencies in product descriptions and naming conventions, particularly within the Peruvian market. A data set of local product listings was compiled and used to train and evaluate multiple classifiers, the XGBoost model achieving 92. 7% precision and a 93. 6% F1 score. Beyond local performance, the model was tested in additional South American markets, including Argentina, Brazil, Chile, and Colombia, demonstrating robustness against linguistic and cultural differences. The proposed system enables more accurate product discovery, price comparison, and competitor monitoring, offering practical benefits for both consumers and businesses. Ultimately, this work contributes to the advancement of E-Commerce infrastructure in emerging markets and supports more informed and efficient decision-making across diverse retail ecosystems.}
}
@article{HAGHGOO2022101081,
title = {ENTIRETY — sEmanNTIc pRovisioning and govErning ioT devices in smart energY domain},
journal = {SoftwareX},
volume = {18},
pages = {101081},
year = {2022},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2022.101081},
url = {https://www.sciencedirect.com/science/article/pii/S2352711022000589},
author = {Maliheh Haghgoo and Ilya Sychev and Antonello Monti and Frank H.P. Fitzek},
keywords = {Internet of Things, Device provisioning, Semantic IoT provisioning},
abstract = {ENTIRETY is an open-source web application that provides consistency for the devices in IoT platforms with a simple and unified way of governing and provisioning. This service primarily targets smart energy domain applications in a large-scale scenario but can be easily extended to support other domain-specific devices. To overcome the IoT diversity challenges, the core of the ENTIRETY is a SARGON ontology which is the semantic data model designed for cross-cutting the domain-specific information in the smart energy system which is modular, extendable and reusable. Consequently, ENTIRETY is intended on underlying principles such as reusability and extensibility, it applies a template-based approach and can be run as a Docker image.}
}
@article{MARCEL20191,
title = {Special issue on DOLAP 2017: Design, Optimization, Languages and Analytical Processing of Big Data},
journal = {Information Systems},
volume = {79},
pages = {1-2},
year = {2019},
note = {Special issue on DOLAP 2017: Design, Optimization, Languages and Analytical Processing of Big Data},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2018.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0306437918305611},
author = {Patrick Marcel and Il-Yeol Song}
}
@article{LI2020,
title = {A Personalized Voice-Based Diet Assistant for Caregivers of Alzheimer Disease and Related Dementias: System Development and Validation},
journal = {Journal of Medical Internet Research},
volume = {22},
number = {9},
year = {2020},
issn = {1438-8871},
doi = {https://doi.org/10.2196/19897},
url = {https://www.sciencedirect.com/science/article/pii/S1438887120008808},
author = {Juan Li and Bikesh Maharjan and Bo Xie and Cui Tao},
keywords = {Alzheimer disease, dementia, diet, knowledge, ontology, voice assistant},
abstract = {Background
The world’s aging population is increasing, with an expected increase in the prevalence of Alzheimer disease and related dementias (ADRD). Proper nutrition and good eating behavior show promise for preventing and slowing the progression of ADRD and consequently improving patients with ADRD’s health status and quality of life. Most ADRD care is provided by informal caregivers, so assisting caregivers to manage patients with ADRD’s diet is important.
Objective
This study aims to design, develop, and test an artificial intelligence–powered voice assistant to help informal caregivers manage the daily diet of patients with ADRD and learn food and nutrition-related knowledge.
Methods
The voice assistant is being implemented in several steps: construction of a comprehensive knowledge base with ontologies that define ADRD diet care and user profiles, and is extended with external knowledge graphs; management of conversation between users and the voice assistant; personalized ADRD diet services provided through a semantics-based knowledge graph search and reasoning engine; and system evaluation in use cases with additional qualitative evaluations.
Results
A prototype voice assistant was evaluated in the lab using various use cases. Preliminary qualitative test results demonstrate reasonable rates of dialogue success and recommendation correctness.
Conclusions
The voice assistant provides a natural, interactive interface for users, and it does not require the user to have a technical background, which may facilitate senior caregivers’ use in their daily care tasks. This study suggests the feasibility of using the intelligent voice assistant to help caregivers manage patients with ADRD’s diet.}
}
@article{GENG2023100757,
title = {Benchmarking knowledge-driven zero-shot learning},
journal = {Journal of Web Semantics},
volume = {75},
pages = {100757},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2022.100757},
url = {https://www.sciencedirect.com/science/article/pii/S1570826822000415},
author = {Yuxia Geng and Jiaoyan Chen and Xiang Zhuang and Zhuo Chen and Jeff Z. Pan and Juan Li and Zonggang Yuan and Huajun Chen},
keywords = {Zero-shot learning, Knowledge Graph, Image classification, Relation extraction, Knowledge Graph completion, Ontology, Semantic embedding},
abstract = {External knowledge (a.k.a. side information) plays a critical role in zero-shot learning (ZSL) which aims to predict with unseen classes that have never appeared in training data. Several kinds of external knowledge, such as text and attribute, have been widely investigated, but they alone are limited with incomplete semantics. Some very recent studies thus propose to use Knowledge Graph (KG) due to its high expressivity and compatibility for representing kinds of knowledge. However, the ZSL community is still in short of standard benchmarks for studying and comparing different external knowledge settings and different KG-based ZSL methods. In this paper, we proposed six resources covering three tasks, i.e., zero-shot image classification (ZS-IMGC), zero-shot relation extraction (ZS-RE), and zero-shot KG completion (ZS-KGC). Each resource has a normal ZSL benchmark and a KG containing semantics ranging from text to attribute, from relational knowledge to logical expressions. We have clearly presented these resources including their construction, statistics, data formats and usage cases w.r.t. different ZSL methods. More importantly, we have conducted a comprehensive benchmarking study, with a few classic and state-of-the-art methods for each task, including a method with KG augmented explanation. We discussed and compared different ZSL paradigms w.r.t. different external knowledge settings, and found that our resources have great potential for developing more advanced ZSL methods and more solutions for applying KGs for augmenting machine learning. All the resources are available at https://github.com/China-UK-ZSL/Resources_for_KZSL.}
}
@article{SEMJEN2025100253,
title = {Integrating generative and parametric design with BIM: A literature review of challenges and research gaps in construction design},
journal = {Applications in Engineering Science},
volume = {23},
pages = {100253},
year = {2025},
issn = {2666-4968},
doi = {https://doi.org/10.1016/j.apples.2025.100253},
url = {https://www.sciencedirect.com/science/article/pii/S2666496825000512},
author = {Álmos Á. Semjén and János Szép},
keywords = {Parametric design, Building Information Modelling, Construction industry, Optimization, Structural design, Design automation, Generative design, Artificial intelligence},
abstract = {Parametric Design (PD), Generative Design (GD), and Building Information Modelling (BIM) have emerged as transformative tools in the construction industry, offering significant potential for design optimisation, interdisciplinary collaboration, and data-driven decision making. This paper presents a comprehensive literature review to evaluate the current state of PD, GD, and BIM integration, highlighting practical applications and identifying research gaps. In addition to mapping the academic discourse, the review also highlights selected practical implementations from existing literature to illustrate how these technologies are being translated into applied workflows. Furthermore, the methodology section critically reflects on the limitations of the keyword-based search strategy and suggests future directions to mitigate potential literature gaps. While many studies demonstrate efficiency gains in early design phases, the integration of these technologies across the full building lifecycle remains limited. Key challenges include insufficient interoperability between platforms, lack of standardisation, and minimal adoption of GD-BIM combinations in construction and logistics. Furthermore, few studies address the regulatory compliance and real-world scalability of AI-assisted generative models. The review concludes that although these digital methods can accelerate innovation and sustainability, their practical implementation requires further research in construction management, code-based automation, and human-in-the-loop design workflows.}
}
@article{KINCL2025127888,
title = {Comprehensive benchmarking of knowledge graph embeddings methods for Android malware detection},
journal = {Expert Systems with Applications},
volume = {288},
pages = {127888},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127888},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425015106},
author = {Jan Kincl and Tome Eftimov and Adam Viktorin and Roman Šenkeřík and Tanja Pavleska},
keywords = {Mobile android security, Knowledge graphs embeddings, Machine learning, Android malware detection},
abstract = {The rising popularity and open-source model of the Android operating system has made it a main target for attackers creating malware applications. With the mobile industry being an expanding device ecosystem, there is a critical need for developing effective methods to protect against mobile malware. Recognizing the latest approaches and their limitations, we have conducted a comprehensive empirical analysis on the applicability of knowledge graphs for malware detection in view of the influence of the scoring functions, the vector dimension, the stability of the obtained results, the performance of the individual classifiers, and other important time dependencies. In addition, we propose a knowledge-graph based method aimed at improving the quality of classification input data, while offering greater interfacing capabilities with external knowledge and lower computational complexity. The proposed method offers a new perspective on working with Android malware, demonstrating a unique data processing pipeline for malware sample identification and encouraging further innovation in the field. Our findings demonstrate that knowledge graph representation is not only feasible, but also provides well-performing results, remaining competitive with state-of-the-art approaches.}
}
@article{REINHOLD20181120,
title = {Business models in tourism – state of the art},
journal = {Tourism Review},
volume = {74},
number = {6},
pages = {1120-1134},
year = {2018},
issn = {1660-5373},
doi = {https://doi.org/10.1108/TR-02-2018-0027},
url = {https://www.sciencedirect.com/science/article/pii/S1660537318000231},
author = {Stephan Reinhold and Florian J. Zach and Dejan Krizaj},
keywords = {Tourism, Review, Business model, Special issue, Avenues},
abstract = {Purpose
This paper aims to review the state of the art for the Tourism Review special issue on “Business Models in Tourism”. The authors’ purpose is twofold: first, to contextualize the empirical and conceptual contributions featured in the special issue in relation to the state of research on business models in tourism. Second, the authors position the special issue in the broader scholarly conversation on business models to identify avenues for future research.
Design/methodology/approach
The authors systematically review the content of tourism-specific business model studies from leading literature databases to answer four questions relevant for future work on business models in tourism: First, how do tourism scholars define the business model concept? Second, what is the ontological stance (object, schema or tool) of existing studies of tourism business models? Third, what are the methodological preferences of existing work on business models in tourism? And finally, what qualifies as rigorous business model research?
Findings
From the critical review of 32 contributions, the authors identify a minimal consensus and dominant approach to conceptualizing the business model concept in tourism studies. In addition, the authors reveal a strong preference for small-n case study research designs. In sum, those findings point to important gaps and design decisions for future business model studies in tourism.
Originality/value
This review of the state of research on business models in tourism details research opportunities with regard to theory, methods and applications that tourism scholars can investigate to contribute to the theory and practice of business model management.}
}
@article{VIRVOU2024120759,
title = {VIRTSI: A novel trust dynamics model enhancing Artificial Intelligence collaboration with human users – Insights from a ChatGPT evaluation study},
journal = {Information Sciences},
volume = {675},
pages = {120759},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120759},
url = {https://www.sciencedirect.com/science/article/pii/S002002552400673X},
author = {Maria Virvou and George A. Tsihrintzis and Evangelia-Aikaterini Tsichrintzi},
keywords = {Artificial Intelligence, AI-Empowered Software, Autonomous Systems, AI Trust, Human-AI Interaction, Human-Centered Artificial Intelligence, User Modelling, Finite State Modelling, Confusion Matrix, AI in Education},
abstract = {The rapid integration of intelligent processes and methods into information systems in the Artificial Intelligence (AI) era has led to a substantial shift towards autonomous software decision-making. This evolution necessitates robust human oversight, especially in critical domains like Healthcare, Education, and Energy. Human trust in AI plays a vital role in influencing decision-making processes of users interacting with AI. This paper presents VIRTSI (Variability and Impact of Reciprocal Trust States towards Intelligent systems), a novel rigorous computational model for human-AI Interaction. VIRTSI simulates human trust states, spanning from overtrust to distrust, through user modelling. It comprises: 1. A trust dynamics representational model based on Deterministic Finite State Automata (DFAs), illustrating transitions among cognitive trust states in response to AI-generated replies. 2. A trust evaluation model based on Confusion Matrices, originating from machine learning and Accuracy Metrics, providing a quantitative framework for analysing human trust dynamics. As a result, this is the first time that trust dynamics have been thoroughly traced in a representational model and a method has been developed to assess the impact of possibly harmful states like overtrust and distrust. An empirical study on the recently launched Large Language Model of generative AI, ChatGPT (version 3.5), provides a radical underexplored AI-generated platform for evaluating the human-AI interaction through VIRTSI. The study involved 1200 interactions of real users as well as AI experts together with experts in two very different domains of evaluation, namely software engineering and poetry. This study traces trust dynamics and the emerging human-AI interaction, in concrete examples of real user synergies with generative AI. The research reveals the vital role of maintaining normal trust states for optimal human-AI interaction and that both AI and human users need further steps towards this goal. The real-world implications of this research can guide the creation and evaluation of user interfaces with AI and the incorporation of functionalities in the development of generative AI chatbots in terms of trust by providing a new rigorous DFA representational method of trust dynamics and a corresponding new perspective of confusion matrix evaluation method of the dynamics’ impact in the efficiency of human-AI dialogues.}
}
@article{PARSONS2020124,
title = {Indigenous peoples and transformations in freshwater governance and management},
journal = {Current Opinion in Environmental Sustainability},
volume = {44},
pages = {124-139},
year = {2020},
note = {Resilience and complexity:Frameworks and models to capture social-ecological interactions},
issn = {1877-3435},
doi = {https://doi.org/10.1016/j.cosust.2020.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S187734352030018X},
author = {Meg Parsons and Karen Fisher},
abstract = {In this paper, we consider how Indigenous peoples are contesting freshwater management regimes based on Western ontologies and epistemologies, and are seeking recognition for the inclusion of Indigenous knowledge, practices, and authority over freshwater. We focus on the potential of new governance arrangements emerging around the globe to contribute to broader societal deliberations about, and transformations in human-environmental relationships. In particular, we draw attention to research exploring and interrogating matters of equity and inclusion central to sustainability and how Aotearoa New Zealand Māori are seeking to enact their values and aspirations for sustainable freshwater futures underpinned by a relational ontology that connects humans to other humans and nonhumans across time and space.}
}
@article{DAHIR2021100043,
title = {A query expansion method based on topic modeling and DBpedia features},
journal = {International Journal of Information Management Data Insights},
volume = {1},
number = {2},
pages = {100043},
year = {2021},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2021.100043},
url = {https://www.sciencedirect.com/science/article/pii/S2667096821000367},
author = {Sarah Dahir and Abderrahim El Qadi},
keywords = {Information retrieval, Query expansion, DBpedia, Term distribution, Topic modeling, Language model},
abstract = {Query Expansion (QE) is a method used for improving Information Retrieval (IR) by adding the terms that are almost selected from feedback documents, and similar to the user query terms. But, due to the very small average number of query keywords; it is sometimes difficult to detect the context around the user query, and expand the query accordingly, especially when it contains ambiguous terms(i.e. polysemy terms). To this end, Linked Open Data (LOD) sources may be exploited. Yet, most attributes from linked data are multi-valued which makes a system unable to determine the right one(s) to use for expansion. And few other attributes are single-valued but too long and noisy to use directly. To deal with the previous issues, integration of the topic modeling process has been proposed to predict the latent semantic attribute-topics to use for expansion. This approach reconstructs candidate documents for a given query using distribution technique Bose-Einstein statistics (Bo1) and DBpedia attributes. The Latent Dirichlet Allocation(LDA) based topic models are then generated by considering these documents and the relevant expansion terms are then determined. The proposed method has been evaluated using the AP dataset collection, and the experiments revealed significant improvements according to the retrieval results using the distribution technique Bo1. Also, the proposed “LDA-LinkedBo1” approach outperformed DBpedia association based approaches in terms of MRR@N.}
}
@article{ADAM202263,
title = {A novel tool that allows interactive screening of PubMed citations showed promise for the semi-automation of identification of Biomedical Literature},
journal = {Journal of Clinical Epidemiology},
volume = {150},
pages = {63-71},
year = {2022},
issn = {0895-4356},
doi = {https://doi.org/10.1016/j.jclinepi.2022.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0895435622001536},
author = {Gaelen P. Adam and Dimitris Pappas and Haris Papageorgiou and Evangelos Evangelou and Thomas A. Trikalinos},
keywords = {Evidence synthesis, Systematic review methods, Literature identification, Abstract screening, Text mining, Machine learning},
abstract = {Background and Objectives
Systematic reviews form the basis of evidence-based medicine, but are expensive and time-consuming to produce. To address this burden, we have developed a literature identification system (Pythia) that combines the query formulation and citation screening steps.
Methods
Pythia incorporates a set of natural-language questions with machine-learning algorithms to rank all PubMed citations based on relevance, returning the 100 top-ranked citations for human screening. The tagged citations are iteratively exploited by Pythia to refine the search and re-rank the citations.
Results
Across seven systematic reviews, the ability of Pythia to identify the relevant citations (sensitivity) ranged from 0.09 to 0.58. The number of abstracts reviewed per relevant abstract number needed to read (NNR) was lower than in the manually screened project in four reviews, higher in two, and had mixed results in one. The reviews that had greater overall sensitivity retrieved more relevant citations in early batches, but retrieval was generally unaffected by other aspects, such as study design, study size, and specific key question.
Conclusion
Due to its low sensitivity, Pythia is not ready for widespread use. Future research should explore ways to encode domain knowledge in query formulation to better enrich the questions used in the search.}
}
@article{CHEN202537,
title = {A Survey of Large-Scale Deep Learning Models in Medicine and Healthcare},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {144},
number = {1},
pages = {37-81},
year = {2025},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2025.067809},
url = {https://www.sciencedirect.com/science/article/pii/S1526149225002206},
author = {Zhiwei Chen and Runze Liu and Shitao Huang and Yangyang Guo and Yongjun Ren},
keywords = {Large models, healthcare, artificial intelligence, data management, medical applications},
abstract = {The rapid advancement of artificial intelligence technology is driving transformative changes in medical diagnosis, treatment, and management systems through large-scale deep learning models—a process that brings both groundbreaking opportunities and multifaceted challenges. This study focuses on the medical and healthcare applications of large-scale deep learning architectures, conducting a comprehensive survey to categorize and analyze their diverse uses. The survey results reveal that current applications of large models in healthcare encompass medical data management, healthcare services, medical devices, and preventive medicine, among others. Concurrently, large models demonstrate significant advantages in the medical domain, especially in high-precision diagnosis and prediction, data analysis and knowledge discovery, and enhancing operational efficiency. Nevertheless, we identify several challenges that need urgent attention, including improving the interpretability of large models, strengthening privacy protection, and addressing issues related to handling incomplete data. This research is dedicated to systematically elucidating the deep collaborative mechanisms between artificial intelligence and the healthcare field, providing theoretical references and practical guidance for both academia and industry.}
}
@article{KIM20252214,
title = {Enzyme functional classification using artificial intelligence},
journal = {Trends in Biotechnology},
volume = {43},
number = {9},
pages = {2214-2231},
year = {2025},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2025.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167779925000885},
author = {Ha Rim Kim and Hongkeun Ji and Gi Bae Kim and Sang Yup Lee},
keywords = {deep learning, EC number, enzyme, GO term, machine learning, metabolism},
abstract = {Enzymes are essential for cellular metabolism, and elucidating their functions is critical for advancing biochemical research. However, experimental methods are often time consuming and resource intensive. To address this, significant efforts have been directed toward applying artificial intelligence (AI) to enzyme function prediction, enabling high-throughput and scalable approaches. In this review, we discuss advances in AI-driven enzyme functional annotation, transitioning from traditional machine learning (ML) methods to state-of-the-art deep learning approaches. We highlight how deep learning enables models to automatically extract features from raw data without manual intervention, leading to enhanced performance. Finally, we discuss the discovery of novel enzyme functions and generation of de novo enzymes through the integration of generative AIs and bio big data as future research directions.}
}
@article{DUBOVA2023656,
title = {Carving joints into nature: reengineering scientific concepts in light of concept-laden evidence},
journal = {Trends in Cognitive Sciences},
volume = {27},
number = {7},
pages = {656-670},
year = {2023},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2023.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S1364661323000967},
author = {Marina Dubova and Robert L. Goldstone},
keywords = {concept reengineering, scientific concepts, ontology, concept-ladenness of scientific observation, top-down influences on perception},
abstract = {A new wave of proposals suggests that scientists must reassess scientific concepts in light of accumulated evidence. However, reengineering scientific concepts in light of data is challenging because scientific concepts affect the evidence itself in multiple ways. Among other possible influences, concepts (i) prime scientists to overemphasize within-concept similarities and between-concept differences; (ii) lead scientists to measure conceptually relevant dimensions more accurately; (iii) serve as units of scientific experimentation, communication, and theory-building; and (iv) affect the phenomena themselves. When looking for improved ways to carve nature at its joints, scholars must take the concept-laden nature of evidence into account to avoid entering a vicious circle of concept-evidence mutual substantiation.}
}
@article{MARFOGLIA2025109745,
title = {Towards real-world clinical data standardization: A modular FHIR-driven transformation pipeline to enhance semantic interoperability in healthcare},
journal = {Computers in Biology and Medicine},
volume = {187},
pages = {109745},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.109745},
url = {https://www.sciencedirect.com/science/article/pii/S0010482525000952},
author = {Alberto Marfoglia and Filippo Nardini and Valerio Antonio Arcobelli and Serena Moscato and Sabato Mellone and Antonella Carbonaro},
keywords = {Health informatics, Standardization, Data conversion, Semantic technology, Software architecture},
abstract = {Background:
Given the exponential increase in clinical data, which accounts for around 30% of global data volume, effective information management has become crucial to ensuring robust interoperability. This trend is further expedited by implementing consumer-oriented Internet of Things platforms, contributing to the growth of the $8.3 trillion healthcare industry. These advancements, combined with challenges such as heterogeneous data formats and a lack of incentives, necessitate the development of pragmatic infrastructures and tools that harness contemporary clinical standards like Fast Healthcare Interoperability Resources (FHIR).
Objective:
This study aims to present a modular conversion pipeline employing a templating strategy for translating clinical data into the FHIR model. Emphasis is placed on utilizing a standard mapping specification like FHIR Mapping Language. This ensures essential properties such as platform independence, portability, and code reusability.
Methods:
The pipeline was developed incrementally, dividing its core functionalities into five modules: Input, Refinement, Mapping, Validation, and Export. These were subsequently validated by converting a dataset from a prosthetic fitting and rehabilitation center to demonstrate the approach’s validity in a real-world data context.
Results:
A total of 1962 hospital stay records of 1006 unique patients were converted successfully to 15 distinct types of FHIR resources. The successful conversion states the pipeline’s effectiveness, additionally showcasing its capabilities for enhancing semantic interoperability and facilitating the reuse of real-world data.
Conclusion:
Our approach emerges as a modular data conversion framework that addresses the limitations of existing solutions, making significant contributions to the creation of standardized, interoperable, and high-quality clinical datasets that serve as a foundation for further work.}
}
@article{LUO2024128280,
title = {A comprehensive survey for automatic text summarization: Techniques, approaches and perspectives},
journal = {Neurocomputing},
volume = {603},
pages = {128280},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128280},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224010518},
author = {Mengqi Luo and Bowen Xue and Ben Niu},
keywords = {Automatic text summarization, Natural language processing, Machine learning, Large language model, Knowledge discovery},
abstract = {The enormous quantity of text makes it challenging for users to obtain the key information and knowledge. Automatic text summarization can alleviate this problem by providing reliable summaries for massive text documents. During the last decade, significant achievements have been made in text summarization. We conduct this survey to explore what research community is focused on, the application scenarios of summarization, the state-of-the-art techniques and methods, and to analyze the challenges and future direction. We summarize that incorporating with natural language processing, previous text summarization research applied knowledge-based methods, graph-based methods, statistical learning methods, and deep learning methods. Applying large language model to text summarization is still in its early stages. By analyzing current research progress, we conclude that understand semantic information and specific domain knowledge is required for text summarization, and the conciseness and readability of the summary should be ensured. The future research opportunity is automatic knowledge summarization, and more research effort is urgently needed to explore.}
}
@article{MANNAA20228909,
title = {Computer-assisted i‘raab of Arabic sentences for teaching grammar to students},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {10, Part B},
pages = {8909-8926},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2022.08.020},
url = {https://www.sciencedirect.com/science/article/pii/S1319157822002944},
author = {Zarah M. Mannaa and Aqil M. Azmi and Hatim A. Aboalsamh},
keywords = {Arabic syntactic parsing, Grammatical analysis, Context-free grammar, Head-driven phrase structure grammar, Link-grammar, Computer-assisted learning},
abstract = {The end-case analysis of Arabic sentences is one of the keys to their meaning. This process is called i‘raab, a daunting task for the students. The outcome of the analysis is twofold: (a) placing a proper diacritical marking on the end-cases of individual words, and (b) providing a logical justification. Our objective is to generate a full i‘raabof the sentences, which could be incorporated in a computer-assisted Arabic grammar learning environment, teaching and helping the students with the i‘raabprocess. Our system comprises four components that work together to generate a proper i‘raabfor the sentence. We devise an enhanced context-free grammar (eCFG) that covers all the cases and rules taught in Saudi schools’ grammar textbooks (grades 7–12). Moreover, our eCFGgrammar eliminates the need for specialized grammars, e.g., head-driven phrase structure grammar and link grammar, to resolve complex cases involving dependencies. Furthermore, we utilize ontology to determine the correct semantics. The system was tested on 300 sentences varying in complexity from the Arabic grammar textbooks used in the schools. Our system achieved an overall accuracy of 88.33%.}
}
@article{CRISAFULLI2023104549,
title = {Modeling and analysing Cyber–Physical Systems in HOL-CSP},
journal = {Robotics and Autonomous Systems},
volume = {170},
pages = {104549},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104549},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023001884},
author = {Paolo Crisafulli and Safouan Taha and Burkhart Wolff},
keywords = {Cyber–Physical Systems, Autonomous cars, Safety-critical systems, Process-algebra, Concurrency, Proof-based verification},
abstract = {Modelling and Analysing Cyber–Physical Systems (CPS) is a challenge for Formal Methods and therefore a field of active research. It is characteristic of CPSs that models comprise aspects of Newtonian Physics appearing in system environments, the difficulties of their discretization, the problems of communication and interaction between actors in this environment as well as calculations respecting time-bounds. We present a novel framework to address these problems developed with industrial partners involved in the Autonomous Car domain. Based on HOL-CSP, we model time, physical evolution, “scenes” (global states) and “scenarios” (traces) as well as the interaction of “actors” (vehicles, pedestrians, traffic lights) inside this framework. In particular, discrete samplings are modelled by infinite internal choices. For several instances of the modelling framework, we give formal proofs of a particular safety property for Autonomous Cars: if each car follows the same driving strategy defined by the so-called Responsibility-Sensitive Safety (RSS), no collision will occur. The proofs give rise to a number of variants of RSS and optimizations as well as a test-case partitioning of abstract test cases and a test-strategy for integration tests.}
}
@article{ESFEHANI20183158,
title = {Lost in translation? Cross-language thematic analysis in tourism and hospitality research},
journal = {International Journal of Contemporary Hospitality Management},
volume = {30},
number = {11},
pages = {3158-3174},
year = {2018},
issn = {0959-6119},
doi = {https://doi.org/10.1108/IJCHM-10-2017-0701},
url = {https://www.sciencedirect.com/science/article/pii/S0959611918001742},
author = {Minoo H. Esfehani and Trudie Walters},
keywords = {Methodology, Qualitative research, Thematic analysis, Translation, Tourism and hospitality, Cross-language},
abstract = {Purpose
Tourism and hospitality research is frequently cross language in nature; yet, English is the most used language to disseminate research findings. The use of thematic analysis is increasing; yet, critical discussions of the implications of the timing of translation when applying this method are rare. The purpose of this study is to present a model for bilingual researchers undertaking qualitative studies in their mother language who are reliant on their own language skills to translate and overcome language differences, and who are using thematic analysis.
Design/methodology/approach
Thematic analysis is a six-phase iterative analysis process during which the main themes are identified and a network of related themes is constructed to facilitate the interpretation of the material. The model is illustrated through reference to a research project carried out by the first author on the role and manifestation of intangible cultural heritage in tourism in protected areas in Iran.
Findings
The model introduces translation as an internal procedure within thematic analysis, situating it between the second and third phases when the codes are being consolidated into basic themes. Translation is viewed as a part of the iterative process of thematic analysis.
Originality/value
This model is the first to provide bilingual cross-language researchers with a practical and epistemologically, methodologically and ethically sound rationale for the timing of translation when using thematic analysis. While it was developed on a tourism case study, the authors believe it is applicable to research in other disciplines where cross-language qualitative analysis is used.}
}
@article{YANG2025149231,
title = {VAMP8 as a biomarker and potential therapeutic target for endothelial cell dysfunction in atherosclerosis},
journal = {Gene},
volume = {942},
pages = {149231},
year = {2025},
issn = {0378-1119},
doi = {https://doi.org/10.1016/j.gene.2025.149231},
url = {https://www.sciencedirect.com/science/article/pii/S0378111925000198},
author = {Luqun Yang and Xin Guan and Jiangwei Cheng and Lin Ni and Huijing Yao and Yuping Gao and Kaiyi Zhu and Xiushan Shi and Bingjie Li and Yuanyuan Lin},
keywords = {Atherosclerosis, Endothelial cell dysfunction, DEGs, HUVECs, SMR, },
abstract = {Background
Endothelial cell dysfunction has a critical role in the pathophysiology of atherosclerosis. This study aims to uncover pivotal genes and pathways linked to endothelial cell dysfunction in atherosclerosis, as well as to ascertain the assumed causal effects and potential mechanisms.
Methods
Datasets relevant to endothelial cell dysfunction in atherosclerosis were collected and divided into training and validation sets. Following differential analysis, we constructed a protein–protein interaction (PPI) network and a molecular interaction map of common-differentially expressed genes (co-DEGs) with proteins known to be involved in atherosclerotic endothelial cell dysfunction. Gene Ontology (GO), Kyoto Encyclopedia of Genes and Genome (KEGG), and Gene Set Enrichment Analysis (GSEA) were also conducted. Moreover, human umbilical vein endothelial cells (HUVECs) were cultured in circumstances characterized by elevated glucose levels to establish a cellular injury model simulating atherosclerotic conditions, and quantitative Polymerase Chain Reaction (qPCR) experiments were conducted to validate the differences of co-DEGs. Subsequently, the Summary-data-based Mendelian Randomization (SMR) method was employed. Additionally, we employed the Western Blot (WB) technique to validate the differential expression of VAMP8. Finally, we identified the differential expression of VAMP8 in the validation set and further validated its differential expression by collecting fresh blood samples from 20 patients with atherosclerosis and 20 healthy individuals.
Results
14 co-DEGs (FABP5, GULP1, COL4A5, VAMP8, FABP4, PFN2, ANGPT2, TFPI2, NUPR1, SULF1, FGF13, BASP1, EPB41L3, and PBK) were identified. SMR analysis confirmed 10 potential causal effect genes: PSRC1, VAMP8, FES, HNRNPUL1, CFDP1, SAP130, MDN1, OPRL1, UTP11, and HOXC4. The qPCR and WB experiments demonstrated that VAMP8 was significantly upregulated in the injured HUVECs group (p < 0.0001). Compared to the control group, VAMP8 was markedly increased in the blood samples of patients with atherosclerosis (p < 0.0001).
Conclusions
VAMP8 may potentially serve as a pathogenic gene in the process of endothelial dysfunction in atherosclerosis.}
}
@article{RODRIGUES2018146,
title = {The role of experienced teachers in the development of pre-service language teachers’ professional identity: Revisiting school memories and constructing future teacher selves},
journal = {International Journal of Educational Research},
volume = {88},
pages = {146-155},
year = {2018},
issn = {0883-0355},
doi = {https://doi.org/10.1016/j.ijer.2018.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S088303551731618X},
author = {Livia de Araujo Donnini Rodrigues and Emerson {de Pietri} and Hugo Santiago Sanchez and Kuchah Kuchah},
keywords = {Language teacher cognition, Teacher identity, Preservice teacher education, Critical perspective},
abstract = {This paper examines how student teachers perceive the role of more experienced teachers in fostering their pedagogical cultural identities (Burgess, 2016). It reports on a project within the ‘Programa Institucional de Bolsas de Iniciação à Docência’ (PIBID), a national programme in Brazil to promote teacher recruitment and encourage undergraduates to pursue a career in teaching. The study investigated in-school experiences of four pre-service teachers working in state schools located in peripheral urban areas in São Paulo. Specifically, it examined the ways these student teachers conceptualise their pedagogical contexts and related them to the concrete or symbolic presence of other experienced professionals. The findings are expected to support professional development and inform the revision of teacher education programmes in Brazil.}
}
@article{SILVALLANES2025251,
title = {Targeting CB2 receptor with a novel antagonist reverses cognitive decline, neurodegeneration and pyroptosis in a TAU-dependent frontotemporal dementia mouse model},
journal = {Brain, Behavior, and Immunity},
volume = {127},
pages = {251-268},
year = {2025},
issn = {0889-1591},
doi = {https://doi.org/10.1016/j.bbi.2025.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0889159125000935},
author = {Ignacio Silva-Llanes and Silvia Rodríguez-López and Pedro González-Naranjo and Eric del Sastre and Manuela G. López and Juan Antonio Páez and Nuria Campillo and Isabel Lastres-Becker},
keywords = {CB antagonists, TAU, FTD, Neurodegeneration, Neuroinflammation, Pyroptosis, GASDERMIN D},
abstract = {Frontotemporal dementia (FTD) comprises a group of disorders characterized by a progressive decline in behavior or language linked to the degeneration of the frontal and anterior temporal lobes followed by hippocampal atrophy. There are no effective treatments for FTD and for this reason, novel pharmacological targets, such as the endocannabinoid system (ECS), are being explored. Previous results from our laboratory showed a TAUP301L-dependent increase in CB2 receptor expression in hippocampal neurons of a FTD mouse model, alongside the neuroprotective impact of CB2 ablation. In this study, we evaluated the therapeutic potential of a new CB2 antagonist (PGN36) in our TAU-dependent FTD mouse model. Six-month-old mice received stereotaxic injections of an adeno-associated virus expressing human TAUP301L protein (AAV-TAUP301L) into the right hippocampus and were treated daily with PGN36 (5 mg/kg, i.p.) or vehicle for three weeks. By integrating behavioral tests, RNA-seq, qPCR expression analysis, and immunofluorescence in the AAV expressing TAU mouse model, we found that PGN36 treatment reverses key features of the neurodegenerative process triggered by TAUP301L overexpression. PGN36 treatment effectively countered TAUP301L-induced cognitive decline by reducing TAU protein expression levels and restoring markers of synaptic plasticity. Notably, we observed neuroprotection in the dentate gyrus granular layer, which we attribute to the modulation of pyroptosis. This programmed cell death pathway, is triggered by TAUP301L overexpression. PGN36 appears to modulate the pyroptotic cascade, thereby preventing the pyroptosis-induced neuronal loss. These findings collectively underscore the neuroprotective potential of this novel CB2 antagonist treatment against TAU-associated FTD.}
}
@article{PU2023104464,
title = {Graph embedding-based link prediction for literature-based discovery in Alzheimer’s Disease},
journal = {Journal of Biomedical Informatics},
volume = {145},
pages = {104464},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104464},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423001855},
author = {Yiyuan Pu and Daniel Beck and Karin Verspoor},
keywords = {Literature-based discovery, Alzheimer’s Disease, Link prediction, Knowledge graph, Text mining, Graph embedding},
abstract = {Objective:
We explore the framing of literature-based discovery (LBD) as link prediction and graph embedding learning, with Alzheimer’s Disease (AD) as our focus disease context. The key link prediction setting of prediction window length is specifically examined in the context of a time-sliced evaluation methodology.
Methods:
We propose a four-stage approach to explore literature-based discovery for Alzheimer’s Disease, creating and analyzing a knowledge graph tailored to the AD context, and predicting and evaluating new knowledge based on time-sliced link prediction. The first stage is to collect an AD-specific corpus. The second stage involves constructing an AD knowledge graph with identified AD-specific concepts and relations from the corpus. In the third stage, 20 pairs of training and testing datasets are constructed with the time-slicing methodology. Finally, we infer new knowledge with graph embedding-based link prediction methods. We compare different link prediction methods in this context. The impact of limiting prediction evaluation of LBD models in the context of short-term and longer-term knowledge evolution for Alzheimer’s Disease is assessed.
Results:
We constructed an AD corpus of over 16 k papers published in 1977–2021, and automatically annotated it with concepts and relations covering 11 AD-specific semantic entity types. The knowledge graph of Alzheimer’s Disease derived from this resource consisted of ∼11 k nodes and ∼394 k edges, among which 34% were genotype-phenotype relationships, 57% were genotype-genotype relationships, and 9% were phenotype-phenotype relationships. A Structural Deep Network Embedding (SDNE) model consistently showed the best performance in terms of returning the most confident set of link predictions as time progresses over 20 years. A huge improvement in model performance was observed when changing the link prediction evaluation setting to consider a more distant future, reflecting the time required for knowledge accumulation.
Conclusion:
Neural network graph-embedding link prediction methods show promise for the literature-based discovery context, although the prediction setting is extremely challenging, with graph densities of less than 1%. Varying prediction window length on the time-sliced evaluation methodology leads to hugely different results and interpretations of LBD studies. Our approach can be generalized to enable knowledge discovery for other diseases.
Availability:
Code, AD ontology, and data are available at https://github.com/READ-BioMed/readbiomed-lbd.}
}
@article{ZHU2025101741,
title = {ECDG-DST: A dialogue state tracking model based on efficient context and domain guidance for smart dialogue systems},
journal = {Computer Speech & Language},
volume = {90},
pages = {101741},
year = {2025},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101741},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824001244},
author = {Meng Zhu and Xiaolong Xu},
keywords = {Deep learning, Dialogue system, Dialogue state tracking, Task based dialogue, Open vocabulary},
abstract = {Dialogue state tracking (DST) is an important component of smart dialogue systems, with the goal of predicting the current dialogue state at conversation turn. However, most of the previous works had problems with storing a large amount of data and storing a large amount of noisy information when the conversation takes many turns. In addition, they also overlooked the effect of the domain in the task of dialogue state tracking. In this paper, we propose ECDG-DST 1 (A dialogue state tracking model based on efficient context and domain guidance) for smart dialogue systems, which preserves key information but retains less dialogue history, and masks the domain effectively in dialogue state tracking. Our model utilizes the efficient conversation context, the previous conversation state and the relationship between domains and slots to narrow the range of slots to be updated, and also limit the directions of values to reduce the generation of irrelevant words. The ECDG-DST model consists of four main components, including an encoder, a domain guide, an operation predictor, and a value generator. We conducted experiments on three popular task-oriented dialogue datasets, Wizard-of-Oz2.0, MultiWOZ2.0, and MultiWOZ2.1, and the empirical results demonstrate that ECDG-DST respectively improved joint goal accuracy by 0.45 % on Wizard-of-Oz2.0, 2.44 % on MultiWOZ2.0 and 2.05 % on MultiWOZ2.1 compared to the baselines. In addition, we analyzed the scope of the efficient context through experiments and validate the effectiveness of our proposed domain guide mechanism through ablation study.}
}
@article{HENDAWI2021,
title = {ADDietCoach:},
journal = {International Journal of E-Health and Medical Communications},
volume = {12},
number = {6},
year = {2021},
issn = {1947-315X},
doi = {https://doi.org/10.4018/IJEHMC.20211101.oa6},
url = {https://www.sciencedirect.com/science/article/pii/S1947315X21000061},
author = {Rasha Hendawi and Juan Li and Shadi Alian},
keywords = {Alzheimer’s Disease, Knowledge, Logic, Ontology, Personalization, Reasoning, Recommendation},
abstract = {ABSTRACT
The aging population worldwide is expected to increase the prevalence of Alzheimer’s disease. As there is no medical curative treatment for this disease to date, alternative treatments have been applied to improve the patient’s brain and general health. One of these efforts includes providing Alzheimer’s patients with proper food and nutrition. In this paper, the authors propose a knowledge-powered personalized virtual coach to provide diet and nutrition assistance to patients of Alzheimer’s and/or their informal caregivers. The virtual coach is built on top of an ontology-enhanced knowledge base containing knowledge about patients, Alzheimer’s disease, food, and nutrition. Semantics-based searching and reasoning are performed on the knowledge base to get personalized context-aware recommendation and education about healthy eating for Alzheimer’s patients. The proposed system has been implemented as a mobile application. Evaluation based on use cases has demonstrated the usefulness of this tool.}
}
@article{ALBERTI2025104361,
title = {A semantics for probabilistic hybrid knowledge bases with function symbols},
journal = {Artificial Intelligence},
volume = {346},
pages = {104361},
year = {2025},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2025.104361},
url = {https://www.sciencedirect.com/science/article/pii/S0004370225000803},
author = {Marco Alberti and Evelina Lamma and Fabrizio Riguzzi and Riccardo Zese},
keywords = {Hybrid knowledge bases, Minimal knowledge with negation as failure, Probability, Distribution semantics},
abstract = {Hybrid Knowledge Bases (HKBs) successfully integrate Logic Programming (LP) and Description Logics (DL) under the Minimal Knowledge with Negation as Failure semantics. Both world closure assumptions (open and closed) can be used in the same HKB, a feature required in many domains, such as the legal and health-care ones. In previous work, we proposed (function-free) Probabilistic HKBs, whose semantics applied Sato's distribution semantics approach to the well-founded HKB semantics proposed by Knorr et al. and Lyu and You. This semantics relied on the fact that the grounding of a function-free Probabilistic HKB (PHKB) is finite. In this article, we extend the PHKB language to allow function symbols, obtaining PHKBFS. Because the grounding of a PHKBFS can be infinite, we propose a novel semantics which does not require the PHKBFS's grounding to be finite. We show that the proposed semantics extends the previously proposed semantics and that, for a large class of PHKBFS, every query can be assigned a probability.}
}
@article{WANG2025100759,
title = {Towards cognitive intelligence-enabled product design: The evolution, state-of-the-art, and future of AI-enabled product design},
journal = {Journal of Industrial Information Integration},
volume = {43},
pages = {100759},
year = {2025},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100759},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24002024},
author = {Zuoxu Wang and Xinxin Liang and Mingrui Li and Shufei Li and Jihong Liu and Lianyu Zheng},
keywords = {Engineering product design, Cognitive computing, Knowledge graph, Industrial products and services design, Design knowledge support, Knowledge reasoning},
abstract = {Engineering design researchers have increasing interests in leveraging artificial intelligence (AI) techniques to a wide range of product design tasks, such as customer requirement analysis, product concept generation, design synthesis, and decision-making in product design. Indeed, AI techniques perform excellently on well-defined design tasks with clear problem definition, specialized solutions, and abundant training data. However, facing the ever-evolving AI techniques rapidly and radically changing the product design manner, there is still a lack of a systematic summary about the current stage of AI-enabled product design. Besides, although the current AI-enabled product design performs excellently on the well-defined tasks, the other advanced design tasks that need cognitive capability can still hardly be satisfyingly completed by the current product design system. This study systematically reviewed the literature on AI-enabled product design to understand its evolution and state-of-the-arts. To bridge the semantic gap between humans and systems, a novel cognitive intelligence-enabled product design (CIPD) framework is proposed, in which cognitive intelligence is the key enabler. The CIPD's key aspects, including its system architecture, human-like capabilities, enabling technologies, and potential applications, are also systematically discussed. It is hoped that this study could contribute to the future directions of the product design field and offer insightful guidance to the practitioners and researchers in their product design process.}
}
@article{ANDREASEN2024102246,
title = {The power and potentials of Flexible Query Answering Systems: A critical and comprehensive analysis},
journal = {Data & Knowledge Engineering},
volume = {149},
pages = {102246},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2023.102246},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X23001064},
author = {Troels Andreasen and Gloria Bordogna and Guy De Tré and Janusz Kacprzyk and Henrik Legind Larsen and Sławomir Zadrożny},
keywords = {Flexible query answering, Model-based query answering, Data-driven query answering},
abstract = {The popularity of chatbots, such as ChatGPT, has brought research attention to question answering systems, capable to generate natural language answers to user’s natural language queries. However, also in other kinds of systems, flexibility of querying, including but also going beyond the use of natural language, is an important feature. With this consideration in mind the paper presents a critical and comprehensive analysis of recent developments, trends and challenges of Flexible Query Answering Systems (FQASs). Flexible query answering is a multidisciplinary research field that is not limited to question answering in natural language, but comprises other query forms and interaction modalities, which aim to provide powerful means and techniques for better reflecting human preferences and intentions to retrieve relevant information. It adopts methods at the crossroad of several disciplines among which Information Retrieval (IR), databases, knowledge based systems, knowledge and data engineering, Natural Language Processing (NLP) and the semantic web may be mentioned. The analysis principles are inspired by the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) framework, characterized by a top-down process, starting with relevant keywords for the topic of interest to retrieve relevant articles from meta-sources And complementing these articles with other relevant articles from seed sources Identified by a bottom-up process. to mine the retrieved publication data a network analysis is performed Which allows to present in a synthetic way intrinsic topics of the selected publications. issues dealt with are related to query answering methods Both model-based and data-driven (the latter based on either machine learning or deep learning) And to their needs for explainability and fairness to deal with big data Notably by taking into account data veracity. conclusions point out trends and challenges to help better shaping the future of the FQAS field.}
}
@article{LOWE20201014,
title = {Constitutive rules for guiding the use of the viable system model: Reflections on practice},
journal = {European Journal of Operational Research},
volume = {287},
number = {3},
pages = {1014-1035},
year = {2020},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2020.05.030},
url = {https://www.sciencedirect.com/science/article/pii/S0377221720304768},
author = {David Lowe and Angela Espinosa and Mike Yearworth},
keywords = {Viable system model, Problem structuring methods, Constitutive rules, Epistemology, Hierarchical Process Model},
abstract = {The Viable System Model (VSM) provides a well-established framework to aid the design and diagnosis of organisations to survive and thrive in complex operating environments. However, the cognitive accessibility of the VSM presents a significant barrier to its application with non-expert stakeholders. In the face of such difficulties, VSM practitioners will often take steps to adapt the classic presentation of VSM to suit the needs of their particular operational context. We propose a set of constitutive rules, including an explicit epistemology, that can both account for the variety of VSM practice reported in the literature and also be used to guide practitioners in their application of the VSM and thus make rigorous use of VSM theory. The epistemology is expressed as a performative model, expressed as a Hierarchical Process Model (HPM), of the practitioner's use of the VSM in an engagement. We use this model to describe, reflect upon, and learn about VSM practice by the cross-case analysis of three recent VSM interventions. The combination of variability in problem structuring and specificity to the VSM afforded by the constitutive rules and the performative epistemology in combination has provided insight into the social ontology of VSM practice and the boundaries of what should be considered acceptable practice from a competence perspective. Our approach is intended to encourage wider and better application of VSM theory in preparing organisations to maintain performance in uncertain futures.}
}
@article{VIRY202087,
title = {Ontologie d’Alerte Choucas : de la modélisation des connaissances à un outil support d’un raisonnement géovisuel1},
journal = {Geomatica},
volume = {74},
number = {3},
pages = {87-103},
year = {2020},
issn = {1195-1036},
doi = {https://doi.org/10.1139/geomat-2020-0005},
url = {https://www.sciencedirect.com/science/article/pii/S1195103624005433},
author = {Matthieu Viry and Marlène Villanova-Oliver},
keywords = {représentation des connaissances, raisonnement géovisuel, secours en montagne, knowledge representation, geovisual reasoning, mountain search and rescue},
abstract = {Lorsqu’une intervention de secours est nécessaire, localiser précisément et rapidement le site sur lequel envoyer les équipes est primordial. La littérature montre que des outils de géovisualisation constituent des solutions pertinentes pour supporter des processus d’analyse d’informations dans des contextes variés. Nous nous intéressons ici au raisonnement du secouriste réceptionnant un appel à l’aide et visons des solutions conceptuelles et logicielles dédiées à la tâche de détermination de la localisation de la victime, plus particulièrement dans le contexte du secours en montagne. Nous avons formalisé le raisonnement du secouriste et les informations sur lesquelles il ou elle s’appuie à l’aide d’une ontologie. L’Ontologie d’Alerte Choucas structure les concepts exploités par le secouriste qui élabore des hypothèses de localisation probable de la victime à partir d’informations (telles qu’une position relative, un temps de marche, une direction) fournies lors d’un échange verbal. Dans notre approche, l’ontologie est en outre exploitée pour dériver les composants d’interface d’un prototype de géovisualisation facilitant le raisonnement du secouriste. Ces composants sont une aide à la saisie des informations, en fournissant une restitution cartographique adaptée, et contribuent à construire et à affiner la zone de localisation. Notre approche présente une chaine de traitements originale, menant de la représentation des connaissances à la génération automatisée d’une interface fonctionnelle d’aide au raisonnement visant à localiser des victimes en montagne. When a search and rescue intervention is required, it is essential to quickly and accurately locate the site where rescuers have to be deployed. The literature shows that geovisualization tools are relevant solutions to support various information analysis processes in various contexts. In this work, we are interested in the reasoning of the rescuer receiving a call for help and we aim at conceptual and software solutions dedicated to determining the victim’s location, more particularly in the context of mountain rescue. We have formalized the rescuer’s reasoning and the information he or she relies on by using an ontology. The Choucas Alert Ontology structures the concepts used by the rescuer, who develops assumptions of probable victim location based on information (such as relative position, walking time, direction) provided during a verbal exchange. In our approach, the ontology is also used to derive interface components for a geovisualization prototype that facilitates the rescuer’s reasoning. These components assist the information capture, providing an adapted cartographic restitution, and contribute to the construction and refinement of the localization zone. Our approach presents an original processing chain, from the representation of knowledge to the automated generation of a functional interface to assist reasoning in locating victims in the mountains.}
}
@article{ANWAR2022793,
title = {CD-SPM: Cross-domain book recommendation using sequential pattern mining and rule mining},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {3},
pages = {793-800},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2019.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S131915781831142X},
author = {Taushif Anwar and V. Uma},
keywords = {Semantic similarity, Ontology, Cross-domain recommendation, Collaborative filtering, PrefixSpan},
abstract = {Recommender system suggests a personalized recommendation by filtering the information based on users interest. Nowadays, users like to purchase the best possible items and services to spend the shortest span of time. The cross-domain recommendation system is a method of recommendation wherein knowledge is gathered from multiple domains. With respect to the user’s search term from the source domain, most similar items are recommended from the target domain. Semantic similarity between two different items can be achieved through Wpath method using Ontology. PrefixSpan is used for generating sequential patterns and Topseq rule mining algorithm is used for finding the frequent sequential rule. So, this work tries to extend cross domain recommendation by 1) finding the semantic similarity of items using Ontology; 2) applying Collaborative Filtering for finding similar items and users; 3) generating frequent item sequences using PrefixSpan sequential pattern mining algorithm and 4) recommending user preferred items using Topseq rule mining algorithm. The recommender system is evaluated considering precision, recall and F1 Score measures. It finds CD-SPM which yields better F1 Score. The proposed approach also alleviates the new user problem and sparsity problem to some extent.}
}
@article{ZECH2025115184,
title = {From BIM to Digital Twin: A transformation process through advanced control modeling and automated commissioning using daylight and artificial lighting as examples},
journal = {Energy and Buildings},
volume = {329},
pages = {115184},
year = {2025},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.115184},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824013008},
author = {Philipp Zech and Sascha Hammes and Emanuele Goldin and David Geisler-Moroder and Ruth Breu and Rainer Pfluger},
keywords = {Model-driven engineering, Digital Twins, Domain-specific languages, Building information modeling, Building control and automation, Automated commissioning},
abstract = {An essential part of commissioning is verifying that the implemented systems are operating in accordance with the functional specifications. Commissioning also represents the handover between design and operation and is therefore an interface between different stakeholders. Building Information Modeling (BIM), as a platform for collaboration and information sharing, offers potential for both pre-commissioning, by providing evaluation options prior to implementation, and for automated commissioning of building control systems, by reducing on-site effort and the risk of error. However, current BIM approaches largely neglect this step due to a lack of modeling support. BIM is often seen as a key technology for digital twins. This transformation process can take place through the modeling of building control systems, which represent the interface between operation and design and can capture all relevant operating parameters. By integrating the model with data from simulation during early design and during the life of a building, a digital twin-based assessment of a building's energy performance throughout its life-cycle becomes possible. To realize these benefits, model-based engineering for model-based building commissioning is used to develop a digital twin that improves building operations through data-driven simulation. The resulting model-based tool environment encompasses an agile and continuous engineering process that is aligned with the building life-cycle and addresses current BIM characteristics according to the standard. Our evaluation using the Technology Acceptance Model confirms that the proposal presented is practical to implement and contributes to improving building operations. This is achieved by simplifying the pre-configuration, simulation-based evaluation and automated commissioning of building control systems for the subsequent commissioning of digital twins.}
}
@article{ARENASGUERRERO2025113179,
title = {Intermediate triple table: A general architecture for virtual knowledge graphs},
journal = {Knowledge-Based Systems},
volume = {314},
pages = {113179},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113179},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125002266},
author = {Julián Arenas-Guerrero and Oscar Corcho and María S. Pérez},
keywords = {Virtual knowledge graph, Data integration, Graph querying, Data virtualization, Declarative mappings},
abstract = {Virtual knowledge graphs (VKGs) have been widely applied to access relational data with a semantic layer by using an ontology in use cases that are dynamic in nature. However, current VKG techniques focus mainly on accessing a single relational database and remain largely unstudied for data integration with several heterogeneous data sources. To overcome this limitation, we propose intermediate triple table (ITT), a general VKG architecture to access multiple and diverse data sources. Our proposal is based on data shipping and addresses heterogeneity by adopting a schema-oblivious graph representation that intervenes between the sources and the queries. We minimize data computation by just materializing a relevant subgraph for a specific query. We employ star-shaped query processing and extend this technique to mapping candidate selection. For rapid materialization of the ITT, we apply a mapping partitioning technique to parallelize mapping execution, which also guarantees duplicate-free subgraphs and reduces memory consumption. We use SPARQL-to-SQL query translation to homogeneously evaluate queries over the ITT and execute them with an in-process analytical store. We implemented ITT on top of a knowledge graph materialization engine and evaluated it with two VKG benchmarks. The experimental results show that our proposal outperforms state-of-the-art techniques for complex graph queries in terms of execution time. It also decreases the number of timeouts although it uses more memory as a trade-off. The experiments also demonstrate the source independence of the architecture on a mixed distribution of data with SQL and document stores together with various file formats.}
}
@incollection{PSYCHA2018271,
title = {Systems Platform as a Synthesis and Decision Tool in Algae Biorefineries},
editor = {Mario R. Eden and Marianthi G. Ierapetritou and Gavin P. Towler},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {44},
pages = {271-276},
year = {2018},
booktitle = {13th International Symposium on Process Systems Engineering (PSE 2018)},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-64241-7.50040-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780444642417500409},
author = {Melina Psycha and Filopoimin Lykokanellos and Antonis C. Kokossis},
keywords = {systems platform, synthesis, algae, biorefineries, ontologies},
abstract = {The collection and exploitation of emerging data regarding algae systems is a difficult and challenging task, which requires coordination between the different sources. Relations between algae feedstocks, technologies and products are ambiguous at best and the ones that are established need to be documented. The systems platform presented in this paper deals with the challenges and sets a benchmark for a wide reange of products and pathways. Drawing on the platform, relations between the elements of the database are formed on a conceptual level indicating feasible solutions to the respective problem. Ontology engineering is the tool to achieve classification of the elements as well as to form connections between them based on provided properties. In combination with optimization and knowledge engineering, the platform offers high-capacity screening with featuring criteria (techno-economic, environmental etc.), hosts models of different scales, enables bounds on the number of options to screen and delivers the best set of solutions according to each different criterion.}
}
@article{TOMASEVIC2018993,
title = {Managing mining project documentation using human language technology},
journal = {The Electronic Library},
volume = {36},
number = {6},
pages = {993-1009},
year = {2018},
issn = {0264-0473},
doi = {https://doi.org/10.1108/EL-11-2017-0239},
url = {https://www.sciencedirect.com/science/article/pii/S0264047318000012},
author = {Aleksandra Tomašević and Ranka Stanković and Miloš Utvić and Ivan Obradović and Božo Kolonja},
keywords = {Digital libraries, Information retrieval, Data mining, Human language technologies, Project documentation},
abstract = {Purpose
This paper aims to develop a system, which would enable efficient management and exploitation of documentation in electronic form, related to mining projects, with information retrieval and information extraction (IE) features, using various language resources and natural language processing.
Design/methodology/approach
The system is designed to integrate textual, lexical, semantic and terminological resources, enabling advanced document search and extraction of information. These resources are integrated with a set of Web services and applications, for different user profiles and use-cases.
Findings
The use of the system is illustrated by examples demonstrating keyword search supported by Web query expansion services, search based on regular expressions, corpus search based on local grammars, followed by extraction of information based on this search and finally, search with lexical masks using domain and semantic markers.
Originality/value
The presented system is the first software solution for implementation of human language technology in management of documentation from the mining engineering domain, but it is also applicable to other engineering and non-engineering domains. The system is independent of the type of alphabet (Cyrillic and Latin), which makes it applicable to other languages of the Balkan region related to Serbian, and its support for morphological dictionaries can be applied in most morphologically complex languages, such as Slavic languages. Significant search improvements and the efficiency of IE are based on semantic networks and terminology dictionaries, with the support of local grammars.}
}
@article{PABLE2022104,
title = {Linguistics for the apocalypse},
journal = {Language & Communication},
volume = {86},
pages = {104-110},
year = {2022},
issn = {0271-5309},
doi = {https://doi.org/10.1016/j.langcom.2022.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0271530922000428},
author = {Adrian Pablé},
keywords = {Posthumanism, Human exceptionalism, Apocalyptic linguistics, Object animacy, Anthropocentrism, Linguistic ideologies},
abstract = {Linguistics has, in recent years, gone through several turns that have been heralded as pushing the discipline beyond its traditional borders: among them are the agentive, the post- and transhumanist, the postdualist, the animal, the new materialist, the ontological and even the spectral turn. Now linguistics is writing the final chapter in its short disciplinary history: I call it the ‘apocalyptic turn’. Such an endtimes linguistics has been made possible because some of us have convinced ourselves and others that human linguistic exceptionalism is a harmful ideology that has held us captive for millennia in our thinking about human nature, nonhuman animals, the flora, the planet, the invisible world, and the importance of matter/materiality in particular. In this short piece I will discuss some examples indicating the intellectual sources this neo-pagan form of linguistics draws from as well as the consequences of its wide academic acceptance.}
}
@article{HANNOUSSE2021100415,
title = {Securing microservices and microservice architectures: A systematic mapping study},
journal = {Computer Science Review},
volume = {41},
pages = {100415},
year = {2021},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2021.100415},
url = {https://www.sciencedirect.com/science/article/pii/S1574013721000551},
author = {Abdelhakim Hannousse and Salima Yahiouche},
keywords = {Microservices, Microservice architectures, Security, Systematic mapping},
abstract = {Microservice architectures (MSA) are becoming trending alternatives to existing software development paradigms notably for developing complex and distributed applications. Microservices emerged as an architectural design pattern aiming to address the scalability and ease the maintenance of online services. However, security breaches have increased threatening availability, integrity and confidentiality of microservice-based systems. A growing body of literature is found addressing security threats and security mechanisms to individual microservices and microservice architectures. The aim of this study is to provide a helpful guide to developers about already recognized threats on microservices and how they can be detected, mitigated or prevented; we also aim to identify potential research gaps on securing MSA. In this paper, we conduct a systematic mapping in order to categorize threats on MSA with their security proposals. Therefore, we extracted threats and details of proposed solutions reported in selected studies. Obtained results are used to design a lightweight ontology for security patterns of MSA. The ontology can be queried to identify source of threats, security mechanisms used to prevent each threat, applicability layer and validation techniques used for each mechanism. The systematic search yielded 1067 studies of which 46 are selected as primary studies. The results of the mapping revealed an unbalanced research focus in favor of external attacks; auditing and enforcing access control are the most investigated techniques compared with prevention and mitigation. Additionally, we found that most proposed solutions are soft-infrastructure applicable layer compared with other layers such as communication and deployment. We also found that performance analysis and case studies are the most used validation techniques of security proposals.}
}
@article{CHIZHIKOVA2023106581,
title = {CARES: A Corpus for classification of Spanish Radiological reports},
journal = {Computers in Biology and Medicine},
volume = {154},
pages = {106581},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.106581},
url = {https://www.sciencedirect.com/science/article/pii/S001048252300046X},
author = {Mariia Chizhikova and Pilar López-Úbeda and Jaime Collado-Montañez and Teodoro Martín-Noguerol and Manuel C. Díaz-Galiano and Antonio Luna and L. Alfonso Ureña-López and M. Teresa Martín-Valdivia},
keywords = {Biomedical natural language processing, Radiology report, Text classification, Medical corpus, ICD-10, Transformer language model, Spanish medical resource},
abstract = {This paper presents a new corpus of radiology medical reports written in Spanish and labeled with ICD-10. CARES (Corpus of Anonymised Radiological Evidences in Spanish) is a high-quality corpus manually labeled and reviewed by radiologists that is freely available for the research community on HuggingFace. These types of resources are essential for developing automatic text classification tools as they are necessary for training and tuning computational systems. However, in the medical domain these are very difficult to obtain for different reasons including privacy and data protection issues or the involvement of medical specialists in the generation of these resources. We present a corpus labeled and reviewed by radiologists in their daily practice that is available for research purposes. In addition, after describing the corpus and explaining how it has been generated, a first experimental approach is carried out using several machine learning algorithms based on transformer language models such as BioBERT and RoBERTa to test the validity of this linguistic resource. The best performing classifier achieved 0.8676 micro and 0.8328 macro f1-score and these results encourage us to continue working in this research line.}
}
@article{LI2023110061,
title = {Variational autoencoder densified graph attention for fusing synonymous entities: Model and protocol},
journal = {Knowledge-Based Systems},
volume = {259},
pages = {110061},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.110061},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122011546},
author = {Qian Li and Daling Wang and Shi Feng and Kaisong Song and Yifei Zhang and Ge Yu},
keywords = {Open knowledge graph, Knowledge graph representation, Cluster ranking, Link prediction},
abstract = {The prediction of missing links of open knowledge graphs (OpenKGs) poses unique challenges compared with well-studied curated knowledge graphs (CuratedKGs). Unlike CuratedKGs whose entities are fully disambiguated against a fixed vocabulary, OpenKGs consist of entities represented by non-canonicalized free-form noun phrases and do not require an ontology specification, which drives the synonymity (multiple entities with different surface forms have the same meaning) and sparsity (a large portion of entities with few links). How to capture synonymous features in such sparse situations and how to evaluate the multiple answers pose challenges to existing models and evaluation protocols. In this paper, we propose VGAT, a variational autoencoder densified graph attention model to automatically mine synonymity features, and propose CR, a cluster ranking protocol to evaluate multiple answers in OpenKGs. For the model, VGAT investigates the following key ideas: (1) phrasal synonymity encoder attempts to capture phrasal features, which can automatically make entities with synonymous texts have closer representations; (2) neighbor synonymity encoder mines structural features with a graph attention network, which can recursively make entities with synonymous neighbors closer in representations. (3) densification attempts to densify the OpenKGs by generating similar embeddings and negative samples. For the protocol, CR is designed from the significance and compactness perspectives to comprehensively evaluate multiple answers. Extensive experiments and analysis show the effectiveness of the VGAT model and rationality of the CR protocol.}
}
@article{ZARGAR2024127561,
title = {Elucidation of molecular mechanisms, pathways, and diseases modulated by arsenicals through toxogenomics and multi-omics analysis},
journal = {Journal of Trace Elements in Medicine and Biology},
volume = {86},
pages = {127561},
year = {2024},
issn = {0946-672X},
doi = {https://doi.org/10.1016/j.jtemb.2024.127561},
url = {https://www.sciencedirect.com/science/article/pii/S0946672X24001810},
author = {Seema Zargar and Nojood Altwaijry and Humidah Alanazi and Atekah Hazzaa Alshammari and Hamad M. Alkahtani and Tanveer A. Wani},
keywords = {CBioPortal, GSEA, Hepatocellular carcinoma, Signaling pathways},
abstract = {Arsenic compounds exist in inorganic and organic forms with inorganic form confirmed as a potent carcinogen. Toxogenomics and multi-omics analysis were used to explore the molecular mechanisms of carcinogenecity induced by arsenicals. Comparative toxogenomics revealed sodium arsenite and arsenate as the most toxic arsenicals to humans, interacting with various genes and altering gene expression through mRNA binding proteins. Both metalloids were classified as Class II toxins by the ProTox II prediction tool, with a lethal dose (LD50) of 149 mg/kg body weight. The most frequently interacting genes were HMOX1, CAT, NFE2L2, CASP3, MAPK1, CXCL8, PARP1, TNF, and PYGM. Analysis of TCGA pan-cancer data revealed that 46 % of hepatocellular carcinoma patients exhibited alterations in the genes HMOX1, CAT, NFE2L2, CASP3, MAPK1, CXCL8, PARP1, TNF, and PYGM, suggesting their significant role in the development of this disease. The alteration in the gene list decreased the overall patient survival but insignificantly in the Kaplan-Meier curves revealing insignificant role in survival. GSEA suggested significant enrichment of the gene list in pathways involved in the G2M checkpoint, apoptosis, hypoxia, TNFA signaling via NFKB, PI3K AKTMTOR signaling, P53, IFN gamma and inflammatory response pathways revealing the involvement of these pathways. Ten microRNAs (miRNAs) regulated the expressions of the genes involved in the above-mentioned pathways with the significant enrichment in miR-21–3p, miR-206 and mir486a-5p. The relevant pathway and graphical representation of the network of miRNA-target interactions identified by the enrichment analysis along with disease ontologies were predicted. This study will be helpful insight into setting of laboratory experiments.}
}
@article{LIU2025111048,
title = {Enhancing machine tool predictive maintenance: A dual-model approach integrating improved deep autoencoders and graph attention network},
journal = {Computers & Industrial Engineering},
volume = {203},
pages = {111048},
year = {2025},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2025.111048},
url = {https://www.sciencedirect.com/science/article/pii/S0360835225001949},
author = {Changchun Liu and Dunbing Tang and Haihua Zhu and Qixiang Cai and Zequn Zhang and Qingwei Nie},
keywords = {Predictive Maintenance, CNN-BiLSTM-Autoencoder, Fault Prediction, Improved Graph Attention Network, Maintenance Service Recommendation},
abstract = {Along with the increasing amount and complication of machine tools, various machine faults appear and the stability of the inherent manufacturing process may be threatened. On the one hand, high-dimensional data is difficult to be effectively used to accurately predict the timing of a fault. On the other hand, even with a rough estimate of the fault time, it is difficult for maintenance personnel to quickly find the root cause of the fault during the stage of predictive maintenance. The fundamental reason is the lack of cognition and reasoning about the correlation of a large amount of underlying knowledge in fault prediction and maintenance. To address this issue, a dual-model driven predictive maintenance approach is proposed by using deep autoencoder and graph attention neural network to enhance the stability of machine tools. Firstly, a system architecture of the proposed dual-model driven predictive maintenance is designed with fault data acquisition of machine tools, fault prediction model driven by CNN-BiLSTM-Autoencoder, and Graph Attention Network-driven maintenance service recommendation, which can be illustrated minutely as follows. Based on the acquires high-dimensional fault data, a CNN-BiLSTM-Autoencoder-based fault prediction model is proposed for machine tools, which can capture the underlying relationships among fault features to calculate accurate fault prediction results. Based on the accurate fault prediction result, an effective maintenance service recommendation approach is proposed based on the improved Graph Attention Network combined with Neural Tensor Networks, which can capture more complex relationships among fault causes and maintenance services. Based on this, maintenance services that well match maintenance requirements for actual faults can be recommended. Finally, comparative experiments are conducted within a machining workshop featuring a diversity of machine tools, which confirm that the proposed approach can exceed traditional methods in terms of fault prediction and maintenance recommendation performance.}
}
@article{LEE20191952,
title = {An Efficient Design Support System based on Automatic Rule Checking and Case-based Reasoning},
journal = {KSCE Journal of Civil Engineering},
volume = {23},
number = {5},
pages = {1952-1962},
year = {2019},
issn = {1226-7988},
doi = {https://doi.org/10.1007/s12205-019-1750-2},
url = {https://www.sciencedirect.com/science/article/pii/S1226798824033658},
author = {Pin-Chan Lee and Tzu-Ping Lo and Ming-Yang Tian and Danbing Long},
keywords = {automatic rule checking, building information modelling, case-based reasoning, design support system, AHP, TOPSIS},
abstract = {A well building design support system can not only meet the rules but also automatically recommend the appropriate alternatives for designers, but most modifications now are conducted in the manual way. Although the method of automatic rule checking can effectively identify the compliance of rules in Building Information Modeling (BIM) models, recommendation supports are still lacked in applications. This paper aims to propose a design support system, using automatic rule checking to identify the compliance of rules and adopting case-based reasoning to provide recommendations via ontology and semantics. The AHP-TOPSIS (Analytic hierarchy process-Technique for Order Preference by Similarity to an Ideal Solution) method is used to give reliable recommendations rank. A real case is adopted as an illustrative example. Results show that the proposed system can increase the design efficiency in both design checking and modifying. Similar applications can be extended to other fields and rules.}
}
@incollection{MARTINIS202568,
title = {Text Mining: Named Entity Recognition},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {68-72},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00013-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027000130},
author = {Maria C. Martinis},
keywords = {Clinical domain., NER, NLP},
abstract = {Named Entity Recognition (NER) plays a fundamental role in extracting valuable information from unstructured clinical and bioinformatic text data. In the context of clinical and bioinformatic research, NER algorithms are essential for identifying and categorizing specific entities such as genes, proteins, diseases, medications, and clinical parameters from extensive textual sources. The clinical sector heavily relies on electronic health records (EHRs) and medical literature, which contain a wealth of patient information. NER aids in automating the extraction of crucial patient-specific details, enabling healthcare professionals to make informed decisions, improve patient care, and support clinical research. In bioinformatics, it assists in the analysis of genomic and proteomic data by accurately identifying genes, proteins, and their interactions in scientific literature. However, NER in these domains faces various challenges, including the ambiguity of entity mentions, variations in terminology, and domain-specific jargon. Recent advancements in deep learning techniques, such as transformer-based models, have shown significant performance improvements in NER tasks in clinical and bioinformatic contexts. These models leverage pre-trained representations and can be fine-tuned on domain-specific datasets to achieve state-of-the-art results. Additionally, the availability of large-scale annotated corpora and open-source tools has made NER more accessible to researchers and practitioners in these fields. In conclusion, NER is a crucial component in the extraction of structured information from unstructured text data in clinical and bioinformatic domains. Despite the challenges, recent advancements in machine learning and natural language processing techniques have significantly enhanced NER׳s accuracy and efficiency, opening up new possibilities for knowledge extraction and discovery in these critical domains.}
}
@article{DEEKEN2018146,
title = {Grounding semantic maps in spatial databases},
journal = {Robotics and Autonomous Systems},
volume = {105},
pages = {146-165},
year = {2018},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0921889017306565},
author = {Henning Deeken and Thomas Wiemann and Joachim Hertzberg},
keywords = {Semantic mapping, Spatial analysis, Knowledge representation},
abstract = {Semantic maps add to classic robot maps spatially grounded object instances anchored in a suitable way for knowledge representation and reasoning. They enable a robot to solve reasoning problems of geometrical, topological, ontological and logical nature in addition to localization and path planning. Recent literature on semantic mapping lacks effective and efficient approaches for grounding qualitative spatial relations through analysis of the quantitative geometric data of the mapped entities. Yet, such qualitative relations are essential to perform spatial and ontological reasoning about objects in the robot’s surroundings. This article contributes a framework for semantic map representation, called SEMAP, to overcome this missing aspect. It is able to manage full 3D maps with geometric object models and the corresponding semantic annotations as well as their relative spatial relations. For that, spatial database technology is used to solve the representational and querying problems efficiently. This article describes the extensions necessary to make a spatial database suitable for robotic applications. Especially, we add 3D spatial operators and a tree of transformations to represent relative position information. We evaluate the implemented capabilities and present real life use cases of SEMAP in different application domains.}
}
@article{HELBIG2020107970,
title = {Personalized medicine in genetic epilepsies – possibilities, challenges, and new frontiers},
journal = {Neuropharmacology},
volume = {172},
pages = {107970},
year = {2020},
issn = {0028-3908},
doi = {https://doi.org/10.1016/j.neuropharm.2020.107970},
url = {https://www.sciencedirect.com/science/article/pii/S0028390820300368},
author = {Ingo Helbig and Colin A. Ellis},
keywords = {Epilepsy, Neurogenetics, Precision medicine, Electronic medical records, Human phenotype ontology},
abstract = {Identifying the optimal treatment based on specific characteristics of each patient is the main promise of precision medicine. In the field of epilepsy, the identification of more than 100 causative genes provides the enticing possibility of treatments targeted to specific disease etiologies. These conditions include classical examples, such as the use of vitamin B6 in antiquitin deficiency or the ketogenic diet in GLUT1 deficiency, where the disease mechanism can be directly addressed by the selection of a specific therapeutic compound. For epilepsies caused by channelopathies there have been advances in understanding how the selection of existing medications can be targeted to the functional consequences of genetic alterations. We discuss the examples of the use of sodium channel blockers such as phenytoin and oxcarbazepine in the sodium channelopathies, quinidine in KCNT1-related epilepsies, and strategies in GRIN-related epilepsies as examples of epilepsy precision medicine. Assessing the clinical response to targeted treatments of these conditions has been complicated by genetic and phenotypic heterogeneity, as well as by various neurological and non-neurological comorbidities. Moving forward, the development of standardized outcome measures will be critical to successful precision medicine trials in complex and heterogeneous disorders like the epilepsies. Finally, we address new frontiers in epilepsy precision medicine, including the need to match the growing volume of genetic data with high-throughput functional assays to assess the functional consequences of genetic variants and the ability to extract clinical data at large scale from electronic medical records and apply quantitative methods based on standardized phenotyping language.}
}
@article{GENTILE2024168,
title = {On compatibility between realism and fictionalism: A response to Suárez' proposal},
journal = {Studies in History and Philosophy of Science},
volume = {103},
pages = {168-175},
year = {2024},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2023.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0039368123001711},
author = {Nélida Gentile and Susana Lucero},
keywords = {Fictionalism, Scientific realism, Instrumentalism, Pragmatism, Mauricio suárez},
abstract = {In a series of articles, Mauricio Suárez defends the neutrality of fictionalism with respect to the scientific realism-anti-realism debate. Suárez understands fictionalism from a strictly methodological point of view, linked to the practice of model building in the context of the philosophy of science. He moves away from the type of fictionalism analysed in other areas of philosophy such as metaphysics, the philosophy of language, aesthetics or the philosophy of mathematics. Following Vaihinger's position, he emphasizes the inferential role of fiction in scientific modelling and argues that scientific fictionalism is not incompatible with scientific realism, as is often believed. We argue against Suárez's position and reject the ubiquitous character assigned to fictions in scientific discourse, as well as the deflationary view of scientific realism defended by Suárez. We conclude that when the semantic, epistemic, and metaphysical aspects at stake in the realism-antirealism debate are taken into account, the alleged compatibility between scientific realism and fictionalism starts to generate some tension.}
}
@article{CHEN2022100001,
title = {Vision, status, and research topics of Natural Language Processing},
journal = {Natural Language Processing Journal},
volume = {1},
pages = {100001},
year = {2022},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2022.100001},
url = {https://www.sciencedirect.com/science/article/pii/S2949719122000012},
author = {Xieling Chen and Haoran Xie and Xiaohui Tao},
keywords = {Natural Language Processing, NLP, , Trustworthy Artificial Intelligence},
abstract = {The field of Natural Language Processing (NLP) has evolved with, and as well as influenced, recent advances in Artificial Intelligence (AI) and computing technologies, opening up new applications and novel interactions with humans. Modern NLP involves machines’ interaction with human languages for the study of patterns and obtaining meaningful insights. NLP is increasingly receiving attention across academia and industry and demonstrates extraordinary opportunities and across AI applications (e.g., question answering, information retrieval, sentiment analysis, and recommender systems) and helps to deal with new tasks such as machine translation and reading comprehension, with real world performance improving all the time. This editorial first provides an overview of the field of NLP in terms of research grants, publication venues, and research topics. We then introduce the mission of Natural Language Processing Journal, a new NLP-focused Elsevier journal intended as a forum for researchers and practitioners to publish theoretical, practical, and methodological achievements related to trustworthy AI development and applications for analyzing, processing, and modeling human languages.}
}
@article{DECARVALHO202196,
title = {The enactive computational basis of cognition and the explanatory cognitive basis for computing},
journal = {Cognitive Systems Research},
volume = {67},
pages = {96-103},
year = {2021},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2020.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1389041720301108},
author = {Leonardo Lana {de Carvalho} and João Eduardo Kogler},
keywords = {Cognitive systems, Enaction, Computing, Socio-natural practices},
abstract = {The computational theory of cognition, or computationalism, holds that cognition is a form of computation. Two issues related to this view are comprised by the goal of this paper: A) Computing systems are traditionally seen as representational systems, but functional and enactive approaches support non-representational theories; B) Recently, a sociocultural theory against computationalism was proposed with the aim of ontologically reducing computing to cognition. We defend, however, that cognition and computation are in action, thus cognition is just a form of computing and that cognition is the explanatory basis for computation. We state that: 1. Representational theories of computing recurring to intentional content run into metaphysical problems. 2. Functional non-representational theories do not incur this metaphysical problem when describing computing in terms of the abstract machine. 3. Functional theories are consistent with enactive in describing computing machines not in a strictly functional way, but especially in terms of their organization. 4. Enactive cognition is consistent with the computationalism in describing Turing machines as functionally and organizationally closed systems. 5. The cognitive explanatory basis for computing improves the computational theory of cognition. When developed in the human linguistic domain, computer science is seen as a product of human socionatural normative practices, however, cognition is just an explanatory, not ontological, basis for computing. The paper concludes by supporting that computation is in action, that cognition is just one form of computing in the world and the explanatory basis for computation.}
}
@article{FIMMERS20211239,
title = {An Approach for Knowledge-Driven, Flexible Process Generation},
journal = {Procedia CIRP},
volume = {104},
pages = {1239-1244},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.208},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121011069},
author = {Christian Fimmers and Simon Storms and Werner Herfs and Christian Brecher},
keywords = {Knowledge-Based Systems, Flexible Production, Computer-Aided Process Planning},
abstract = {Increasing complexity in processes and individualization of products results in complicated planning processes. Multi-variant products and frequently changing production requirements must be met by flexibility in the processes. For loosely coupled production systems, it is difficult to keep track of the available resources. Therefore, planning processes must be supported with suitable systems. This contribution presents a support system, which transfers the knowledge into ontologies and uses these to support the planning. Resources and their capabilities can be combined freely to achieve maximum flexibility. The procedure is demonstrated within the supply chain in forestry – a distributed and loosely coupled domain.}
}
@article{PAUWELS2024102426,
title = {Validation of technical requirements for a BIM model using semantic web technologies},
journal = {Advanced Engineering Informatics},
volume = {60},
pages = {102426},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102426},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624000740},
author = {Pieter Pauwels and Ellen {van den Bersselaar} and Lucas Verhelst},
keywords = {Code compliance checking, Semantic web, Rule classification, SHACL},
abstract = {While progressing through the different stages of a project in the architecture, engineering, and construction (AEC) sector, a project must continuously comply with several requirements. Yet, human skills are often essential when it comes to validating these requirements, which can be error-prone and contributes to the loss of information in the information stream of a project. As has been indicated before, higher levels of automation can be achieved by describing, querying, and validating AEC project data using the technology stack offered by the Semantic Web (SW). Particularly the logical basis of these technologies (semantic rule-checking) is promising, as it allows a more consistent representation and checking of such requirements. As part of the development of a decentralized rule-checking system using SW open standards, this study investigates particularly to what extent the more challenging requirements can be checked, such as 3D geometric requirements and more ambiguous or vague requirements. Tangible and clear results described in this report are (1) a concise classification of rule types that apply to BIM models, (2) a system architecture for a semantic rule-checking system, and (3) a demonstration of this system through three use cases. The results show how 66% of a document containing human-readable requirements can be validated automatically. Furthermore, a clear indication is given of how the semantic compliance check can be combined with 3D geometric compliance checks into a scalable web-friendly architecture.}
}
@article{SALMONGOMEZ2025205,
title = {FoxP2 and Schizophrenia: a systematic review},
journal = {Journal of Psychiatric Research},
volume = {190},
pages = {205-215},
year = {2025},
issn = {0022-3956},
doi = {https://doi.org/10.1016/j.jpsychires.2025.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S0022395625004558},
author = {Gabriel Salmón-Gómez and Paula Suárez-Pinilla and Esther Setién-Suero and Carlos Martínez-Asensi and Rosa Ayesa-Arriola},
keywords = {Schizophrenia, Psychosis, Language, Speech, Systematic review, Neurobiology},
abstract = {Schizophrenia (SCZ) is a neurodevelopmental psychiatric disorder characterized by impaired information processing and neural circuit dysfunction. FoxP2, an ontological transcription factor, is crucial for brain development and neuronal differentiation. This systematic review explores the association between FoxP2 polymorphisms and SCZ using PRISMA guidelines to search PubMed and EMBASE. Articles were selected based on predefined criteria, and their findings were systematically evaluated. While no FoxP2 polymorphism was significantly associated with SCZ risk, specific variants showed relevance to clinical manifestations. Rs10447760 is linked to symptom severity and Body Mass Index (BMI), rs1456031 correlated with childhood parental abuse and auditory verbal hallucinations (AVH), rs2253478 is associated with poverty of speech, and rs2396753 is significantly related to reduced grey matter density (GMD) in SCZ patients. These findings suggest that FoxP2 polymorphisms may influence SCZ-related traits such as weight gain, language impairments, reduced GMD, and trauma-associated AVH. However, the limited sample sizes and scope of current studies highlight the need for further research to clarify FoxP2's role in less explored aspects of SCZ.}
}
@article{GARG2025100743,
title = {An analysis of acoustic features for accented speech classification},
journal = {Egyptian Informatics Journal},
volume = {31},
pages = {100743},
year = {2025},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2025.100743},
url = {https://www.sciencedirect.com/science/article/pii/S1110866525001367},
author = {Apar Garg and Yassine Aribi and Turke Althobaiti and Tanmay Bhowmik},
keywords = {Speech accent classification, Feature extraction, Native accent, Non-native accent, Speech acoustic features},
abstract = {Spoken language is a topic which lured researchers for a long duration. Due to the variety of different voice-based products, the application of spoken language can be observed in various places. Several home assistant systems have become an integral part of our lives as they make mundane tasks such as setting up reminders and checking emails easy. However, non-native English speakers frequently face problems in using automated assistants because of accented speech. This study presents an analysis of speech accent features for accented speech classification. The aim is to identify which speech features are the most important for accurately classifying accents in spoken language. We collected a dataset of accented speech samples and used various feature extraction techniques to extract relevant features from the speech signal. These features included mel frequency cepstral coefficients, zero-crossing rate, spectral features, chroma features, etc. Machine learning algorithms are used to classify the accents based on the extracted features and achieve an overall accuracy of 86.67%. This research work is prompted by the increasing need to develop robust speech recognition systems that can generalize across regional accents. The performance of standard automatic speech recognition systems drops very often due to accented speech. Several studies tend towards deep learning-based solutions; however, there is a lack of focused analysis of the performance of traditional acoustic features in accent discrimination tasks. This study targets to bridge that gap by performing a comparative study on selected acoustic features. The analysis of speech accent features presented in this study can be useful to develop robust speech accent classification systems for applications such as language learning, speech recognition, and accent identification.}
}
@article{TRAGER2024466,
title = {Artificial intelligence for nonmelanoma skin cancer},
journal = {Clinics in Dermatology},
volume = {42},
number = {5},
pages = {466-476},
year = {2024},
note = {Artificial Intelligence II},
issn = {0738-081X},
doi = {https://doi.org/10.1016/j.clindermatol.2024.06.016},
url = {https://www.sciencedirect.com/science/article/pii/S0738081X24001007},
author = {Megan H. Trager and Emily R. Gordon and Alyssa Breneman and Chunhua Weng and Faramarz H. Samie},
abstract = {Nonmelanoma skin cancers (NMSCs) are among the top five most common cancers globally. NMSC is an area with great potential for novel application of diagnostic tools including artificial intelligence (AI). In this scoping review, we aimed to describe the applications of AI in the diagnosis and treatment of NMSC. Twenty-nine publications described AI applications to dermatopathology including lesion classification and margin assessment. Twenty-five publications discussed AI use in clinical image analysis, showing that algorithms are not superior to dermatologists and may rely on unbalanced, nonrepresentative, and nontransparent training data sets. Sixteen publications described the use of AI in cutaneous surgery for NMSC including use in margin assessment during excisions and Mohs surgery, as well as predicting procedural complexity. Eleven publications discussed spectroscopy, confocal microscopy, thermography, and the AI algorithms that analyze and interpret their data. Ten publications pertained to AI applications for the discovery and use of NMSC biomarkers. Eight publications discussed the use of smartphones and AI, specifically how they enable clinicians and patients to have increased access to instant dermatologic assessments but with varying accuracies. Five publications discussed large language models and NMSC, including how they may facilitate or hinder patient education and medical decision-making. Three publications pertaining to the skin of color and AI for NMSC discussed concerns regarding limited diverse data sets for the training of convolutional neural networks. AI demonstrates tremendous potential to improve diagnosis, patient and clinician education, and management of NMSC. Despite excitement regarding AI, data sets are often not transparently reported, may include low-quality images, and may not include diverse skin types, limiting generalizability. AI may serve as a tool to increase access to dermatology services for patients in rural areas and save health care dollars. These benefits can only be achieved, however, with consideration of potential ethical costs.}
}
@article{ZAVARELLA2024e32479,
title = {Triplétoile: Extraction of knowledge from microblogging text},
journal = {Heliyon},
volume = {10},
number = {12},
pages = {e32479},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e32479},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024085104},
author = {Vanni Zavarella and Sergio Consoli and Diego {Reforgiato Recupero} and Gianni Fenu and Simone Angioni and Davide Buscaldi and Danilo Dessí and Francesco Osborne},
keywords = {Information extraction, Knowledge graphs, Social media analysis, Named entity recognition, Hierarchical clustering, Word embeddings},
abstract = {Numerous methods and pipelines have recently emerged for the automatic extraction of knowledge graphs from documents such as scientific publications and patents. However, adapting these methods to incorporate alternative text sources like micro-blogging posts and news has proven challenging as they struggle to model open-domain entities and relations, typically found in these sources. In this paper, we propose an enhanced information extraction pipeline tailored to the extraction of a knowledge graph comprising open-domain entities from micro-blogging posts on social media platforms. Our pipeline leverages dependency parsing and classifies entity relations in an unsupervised manner through hierarchical clustering over word embeddings. We provide a use case on extracting semantic triples from a corpus of 100 thousand tweets about digital transformation and publicly release the generated knowledge graph. On the same dataset, we conduct two experimental evaluations, showing that the system produces triples with precision over 95% and outperforms similar pipelines of around 5% in terms of precision, while generating a comparatively higher number of triples.}
}
@article{CHANG2023101254,
title = {On bridging consumer health search across languages using cross-lingual word space},
journal = {Electronic Commerce Research and Applications},
volume = {59},
pages = {101254},
year = {2023},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2023.101254},
url = {https://www.sciencedirect.com/science/article/pii/S1567422323000194},
author = {Chia-Hsuan Chang and Christopher C. Yang},
keywords = {Consumer health search, Consumer health vocabulary, Cross-lingual word space, Cross-lingual information retrieval, Search engine},
abstract = {Information search is a lucrative activity for electronic commerce, contributing to significant revenues. As search engine becomes the primary source for seeking health information, the search quality concerns consumers. Specifically, the health vocabulary used by general consumers differs from the professionals, leading to unsatisfied retrieval performance and misleading information. The situation is increasingly complex when consumers search health information across languages. To study consumer health search (CHS) across languages, we propose a cross-lingual retrieval framework comprising semantic space construction and information retrieval modules. The semantic space construction module adopts a weakly-supervised approach to determine a cross-lingual word space (CLWS) from collected consumer-generated health content, which helps capture consumers’ vocabulary and identify medical expression translations. The information retrieval module suggests strategies that utilize the translations of CLWS for the subsequent retrieval tasks. By evaluating the performance on two health information search engines, we found that Google Translate (GT), a widely-adopted translation service, is prone to generate mistranslations to the colloquial medical expressions. The CLWS helps identify GT’s mistranslations and filter out the results retrieved by those. More importantly, our framework demonstrates a strategy to integrate translations of CLWS and GT to reach the best retrieval performance for the cross-lingual CHS task.}
}
@article{ROBSON2020103621,
title = {Extension of the Quantum Universal Exchange Language to precision medicine and drug lead discovery. Preliminary example studies using the mitochondrial genome},
journal = {Computers in Biology and Medicine},
volume = {117},
pages = {103621},
year = {2020},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2020.103621},
url = {https://www.sciencedirect.com/science/article/pii/S0010482520300202},
author = {Barry Robson},
keywords = {Data analytics, Data mining, Genomics, Bioinformatics, Universal exchange language, Quantum mechanics, Dirac notation, Inference net, Hyperbolic Dirac net, Bayes net, Clinical decision support},
abstract = {The Quantum Universal Exchange Language (Q-UEL) based on Dirac notation and algebra from quantum mechanics, along with its associated data mining and Hyperbolic Dirac Net (HDN) for probabilistic inference, has proven to be a useful architectural principle for knowledge management, analysis and prediction systems in medicine. It has been described in several papers; here is described its extension to clinical genomics and precision medicine. Two use cases are studied: (a) bioinformatics in clinical decision support especially for risk for type 2 diabetes using mitochondrial patient DNA sequences, and (b) bioinformatics and computational biology (conformational) research examples related to drug discovery involving the recently discovered class of mitochondrial derived peptides (MDPs). MDPs were surprising when first discovered as coded in small open reading frames (sORFs), and are emerging as having a fundamental role in metabolic control, longevity and disease. This project originally represented a language specification study relating to what information related to genomics is essential or useful to carry, and what processing will be needed. However, novel aspects introduced or discovered include the HDN-like neural nets and their use, along with more established methods, for prediction of type 2 diabetes, and in particular for proposals for over 80 natural MDPs most of which that have not previously been described at the time of the study, as potential drug lead targets. Also, use of many medical records with simulated joining of mtDNA as performance tests led to some insightful observations regarding the behavior of HDN predictions where independent factors are involved.}
}
@article{LIETO2021107166,
title = {A commonsense reasoning framework for explanatory emotion attribution, generation and re-classification},
journal = {Knowledge-Based Systems},
volume = {227},
pages = {107166},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107166},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121004299},
author = {Antonio Lieto and Gian Luca Pozzato and Stefano Zoia and Viviana Patti and Rossana Damiano},
keywords = {Explainable AI, Commonsense reasoning, Knowledge generation, Concept combination, Computational models of emotion},
abstract = {We present DEGARI (Dynamic Emotion Generator And ReclassIfier), an explainable system for emotion attribution and recommendation. This system relies on a recently introduced commonsense reasoning framework, the TCL logic, which is based on a human-like procedure for the automatic generation of novel concepts in a Description Logics knowledge base. Starting from an ontological formalization of emotions based on the Plutchik model, known as ArsEmotica, the system exploits the logic TCL to automatically generate novel commonsense semantic representations of compound emotions (e.g. Love as derived from the combination of Joy and Trust according to Plutchik). The generated emotions correspond to prototypes, i.e. commonsense representations of given concepts, and have been used to reclassify emotion-related contents in a variety of artistic domains, ranging from art datasets to the editorial contents available in RaiPlay, the online platform of RAI Radiotelevisione Italiana (the Italian public broadcasting company). We show how the reported results (evaluated in the light of the obtained reclassifications, the user ratings assigned to such reclassifications, and their explainability) are encouraging, and pave the way to many further research directions.}
}
@article{GARDINER2024103944,
title = {Rise of the Allotrope Simple Model: Update from 2023 Fall Allotrope Connect},
journal = {Drug Discovery Today},
volume = {29},
number = {4},
pages = {103944},
year = {2024},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2024.103944},
url = {https://www.sciencedirect.com/science/article/pii/S1359644624000692},
author = {Spencer Gardiner and Christopher Haynie and Dennis {Della Corte}},
keywords = {Allotrope, JSON, ASM, Precompetitive, Data Science},
abstract = {The Allotrope Foundation (AF) started as a group of pharmaceutical companies, instrument, and software vendors that set out to simplify the exchange of data in the laboratory. After a decade of work, they released products that have found adoption in various companies. Most recently, the Allotrope Simple Model (ASM) was developed to speed up and widen the adoption. As a result, the Foundation has recently added chemical companies and, importantly, is reworking its business model to lower the entry barrier for smaller companies. Here, we present the proceedings from the Allotrope Connect Fall 2023 conference and summarize the technical and organizational developments at the Foundation since 2020.}
}
@article{TOP2022106909,
title = {Cultivating FAIR principles for agri-food data},
journal = {Computers and Electronics in Agriculture},
volume = {196},
pages = {106909},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.106909},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922002265},
author = {Jan Top and Sander Janssen and Hendrik Boogaard and Rob Knapen and Görkem Şimşek-Şenel},
keywords = {Agriculture, Food supply chain, Data sharing, FAIR principles, Ontology, Controlled vocabulary},
abstract = {Data generated by the global food system is crucial in the transformation towards sustainable, resilient, and high-quality food production. Although the amount of potentially useful data is growing rapidly, its (re)use is still limited. The FAIR-principles have been developed for making data findable, accessible, interoperable, and reusable both by humans and machines. This paper explores the further operationalization of the FAIR principles in agriculture and food. Experience shows that several conditions must be fulfilled before data can be effectively shared and reused. First, automated tools must be available for data providers and users. Secondly, we need a community-based approach in developing tools and vocabularies. Thirdly, data cannot be shared by an open-by-default policy only. Finally, scientific insight is needed in how data is actually (re)used in scientific communities. We conclude that bringing the FAIR-principles to full maturity requires a fair balance of efforts within the agri-food communities, supported by a proper infrastructure.}
}
@article{NEJI20211111,
title = {HyRa: An Effective Hybrid Ranking Model},
journal = {Procedia Computer Science},
volume = {192},
pages = {1111-1120},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.114},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921016021},
author = {Sameh Neji and Tarek Chenaina and Abdullah M. Shoeb and Leila Ben Ayed},
keywords = {ranking model, language model, information retrieval, semantic information retrieval, semantic similarity},
abstract = {Information retrieval (IR) systems are concerned of the processing of the textual content of the document, the query, and issues of system effectiveness. IR models are means to integrate many sources of evidence on documents to achieve an effective retrieval system. Unlike traditional parameters such as word frequency in documents, semantic information retrieval is an environment that requires the application of semantic techniques. These techniques calculate a degree of query-document relevance to assign the highest rankings to the documents which are semantically closer to the query. To meet the user’s information needs and to optimize the performance of IR system, an innovative model was proposed to calculate the relevance of query-document pairs. The proposed model is a hybrid one that based on a query likelihood language model and a conceptual weighting model. It exploits the semantic similarity between document and query concepts and some document/collection statistics. The conducted experiments on the proposed model show improvement of the standard performance measures over the benchmark competitors. A statistical significance test is conducted to exclude the performance random variation of the compared models. Paired t-tests show that the proposed model is highly significant improves the most important measures.}
}
@article{SCHNEIDER2019189,
title = {Knowledge-based Conversion of Finite State Machines in Manufacturing Automation},
journal = {Procedia Manufacturing},
volume = {28},
pages = {189-194},
year = {2019},
note = {7th International conference on Changeable, Agile, Reconfigurable and Virtual Production (CARV2018)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.12.031},
url = {https://www.sciencedirect.com/science/article/pii/S235197891831374X},
author = {Georg Ferdinand Schneider and Georg Ambrosius Peßler and Walter Terkaj},
keywords = {Automation, Control, Finite State Machines, Ontology, Knowledge-based Method},
abstract = {More and more information and communication technologies originating from the web are introduced in industrial automation systems. The vision for future automation systems includes intelligent self-descriptive components, which exchange information and potentially reason by themselves through knowledge-assisted methods. Formal domain descriptions are required to enable this vision, including knowledge related to mechanical, electrical and control domains. This work focuses on formalizing knowledge of the automation and control domain and investigates how knowledge-based methods can support the automated conversion between different formalisms for modelling discrete behaviour in manufacturing automation: finite state machines. We detail our approach by deploying the presented method in a use case related to the automation of a pick and place unit available from the literature.}
}
@article{RAWASHDEH201886,
title = {Reliable service delivery in Tele-health care systems},
journal = {Journal of Network and Computer Applications},
volume = {115},
pages = {86-93},
year = {2018},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2018.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S1084804518301462},
author = {Majdi Rawashdeh and Mohammed GH. {AL Zamil} and M. Shamim Hossain and Samer Samarah and Syed Umar Amin and Ghulam Muhammad},
keywords = {Tele-health, IoT, Cloud computing, Network applications, Data mining},
abstract = {Modern ICT Applications on Tele-health focuses on providing the smart infrastructure that facilitates the delivery of health services. While Internet-of-Things (IoT) and cloud-computing platforms assist the implementation of such architecture, the reliability of service delivery during network disconnection is still an open issue in this domain. This paper proposes a prediction methodology that is able to deliver reliable services with acceptable accuracy by incorporating domain-specific knowledge into exchanged data. The proposed service will be of a great value in a situation where the network availability is not reliable. The contributions of this work are to 1) measure the impact of ontology enrichment on classifying the health data, 2) develop a prediction model that is able to predict patients' readings with an acceptable accuracy, and 3) minimize communicating messages among the network components. Three experiments have been conducted on a real health dataset to measure the performance of the proposed methodology. The results showed that our proposed methodology improved the reliability of the Tele-health services implemented on the top of IoT and cloud-computing platforms.}
}
@article{LUAN2024927,
title = {Knowledge graph-based Bayesian network risk assessment model for hydrogen accidents},
journal = {International Journal of Hydrogen Energy},
volume = {81},
pages = {927-941},
year = {2024},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2024.07.339},
url = {https://www.sciencedirect.com/science/article/pii/S0360319924030246},
author = {Tingting Luan and Hongru Li and Kai Wang and Xue Zhang and Xiaoyun Li},
keywords = {Hydrogen accidents, Knowledge graph, Bayesian network, Risk assessment, Fuzzy set theory},
abstract = {A Bayesian network risk assessment model for hydrogen accidents based on a knowledge graph is proposed. The study uses hydrogen accident report texts to form hydrogen accident knowledge texts by analyzing, processing, and extracting text fragments with transparent causal relationships. Then, the Bert-BilSTM-CRF algorithm is used to extract knowledge, and the results are stored in the Neo4j database to construct a knowledge map, obtain the risk factors of hydrogen accidents, and complete the structural modeling of the Bayesian network. The improved SAM method is used to aggregate expert opinions, calculate the prior and posterior probability, and realize the parameter learning of the Bayesian network. Based on a knowledge graph, this forms a Bayesian network risk assessment model for hydrogen accidents. This model can evaluate and warn against hydrogen accidents in a data-driven manner and promote knowledge acquisition, analysis, and decision-making on hydrogen energy storage and transportation risks.}
}
@article{CUNLIFFE2022101311,
title = {Natural language processing for under-resourced languages: Developing a Welsh natural language toolkit},
journal = {Computer Speech & Language},
volume = {72},
pages = {101311},
year = {2022},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2021.101311},
url = {https://www.sciencedirect.com/science/article/pii/S088523082100108X},
author = {Daniel Cunliffe and Andreas Vlachidis and Daniel Williams and Douglas Tudhope},
keywords = {Natural language processing, Under-resourced languages, Welsh, Cymraeg, Language technology},
abstract = {Language technology is becoming increasingly important across a variety of application domains which have become common place in large, well-resourced languages. However, there is a danger that small, under-resourced languages are being increasingly pushed to the technological margins. Under-resourced languages face significant challenges in delivering the underlying language resources necessary to support such applications. This paper describes the development of a natural language processing toolkit for an under-resourced language, Cymraeg (Welsh). Rather than creating the Welsh Natural Language Toolkit (WNLT) from scratch, the approach involved adapting and enhancing the language processing functionality provided for other languages within an existing framework and making use of external language resources where available. This paper begins by introducing the GATE NLP framework, which was used as the development platform for the WNLT. It then describes each of the core modules of the WNLT in turn, detailing the extensions and adaptations required for Welsh language processing. An evaluation of the WNLT is then reported. Following this, two demonstration applications are presented. The first is a simple text mining application that analyses wedding announcements. The second describes the development of a Twitter NLP application, which extends the core WNLT pipeline. As a relatively small-scale project, the WNLT makes use of existing external language resources where possible, rather than creating new resources. This approach of adaptation and reuse can provide a practical and achievable route to developing language resources for under-resourced languages.}
}
@article{BRUNDAGE202142,
title = {Technical language processing: Unlocking maintenance knowledge},
journal = {Manufacturing Letters},
volume = {27},
pages = {42-46},
year = {2021},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2020.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S2213846320301668},
author = {Michael P. Brundage and Thurston Sexton and Melinda Hodkiewicz and Alden Dima and Sarah Lukens},
keywords = {Maintenance, Natural language processing, Asset management, Technical language processing, Artificial intelligence},
abstract = {Out-of-the-box natural-language processing (NLP) pipelines need re-imagining to understand and meet the requirements of engineering data. Text-based documents account for a significant portion of data collected during the life cycle of asset management and the valuable information these documents contain is underutilized in analysis. Meanwhile, researchers historically design NLP pipelines with non-technical language in mind. This means industrial implementations are built on tools intended for non-technical use cases, suffering from a lack of verification, validation, and ultimately, personnel trust. To mitigate these sources of risk, we encourage a holistic, domain-driven approach to using NLP in a technical engineering setting, a paradigm we refer to as Technical Language Processing (TLP). Toward this end, the industrial asset management community must collectively redouble efforts toward production of and consensus around key domain-specific resources, including: (1) goal-driven data representations, (2) flexible entity type definitions and dictionaries, and (3) improved access to data-sets – raw and annotated. This collective action allows the maintenance community to follow in the path of other scientific communities, e.g., medicine, to develop and utilize these public resources to make TLP a key contributor to Industry 4.0.}
}
@article{CIVITARESE201988,
title = {newNECTAR: Collaborative active learning for knowledge-based probabilistic activity recognition},
journal = {Pervasive and Mobile Computing},
volume = {56},
pages = {88-105},
year = {2019},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2019.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S1574119218303572},
author = {Gabriele Civitarese and Claudio Bettini and Timo Sztyler and Daniele Riboni and Heiner Stuckenschmidt},
keywords = {Pervasive computing, Activity recognition, Active learning, Temporal pattern mining},
abstract = {The increasing popularity of ambient assisted living solutions is claiming adaptive and scalable tools to monitor activities of daily living. Currently, most sensor-based activity recognition techniques rely on supervised learning algorithms. However, the acquisition of comprehensive training sets of activities in smart homes is expensive and violates the individual’s privacy. In this work, we address this problem by proposing a novel hybrid approach that couples collaborative active learning with probabilistic and knowledge-based reasoning. The rationale of our approach is that a generic, and possibly incomplete, knowledge-based model of activities can be refined to target specific individuals and environments by collaboratively acquiring feedback from inhabitants. Specifically, we propose a collaborative active learning method exploiting users’ feedback to (i) refine correlations among sensor events and activity types that are initially extracted from a high-level ontology, and (ii) mine temporal patterns of sensor events that are frequently generated by the execution of specific activities. A Markov Logic Network is used to recognize activities with probabilistic rules that capture both the ontological knowledge and the information obtained by active learning. We experimented our solution with a real-world dataset of activities carried out by several individuals in an interleaved fashion. Experimental results show that our collaborative and personalized active learning solution significantly improves recognition rates, while triggering a small number of feedback requests. Moreover, the overall recognition rates compare favorably with existing supervised and unsupervised activity recognition methods.}
}
@article{LACHHEB2025101021,
title = {AI in higher education: A bibliometric analysis, synthesis, and a critique of research},
journal = {The Internet and Higher Education},
volume = {67},
pages = {101021},
year = {2025},
issn = {1096-7516},
doi = {https://doi.org/10.1016/j.iheduc.2025.101021},
url = {https://www.sciencedirect.com/science/article/pii/S1096751625000302},
author = {Ahmed Lachheb and Javier Leung and Victoria Abramenka-Lachheb and Rajagopal Sankaranarayanan},
keywords = {Artificial intelligence in higher education, Inclusive education with AI, Generative AI in higher education},
abstract = {To better characterize and understand AI in higher education and its role in relation to educational disparities and inclusivity, this paper presents a comprehensive bibliometric assessment of research on AI in higher education. Using quantitative topic modeling and qualitative analysis methods, this study describes: (1) the research landscape of AI in higher education and (2) the common topics of AI in higher education research, including topics related to inclusive education. Based on these descriptions, this study offers a synthesis and critique of research on AI in higher education on the following issues: (a) the use of AI to address educational disparities and foster inclusivity, (b) the ethics of AI-powered large language learning models and translation tools, and (c) AI literacy. The findings of this study call on higher education scholars/researchers to reaffirm higher education research and educational mission, and the standards of rigorous research to lead the knowledge on AI.}
}
@article{LI2025115773,
title = {Unveiling the evolution of antimicrobial peptides in gut microbes via foundation-model-powered framework},
journal = {Cell Reports},
volume = {44},
number = {6},
pages = {115773},
year = {2025},
issn = {2211-1247},
doi = {https://doi.org/10.1016/j.celrep.2025.115773},
url = {https://www.sciencedirect.com/science/article/pii/S2211124725005443},
author = {Wenhui Li and Baicheng Huang and Menghao Guo and Zihan Zeng and Tao Cai and Linqing Feng and Xinpeng Zhang and Ling Guo and Xianyue Jiang and Yanbin Yin and Ercheng Wang and Xingxu Huang and Jinfang Zheng},
keywords = {antimicrobial peptides, protein language model, protein structure, evolution, human gut microbiota},
abstract = {Summary
Antimicrobial resistance poses a major threat to public health, prompting the development of alternative therapies such as antimicrobial peptides (AMPs). Protein language models (PLMs) have advanced protein structure and function predictions, facilitating AMP discovery. We developed antimicrobial peptide structural evolution miner (AMP-SEMiner), an AI-driven framework that integrates PLMs, structural clustering, and evolutionary analysis to systematically identify AMPs encoded by small open reading frames and AMP-containing proteins in metagenome-assembled genomes. AMP-SEMiner identified over 1.6 million AMP candidates across diverse environments. Experimental validation showed antimicrobial activity in 9 of the 20 tested candidates, with 5 surpassing antibiotic effectiveness; variant peptides derived from these candidates similarly demonstrated strong antimicrobial efficacy. AMPs from human gut microbiomes revealed both conserved and adaptive evolutionary strategies, reflecting their dynamic ecological roles. AMP-SEMiner thus represents a valuable tool for expanding AMP discovery and has significant potential to inform the development of alternative antimicrobial treatments.}
}
@article{CARDOSO201844,
title = {Valuation Languages Along the Coal Chain From Colombia to the Netherlands and to Turkey},
journal = {Ecological Economics},
volume = {146},
pages = {44-59},
year = {2018},
issn = {0921-8009},
doi = {https://doi.org/10.1016/j.ecolecon.2017.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0921800916315373},
author = {Andrea Cardoso},
keywords = {Coal, Commodity chains, Valuation languages, Ecological distribution conflict, Environmental justice},
abstract = {Environmental goods and bads are accumulated and unequally distributed along the coal supply chain, producing environmental injustices where actors deployed values and representations of coal to either resist or legitimize its extraction and consumption. This paper analyzes those valuation languages along the coal chain and their relationships with the territory where coal is extracted and burned. The paper examines and compares the coal chains between Colombia-Netherlands and Colombia-Turkey, assessing the various dimensions of the ecological distribution conflicts. The coal chain is analyzed through different layers and scales. To identify the valuation languages along both coal chains, semi-structured interviews and secondary data analysis were conducted. Discourse analysis methodologies were used to determine the frequency and relevance of the valuation languages. Results show that multiple valuation languages appear, which are peculiar to each country and to the economic and political contexts in which the different stages of coal chain are embedded. Environmental justice actions taken along the coal chain with the focus on acknowledging these multiple valuation languages are also discussed. In conclusion, the analysis reveals that these actions depend on the willingness of social actors to give up or negotiate their valuation languages or on the power to impose them.}
}
@article{TIRONI2023102264,
title = {Artificial intelligence in the new forms of environmental governance in the Chilean State: Towards an eco-algorithmic governance},
journal = {Technology in Society},
volume = {74},
pages = {102264},
year = {2023},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2023.102264},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X23000696},
author = {Martín Tironi and Diego Ignacio {Rivera Lisboa}},
keywords = {Environmental governmentality, Smart earth, Ecology, Sensors, AI, Environmental compliance},
abstract = {One of the most popular fields of experimentation with technological solutions based on Artificial Intelligence (AI) and algorithmic systems is environmental studies, particularly as it relates to climate change. The promise of mitigating the impact of human activity on the environment through the introduction of sensor technologies has given way to a series of narratives around their role and capabilities. Focusing on the case of Environmental Intelligence, an initiative developed by the Chilean government's Superintendency for the Environment that incorporates AI into the monitoring process, we offer arguments regarding the articulation of an eco-algorithmic governmentality in which the environment is desingularized and reduced to a series of metrics associated with regulatory compliance. The operations that serve to prototype and give shape to the initiative created a series of tensions around the possibility of arriving at other forms of involvement in and understanding of the environment. This article shows how this eco-algorithmic governmentality conceptualizes the environment as an entity that can be optimized and rationalized, generating epistemic frictions with other logics of relationality and situated and terrestrial sensibility.}
}
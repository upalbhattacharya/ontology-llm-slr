@article{TAN2021347,
title = {Research on Knowledge Driven Intelligent Question Answering System for Electric Power Customer Service},
journal = {Procedia Computer Science},
volume = {187},
pages = {347-352},
year = {2021},
note = {2020 International Conference on Identification, Information and Knowledge in the Internet of Things, IIKI2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.04.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921008668},
author = {Yuanpeng Tan and Huifang Xu and Yaguang Wu and Zhonghao Zhang and Yeteng An and Yongping Xiong and Fang Wang},
keywords = {knowledge graph, electric power customer service, knowledge engineering, intelligent question answering system},
abstract = {The electric power customer service domain-specific knowledge graph aims to describe the concepts, entities, events and their relationships in the electric power customer service business field, with a structured manner, and provide a more effective data organization, management, and cognitive ability for the customer service business. This paper proposes a method for constructing a knowledge graph in the field of electric customer service, and constructs a knowledge graph with over 15,000 entities and 20,000 relationships. Based on the knowledge graph, an intelligent question answering application architecture is designed, which consists of multiple functional modules such as dialogue process configuration, natural language processing, and business process processing. It provides more efficient and open knowledge retrieval services for the electric customer service business, and improves the intelligence level of customer service question answering.}
}
@article{STRUPCZEWSKI2021105143,
title = {Defining cyber risk},
journal = {Safety Science},
volume = {135},
pages = {105143},
year = {2021},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2020.105143},
url = {https://www.sciencedirect.com/science/article/pii/S0925753520305397},
author = {Grzegorz Strupczewski},
keywords = {Cyber risk, Definition, Meta model of cyber risk, Cybersecurity, Cyber threats},
abstract = {Rapid digitization of the economy and social relations is the main reason why the issues of cyber risk, cyber threats and cybersecurity are continually gaining importance. Despite the increase in the number of research papers in these areas, scholarly articles defining cyber risk are relatively scarce. Moreover, the uniform broadly accepted definition of cyber risk has not been adopted yet, probably due to the interdisciplinary nature of this concept and the dynamics of its change. The paper contributes to the literature on the cyber risk, cybersecurity and cyber risk management. The author presents a comparative content analysis of existing definitions of cyber risk. Based on identification of three key characteristics of the cyber risk concept (source of cyber risk, cyber risk object, impact of cyber risk) in each definition, the analysed definitions are categorised as one-dimensional, two-dimensional or comprehensive definition. Among the collected 20 definitions of cyber risk, there is only one that can be called comprehensive. The remaining definitions address only selected aspects of this notion. The author proposes a new, comprehensive and universal definition of cyber risk. As an extension to the proposed approach, the ontological meta model of the cyber risk concept is developed. It supports deeper description of the cyber risk concept by depicting functional interdependencies with other terms and factors that constitute the cyber risk framework.}
}
@article{TERHORST2020101764,
title = {Learning soft domain constraints in a factor graph model for template-based information extraction},
journal = {Data & Knowledge Engineering},
volume = {125},
pages = {101764},
year = {2020},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2019.101764},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X19300345},
author = {Hendrik {ter Horst} and Matthias Hartung and Philipp Cimiano and Nicole Brazda and Hans Werner Müller and Roman Klinger},
keywords = {Template-based information extraction, Slot-filling, Probabilistic graphical models, Learning domain constraints, Database population},
abstract = {The ability to accurately extract key information from textual documents is necessary in several downstream applications e.g., automatic knowledge base population from text, semantic information retrieval, question answering, or text summarization. However, information extraction (IE) systems are far from being errorless and in some cases commit errors that seem obvious to a human expert as they violate common sense or domain knowledge. Towards improving the performance of IE systems, we focus on the question of how domain knowledge can be incorporated into IE models to reduce the number of spurious extractions. Starting from the assumption that such domain knowledge cannot be incorporated explicitly and manually by domain experts due to the amount of effort and technical complexities involved, we propose a machine learning approach in which domain constraints are acquired as a byproduct of learning a model that learns to extract key information in a supervised setting. We frame the task as a template-based information extraction problem in which several dependent slots need to be automatically filled and propose a factor graph based approach to model the joint distribution of slot assignments given a text. Beyond using standard textual features in factors that score the compatibility of slot fillers in relation to the text, we use additional features that are text-independent and capture soft domain constraints. During the training process, these constraints receive a weight as part of the parameter learning process indicating how strongly a constraint should be enforced. These domain constraints are thus ‘soft’ in the sense that they can be violated, but the system learns to penalize solutions that violate them. The soft constraints we introduce come in two flavors: on the one hand we incorporate information about the mean of numerical attributes and use features that indicate how far a certain value is from the mean. We call these features single slot soft constraints. On the other hand, we model the pairwise compatibility between slot filler assignments independent of the textual context, thus modeling the (domain) compatibility of the slot assignments. We call the latter ones pairwise slot soft constraints. As main result of our work, we show that learning pairwise slot soft constraints improves the performance of our extraction model compared to single slot soft constraints by up to 6 points in F1, leading to an F1 score of 0.91 for individual template types. Further, the human readable output format of our model enables the extraction and interpretation of the learned soft constraints. Based on this, we show in an evaluation by domain experts that more than 68% of the learned soft constraints are regarded as plausible.}
}
@article{KELLY20242048,
title = {Optimizing CAR Costimulatory Domains Using Contrastive Learning and Optimal Transport on High-Throughput Screening Data},
journal = {Blood},
volume = {144},
pages = {2048},
year = {2024},
note = {66th ASH Annual Meeting Abstracts},
issn = {0006-4971},
doi = {https://doi.org/10.1182/blood-2024-211466},
url = {https://www.sciencedirect.com/science/article/pii/S0006497124047955},
author = {Conor Kelly and Wenjun Zhu and Keyvan Keyvanfar and Pradeep Dagur and Stefan Cordes},
abstract = {Chimeric Antigen Receptors (CARs) are engineered into immune cells to bestow specificity towards identified targets on cancer cells. CAR T cells have established themselves as therapies for B cell and plasma cell malignancies, where they induce prolonged remission in most patients, though fewer than half achieve durable control of their disease. CARs follow a modular design, with an extracellular portion conferring target-specificity, a transmembrane hinge, and two or more intracellular signaling domains. One of the signaling components - typically CD3z - initiates the signaling cascades crucial for T-cell activation, proliferation, attainment of cytotoxic function, and ultimately immune memory. The other signaling component, called the costimulatory domain, is crucial for ensuring proper and controlled immune responses. Absence of this domain results in T cell anergy, but the optimal costimulatory domains remain unknown. Intracellular immune receptor signaling domains often comprise disordered regions lacking fixed tertiary protein structure. Their primary function is fulfilled by their primary structure consisting of one or more short (3 - 10 amino acid long) conserved sequences called Eukaryotic Linear Motifs. Adhering to this design principle, we systematically identified all transmembrane proteins within the Gene Ontology database associated with immune cell differentiation, proliferation, regulation or signaling. We specifically focused on proteins with known topologies in UniProt, extracting the primary sequences of the cytosolic portions. Through this process we curated a pool of over 1,200 distinct costimulatory domains, encompassing receptors not native to T cells, including cytokine/chemokine and inhibitory receptors. We lentivirally transduced anti-CD20 CAR with this pool of costimulatory domains into CD8+ T cells from 2 healthy volunteers. We cultured the resulting CAR T in triplicate, alone, activated by CD2/CD28 beads, in the presence of Raji (target) cells or K562 (negative control) cells over a 14-day period. At the conclusion of the experiment, we immunophenotypically sorted the cells into 6 sets: distinguishing central memory, effector and naïve T cell subsets each characterized by low or high PD1 expression. We prepared targeted sequencing libraries from the sorted cells to identify the costimulatory domains. Our screen yielded several promising novel ‘hits’ that we are evaluating through arrayed experiments. Notably, CD74 displayed enrichment in both the non-exhausted memory and effector subsets. Recognized as the Major Histocompatibility Complex (MHC) class II-associated invariant chain, CD74 is primarily known for its role in facilitating the assembly and trafficking of MHC class II molecules within cells. Beyond this canonical function, CD74 also harbors intracellular signaling capabilities that contribute to immune responses, engaging the ERK1/2, PI3K/AKT, and NF-κB pathways. By contrast, 4-1BB, which is used in most commercial CAR, activates a superset of these pathways, MAPK, PI3K/AKT and NF-κB. ERK1/2 is a subset of the MAPK pathway that primarily responds to growth factors and mitogens. Large language models have found powerful application in biology. ESM-2 (Evolutionary Scale Modeling) is a foundation model developed by Meta AI, that maps primary sequences to a representation (‘embedding‘) that captures evolutionary and structural information. To gain a predictive understanding of proportions of immunophenotypic subsets resulting from different costimulatory domains, we trained a contrastive learning model to learn a shared embedding between ESM-2 representations (a 5,120-dimensional space) and our experimentally determined proportions of immunophenotypic proportions under different co-culture conditions (a 24-dimensional space). We trained two dense neural networks with 2 hidden layers to project to a 20-dimensional shared embedding using 90% of our data, we achieved a moderate Fraction of Samples Closer than the True Match (FOSCTTM) of 0.45 in our validation sample. We further refined this shared embedding with fused Gromov-Wasserstein optimal transport to achieve a respectable FOSCTTM of 0.13. Our model predicts proportions of immunophenotypic subsets for different costimulatory domains in the context of our anti-CD20 CAR. We plan arrayed testing of domains predicted to improve those in our previous pool.}
}
@article{GLEESON20231526,
title = {Integrating model development across computational neuroscience, cognitive science, and machine learning},
journal = {Neuron},
volume = {111},
number = {10},
pages = {1526-1530},
year = {2023},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2023.03.037},
url = {https://www.sciencedirect.com/science/article/pii/S0896627323002611},
author = {Padraig Gleeson and Sharon Crook and David Turner and Katherine Mantel and Mayank Raunak and Ted Willke and Jonathan D. Cohen},
abstract = {Neuroscience, cognitive science, and computer science are increasingly benefiting through their interactions. This could be accelerated by direct sharing of computational models across disparate modeling software used in each. We describe a Model Description Format designed to meet this challenge.}
}
@article{BERARDINUCCI2022411,
title = {A learning workflow based on an integrated digital toolkit to support education in manufacturing system engineering},
journal = {Journal of Manufacturing Systems},
volume = {63},
pages = {411-423},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S027861252200053X},
author = {Francesco Berardinucci and Giorgio Colombo and Marcello Lorusso and Massimo Manzini and Walter Terkaj and Marcello Urgo},
keywords = {Learning workflows, Digital tools, Manufacturing systems engineering},
abstract = {Digital modelling of manufacturing systems is experiencing a fast development, but it still shows significant limitations when considering integration and interoperability of enabling technologies. Indeed, there is still a lack of reference integrated workflows to perform the wide span of tasks, ranging from layout configuration and performance evaluations to 3D representations. Commercial software tools are either too complex or expensive to be approached by non-specialists, therefore it is hard to design effective learning activities in manufacturing system engineering. This paper proposes a structured learning workflow based on an open toolkit that takes advantage of a common ontology-based data model to smoothly integrate digital tools for manufacturing system modelling, performance evaluation, and virtual reality representation. After detailing methodologies and digital tools, the proposed workflow is applied to a pilot case in higher education.}
}
@article{BULIUNG2021114237,
title = {Living the journey to school: Conceptual asymmetry between parents and planners on the journey to school},
journal = {Social Science & Medicine},
volume = {284},
pages = {114237},
year = {2021},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2021.114237},
url = {https://www.sciencedirect.com/science/article/pii/S0277953621005694},
author = {Ronald Buliung and Paul Hess and Lori Flowers and Fiona J. Moola and Guy Faulkner},
keywords = {Active school transportation, Built environment, Thematic analysis, Social ecological framework, Onto-epistemology, Parents},
abstract = {Research about school travel and the built environment developed using positivist and post-positivist onto-epistemologies often relies heavily on travel surveys, activity diaries, GPS tracking, and the “objective” measurement of built environment features using geographical information systems and planimetric data. That work takes up and applies specialized disciplinary and practice-based language (e.g., planning and engineering) and concepts that are used to describe, measure, and design the built environment. In this paper, we explore differences in how parents think about the built environment and school transport and the ways in which the built environment and transport are conceptualized in planning. The presence of conceptual asymmetry between a scholar's “model” and the “lived experience” of parents and children may have implications for the efficacy of school travel-related policy and planning. We use Bronfenbrenner's social ecological model to guide a thematic analysis of 37 interviews with parents about school travel behaviour in Toronto, Canada. We found that parents' experiences of the built environment are complex and varied, with different features influencing individual parents differently, and at varying levels of the ecological model. For example, mixed-use development, often held up as a necessary condition for tackling automobility, was cited as a desirable aesthetic background for driving. We were able to locate examples of conceptual asymmetry but also agreement – particularly about traffic around schools. For example, parents expressed divergent views on the impact of heavy traffic on walking, with some describing traffic and traffic safety as barriers to walking, while others indicated that resistance to driving in traffic motivated a choice to walk. Our study serves as a call to planners and geographers to better attend to the lay, everyday onto-epistemologies that shape parents' lived experiences of travel to school.}
}
@article{CARBONNEL2019341,
title = {Towards complex product line variability modelling: Mining relationships from non-boolean descriptions},
journal = {Journal of Systems and Software},
volume = {156},
pages = {341-360},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219301311},
author = {Jessie Carbonnel and Marianne Huchard and Clémentine Nebut},
keywords = {Complex software product line, Reverse engineering, Variability modelling, Extended feature models, Formal concept analysis, Pattern structures},
abstract = {Software product line engineering relies on systematic reuse and mass customisation to reduce the development time and cost of a software system family. The extractive adoption of a product line requires to extract variability information from the description of a collection of existing software systems to model their variability. With the increasing complexity of software systems, software product line engineering faces new challenges including variability extraction and modelling. Extensions of existing boolean variability models, such as multi-valued attributes or UML-like cardinalities, were proposed to enhance their expressiveness and support variability modelling in complex product lines. In this paper, we propose an approach to extract complex variability information, i.e., involving features as well as multi-valued attributes and cardinalities, in the form of logical relationships. This approach is based on Formal Concept Analysis and Pattern Structures, two mathematical frameworks for knowledge discovery that bring theoretical foundations to complex variability extraction algorithms. We present an application on product comparison matrices representing complex descriptions of software system families. We show that our method does not suffer from scalability issues and extracts all pertinent relationships, but that it also extracts numerous accidental relationships that need to be filtered.}
}
@article{SAAD20252974,
title = {DNA binding and mitotic phosphorylation protect polyglutamine proteins from assembly formation},
journal = {Cell},
volume = {188},
number = {11},
pages = {2974-2991.e20},
year = {2025},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2025.03.031},
url = {https://www.sciencedirect.com/science/article/pii/S0092867425003496},
author = {Shady Saad and Tomek Swigut and Saman Tabatabaee and Pranav Lalgudi and Daniel F. Jarosz and Joanna Wysocka},
keywords = {FOXP2, transcription factors, glutamine-rich proteins, polyglutamine, Huntington's disease, protein assemblies, amyloid, human speech, evolution, language},
abstract = {Summary
Polyglutamine (polyQ) expansion is associated with pathogenic protein aggregation in neurodegenerative disorders. However, long polyQ tracts are also found in many transcription factors (TFs), such as FOXP2, a TF implicated in human speech. Here, we explore how FOXP2 and other glutamine-rich TFs avoid unscheduled assembly. Throughout interphase, DNA binding, irrespective of sequence specificity, has a solubilizing effect. During mitosis, multiple phosphorylation events promote FOXP2’s eviction from chromatin and supplant the solubilizing function of DNA. Further, human-specific amino acid substitutions linked to the evolution of speech map to a mitotic phospho-patch, the “EVO patch,” and reduce the propensity of the human FOXP2 to assemble. Fusing the pathogenic form of Huntingtin to either a DNA-binding domain, a phosphomimetic variant of this EVO patch, or a negatively charged peptide is sufficient to diminish assembly formation, suggesting that hijacking mechanisms governing solubility of glutamine-rich TFs may offer new strategies for treatment of polyQ expansion diseases.}
}
@article{HONG2023107049,
title = {Railway accident causation analysis: Current approaches, challenges and potential solutions},
journal = {Accident Analysis & Prevention},
volume = {186},
pages = {107049},
year = {2023},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2023.107049},
url = {https://www.sciencedirect.com/science/article/pii/S0001457523000969},
author = {Wei-Ting Hong and Geoffrey Clifton and John D. Nelson},
keywords = {Causation analysis, Railway accident, Natural Language Processing (NLP), Textual data, Scoping review},
abstract = {Railway accident causation analysis is fundamental to understanding the nature of railway safety. Although a considerable number of prior studies have investigated this context, many of them suffer from the need to deal with a large amount of textual data given that most railway safety-related information is recorded and stored in the form of text. To gain a better understanding of the limitations imposed by overreliance on textual analysis, a scoping review of the academic literature on how railway accident causation analysis is addressed has been conducted. The results confirm the high frequency of using textual data, a single case study, and in-depth analysis frameworks. While the value of exploring causational factors is clear, the high level of human intervention and the labour-intensive analysis processes based on a large volume of textual data hinder researchers from understanding the complex nature of the rail safety system. Recently, growing attention has been given to the application of Natural Language Processing (NLP) to aid the practice of analysing a large corpus of textual data, but only limited studies to date in railway safety use such techniques and none address railway accident causation analysis. To fill this gap, a supplementary review is conducted to identify opportunities, challenges, boundaries and limitations in the application of NLP approaches to railway accident causation analysis. Findings indicate that novel techniques using off-the-shelf tools have strong potential to overcome the limitations of overreliance on manual analysis in practice and theory, but the absence of shared railway safety-related benchmark corpora restricts implementation. This study sheds light on a new approach to railway accident causation analysis and clarifies future applicable utilisations for further research.}
}
@article{XU202042,
title = {Incorporating context-relevant concepts into convolutional neural networks for short text classification},
journal = {Neurocomputing},
volume = {386},
pages = {42-53},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.08.080},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219312184},
author = {Jingyun Xu and Yi Cai and Xin Wu and Xue Lei and Qingbao Huang and Ho-fung Leung and Qing Li},
keywords = {Short text classification, Knowledge base, Attention mechanism, Neural networks},
abstract = {Text classification is an important task in natural language processing. Previous text classification models do not perform well on short texts due to the data sparsity problem. In order to solve this problem, recent research extracts concepts of words to enrich text representation. However, this approach might bring general concepts, which might not be helpful in discriminating categories in text classification. Furthermore, it might bring noise into text representation and lead to performance degradation. To tackle these problems, we propose a neural network called DE-CNN, which can incorporate context-relevant concepts into a convolutional neural network for short text classification. Our model firstly utilizes two layers to extract concepts and context respectively and then employs an attention layer to extract context-relevant concepts. Then the concepts are incorporated into text representation for short text classification. The experimental results on three text classification tasks show that our proposed model outperforms compared state-of-the-art models.}
}
@incollection{SHANKS202484,
title = {Social Theory in Archaeology},
editor = {Efthymia Nikita and Thilo Rehren},
booktitle = {Encyclopedia of Archaeology (Second Edition) (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {84-93},
year = {2024},
isbn = {978-0-323-91856-5},
doi = {https://doi.org/10.1016/B978-0-323-90799-6.00214-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323907996002147},
author = {Michael Shanks},
keywords = {Archaeological theory, History of archaeology, Pragmatism, Science studies, Social archaeology, Social theory},
abstract = {Social theory in archaeology is conventionally summarized as different schools of thought (paradigms and isms), usually associated with thought leaders in the discipline, that aim to reconstruct past societies. Involved are questions of social ontology (asking—what is society and its components?), and epistemology (asking—how might we come to know societies?). In contrast, this contribution avoids the description of schools of thought and, following a pragmatist line, treats social theory in archaeology as a key component of thoughtful practice; what archaeologists do. The components of practice such as opening brief, orientation, deliverable, frame, and project management are related to social theory. The concepts of social theory are argued to form a tool kit mobilized in archaeological research. A major section outlines some key concepts of social theory, including society, culture, systems, networks, agency, power, identity, personhood, and complexity.}
}
@article{KERSLOOT2021103897,
title = {De-novo FAIRification via an Electronic Data Capture system by automated transformation of filled electronic Case Report Forms into machine-readable data},
journal = {Journal of Biomedical Informatics},
volume = {122},
pages = {103897},
year = {2021},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2021.103897},
url = {https://www.sciencedirect.com/science/article/pii/S1532046421002264},
author = {Martijn G. Kersloot and Annika Jacobsen and Karlijn H.J. Groenen and Bruna {dos Santos Vieira} and Rajaram Kaliyaperumal and Ameen Abu-Hanna and Ronald Cornet and Peter A.C. {‘t Hoen} and Marco Roos and Leo {Schultze Kool} and Derk L. Arts},
keywords = {Electronic Case Report Forms, FAIR Data, Machine-readable data, Interoperability, Patient registry},
abstract = {Introduction
Existing methods to make data Findable, Accessible, Interoperable, and Reusable (FAIR) are usually carried out in a post hoc manner: after the research project is conducted and data are collected. De-novo FAIRification, on the other hand, incorporates the FAIRification steps in the process of a research project. In medical research, data is often collected and stored via electronic Case Report Forms (eCRFs) in Electronic Data Capture (EDC) systems. By implementing a de novo FAIRification process in such a system, the reusability and, thus, scalability of FAIRification across research projects can be greatly improved. In this study, we developed and implemented a novel method for de novo FAIRification via an EDC system. We evaluated our method by applying it to the Registry of Vascular Anomalies (VASCA).
Methods
Our EDC and research project independent method ensures that eCRF data entered into an EDC system can be transformed into machine-readable, FAIR data using a semantic data model (a canonical representation of the data, based on ontology concepts and semantic web standards) and mappings from the model to questions on the eCRF. The FAIRified data are stored in a triple store and can, together with associated metadata, be accessed and queried through a FAIR Data Point. The method was implemented in Castor EDC, an EDC system, through a data transformation application. The FAIRness of the output of the method, the FAIRified data and metadata, was evaluated using the FAIR Evaluation Services.
Results
We successfully applied our FAIRification method to the VASCA registry. Data entered on eCRFs is automatically transformed into machine-readable data and can be accessed and queried using SPARQL queries in the FAIR Data Point. Twenty-one FAIR Evaluator tests pass and one test regarding the metadata persistence policy fails, since this policy is not in place yet.
Conclusion
In this study, we developed a novel method for de novo FAIRification via an EDC system. Its application in the VASCA registry and the automated FAIR evaluation show that the method can be used to make clinical research data FAIR when they are entered in an eCRF without any intervention from data management and data entry personnel. Due to the generic approach and developed tooling, we believe that our method can be used in other registries and clinical trials as well.}
}
@article{RONCA2022103668,
title = {The delay and window size problems in rule-based stream reasoning},
journal = {Artificial Intelligence},
volume = {306},
pages = {103668},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103668},
url = {https://www.sciencedirect.com/science/article/pii/S000437022200008X},
author = {Alessandro Ronca and Mark Kaminski and Bernardo {Cuenca Grau} and Ian Horrocks},
keywords = {Knowledge representation and reasoning, Ontologies, Temporal reasoning, Stream reasoning, Stream processing},
abstract = {In recent years, there has been an increasing interest in extending stream processing engines with rule-based temporal reasoning capabilities. To ensure correctness, such systems must be able to output results over the partial data received so far as if the entire (infinite) stream had been available; furthermore, these results must be streamed out as soon as the relevant data is received, thus incurring the minimum possible delay; finally, due to memory limitations, systems can only keep a limited history of previous facts in memory to perform further computations. These requirements pose significant theoretical and practical challenges since temporal rules can derive new information and propagate it both towards past and future time points; as a result, streamed answers can depend on data that has not yet been received, as well as on data that arrived far in the past. Towards developing a solid foundation for practical rule-based stream reasoning, we propose and study in this paper a suite of decision problems that can be exploited by stream reasoning algorithms to tackle the aforementioned challenges, and provide tight complexity bounds for a core temporal extension of Datalog. All of the problems we consider can be solved at design time (under reasonable assumptions), prior to the processing of any data. Solving these problems enables the use of reasoning algorithms that process the input streams incrementally using a sliding window, while at the same time supporting an expressive rule-based knowledge representation language and minimising both latency and memory consumption.}
}
@article{JANSSON2024107610,
title = {Working with emotions in social work practice. A pride-building model for institutional care of young people},
journal = {Children and Youth Services Review},
volume = {161},
pages = {107610},
year = {2024},
issn = {0190-7409},
doi = {https://doi.org/10.1016/j.childyouth.2024.107610},
url = {https://www.sciencedirect.com/science/article/pii/S0190740924001828},
author = {Peter M. Jansson and Nina V. Gunnarsson},
keywords = {Social bonds, Shame, Pride, Social work, Treatment, Adolescents, Rehabilitation},
abstract = {In this article we point out why social workers and treatment staff must have knowledge of how to identify emotions, understand their own emotions and understand the emotions they elicit in others as a prerequisite for successful rehabilitation. In particular, the emotions of shame and pride play a crucial role in the interaction between social workers and clients. There is currently a need for empirically applicable models that facilitate social workers and therapists in institutional care to identify shame and pride in the interaction with clients. Here we provide a model that can be used to analyze the quality of the social bonds between treatment staff and young clients in institutional care. Institutionalized treatment of young people is often based on an asymmetrical power relationship and the transformation of deviant young people’s identity into normal ones. This is fraught with risks, as the power imbalance can preserve and reinforce deviant identities. To encourage the emergence of a normalized identity, the client's good qualities must form the basis of treatment. Greater understanding of the emotions evoked in a treatment situation is necessary for successful rehabilitation.}
}
@article{MATEOTRUJILLO2025114309,
title = {Optimizing dyslexia intervention through an adaptive sequential recommender system},
journal = {Knowledge-Based Systems},
volume = {329},
pages = {114309},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114309},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125013504},
author = {J. Ignacio {Mateo Trujillo} and Ignacio Rodríguez-Rodríguez and Diego Castillo-Barnes and Andrés Ortiz and Auxiliadora Sánchez and Juan L. Luque},
keywords = {Recommendation system, Bayesian knowledge tracking, Virtual children, Intervention test, Phonemes, Dyslexia, Word-generator,},
abstract = {Children with dyslexia face significant learning difficulties that require personalized and intensive interventions. Although computer-based support programs exist, they often fail to adapt to the unique needs of each child, representing a major challenge in the field of educational intervention. This article presents a new adaptive sequential guidance system for personalized dyslexia intervention that addresses these limitations. The proposed methodology incorporates several key innovations: (1) a dynamic word generator that creates phonetically modified words and pseudowords from seed words, (2) a three-dimensional matrix structure (E, W,and F) to effectively manage word difficulty and user performance, and (3) a recommendation algorithm based on matrix factorization. To mitigate cold-start problems, the system implements a heuristic initiation process and uses an extension technique to detect difficulties in specific derived words. Additionally, the concept of “virtual children” generated from real data and based on Bayesian Knowledge Tracking is introduced, allowing thorough testing and optimization of the system prior to its actual implementation. The evaluation of the system demonstrates three main results: (1) the use of heat maps and 3D visualization of the E matrix allows identifying specific areas of difficulty for each user, facilitating more targeted interventions; (2) extensive testing confirms the robustness of the system to reduce error rates in multiple trials; and (3) a parametric study evidences the ability of the system to adapt through adjustable parameters, keeping each child in his or her optimal learning zone.}
}
@article{OUALI2022158,
title = {Augmented Reality for Scene Text Recognition, Visualization and Reading to Assist Visually Impaired People},
journal = {Procedia Computer Science},
volume = {207},
pages = {158-167},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.048},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922009218},
author = {Imene OUALI and Mohamed BEN HALIMA and Ali WALI},
keywords = {Text Visualization, Text detection, Text recognition, Natural Scene, Augmented Reality, VGG19},
abstract = {Reading traffic signs while driving a car for visually impaired people and people with visual problems is a very difficult task for them. This task is encountered every day, sometimes incorrect reading of traffic signs can lead to very serious results. In particular, the Arabic language is very difficult, making recognizing and viewing Arabic text a difficult task. In this context, we are looking for an effective solution to remove errors and results that can sometimes end someone's life. This article aims to correctly read traffic signs with Arabic text using augmented reality technology. Our system is composed of three modules. The first is text detection and recognition. The second is Text visualization. The third is Text to speech methods conversion. With this system, the user can have two different results. The first result is visual with much-improved text and enhancement. The second result is sound, he can hear the text aloud. This system is very applicable and effective for daily life. To assess the effectiveness of our work, we offer a survey to a group of visually impaired people to give their opinion on the use of our application. The results have been good for most people.}
}
@article{PRADEEP201944,
title = {The MOM of context-aware systems: A survey},
journal = {Computer Communications},
volume = {137},
pages = {44-69},
year = {2019},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2019.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0140366418309472},
author = {Preeja Pradeep and Shivsubramani Krishnamoorthy},
keywords = {Context-awareness, Context-aware computing, Context-aware systems, Context modeling, Context organization, Ontology, Middleware, Pervasive and ubiquitous computing},
abstract = {Context-aware computing enriches the capabilities of intelligent devices complemented with smart applications and helps establish smart ecosystems in fields such as Ambient Intelligence, Internet of Things, Mobile Computing, and Pervasive and Ubiquitous Computing. Though the literature has many surveys that outline existing systems, it still remains critical to elucidate the basics of actually building an effective context-aware ecosystem. We outline the basic components required and essential for the same. We believe that a context-aware ecosystem becomes effective when these components are designed and implemented effectively. We call it the MOM of context-aware systems: generic and effective context Modeling, an efficient context Organization, and a robust context Middleware. Context modeling affords a syntax to the raw pieces of relevant information, the organization mechanism furnishes semantic import to the information and relationships, and the middleware compiles and integrates the information, enabling sharing of context. We discuss various context-aware ecosystems and middleware from the literature and highlight how the three building components function in each case. This paper will benefit newcomers to the field who are looking to learn about and build context-aware ecosystems.}
}
@article{MAMOUDOU2025101291,
title = {AI-driven bioactive peptide discovery of next-generation metabolic biotherapeutics},
journal = {Applied Food Research},
volume = {5},
number = {2},
pages = {101291},
year = {2025},
issn = {2772-5022},
doi = {https://doi.org/10.1016/j.afres.2025.101291},
url = {https://www.sciencedirect.com/science/article/pii/S2772502225005967},
author = {Hamadou Mamoudou and Martin Alain Mune Mune},
keywords = {Bioactive peptides, Metabolic diseases, Drug discovery, AI-driven},
abstract = {Metabolic diseases, including obesity and type 2 diabetes, pose a significant global health burden, demanding innovative therapeutic solutions. Traditional drug discovery is often slow and costly, struggling with the complex nature of these disorders. Bioactive peptides offer a promising alternative due characterized by their specificity, low toxicity, and diverse mechanisms. However, challenges in their screening, stability, and target identification have limited their clinical use. Artificial intelligence (AI) and machine learning (ML) are now revolutionizing peptide discovery. These technologies enable rapid prediction, de novo design, and optimization of bioactive sequences. This review critically evaluates AI's role in identifying and developing peptides for metabolic disease pathways. We examine key computational methods, including sequence-based features, advanced deep learning models (CNNs, LSTMs, Transformers), and generative approaches. The manuscript also covers essential datasets, validation frameworks, and illustrative case studies. We explore the integration of molecular dynamics, network pharmacology, and reinforcement learning for advanced peptide engineering. Despite significant progress, challenges persist, such as data heterogeneity, model generalizability, and the gap between in silico predictions and experimental validation. Looking ahead, we highlight future opportunities, including multi-omics integration, explainable AI, the discovery of microbiome-derived peptides, and synthetic biology-driven design. This review underscores AI’s transformative potential in advancing peptide-based interventions for metabolic diseases, offering a roadmap for novel, targeted, and preventive therapies.}
}
@article{LIU2021106611,
title = {Decomposing word embedding with the capsule network},
journal = {Knowledge-Based Systems},
volume = {212},
pages = {106611},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.106611},
url = {https://www.sciencedirect.com/science/article/pii/S0950705120307401},
author = {Xin Liu and Qingcai Chen and Yan Liu and Joanna Siebert and Baotian Hu and Xiangping Wu and Buzhou Tang},
keywords = {Word sense learning, Capsule network, Word-in-Context, English all-words Word Sense Disambiguation},
abstract = {Word sense disambiguation tries to learn the appropriate sense of an ambiguous word in a given context. The existing pre-trained language methods and the methods based on multi-embeddings of word did not explore the power of the unsupervised word embedding sufficiently. In this paper, we discuss a capsule network-based approach, taking advantage of capsule’s potential for recognizing highly overlapping features and dealing with segmentation. We propose a capsule network-based method to decompose the unsupervised word embedding of an ambiguous word into context specific sense embedding, called CapsDecE2S. In this approach, the unsupervised ambiguous embedding is fed into capsule network to produce its multiple morpheme-like vectors, which are defined as the basic semantic language units of meaning. With attention operations, CapsDecE2S integrates the word context to reconstruct the multiple morpheme-like vectors into the context-specific sense embedding. To train CapsDecE2S, we propose a sense matching training method. In this method, we convert the sense learning into a binary classification that explicitly learns the relation between senses by the label of matching and non-matching. The CapsDecE2S was experimentally evaluated on two sense learning tasks, i.e., word in context and word sense disambiguation. Results on two public corpora Word-in-Context and English all-words Word Sense Disambiguation show that, the CapsDecE2S model achieves the new state-of-the-art for the word in context and word sense disambiguation tasks. The source code can be downloaded from the Github page1 1https://github.com/Gdls/CapsDecE2S..}
}
@article{MORALES2025112363,
title = {Thermal comfort and indoor overheating risks of urban building stock - A review of modelling methods and future climate challenges},
journal = {Building and Environment},
volume = {269},
pages = {112363},
year = {2025},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2024.112363},
url = {https://www.sciencedirect.com/science/article/pii/S0360132324012058},
author = {Richard Dean Morales and Amaryllis Audenaert and Stijn Verbeke},
keywords = {Indoor overheating, Urban climate, Urban heat island, Microclimate, Climate change, Urban building energy modelling, Building performance simulation},
abstract = {Urban microclimate often exhibits higher temperatures than rural areas due to Urban Heat Island (UHI). While UHI reflects outdoor overheating risks, people spend most of their time indoors, where thermal conditions are shaped by complex interactions of external weather, building characteristics, and human behaviour. Thus, UHI effects do not straightforwardly translate to indoor temperature increases. The intricacies of assessing indoor overheating risks in urban settings can be captured using Urban Building Energy Modelling (UBEM) with Building Performance Simulation (BPS). This paper reviews the latest research on BPS in an urban context, offering critical analysis and identifying challenges to guide future research. Conventionally, UBEM has primarily focused on energy simulation rather than indoor thermal comfort. Also, it often uses a generic approach, modelling a limited set of archetype buildings. This simplification can result in suboptimal climate adaptation and mitigation strategies, as indoor thermal comfort depends on specific building conditions like microclimate, window size, orientation, and shading. However, recent advances in UBEM have begun incorporating microclimate data and climate change scenarios. New thermal comfort metrics tailored to climate change are emerging, building upon existing standards. Progress has been made in modelling significant factors affecting building heat balance, including the generation of multizone models, incorporation of trees and vegetation, and improved calculations of longwave radiation and surface albedo. Despite this progress, current research often focuses on a limited set of these factors and the relative contribution to the UBEM model outcomes is often not quantified. Future endeavours should strive for explicit quantification and efficient integration of these interacting elements within urban models, balancing the modelling complexity with computational demands while improving simulation accuracy. Limitations may arise from data availability, necessitating the adoption of alternative data sources and novel modelling techniques. Ultimately, enhanced assessments of urban-level indoor overheating will support better-informed urban planning and building design strategies, bolstering resilience against overheating.}
}
@article{CASSIDY2024105623,
title = {Flipping healthcare by including the patient perspective in integrated care pathway design: A scoping review},
journal = {International Journal of Medical Informatics},
volume = {192},
pages = {105623},
year = {2024},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2024.105623},
url = {https://www.sciencedirect.com/science/article/pii/S1386505624002867},
author = {Sonja Cassidy and Øivind Skeidsvoll Solvang and Conceição Granja and Terje Solvoll},
keywords = {Healthcare service design, Patient perspective, Integrated care, Care pathway modelling, eHealth, Electronic health record},
abstract = {Background
Despite the recognized benefits of integrating patient perspectives into healthcare design and clinical decision support, theoretical approaches and standardized methods are lacking. Various strategies, such as developing pathways, have evolved to address these challenges. Previous research emphasized the need for a framework for care pathways that includes theoretical principles, extensive user involvement, and data from electronic health records to bridge the gap between different fields and disciplines. Standardizing the representation of the patient perspective could facilitate its sharing across healthcare organizations and domains and its integration into journal systems, shifting the balance of power from the provider to the patient.
Objectives
This study aims to 1) Identify research approaches taken to develop patient-centred, integrated, care pathways supported by electronic health records 2) Propose a socio-technical framework for designing patient-centred care pathways across multiple healthcare levels that integrates the voice of the patient with the knowledge of the care provider and technological perspectives.
Methods
This study conducted a scoping review following the Joanna Briggs Institute guidelines and PRISMA-ScR protocol. The databases PubMed, Scopus, Web of Science, ProQuest, IEEE, and Google Scholar were searched using a key term search strategy including variations of patient-centred, integrated care, pathway, framework and model to identify relevant studies. Eligible articles included peer-reviewed literature documenting methodologies for mapping patient-centred, integrated care pathways in healthcare service design.
Results
This review summarizes the application of care pathway modelling practices across various areas of healthcare innovation. The search resulted in 410 studies, with 16 articles included after the full review and grey literature search.
Conclusions
Our research illustrated incorporating patient perspectives into modelling care pathways and healthcare service design. Regardless of the medical domain, our methodology proposes an approach for modelling patient-centred, integrated care pathways across the care continuum, including using electronic health records to support the pathways.}
}
@article{LEVY2022100165,
title = {Comparison of machine-learning algorithms for the prediction of Current Procedural Terminology (CPT) codes from pathology reports},
journal = {Journal of Pathology Informatics},
volume = {13},
pages = {100165},
year = {2022},
issn = {2153-3539},
doi = {https://doi.org/10.4103/jpi.jpi_52_21},
url = {https://www.sciencedirect.com/science/article/pii/S2153353922007593},
author = {Joshua Levy and Nishitha Vattikonda and Christian Haudenschild and Brock Christensen and Louis Vaickus},
keywords = {BERT, Current procedural terminology, Deep learning, Machine learning, Pathology reports, XGBoost},
abstract = {Background
Pathology reports serve as an auditable trial of a patient’s clinical narrative, containing text pertaining to diagnosis, prognosis, and specimen processing. Recent works have utilized natural language processing (NLP) pipelines, which include rule-based or machine-learning analytics, to uncover textual patterns that inform clinical endpoints and biomarker information. Although deep learning methods have come to the forefront of NLP, there have been limited comparisons with the performance of other machine-learning methods in extracting key insights for the prediction of medical procedure information, which is used to inform reimbursement for pathology departments. In addition, the utility of combining and ranking information from multiple report subfields as compared with exclusively using the diagnostic field for the prediction of Current Procedural Terminology (CPT) codes and signing pathologists remains unclear.
Methods
After preprocessing pathology reports, we utilized advanced topic modeling to identify topics that characterize a cohort of 93,039 pathology reports at the Dartmouth-Hitchcock Department of Pathology and Laboratory Medicine (DPLM). We separately compared XGBoost, SVM, and BERT (Bidirectional Encoder Representation from Transformers) methodologies for the prediction of primary CPT codes (CPT 88302, 88304, 88305, 88307, 88309) as well as 38 ancillary CPT codes, using both the diagnostic text alone and text from all subfields. We performed similar analyses for characterizing text from a group of the 20 pathologists with the most pathology report sign-outs. Finally, we uncovered important report subcomponents by using model explanation techniques.
Results
We identified 20 topics that pertained to diagnostic and procedural information. Operating on diagnostic text alone, BERT outperformed XGBoost for the prediction of primary CPT codes. When utilizing all report subfields, XGBoost outperformed BERT for the prediction of primary CPT codes. Utilizing additional subfields of the pathology report increased prediction accuracy across ancillary CPT codes, and performance gains for using additional report subfields were high for the XGBoost model for primary CPT codes. Misclassifications of CPT codes were between codes of a similar complexity, and misclassifications between pathologists were subspecialty related.
Conclusions
Our approach generated CPT code predictions with an accuracy that was higher than previously reported. Although diagnostic text is an important source of information, additional insights may be extracted from other report subfields. Although BERT approaches performed comparably to the XGBoost approaches, they may lend valuable information to pipelines that combine image, text, and -omics information. Future resource-saving opportunities exist to help hospitals detect mis-billing, standardize report text, and estimate productivity metrics that pertain to pathologist compensation (RVUs).}
}
@article{YANG202030,
title = {Introspection unit in memory network: Learning to generalize inference in OOV scenarios},
journal = {Neurocomputing},
volume = {379},
pages = {30-40},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.07.111},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219314778},
author = {Qichuan Yang and Zhiqiang He and Zhiqiang Zhan and Yang Zhang and Rang Li and Changjian Hu},
keywords = {Memory network, Out of vocabulary, Introspection unit, Language inference},
abstract = {Inference in natural language processing (NLP) is a tough task. Although plenty of models have been proposed in recent years, they are usually restricted to infer within a limited vocabulary or handcrafted training templates. In this paper, we propose the introspection unit (IU), a new neural module which can be incorporated with memory networks to deal with inference tasks in out of vocabulary (OOV) and rare named entities (RNEs) scenarios. Specifically, when encountering a new word, IU compares its part-of-speech context with the training dataset to extract a similar sample, and then embeds the new word into a target position to construct a simulated sample. The target position is located by the result of part-of-speech tagging. Finally, using the simulated sample, IU helps memory networks to learn the context and characteristic of the new word. In experiments, we evaluate the effectiveness of IU with the memory network on four inference datasets: a name OOV dataset, a place OOV dataset, a more challenging synthetical mixture OOV dataset and a realistic dialogue dataset. The experimental results demonstrate that IU effectively generalizes the inference ability of memory networks to OOV scenarios and improves the inference accuracies significantly. Furthermore, we visualize both the introspection process and the effect of IU in word embeddings and memories.}
}
@article{LU2020330,
title = {DCEM: A data cell evolution model for service composition based on bigraph theory},
journal = {Future Generation Computer Systems},
volume = {112},
pages = {330-347},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19330006},
author = {Jiawei Lu and Huan Zhou and Haotian Zhu and Yuanming Zhang and Qianhui Liang and Gang Xiao},
keywords = {Service composition, Data service, Bigraph, Data cell, Service evolution},
abstract = {Service composition is an important means of integrating individual web services to create new value-added systems that satisfy complex requirements. Effectively analyzing different types of services and identifying the matching similarity between services is a crucial component of service composition, as it allows failed services to be efficiently substituted in a distributed and dynamic environment. In this paper, we propose a data cell evolution model (DCEM) that supports the dynamic adaptation of service compositions. DCEM combines data service information and biological cell behavior analysis to encapsulate data services into data cells. To optimize the adaptations, we analyze the static and dynamic structure of data cells based on bigraph theory to guarantee the consistency of service evolution. To evaluate the proposed approach, a series of simulation experiments and comparisons are conducted to demonstrate the effectiveness of service composition.}
}
@article{AMIN20231165,
title = {Intelligent Service Search Model Using Emerging Technologies},
journal = {Computers, Materials and Continua},
volume = {77},
number = {1},
pages = {1165-1181},
year = {2023},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.040693},
url = {https://www.sciencedirect.com/science/article/pii/S1546221823001042},
author = {Farhan Amin and Gyu Sang Choi},
keywords = {Internet of Things, Social Internet of Things, Cyber-Physical Social systems, smart cities, service search, smart cities},
abstract = {In recent years, the Internet of Things (IoT) has played a vital role in providing various services to users in a smart city. However, searching for services, objects, data, and frameworks remains a concern. The technological advancements in Cyber-Physical Systems (CPSs) and the Social Internet of Things (SIoT) open a new era of research. Thus, we propose a Cyber-Physical-Social Systems (CPSs) for service search. Herein, service search and object discovery operation carries with the suitable selection of friends in the network. Our proposed model constructs a graph and performs social network analysis (SNA). We suggest degree centrality, clustering, and scale-free emergence and show that a rational selection of friends per service exploration increases the overall network navigability. The efficiency of our proposed system is verified using real-world datasets based on service processing time, path length, giant component, and network diameter. The simulation results proved that our proposed system is efficient, robust, and scalable.}
}
@article{ZIDOUN2022,
title = {Contextual Conversational Agent to Address Vaccine Hesitancy: Protocol for a Design-Based Research Study},
journal = {JMIR Research Protocols},
volume = {11},
number = {8},
year = {2022},
issn = {1929-0748},
doi = {https://doi.org/10.2196/38043},
url = {https://www.sciencedirect.com/science/article/pii/S1929074822005236},
author = {Youness Zidoun and Sreelekshmi Kaladhara and Leigh Powell and Radwa Nour and Hanan {Al Suwaidi} and Nabil Zary},
keywords = {conversational agent, design-based research, chatbot, Rasa, NLU, COVID-19, vaccine hesitancy, misinformation, vaccination, iterative design, health communication, health information, System Usability Scale},
abstract = {Background
Since the beginning of the COVID-19 pandemic, people have been exposed to misinformation, leading to many myths about SARS-CoV-2 and the vaccines against it. As this situation does not seem to end soon, many authorities and health organizations, including the World Health Organization (WHO), are utilizing conversational agents (CAs) in their fight against it. Although the impact and usage of these novel digital strategies are noticeable, the design of the CAs remains key to their success.
Objective
This study describes the use of design-based research (DBR) for contextual CA design to address vaccine hesitancy. In addition, this protocol will examine the impact of DBR on CA design to understand how this iterative process can enhance accuracy and performance.
Methods
A DBR methodology will be used for this study. Each phase of analysis, design, and evaluation of each design cycle inform the next one via its outcomes. An anticipated generic strategy will be formed after completing the first iteration. Using multiple research studies, frameworks and theoretical approaches are tested and evaluated through the different design cycles. User perception of the CA will be analyzed or collected by implementing a usability assessment during every evaluation phase using the System Usability Scale. The PARADISE (PARAdigm for Dialogue System Evaluation) method will be adopted to calculate the performance of this text-based CA.
Results
Two phases of the first design cycle (design and evaluation) were completed at the time of this writing (April 2022). The research team is currently reviewing the natural-language understanding model as part of the conversation-driven development (CDD) process in preparation for the first pilot intervention, which will conclude the CA’s first design cycle. In addition, conversational data will be analyzed quantitatively and qualitatively as part of the reflection and revision process to inform the subsequent design cycles. This project plans for three rounds of design cycles, resulting in various studies spreading outcomes and conclusions. The results of the first study describing the entire first design cycle are expected to be submitted for publication before the end of 2022.
Conclusions
CAs constitute an innovative way of delivering health communication information. However, they are primarily used to contribute to behavioral change or educate people about health issues. Therefore, health chatbots’ impact should be carefully designed to meet outcomes. DBR can help shape a holistic understanding of the process of CA conception. This protocol describes the design of VWise, a contextual CA that aims to address vaccine hesitancy using the DBR methodology. The results of this study will help identify the strengths and flaws of DBR’s application to such innovative projects.}
}
@article{ALVAREZRODRIGUEZ2023103744,
title = {Towards a method to quantitatively measure toolchain interoperability in the engineering lifecycle: A case study of digital hardware design},
journal = {Computer Standards & Interfaces},
volume = {86},
pages = {103744},
year = {2023},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2023.103744},
url = {https://www.sciencedirect.com/science/article/pii/S0920548923000259},
author = {Jose María Alvarez-Rodríguez and Roy Mendieta and Eduardo Cibrián and Juan Llorens},
keywords = {Software tools, Software reusability, Web services, Software as a service, Internet},
abstract = {The engineering lifecycle of cyber-physical systems is becoming more challenging than ever. Multiple engineering disciplines must be orchestrated to produce both a virtual and physical version of the system. Each engineering discipline makes use of their own methods and tools generating different types of work products that must be consistently linked together and reused throughout the lifecycle. Requirements, logical/descriptive and physical/analytical models, 3D designs, test case descriptions, product lines, ontologies, evidence argumentations, and many other work products are continuously being produced and integrated to implement the technical engineering and technical management processes established in standards such as the ISO/IEC/IEEE 15288:2015 “Systems and software engineering-System life cycle processes”. Toolchains are then created as a set of collaborative tools to provide an executable version of the required technical processes. In this engineering environment, there is a need for technical interoperability enabling tools to easily exchange data and invoke operations among them under different protocols, formats, and schemas. However, this automation of tasks and lifecycle processes does not come free of charge. Although enterprise integration patterns, shared and standardized data schemas and business process management tools are being used to implement toolchains, the reality shows that in many cases, the integration of tools within a toolchain is implemented through point-to-point connectors or applying some architectural style such as a communication bus to ease data exchange and to invoke operations. In this context, the ability to measure the current and expected degree of interoperability becomes relevant: 1) to understand the implications of defining a toolchain (need of different protocols, formats, schemas and tool interconnections) and 2) to measure the effort to implement the desired toolchain. To improve the management of the engineering lifecycle, a method is defined: 1) to measure the degree of interoperability within a technical engineering process implemented with a toolchain and 2) to estimate the effort to transition from an existing toolchain to another. A case study in the field of digital hardware design comprising 6 different technical engineering processes and 7 domain engineering tools is conducted to demonstrate and validate the proposed method.}
}
@article{LI2019460,
title = {Pattern match query over fuzzy RDF graph},
journal = {Knowledge-Based Systems},
volume = {165},
pages = {460-473},
year = {2019},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2018.12.014},
url = {https://www.sciencedirect.com/science/article/pii/S0950705118306099},
author = {Guanfeng Li and Li Yan and Zongmin Ma},
keywords = {Fuzzy RDF, Graph patterns, Regular expression, Graph homomorphism},
abstract = {In real applications, there are many domains that are full of uncertainty. As the resource description language of the Semantic Web, vagueness RDF data are playing an increasingly important role. And efficient querying of fuzzy RDF data is therefore of increasing importance. In this paper, we firstly introduce practical extensions of the RDF model to represent vagueness based on the fuzzy graph. Then we study the problem of querying subgraph from fuzzy RDF graph that matches with a given pattern graph with high satisfaction degree. The graph pattern surpasses previous proposals in terms of expressiveness because it can use regular expressions to specify the structural conditions of the graph in addition to the fuzzy conditions on the vertex values. We further develop an algorithm for evaluating graph pattern queries based on a revised notion of graph homomorphism. Finally, we verified the effectiveness, efficiency, and scalability of the algorithm using a comprehensive experimental evaluation of real-life data and synthetic data.}
}
@article{EFFIONG2025,
title = {Exploring Research Methodology and Research Design:},
journal = {International Journal of Business Analytics},
volume = {12},
number = {1},
year = {2025},
issn = {2334-4547},
doi = {https://doi.org/10.4018/IJBAN.381677},
url = {https://www.sciencedirect.com/science/article/pii/S233445472500005X},
author = {Sokomba Hannah Effiong},
keywords = {Reflexivity Research Philosophy Epistemology Ontology Methodological Flexibility Research Practice Organizational Ethnography Autoethnography Thematic Template Analysis},
abstract = {ABSTRACT
This book review examines Exploring Research Methodology and Research Design: Doing Research Across the Business Disciplines, edited by Peter John Sandiford and Sabine Schührer. The book is a reflective and practice-based resource for researchers navigating the complexities of methodology and design in business disciplines. It is structured in three parts: foundational perspectives on research, considerations in design and planning, and the realities of conducting research. Through diverse voices and case-based insights, the text challenges linear, formulaic approaches to research and instead promotes critical engagement, methodological flexibility, and reflexivity. The book equips doctoral students and early-career researchers with tools for thoughtful and context-sensitive research practice by addressing philosophical, ethical, and social dimensions.}
}
@article{QIU20191238,
title = {Causal reasoning of emergency cases based on Fuzzy Cognitive Map},
journal = {Procedia Computer Science},
volume = {159},
pages = {1238-1245},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.293},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919314929},
author = {Jiangnan Qiu and Wenjing Gu and Guangyuan Wang},
keywords = {Emergency cases, Fuzzy cognitive map, Case reasoning},
abstract = {Emergency case reasoning is essential to emergency management. In this paper, we propose a novel emergency case reasoning method based on fuzzy cognitive map (FCM), to model the inherent causal relationships in emergency cases. Specifically, we first obtain emergency domain elements and mine their association rules, by leveraging natural language processing technology and FT-Growth dada mining algorithm. We then design an effective algorithm to learn causal knowledge links from the gathered association rules. Finally, we construct an FCM regarding emergency events and show the reasoning process. Experiments on the gas explosion demonstrate that the proposed method can successfully model the internal causal relationships of emergency elements, and the development of the emergency event can be reflected by the reasoning process of the proposed method according to its varying variables of state. The proposed method can effectively inference and predict the tendency of emergency cases based on the reasoning process, which can further provide valuable decision supports to emergency responders.}
}
@article{OJHA2022700,
title = {Understanding the colonial roots of Indian management thought: An agenda to decolonise and theorise for Indian contexts},
journal = {Journal of Business Research},
volume = {149},
pages = {700-712},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.05.067},
url = {https://www.sciencedirect.com/science/article/pii/S0148296322005057},
author = {Abhoy K. Ojha and Ramya {Tarakad Venkateswaran}},
keywords = {Arthashastra, Decolonisation, Eurocentrism, Indigenous knowledge, India, US-centrism},
abstract = {Despite several calls to develop indigenous theories to contribute to Indian management knowledge, there has been limited success. There is no well-developed alternate Indian paradigm in management that can sustain a rigorous research programme and be relevant to practice. We argue that the intellectual colonisation of Indian academia due to the prevailing Eurocentrism (and US-centrism) and the use of English as a language for research and dissemination of knowledge are two key reasons underlying this failure. We demonstrate this by illustrating the near absence of scholarly work on Kautilya’s Arthashastra despite its wide acceptance in popular writings in India and its use in management practice. Finally, we suggest strategies to achieve intellectual decolonisation or intellectual freedom to enable scholars to engage with Indian issues and phenomena using indigenous knowledge perspectives and to contribute to an indigenous paradigm that might provide unique insights into managing the Indian way.}
}
@article{MARTINEZGIL2022944,
title = {Root Cause Analysis in the Industrial Domain using Knowledge Graphs: A Case Study on Power Transformers},
journal = {Procedia Computer Science},
volume = {200},
pages = {944-953},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.292},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922003015},
author = {Jorge Martinez-Gil and Georg Buchgeher and David Gabauer and Bernhard Freudenthaler and Dominik Filipiak and Anna Fensel},
keywords = {Knowledge Graphs, Root Cause Analysis, Manufacturing, Production},
abstract = {In the industrial domain, developing solutions that allow the identification, understanding, and correction of faults is essential due to the cost of handling such situations. However, to date, there are not many solutions capable of facilitating the human operator to discern the causes and possible solutions for a specific fault. In this work, we present knowledge graph-driven root cause analysis for working with faults in the industrial domain, based on three points of action: reasoning from the current state of machines or processes, failure classification using rules, and advanced querying using graph-query languages. We have conducted a power transformer case study that revealed that our proposed approach could be considered competitive as it has outperformed several alternative machine learning classifiers.}
}
@article{CHABALALA2021S32,
title = {African natural medicine: Toward a health preservation theory and systematization model for clinical application: African natural medicine and health preservation theory},
journal = {Journal of Traditional Chinese Medical Sciences},
volume = {8},
pages = {S32-S43},
year = {2021},
issn = {2095-7548},
doi = {https://doi.org/10.1016/j.jtcms.2021.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S2095754821000892},
author = {Hlupheka Chabalala and Motlalepula Matsabisa and Nceba Gqaleni},
keywords = {African knowledge systems, Systematization, Medical cosmology, Innovation, uBu-Ntu model},
abstract = {All knowledge fields are founded on universal epistemologies and philosophies. This is evident in ancient Traditional Chinese and Ayurvedic (Siddha) medical systems, which are integrated into national health systems of China and India respectively. African natural medicines (ANMs) are not part of national health systems on the African continent because of a lack of systematization frameworks. This article explores classical medical systems drawn from ancient African and Chinese cosmologies. A qualitative research methodology was used to conduct in-depth interviews with 20 respondents selected using a purposeful sampling technique. The data were summarized into systematization frameworks for disease theories, etiology and pathogenesis, diagnosis and prognosis, and treatment including medicines and disease classification. The study findings revealed that in antiquity, Africa had systematic medical cosmologies, remnants of which are evident in current cultural health practices. Therefore, parallels can be drawn in relation to Chinese Taoist and Indian Tantric healing arts. Ancient Africans recognized Ra or iSithunzi (uMbhilini) , which correspond to qi or prana and refer to energetic life force in Asian cultures. Shu and Tefnut (Nkomo weLwandle and Dungha Manzi) correspond to yin and yang or purusha and prakriti, the natural principles of polarised duality. The jing or oojas (physiological essences) and wu xing or pancha tattvas (five elements and modes of nature) and their relationships with the zang and fu (five vital organs and visceral organs, respectively) also found their application in African medical cosmologies. The data revealed that ancient and contemporary ANM systems are based on the concept of consciousness (saa or ntu) and life force energy, ra. These cosmological concepts are predicated upon the Paut Neteru or Amathogo (archetypal forces) that prescribe properties to everything. On this basis, an African-Kemetic Health Preservation Theory was developed, which gives rise to African theories for disease etiology, pathogenesis, differential diagnoses, comprehensive prognosis, and holistic treatment regimens. In addition, we developed an uBu-Ntu bio-innovation model for integral research, inclusive innovation, local technology transfer, holistic enterprising, and conscious commercialization of African natural medicines. It is the intention of the authors to influence policy in the implementation of these theories, models, and frameworks for clinical application and socio-economic development.}
}
@article{EIVAZI2025178119,
title = {Deep Learning-Driven Proteomics Analysis for Gene Annotation in the Renin-Angiotensin System},
journal = {European Journal of Pharmacology},
pages = {178119},
year = {2025},
issn = {0014-2999},
doi = {https://doi.org/10.1016/j.ejphar.2025.178119},
url = {https://www.sciencedirect.com/science/article/pii/S0014299925008738},
author = {Mortaza Eivazi and Kamran Hosseini and Shahin Alipanahi and Huijing Xia and Luke Restivo and Ayushi Patel and Mahdieh Gozali and Tahereh Ebrahimi and Amy Scarborough and Vahideh Tarhriz and Eric Lazartigues},
keywords = {Renin-angiotensin system, Multi-Layer Perceptron, Hypertension, Machine learning},
abstract = {The renin-angiotensin system (RAS) is central to cardiovascular diseases such as hypertension and cardiomyopathy, yet the functions of many RAS genes remain unclear. This study developed a multi-label deep learning model to systematically annotate RAS gene functions and elucidate their roles in biological pathways. A total of 39,463 RAS-related publications from PubMed and PMC were processed into text format. Feature matrices were generated using TF-IDF and token processing, followed by dimensionality reduction via Principal Component Analysis (PCA). A Multi-Layer Perceptron (MLP) was applied for multi-label classification, with performance evaluated using Precision, F1-Score, Ranking Loss, and ROC-AUC metrics. The model outperformed traditional methods (SVM, Random Forest), achieving a Precision of 0.7474 and ROC-AUC of 0.8697. Grouping into three major biological branches improved interpretability and performance (Precision: 0.8312; ROC-AUC: 0.9182). In silico predictions were validated using extracellular vesicle (EV) proteomics and capillary Western assays in DOCA-salt hypertensive mice. Key genes-AGTR2, IRAP (LNPEP), Ywhas (SFN), EDNRA, and ESR2—were identified as critical RAS components. Notably, IRAP was markedly upregulated in hypertension and showed regulatory interactions with 14-3-3 proteins, modulating Nedd4-2, ACE2, and AGTR1 signaling. To our knowledge, this is the first integration of multi-label AI modeling with EV proteomics for RAS pathway annotation. This framework captures complex gene-pathway relationships, advancing systems-level understanding of RAS biology and revealing a novel IRAP/Ywha(s)/Nedd4-2–ACE2 interaction axis as a potential therapeutic target.}
}
@article{HARRISON2021101002,
title = {Showing as sense-making in oral presentations: The speech-gesture-slide interplay in TED talks by Professor Brian Cox},
journal = {Journal of English for Academic Purposes},
volume = {53},
pages = {101002},
year = {2021},
issn = {1475-1585},
doi = {https://doi.org/10.1016/j.jeap.2021.101002},
url = {https://www.sciencedirect.com/science/article/pii/S1475158521000461},
author = {Simon Harrison},
keywords = {Presentations, PowerPoint, TED, Gesture, Depiction, Slides, Visuals, Sense-making, Skilled practice, Brian Cox},
abstract = {Building on research into the visual semiotics of slides and the multimodality of oral presentations, this paper analyses the speech-gesture-slide interplay in TED talks and considers implications for teaching about gesture in the academic presentation genre. Using two examples from presentations by award-winning science communicator Professor Brian Cox, an enactive-ecological approach to embodied communication yields fine-grained descriptions of relations between spoken language, visuals on the slide, and gesture including depictions, eye-gaze shifts, posture, and footwork. These demonstrate how the speaker's activity of showing slides is a person-environment dynamic of sense-making with the audience, and more specifically, how the speaker's speech-gesture-slide interplay animates, discloses, and decomposes aspects of the visuals on his slides while keeping his audience's attention intact. Whereas researchers of English for Academic Purposes have recommended raising students' awareness of multimodality or body language, this paper's findings suggest ways to sensitivise and engage students more directly and implicitly with the ecology of oral presentations.}
}
@article{BROEKHUIS2021,
title = {Conceptualizing Usability for the eHealth Context: Content Analysis of Usability Problems of eHealth Applications},
journal = {JMIR Formative Research},
volume = {5},
number = {7},
year = {2021},
issn = {2561-326X},
doi = {https://doi.org/10.2196/18198},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X21003504},
author = {Marijke Broekhuis and Lex {van Velsen} and Linda Peute and Meilani Halim and Hermie Hermens},
keywords = {usability benchmarking, eHealth systems, content analysis, usability framework, summative evaluation, mobile phone},
abstract = {Background
Usability tests can be either formative (where the aim is to detect usability problems) or summative (where the aim is to benchmark usability). There are ample formative methods that consider user characteristics and contexts (ie, cognitive walkthroughs, interviews, and verbal protocols). This is especially valuable for eHealth applications, as health conditions can influence user-system interactions. However, most summative usability tests do not consider eHealth-specific factors that could potentially affect the usability of a system. One of the reasons for this is the lack of fine-grained frameworks or models of usability factors that are unique to the eHealth domain.
Objective
In this study, we aim to develop an ontology of usability problems, specifically for eHealth applications, with patients as primary end users.
Methods
We analyzed 8 data sets containing the results of 8 formative usability tests for eHealth applications. These data sets contained 400 usability problems that could be used for analysis. Both inductive and deductive coding were used to create an ontology from 6 data sets, and 2 data sets were used to validate the framework by assessing the intercoder agreement.
Results
We identified 8 main categories of usability factors, including basic system performance, task-technology fit, accessibility, interface design, navigation and structure, information and terminology, guidance and support, and satisfaction. These 8 categories contained a total of 21 factors: 14 general usability factors and 7 eHealth-specific factors. Cohen κ was calculated for 2 data sets on both the category and factor levels, and all Cohen κ values were between 0.62 and 0.67, which is acceptable. Descriptive analysis revealed that approximately 69.5% (278/400) of the usability problems can be considered as general usability factors and 30.5% (122/400) as eHealth-specific usability factors.
Conclusions
Our ontology provides a detailed overview of the usability factors for eHealth applications. Current usability benchmarking instruments include only a subset of the factors that emerged from our study and are therefore not fully suited for summative evaluations of eHealth applications. Our findings support the development of new usability benchmarking tools for the eHealth domain.}
}
@article{ASIH2023439,
title = {Interpretable Machine Learning Model For Heart Disease Prediction},
journal = {Procedia Computer Science},
volume = {227},
pages = {439-445},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.544},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923017118},
author = {Putri Sari Asih and Yufis Azhar and Galih Wasis Wicaksono and Denar Regata Akbi},
keywords = {interpretable machine learning, SHAP, LIME},
abstract = {In the medical industry, accurately predicting a patient's likelihood of heart disease requires a high-performance model and explaining how the model arrived at its conclusion. To address this, a study has proposed a way to interpret machine learning models using SHAP and LIME. Four models have been created: Vector Machine, Random Forest, XGBoost, and k-Nearest Neighbor. The SVM and XGBoost models exhibit the highest f1-score performance, reaching up to 88%. These models can then be utilized during the interpretation stage with the aid of SHAP and LIME. Based on the SHAP visualization results, it is evident that the predictions made include various significant variables. Meanwhile, LIME explains the classification of each data point. Additionally, it confirms that SHAP and LIME are valuable tools for interpreting models.}
}
@article{YANG2025106419,
title = {Knowledge graph and mitigation measures recommendation for safety hazards in large-scale hydropower projects using diverse heterogeneous inspection data},
journal = {Automation in Construction},
volume = {178},
pages = {106419},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106419},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525004595},
author = {Yingliu Yang and Pengcheng Xiang and Dianxue Wang},
keywords = {Large-scale hydropower project (LHP), Construction safety hazards (CSHs), Knowledge graph, Sentence-BERT (SBERT), Mitigation measures, Safety management standards},
abstract = {Large-scale hydropower project (LHP) sites are fraught with numerous construction safety hazards (CSHs). When the efficiency of addressing these CSHs falls to meet safety management requirements, accidents may occur. Existing inspection records of CSHs contain a wealth of useful information, yet these unstructured texts hinder their efficient utilization. Furthermore, current mitigation measures for CSHs largely depend on human experience, leading to low efficiency. To address these issues, this paper proposes a BERT-Att-BiLSTM-CRF model based on massive daily inspection data, achieving precise extraction of CSHs entities (F1 > 95 %); construction a multi-dimensional knowledge graph with nine entity types and eight relationships; a mitigation measures recommendation based on Sentence-BERT (SBERT) model demonstrates superior performance (Pearson = 0.92, Spearman = 0.85) through semantic similarity; for novel CSHs, a safety management standards-based semantic model recommends compliant solutions. Validation confirms the research results capability to automate safety knowledge extraction from unstructured texts, establishing a replicable paradigm for infrastructure risk management.}
}
@article{SRIVASTAVA2020809,
title = {Personalized assessment model for alphabets learning with learning objects in e-learning environment for dyslexia},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {32},
number = {7},
pages = {809-817},
year = {2020},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2017.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S1319157817302306},
author = {Bhavana Srivastava and Md. Tanwir Uddin Haider},
keywords = {Dyslexia, Cognitive, E-learning systems, Semantic web},
abstract = {Internet has been source of knowledge for decades. The pool of information cannot be sustained in absence of the network of networks. Internet has many useful applications in commercial, social and educational areas. In today’s scenario, e-learning is also one of the useful applications in the world of Internet. The medium of e-learning has achieved advancement in various fields such as adaptive e-learning systems. The branch of computer science with psycholinguistics has done tremendous job in providing technical solutions to learners. However, learning disorders on the platform of e-learning still require lots of research. Therefore, this paper provides a personalized assessment model for alphabet learning with learning objects for children’s who face dyslexia. The cognitive inclination of dyslexic learner has been determined using assessment model. This paper studies the cognitive potential of dyslexic learner and has built a personalized e-learning platform to alleviate their alphabetical problems.}
}
@article{MOGHADDAM2020100053,
title = {Harvesting Patterns from Textual Web Sources with Tolerance Rough Sets},
journal = {Patterns},
volume = {1},
number = {4},
pages = {100053},
year = {2020},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2020.100053},
url = {https://www.sciencedirect.com/science/article/pii/S2666389920300647},
author = {Hoora Rezaei Moghaddam and Sheela Ramanna},
keywords = {natural language processing, named entity recognition, tolerance rough sets, granular computing, semi-supervised learning, machine learning},
abstract = {Summary
Construction of knowledge repositories from web corpora by harvesting linguistic patterns is of benefit for many natural language-processing applications that rely on question-answering schemes. These methods require minimal or no human intervention and can recursively learn new relational facts-instances in a fully automated and scalable manner. This paper explores the performance of tolerance rough set-based learner with respect to two important issues: scalability and its effect on concept drift, by (1) designing a new version of the semi-supervised tolerance rough set-based pattern learner (TPL 2.0), (2) adapting a tolerance form of rough set methodology to categorize linguistic patterns, and (3) extracting categorical information from a large noisy dataset of crawled web pages. This work demonstrates that the TPL 2.0 learner is promising in terms of precision@30 metric when compared with three benchmark algorithms: Tolerant Pattern Learner 1.0, Fuzzy-Rough Set Pattern Learner, and Coupled Bayesian Sets-based learner.}
}
@article{KUMAR2025108364,
title = {Knowledge graph applications and multi-relation learning for drug repurposing: A scoping review},
journal = {Computational Biology and Chemistry},
volume = {115},
pages = {108364},
year = {2025},
issn = {1476-9271},
doi = {https://doi.org/10.1016/j.compbiolchem.2025.108364},
url = {https://www.sciencedirect.com/science/article/pii/S1476927125000246},
author = {A.Arun Kumar and Samarth Bhandary and Swathi Gopal Hegde and Jhinuk Chatterjee},
keywords = {Drug repurposing, Knowledge graph, Embedding, Multimodal frameworks},
abstract = {Objective
Development of novel drug solutions has always been an expensive endeavour, hence drug repurposing as an approach has gained popularity in recent years. In this review we intend to examine one of the most unique computational methods for drug repurposing, that being knowledge graphs.
Method
Through literature review we looked at the application of knowledge graphs in medicine, specifically at its use in drug repurposing. We also looked at literature embedding methods, integration of machine learning models and approaches to completion of knowledge graphs.
Result
After filtering 43 papers were used for analysis. Timeline, country distribution, application areas of knowledge graph was highlighted. General trends in the use of knowledge graphs for drug repurposing and any shortcomings of the approach was discussed.
Conclusion
This approach has gained popularity only very recently; hence it is in a nascent phase.}
}
@article{MANCHEL2024111322,
title = {From sampling to simulating: Single-cell multiomics in systems pathophysiological modeling},
journal = {iScience},
volume = {27},
number = {12},
pages = {111322},
year = {2024},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.111322},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224025471},
author = {Alexandra Manchel and Michelle Gee and Rajanikanth Vadigepalli},
keywords = {Systems biology, Data processing in systems biology, In silico biology, Biological constraints, Omics},
abstract = {Summary
As single-cell omics data sampling and acquisition methods have accumulated at an unprecedented rate, various data analysis pipelines have been developed for the inference of cell types, cell states and their distribution, state transitions, state trajectories, and state interactions. This presents a new opportunity in which single-cell omics data can be utilized to generate high-resolution, high-fidelity computational models. In this review, we discuss how single-cell omics data can be used to build computational models to simulate biological systems at various scales. We propose that single-cell data can be integrated with physiological information to generate organ-specific models, which can then be assembled to generate multi-organ systems pathophysiological models. Finally, we discuss how generic multi-organ models can be brought to the patient-specific level thus permitting their use in the clinical setting.}
}
@article{KOVALENKO2021183,
title = {Analysis of the emergence of trust in the information field as a decision-making tool},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {13},
pages = {183-187},
year = {2021},
note = {20th IFAC Conference on Technology, Culture, and International Stability TECIS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.10.442},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321018796},
author = {M.A. Kovalenko and R.G. Bolbakov and V.A. Mordvinov},
keywords = {trust, synergetic, fiscal policy, emergence, life cycle},
abstract = {The presented material analyzes the phenomenon of trust, its place in the value system of social regulation. In the development of modern science, there is a clear trend towards the need to diagnose the potential and risks of new technologies, to assess the social and economic consequences of their implementation. Using the language of synergetics as a branch of philosophy, the authors consider the problem of trust in information as a special kind of interaction of certain variables in information processes and systems, raise the question of indicating, reducing and eliminating the risks of the social interaction in the context of trust in information in fiscal policy.}
}
@article{GODDARD2021101327,
title = {“We”: conceptual semantics, linguistic typology and social cognition},
journal = {Language Sciences},
volume = {83},
pages = {101327},
year = {2021},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2020.101327},
url = {https://www.sciencedirect.com/science/article/pii/S0388000120300607},
author = {Cliff Goddard and Anna Wierzbicka},
keywords = {Pronouns, Semantic universals, Co-lexicalisation, Social cognition, NSM semantics},
abstract = {This paper explores “we-words” in the languages of the world, using the NSM method of semantic analysis. A simply phrased, cross-translatable explication for English ‘we’ [1pl] is proposed, suitable also for other languages with a single we-word. At the same time, it is argued that English ‘we’ co-lexicalises a second distinct meaning “we two” [1du], and that the same goes for other languages with a single we-word. The two explications are identical, except for being based on ALL and TWO, respectively. Both explications involve components of “I-inclusion” (roughly, ‘I am one of them’) and “subjective identification” (roughly, ‘I'm thinking about them all in the same way’). It is argued, furthermore, that both meanings (“we-all” and “we-two”) are likely to be found in all languages. To establish this, one has to take account of languages which manifest the “inclusive/exclusive” distinction. For such languages, evidence suggests that one of the two we-words contains a semantic component of “you-inclusion”, while the other is semantically unmarked. Languages whose “we words” encode kinship relations are also briefly considered. The analysis has implications for the typology of pronoun systems, for theorising about human social cognition, and for the lexical semantics of key social concepts.}
}
@article{CHEN2024,
title = {Construction and Improvement Path of Digital Literacy Evaluation Model for Higher Vocational Teachers Based on Deep Learning and Soft Computing},
journal = {International Journal of e-Collaboration},
volume = {20},
number = {1},
year = {2024},
issn = {1548-3673},
doi = {https://doi.org/10.4018/IJeC.347506},
url = {https://www.sciencedirect.com/science/article/pii/S1548367324000061},
author = {Gan Chen},
keywords = {Informatization, Evaluation model, Digital literacy, Improvement path},
abstract = {ABSTRACT
In view of the rapid development of information technology, the cultivation and promotion of digital literacy of higher vocational teachers has become an important issue in the field of education. The application of deep learning and soft computing technology provides strong technical support for this. This paper is to explore the construction and promotion path of digital literacy evaluation model of higher vocational teachers from the perspective of “AI+”. This study deeply analyzes the status quo of digital literacy of higher vocational teachers, and focuses on the combination and application potential of deep learning and intelligent algorithm in the evaluation model and promotion path of digital literacy of higher vocational teachers based on “AI+” perspective. This research plays an important role in promoting personalized education and cultivating talents with high-quality technical skills. Future research will further deepen relevant theories and promote the scientific, standardized and intelligent evaluation model of digital literacy of higher vocational teachers.}
}
@article{ALSAYAT20207,
title = {A comprehensive study for Arabic Sentiment Analysis (Challenges and Applications)},
journal = {Egyptian Informatics Journal},
volume = {21},
number = {1},
pages = {7-12},
year = {2020},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2019.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1110866519300945},
author = {Ahmed Alsayat and Nouh Elmitwally},
keywords = {Semantics, Arabic Sentiment Analysis, Social sciences, Machine learning, ALSA six-level framework},
abstract = {Arabic language processing works on multiple levels; less often, these complementary levels synergize well with each other. Arabic Language Sentiment Analysis (ALSA) levels consist of phonetics, morphology, syntax, lexicology, semantics, and figurative nature. The analysis of opinions and feelings is of interest in English and Indo-European languages, with little emphasis in the Arabic language, which is a language full of rhetorical characteristics and implicit meanings that have positive and negative connotations and meanings across the six linguistic levels. This paper presents a comprehensive and full proposal of a strategy for ALSA. The ALSA framework analyzes the opinions and feelings at all levels of language, in addition to the importance of building an annotated corpus, which helps to understand an Arabic sentence from the level of phonetics to the rhetorical and metonymy levels.}
}
@article{OLEN2024127,
title = {Distressing Discussions in Pediatric Interpreted Medical Encounters: A Qualitative Study of Medical Interpreter Perspectives on Clinician Communication Practices},
journal = {Journal of Pediatric Health Care},
volume = {38},
number = {2},
pages = {127-139},
year = {2024},
note = {A Closer Look at Diversity, Equity, Inclusion, and Belonging in Pediatric Healthcare},
issn = {0891-5245},
doi = {https://doi.org/10.1016/j.pedhc.2023.11.017},
url = {https://www.sciencedirect.com/science/article/pii/S0891524523003619},
author = {Amy Olen and Paulina S. Lim and Sthephany Escandell and Kathryn A. Balistreri and Julia B. Tager and W. Hobart Davies and Matthew C. Scanlon and Charles B. Rothschild},
keywords = {Medical interpreting, interpreted medical encounters, health communication, health equity},
abstract = {Introduction
This study explores pediatric medical interpreters’ perspectives on clinician communication practices in medical encounters characterized by distressing content and difficult discussions.
Method
In this interpretative phenomenological analysis, 13 Spanish-English interpreters at a midwestern pediatric hospital were purposively recruited and, in 2021–2022, completed a demographic survey and semistructured interview on communication in distressing interpreted medical encounters.
Results
Participants described clinician practices for effective cross-cultural interpreted communication. Practices align with recommendations on prebriefing, debriefing, jargon, stakeholder positioning, and teamwork. Novel findings relate to encounters with multiple parties, multilingual patients with monolingual parents, and coordination among clinicians.
Discussion
Findings corroborate recommendations for interpreted communication best practices, extend them to distressing pediatric encounters, and offer recommendations for clinicians using interpreting services in distressing encounters. Participants’ insights are distilled into a series of clinician best practices for high-quality interpreted communication during difficult discussions and for strengthening language access services in pediatric medical settings.}
}
@article{BERTAGNON2021104809,
title = {Branching interval algebra: An almost complete picture},
journal = {Information and Computation},
volume = {281},
pages = {104809},
year = {2021},
issn = {0890-5401},
doi = {https://doi.org/10.1016/j.ic.2021.104809},
url = {https://www.sciencedirect.com/science/article/pii/S0890540121001255},
author = {A. Bertagnon and M. Gavanelli and A. Passantino and G. Sciavicco and S. Trevisani},
keywords = {Interval algebra, Branching time, Tractability, Efficiency},
abstract = {Branching Algebra is the natural branching-time generalization of Allen's Interval Algebra. As in the linear case, the consistency problem for Branching Algebra is NP-hard. Branching Algebra has many potential applications in different areas of Artificial Intelligence; therefore, being able to efficiently solve classical problems expressed in Branching Algebra is very important. This can be achieved in two steps: first, by identifying expressive enough, yet tractable fragments of the whole algebra, and, second, by using such fragments to boost the performances of a backtracking algorithm for the whole language. In this paper we study the properties of several such fragments, both from the algebraic and the computational point of view, and we give an almost complete picture of tractable and non-tractable fragments of the Branching Algebra.}
}
@article{HECKLER2025103094,
title = {Digital phenotyping for mental health based on data analytics: A systematic literature review},
journal = {Artificial Intelligence in Medicine},
volume = {163},
pages = {103094},
year = {2025},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2025.103094},
url = {https://www.sciencedirect.com/science/article/pii/S0933365725000296},
author = {Wesllei Felipe Heckler and Luan Paris Feijó and Juliano Varella {de Carvalho} and Jorge Luis Victória Barbosa},
keywords = {Data analytics, Digital phenotypes, Digital phenotyping, Machine learning, Mental health, Systematic literature review},
abstract = {Even though mental health is a human right, mental disorders still affect millions of people worldwide. Untreated and undertreated mental health conditions may lead to suicide, which generates more than 700,000 deaths annually around the world. The broad adoption of smartphones and wearable devices allowed the recording and analysis of human behaviors in digital devices, which might reveal mental health symptoms. This analysis constitutes digital phenotyping research, referring to frequent and constant measurement of human phenotypes in situ based on data from smartphones and other personal digital devices. Therefore, this article presents a systematic literature review providing a computer science view on data analytics for digital phenotyping in mental health. This study reviewed 5,422 articles from ten academic databases published up to September 2024, generating a final list of 74 studies. The investigated databases are ACM, IEEE Xplore, PsycArticles, PsycInfo, Pubmed, Science Direct, Scopus, Springer, Web of Science, and Wiley. We investigated ten research questions, considering explored data, employed devices, and techniques for data analysis. This review also organizes the application domains and mental health conditions, data analytics techniques, and current research challenges. This study found a growing research interest in digital phenotyping for mental health in recent years. Current approaches still present a high dependence on self-reported measures of mental health status, but there is evidence of the employment of smartphones for leveraging passive data collection. Traditional machine learning techniques are the main explored strategies for analyzing the large amount of collected data. In this regard, published approaches deeply focused on data analysis, generating opportunities concerning the implementation of resources for assisting individuals suffering from mental disorders.}
}
@article{HU2024753,
title = {Development of a cell adhesion-based prognostic model for multiple myeloma: Insights into chemotherapy response and potential reversal of adhesion effects},
journal = {Oncology Research},
volume = {32},
number = {4},
pages = {753-768},
year = {2024},
issn = {0965-0407},
doi = {https://doi.org/10.32604/or.2023.043647},
url = {https://www.sciencedirect.com/science/article/pii/S0965040724000578},
author = {QIAN HU and MENGYAO WANG and JINJIN WANG and YALI TAO and TING NIU},
keywords = {Cell adhesion, Bioinformatics, Prognosis, Multiple myeloma, CAM-DR},
abstract = {Multiple myeloma (MM) is a hematologic malignancy notorious for its high relapse rate and development of drug resistance, in which cell adhesion-mediated drug resistance plays a critical role. This study integrated four RNA sequencing datasets (CoMMpass, GSE136337, GSE9782, and GSE2658) and focused on analyzing 1706 adhesion-related genes. Rigorous univariate Cox regression analysis identified 18 key prognosis-related genes, including KIF14, TROAP, FLNA, MSN, LGALS1, PECAM1, and ALCAM, which demonstrated the strongest associations with poor overall survival (OS) in MM patients. To comprehensively evaluate the impact of cell adhesion on MM prognosis, an adhesion-related risk score (ARRS) model was constructed using Lasso Cox regression analysis. The ARRS model emerged as an independent prognostic factor for predicting OS. Furthermore, our findings revealed that a heightened cell adhesion effect correlated with tumor resistance to DNA-damaging drugs, protein kinase inhibitors, and drugs targeting the PI3K/Akt/mTOR signaling pathway. Nevertheless, we identified promising drug candidates, such as tirofiban, pirenzepine, erlotinib, and bosutinib, which exhibit potential in reversing this resistance. In vitro, experiments employing NCIH929, RPMI8226, and AMO1 cell lines confirmed that MM cell lines with high ARRS exhibited poor sensitivity to the aforementioned candidate drugs. By employing siRNA-mediated knockdown of the key ARRS model gene KIF14, we observed suppressed proliferation of NCIH929 cells, along with decreased adhesion to BMSCs and fibronectin. This study presents compelling evidence establishing cell adhesion as a significant prognostic factor in MM. Additionally, potential molecular mechanisms underlying adhesion-related resistance are proposed, along with viable strategies to overcome such resistance. These findings provide a solid scientific foundation for facilitating clinically stratified treatment of MM.}
}
@article{LUKIC2023101169,
title = {Dementia as a material for co-creative art making: Towards feminist posthumanist caring},
journal = {Journal of Aging Studies},
volume = {67},
pages = {101169},
year = {2023},
issn = {0890-4065},
doi = {https://doi.org/10.1016/j.jaging.2023.101169},
url = {https://www.sciencedirect.com/science/article/pii/S0890406523000701},
author = {Dragana Lukić},
keywords = {Aging, Dementia, More-than-human art materials, Co-creative artmaking practices, Multisensorial entangled intra-actions, Feminist posthumanist caring},
abstract = {This article generates new understandings of dementia through feminist posthumanist and performative engagements with co-creative artmaking practices during a six-month study in a residential care home in Norway. Dementia emerges within multisensorial entanglements of more-than-human materials in three different artmaking sessions, which first materialized in the form of collective photographs and vignettes and culminated in a final exhibition, Gleaming Moments, in the care home. Drawing on these photographs, vignettes, and the author's engagement as a research artist in the sessions, this analysis examined how dementia was enacted as a spark of inspiration, felted warm seat pads, and a friendly more-than-human touch, that is, a touch of human and nonhuman art materials. These findings suggest new ontologies of dementia within multisensorial artmaking practices, in which dementia functions as a material for co-creative artmaking rather than a disease. These findings disrupt dominant biomedical ontologies of Alzheimer's disease and other dementias, as well as humanist person-centered practices in dementia care, which have concretized an individual, rather than relational, focus on dementia. In contrast, this study explores dementia as a phenomenon within the entanglements of human and nonhuman intra-active agencies. By highlighting the significance of these agencies (i.e., sponge holder-painting, wool-felting, choir-singing, chick-making) for different worlds-making with dementia, this study provides an entry point for imagining feminist posthumanist caring. Thus, dementia becomes a matter in life that is not to be managed and defeated to achieve successful aging, but to be interrogated and embraced.}
}
@article{RUAN2021121071,
title = {Rethinking the disruption index as a measure of scientific and technological advances},
journal = {Technological Forecasting and Social Change},
volume = {172},
pages = {121071},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121071},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521005035},
author = {Xuanmin Ruan and Dongqing Lyu and Kaile Gong and Ying Cheng and Jiang Li},
keywords = {Disruption index, Number of references, Regression analysis},
abstract = {Wu et al. (2019) used the disruption(D) index to measure scientific and technological advances in Nature. Their findings spurred extensive discussion in academia on whether we can measure the disruption (i.e., innovation or novelty) of a research paper or a patent based on the number of citations. In this paper, we calculate the D index of ∼0.76 million publications published between 1954 and 2013 in six disciplines including both sciences and social sciences in English and Chinese. We found that the number of references has a negative effect on the D index of a paper with a relatively small number of references, and a positive effect on the D index of a paper with a large number of references. We also found that low coverage of a citation database boosts D values. Specifically, low coverage of non-journal literature in the Web of Science (WOS) boosted D values in social sciences, and the exclusion of non-Chinese language literature in the Chinese Social Sciences Citation Index (CSSCI) resulted in the inflation of D values in Chinese language literature. Limitations of the D index observed in scientific papers also exist in technological patents. This paper sheds light on the use of citation-based measurements of scientific and technological advances and highlights the limitations of this index.}
}
@article{CHAI2025103408,
title = {Cross-domain knowledge transfer in industrial process monitoring: A survey},
journal = {Journal of Process Control},
volume = {149},
pages = {103408},
year = {2025},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2025.103408},
url = {https://www.sciencedirect.com/science/article/pii/S0959152425000368},
author = {Zheng Chai and Chunhui Zhao and Biao Huang},
keywords = {Fault detection and diagnosis, Fault prognosis, Industrial process monitoring, Soft sensors, Transfer learning},
abstract = {The last decades have witnessed rapid progress in machine learning and data analytics-based industrial process monitoring. However, the underlying assumption that the training and test data should have the same feature space and the same distribution is generally challenged in practical industrial applications due to varying working conditions, mechanical wear, feed changes, etc. To this end, knowledge transfer, which reduces the discrepancy between different data and facilitates the target model learning, has given rise to tremendous advances for mitigating this trap. Motivated by the success, in this survey, the state-of-the-art techniques are investigated and a review from a broad perspective in the field of cross-domain industrial process monitoring applications is provided, including fault detection and diagnosis, fault prognosis, and soft sensors. Owing to the extensive developments, the cross-domain knowledge transfer in process monitoring can be divided into three branches in this survey, i.e., the multivariate statistical analysis-based, the shallow neural networks-based, and the deep neural networks-based methods. Benefiting from the theoretical development and elaborately developed approaches, current challenges and instructive perspectives are further conceived for inspiring new directions in this exciting research field. The aim of this paper is to sketch the basic principles and frameworks for cross-domain knowledge transfer in process monitoring and provide inspiration for future studies in the process industry.}
}
@article{WANG2023121136,
title = {Data-driven and Knowledge-based predictive maintenance method for industrial robots for the production stability of intelligent manufacturing},
journal = {Expert Systems with Applications},
volume = {234},
pages = {121136},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121136},
url = {https://www.sciencedirect.com/science/article/pii/S095741742301638X},
author = {Xiaoqiao Wang and Mingzhou Liu and Conghu Liu and Lin Ling and Xi Zhang},
keywords = {Industrial robots, Predictive maintenance, Intelligent manufacturing, Deep learning, Knowledge graph},
abstract = {The service stability of industrial robots (IRs) is considered the basis for ensuring intelligent manufacturing operations. Knowledge-based work plays a central role in the practical application of intelligent manufacturing because the staff have professional knowledge of production and manufacturing after learning, undergoing training, and thinking. They can use their knowledge to analyze complex states, assess them accurately, and make innovative decisions. Therefore, expressing and constructing IR data and knowledge is a key issue in the application of knowledge to the predictive maintenance (PdM) of IRs. Considering the intelligent management of IRs as the research objective of this study, a PdM method based on data and knowledge was developed. A running-state feature-recognition model based on a long short-term memory network was first established to recognize future running states using the history and real-time running data of IRs. Furthermore, the k-nearest neighbor algorithm was used to analyze the correlation between the running-state feature data and faults to predict possible faults. The prediction results were then input into the knowledge graphs (KGs) of IRs for reasoning. PdM strategies were automatically formulated based on the KGs. Finally, a PdM system for IRs was designed and developed. The effectiveness of the proposed method and system were verified by applying them to the welding robots in a new energy automotive welding workshop. The findings of this study provide new concepts and tools for the PdM of IRs, as well as theoretical and methodological support for the production stability of intelligent manufacturing.11Industrial robot (IR), predictive maintenance (PdM), knowledge graph (KG), industrial Internet of Things (IIoT), intelligent manufacturing (IM), intelligent manufacturing systems (IMS), k-nearest neighbor (KNN), long short-term memory (LSTM), process control (OPC), programmable logic controller (PLC), factory automation system (FAS), manufacturing execution system (MES), recipe cluster management system (RCMS), recurrent neural network (RNN), support vector machine (SVM), knowledge ontology (KO), physical production system (PPS), physical modeling modules (PMM), application management modules (AMM), system management module (SMM), fault information management (FIM), knowledge graph management (KGM), maintenance plan management (MPM), information push management (IPM), statistical analysis management (SAM), user interface (UI).}
}
@article{KEJRIWAL2021103497,
title = {An evaluation and annotation methodology for product category matching in e-commerce},
journal = {Computers in Industry},
volume = {131},
pages = {103497},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103497},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521001044},
author = {Mayank Kejriwal and Ke Shen and Chien-Chun Ni and Nicolas Torzec},
keywords = {E-commerce, Product search and recommendation, Annotation, Methodology, Evaluation, Language representation model, Taxonomy alignment},
abstract = {Product category matching is an important task in digital marketplaces and e-commerce, helping to power better search and recommendations in an online context. While variants of the problem have received some attention in academia, there is no documented guidance on how to efficiently acquire annotations for evaluating multiple (current and future) models, many of which rely on modern machine learning techniques such as neural representation learning. In this paper, we motivate and formalize the problem of product category matching in e-commerce, and present a rigorously designed set of guidelines and methodology for acquiring annotations in a cost-effective and reliable manner. We also present a methodology for using the annotations to compare solutions of two or more product category matching methods, including comparing models both before and after annotation. Three widely used e-commerce product category taxonomies, and multiple metrics, are used to demonstrate the utility of our proposals.}
}
@article{DEY2024101984,
title = {An integrated bioinformatics approach for unravelling the molecular insights into psoriasis pathology and therapeutics},
journal = {Gene Reports},
volume = {36},
pages = {101984},
year = {2024},
issn = {2452-0144},
doi = {https://doi.org/10.1016/j.genrep.2024.101984},
url = {https://www.sciencedirect.com/science/article/pii/S2452014424001079},
author = {Rahul Dey and Amitava Das},
keywords = {Psoriasis, RNA-Seq, Microarray, Differentially expressed genes, Hub genes, Drug-gene interactions},
abstract = {Psoriasis is a chronic relapsing inflammatory disease of the skin that affects almost 2–3 % population worldwide. The current treatment strategies include palliative therapeutics targeting the inflammatory pathways that do not completely cure the lesioned skin. The molecular cues in the hyper-proliferative and aberrantly differentiated keratinocytes of the psoriatic lesioned skin remain unknown. Through an integrative in-silico approach, we have analyzed human psoriatic skin samples from 3 RNA-Seq and 3 microarray datasets to identify 340 differentially expressed genes (DEGs). Further, these DEGs were analyzed using gene ontology enrichment, KEGG pathways, and protein-protein interaction networks for their role in disease pathology and the identification of hub genes. The expression of the hub genes was validated in a preclinical murine model of psoriasiform dermatitis. Finally, the ten hub genes were assessed for their drugability, which revealed 74 drugs targeting 7 hub genes (CCNA2, TOP2A, BIRC5, RRM2, CDK1, AURKA, and CCNB1) that can be repurposed for psoriasis treatment. This study provides an understanding of psoriasis pathophysiology and suggests key molecular biomarkers as therapeutic targets for effective mitigation of the disease.}
}
@article{SILVAMUNOZ2019101724,
title = {A time-indexed mereology for SUMO},
journal = {Data & Knowledge Engineering},
volume = {123},
pages = {101724},
year = {2019},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2019.101724},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17303749},
author = {Lydia {Silva Muñoz} and Michael Grüninger},
keywords = {Mereology, Time-indexed mereology, Temporary mereology, Ontology, Upper-level ontology, Foundational ontology, SUMO, DOLCE, Ontology mapping, Change, Mereological change},
abstract = {While the period of time during which a subprocess occurs is precisely the time during which the part-whole relation with its main process takes place, part-whole relations between objects do not obey such a rule. The parts of an object can exist before the object is conformed as such, and can survive its dismantlement. In fact, there are no means for knowing when, during the existence of the part and the whole, their parthood relation holds unless an explicit account of time is represented. A time-indexed mereology characterizes how objects gain and lose parts over time by associating a time index to their part-whole relations. Keeping an account of when objects lose or gain parts is necessary for the correct representation of their spatial location and their participation in processes. Upper-level ontologies characterize the properties of the most basic, domain-independent entities, such as time, space, objects and processes. Two upper-level ontologies broadly used are The Descriptive Ontology for Linguistic and Cognitive Engineering (DOLCE) and The Suggested Upper Merged Ontology (SUMO). However, while DOLCE provides a first-order time-indexed mereology for structuring its entities over time, SUMO provides a weaker axiomatization that does not represent the rules that determine how the mereological structure of objects evolve through time in the real world. This work proposes a first-order logic time-indexed mereology for SUMO based on its current representation of objects, time, and temporal location, thereby characterizing how objects gain and lose parts over time. The proposed theory sets the stage for the development of a time-indexed theory of spatial location, and for the representation of temporal restrictions on the participation of objects in processes. The time-indexed mereology of DOLCE and the proposed theory are formally compared, and their relative strength established by using ontology mapping. In order to achieve such a comparison, the representations of time, and temporal location of both upper-level ontologies are also formally compared.}
}
@article{HANE2020,
title = {Predicting Onset of Dementia Using Clinical Notes and Machine Learning: Case-Control Study},
journal = {JMIR Medical Informatics},
volume = {8},
number = {6},
year = {2020},
issn = {2291-9694},
doi = {https://doi.org/10.2196/17819},
url = {https://www.sciencedirect.com/science/article/pii/S2291969420001714},
author = {Christopher A Hane and Vijay S Nori and William H Crown and Darshak M Sanghavi and Paul Bleicher},
keywords = {Alzheimer disease, dementia, health information systems, machine learning, natural language processing, health information interoperability},
abstract = {Background
Clinical trials need efficient tools to assist in recruiting patients at risk of Alzheimer disease and related dementias (ADRD). Early detection can also assist patients with financial planning for long-term care. Clinical notes are an important, underutilized source of information in machine learning models because of the cost of collection and complexity of analysis.
Objective
This study aimed to investigate the use of deidentified clinical notes from multiple hospital systems collected over 10 years to augment retrospective machine learning models of the risk of developing ADRD.
Methods
We used 2 years of data to predict the future outcome of ADRD onset. Clinical notes are provided in a deidentified format with specific terms and sentiments. Terms in clinical notes are embedded into a 100-dimensional vector space to identify clusters of related terms and abbreviations that differ across hospital systems and individual clinicians.
Results
When using clinical notes, the area under the curve (AUC) improved from 0.85 to 0.94, and positive predictive value (PPV) increased from 45.07% (25,245/56,018) to 68.32% (14,153/20,717) in the model at disease onset. Models with clinical notes improved in both AUC and PPV in years 3-6 when notes’ volume was largest; results are mixed in years 7 and 8 with the smallest cohorts.
Conclusions
Although clinical notes helped in the short term, the presence of ADRD symptomatic terms years earlier than onset adds evidence to other studies that clinicians undercode diagnoses of ADRD. De-identified clinical notes increase the accuracy of risk models. Clinical notes collected across multiple hospital systems via natural language processing can be merged using postprocessing techniques to aid model accuracy.}
}
@article{LEE20191952,
title = {An Efficient Design Support System based on Automatic Rule Checking and Case-based Reasoning},
journal = {KSCE Journal of Civil Engineering},
volume = {23},
number = {5},
pages = {1952-1962},
year = {2019},
issn = {1226-7988},
doi = {https://doi.org/10.1007/s12205-019-1750-2},
url = {https://www.sciencedirect.com/science/article/pii/S1226798824033658},
author = {Pin-Chan Lee and Tzu-Ping Lo and Ming-Yang Tian and Danbing Long},
keywords = {automatic rule checking, building information modelling, case-based reasoning, design support system, AHP, TOPSIS},
abstract = {A well building design support system can not only meet the rules but also automatically recommend the appropriate alternatives for designers, but most modifications now are conducted in the manual way. Although the method of automatic rule checking can effectively identify the compliance of rules in Building Information Modeling (BIM) models, recommendation supports are still lacked in applications. This paper aims to propose a design support system, using automatic rule checking to identify the compliance of rules and adopting case-based reasoning to provide recommendations via ontology and semantics. The AHP-TOPSIS (Analytic hierarchy process-Technique for Order Preference by Similarity to an Ideal Solution) method is used to give reliable recommendations rank. A real case is adopted as an illustrative example. Results show that the proposed system can increase the design efficiency in both design checking and modifying. Similar applications can be extended to other fields and rules.}
}
@article{RADIO2019752,
title = {A survey of time based approaches for linked data},
journal = {Library Hi Tech},
volume = {37},
number = {4},
pages = {752-763},
year = {2019},
issn = {0737-8831},
doi = {https://doi.org/10.1108/LHT-04-2019-0084},
url = {https://www.sciencedirect.com/science/article/pii/S0737883119000010},
author = {Erik Radio},
keywords = {Information retrieval, Technology, Metadata, Ontologies, Resource description framework, Bibliographic standards},
abstract = {Purpose
Linked data technologies promise different ways of querying and retrieving information that enable individuals to have search experiences that are broader and more coordinated than those common in current library technologies. It is vital that information technologies be able to incorporate temporal capabilities or reasoning to allow for the more nuanced interactions with resources, particularly as they change over time. The purpose of this paper is to assess methods currently in use that allow for temporal querying of resources serialized as linked data.
Design/methodology/approach
This paper examines philosophical models, experimental approaches and common standards to identify areas of alignment and divergence in their orientations toward serializing time and change as linked data. By framing approaches and standards within the context of philosophical theories, a clear preference for certain models of time emerge.
Findings
While there have been several approaches to serializing time as linked data, none have found their way into a full implementation by standards in common use. Further, approaches to the issue are largely rooted in one model of philosophical thought that is particularly oriented to computational approaches. As such there is a gap between methods and standards, and a large room for further investigation into temporal models that may be applicable for different contexts. A call for investigation into a model that can cascade in to different temporal approaches is provided.
Originality/value
While there are many papers concerning serializing time as linked data, none have tried to thoroughly align these to philosophical theories of time and further to standards currently in use.}
}
@article{KIM2021101263,
title = {Automated composition and execution of web-based simulation systems through knowledge designing and reasoning},
journal = {Advanced Engineering Informatics},
volume = {48},
pages = {101263},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101263},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621000185},
author = {Dohyun Kim and Dongsu Jeong and Yoonho Seo},
keywords = {System composition and execution, Web-based simulation, Abductive event calculus, Ontology design, Data transformation, Weapon system design},
abstract = {Modeling and simulation (M&S) techniques can be used to improve cooperation among participants by clarifying the various design aspects of a devised system. Whenever an M&S system with specific functions is required, redesigning it from scratch is time and resource intensive. Thus, an approach to reuse or reconfigure existing systems with functions similar to the required ones is a potential alternative to overcome such problems. This paper proposes a method to build composite systems providing specific M&S functions through the web. The method proposed herein offers an integrated process that includes web-based reuse of existing systems, analysis of the requirements of a composite system, derivation of the logical execution order of element systems, and interconnection of the input–output interfaces between linked constituent systems. This approach provides the required tools by dynamically reconfiguring the element systems at the requested locations. The experimental results demonstrate that the proposed procedure can provide automatic generation of the required M&S system (e.g., missile-interception simulation system) and deliver the system to the target location of the cooperative work through the web.}
}
@article{USMANIRANA2024e40744,
title = {Integrative bioinformatic analysis to identify potential phytochemical candidates for glioblastoma},
journal = {Heliyon},
volume = {10},
number = {24},
pages = {e40744},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e40744},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024167758},
author = {Hafiza Maria {Usmani Rana} and Haseeb Nisar and Jignesh Prajapati and Dweipayan Goswami and Ravi Rawat and Volkan Eyupoglu and Samiah Shahid and Anum Javaid and Wardah Nisar},
keywords = {Glioblastoma, Phytochemicals, Hub genes, Molecular docking, MD simulation},
abstract = {Glioblastoma (GBM) is one of the most malignant forms of cancer with the lowest survival ratio. Our study aims to utilize an integrated bioinformatic analysis to identify hub genes against GBM and explore the active phytochemicals with drug-like properties in treating GBM. The study employed databases of DisGenet, GeneCards, and Gene Expression Omnibus to retrieve GBM-associated genes, revealing 142 overlapping genes. Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment were used to analyze the role of these genes, which were involved in cancer-associated cell signaling pathways with tyrosine kinase activities and mainly enriched in the Nucleus. Furthermore, the hub genes identification through Cytoscape identified the top 10 ranked genes in a network, which were used as targets to dock against phytochemicals retrieved from the NPACT database having the ability to pass the blood-brain barrier and drug-likeness properties. The molecular docking and dynamics simulation studies predicted the binding of Isochaihulactone and VismioneB to the active site residues of EGFR and SRC genes. In contrast, Resveratrol binds to key residues of PIK3CA. Further, the binding free energy of the docked complex was calculated by performing MM-GBSA analysis, providing a detailed understanding of the underlying molecular interactions. The results offer interactional and structural insights into candidate phytochemicals towards GBM-associated top-ranked proteins. However, validation studies must be done through both in vitro and in vivo disease models to strengthen our computational results.}
}
@article{SHEN2020101934,
title = {A welding task data model for intelligent process planning of robotic welding},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {64},
pages = {101934},
year = {2020},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2020.101934},
url = {https://www.sciencedirect.com/science/article/pii/S073658451930612X},
author = {Weidong Shen and Tianliang Hu and Chengrui Zhang and Yingxin Ye and Zhengyu Li},
keywords = {Intelligent process planning, Welding task, Robotic welding, Information integration},
abstract = {Nowadays, as an efficient and automatic welding machine that accepts and executes human instructions, welding robots are widely used in industry. However, the lack of intelligence in process planning makes welding preparation complex and time-consuming. In order to realize intelligent process planning of robotic welding, one of the key factors is designing a welding task data model that can support process planning. However, current welding task models have some drawbacks, such as inaccurate geometry information, lacking information on welding requirements, and lacking consideration of machine-readability and compatibility. They cannot provide sufficient information for intelligent process planning. In this paper, a welding task data model, which includes information on accurate geometry, dimension and welding requirement, is presented to solve these problems. Firstly, through requirement analysis, necessary information items of a welding task data model are analyzed and summarized. Then the welding task data model is designed in detail by using EXPRESS. The feasibility of proposed welding task data model is demonstrated through creating the welding task file of an automobile front door subassembly. Moreover, an application framework of the welding task file is presented. Results show that proposed welding task data model is feasible for supporting intelligent process planning and information integration of robotic welding.}
}
@article{YENG2021,
title = {Mapping the Psychosocialcultural Aspects of Healthcare Professionals’ Information Security Practices: Systematic Mapping Study},
journal = {JMIR Human Factors},
volume = {8},
number = {2},
year = {2021},
issn = {2292-9495},
doi = {https://doi.org/10.2196/17604},
url = {https://www.sciencedirect.com/science/article/pii/S2292949521000328},
author = {Prosper Kandabongee Yeng and Adam Szekeres and Bian Yang and Einar Arthur Snekkenes},
keywords = {information security, psychological, sociocultural, health care professionals},
abstract = {Background
Data breaches in health care are on the rise, emphasizing the need for a holistic approach to mitigation efforts.
Objective
The purpose of this study was to develop a comprehensive framework for modeling and analyzing health care professionals’ information security practices related to their individual characteristics, such as their psychological, social, and cultural traits.
Methods
The study area was a hospital setting under an ongoing project called the Healthcare Security Practice Analysis, Modeling, and Incentivization (HSPAMI) project. A literature review was conducted for relevant theories and information security practices. The theories and security practices were used to develop an ontology and a comprehensive framework consisting of psychological, social, cultural, and demographic variables.
Results
In the review, a number of psychological, social, and cultural theories were identified, including the health belief model, protection motivation theory, theory of planned behavior, and social control theory, in addition to some social demographic variables, to form a comprehensive set of health care professionals’ characteristics. Furthermore, an ontology was developed from these theories to systematically organize the concepts. The framework, called the psychosociocultural (PSC) framework, was then developed from the various combined psychological and sociocultural attributes of the ontology. The Human Aspect of Information Security Questionnaire was adopted as a comprehensive tool for gathering staff security practices as mediating variables in the framework.
Conclusions
Data breaches occur often in health care today. This frequency has been attributed to the lack of experience of health care professionals in information security, the lack of development of conscious care security practices, and the lack of motivation to incentivize health care professionals. The frequent data breaches in health care threaten the mutual trust between health care professionals and patients, which implicitly impacts the quality of the health care service. The modeling and analysis of health care professionals’ security practices can be conducted with the PSC framework by combining methods of statistical survey, observations, and interviews in relation to PSC variables, such as perceptions (perceived benefits, perceived threats, and perceived barriers) or psychological traits, social factors, cultural factors, and social demographics.}
}
@article{LIANG2025114647,
title = {LOXL4, CREB5 and steroid hormone biosynthesis pathways are involved in type 1 diabetes with polycystic ovary-like changes},
journal = {European Journal of Obstetrics & Gynecology and Reproductive Biology},
volume = {313},
pages = {114647},
year = {2025},
issn = {0301-2115},
doi = {https://doi.org/10.1016/j.ejogrb.2025.114647},
url = {https://www.sciencedirect.com/science/article/pii/S0301211525009236},
author = {Zefeng Liang and Qiongyin Zhang and Yuzhen Liu and Zi Liu and Yuxi Jiang and Xuesong Yang and Lina Wei and Guang Wang},
keywords = {Type 1 diabetes mellitus, Polycystic Ovary Syndrome, RNA sequencing, Hyperglycemia, Machine learning algorithms},
abstract = {Background
Type 1 Diabetes Mellitus (T1DM) is related to increased Polycystic Ovary Syndrome (PCOS) prevalence in women. However, the molecular mechanisms have not been fully elucidated.
Methods
A meta-analysis was conducted to estimate the prevalence of PCOS in women with T1DM. Then a mouse model of T1DM with polycystic ovaries was established using streptozotocin (STZ). RNA sequencing (RNA-seq) was performed on ovaries to identify differentially expressed genes (DEGs) and enriched pathways. Machine learning algorithms (LASSO, Random Forest, and SVM-RFE) were applied to pinpoint key genes linking T1DM and polycystic ovary-like changes.
Results
Meta-analysis revealed a 25 % PCOS prevalence in T1DM (95 % CI: 0.19–0.32). T1DM mice exhibited reduced ovarian weight and size, decreased serum estradiol levels, and an increased number of antral follicles compared to controls. RNA-seq analysis revealed a significant enrichment of DEGs in the steroid hormone biosynthesis pathway, involving Cyp19a1 and Cyp17a1. Machine learning analysis highlighted LOXL4 and CREB5 as potential key mediators, with receiver operating characteristic (ROC) area under the curve (AUC) values of 0.724 and 0.771, respectively.
Conclusions
This study highlights a key role for steroid hormone biosynthesis in T1DM-associated polycystic ovary-like changes through the identification of enriched DEGs. The identification of LOXL4 and CREB5 as novel mediators offers new insights into the molecular pathogenesis of T1DM-associated polycystic ovary-like changes.}
}
@article{GARGIULO2019125,
title = {Deep neural network for hierarchical extreme multi-label text classification},
journal = {Applied Soft Computing},
volume = {79},
pages = {125-138},
year = {2019},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2019.03.041},
url = {https://www.sciencedirect.com/science/article/pii/S156849461930167X},
author = {Francesco Gargiulo and Stefano Silvestri and Mario Ciampi and Giuseppe {De Pietro}},
keywords = {Extreme multi-label text classification, Semantic indexing, Deep learning, MeSH, Semi-supervised word embeddings},
abstract = {The classification of natural language texts has gained a growing importance in many real world applications due to its significant implications in relation to crucial tasks, such as Information Retrieval, Question Answering, Text Summarization, Natural Language Understanding. In this paper we present an analysis of a Deep Learning architecture devoted to text classification, considering the extreme multi-class and multi-label text classification problem, when a hierarchical label set is defined. The paper presents a methodology named Hierarchical Label Set Expansion (HLSE), used to regularize the data labels, and an analysis of the impact of different Word Embedding (WE) models that explicitly incorporate grammatical and syntactic features. We evaluate the aforementioned methodologies on the PubMed scientific articles collection, where a multi-class and multi-label text classification problem is defined with the Medical Subject Headings (MeSH) label set, a hierarchical set of 27,775 classes. The experimental assessment proves the usefulness of the proposed HLSE methodology and also provides some interesting results relating to the impact of different uses and combinations of WE models as input to the neural network in this kind of application.}
}
@article{CARDILLO2022164,
title = {Fuzzy OWL-Boost: Learning fuzzy concept inclusions via real-valued boosting},
journal = {Fuzzy Sets and Systems},
volume = {438},
pages = {164-186},
year = {2022},
note = {Fuzzy and Neurofuzzy Models},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2021.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0165011421002426},
author = {Franco Alberto Cardillo and Umberto Straccia},
keywords = {OWL 2 ontologies, Machine learning, Real-valued AdaBoost, Fuzzy logic, Concept inclusion axioms},
abstract = {OWL ontologies are nowadays a quite popular way to describe structured knowledge in terms of classes, relations among classes and class instances. In this paper, given an OWL ontology and a target class T, we address the problem of learning fuzzy concept inclusion axioms that describe sufficient conditions for being an individual instance of T (and to which degree). To do so, we present Fuzzy OWL-Boost that relies on the Real AdaBoost boosting algorithm adapted to the (fuzzy) OWL case. We illustrate its effectiveness by means of an experimentation with several ontologies.}
}
@article{SUAREZDELUCCHI2022207,
title = {Becoming a ‘good producer’ in the agri-environmental project economy},
journal = {Journal of Rural Studies},
volume = {96},
pages = {207-216},
year = {2022},
issn = {0743-0167},
doi = {https://doi.org/10.1016/j.jrurstud.2022.10.025},
url = {https://www.sciencedirect.com/science/article/pii/S0743016722002728},
author = {Adriana {Suárez Delucchi} and Erwan Sachet and Mónica Juliana Chavarro and María {Paula Escobar}},
keywords = {Campesinos, Agri-environmental projects, Institutional Ethnography, Project economy, Amazon deforestation frontier},
abstract = {Agri-environmental projects have been portrayed as tools for climate change adaptation and mitigation and to overcome processes of deforestation, soil erosion, issues of water availability, and biodiversity loss. This paper is concerned with the social organisation of knowledge around agri-environmental projects offered to farmers in the department of Caquetá in Colombia. Using Institutional Ethnography (IE), we start with the experiences and work practices of small farmers or campesinos to explore how these are coordinated with the work of other people also involved in the organisation of agri-environmental projects. We identified the ideological code of the ‘good producer’ and argue agri-environmental projects are part of the wider ‘project economy’; an institution that shapes campesinos’ practices. Our data shows that what is portrayed as solutions to achieve sustainable livestock, poverty reduction, and the halting of deforestation, end up eroding the trust and willingness to cooperate of those whose work is crucial to achieve the conservation goals these projects claim to promote. Our research contributes to the growing body of social studies about agrienvironmental systems and explains how such interventions reinforce neoliberal agendas that risk replicating modernising logics of productivity, accountability, and efficiency.}
}
@article{DIMITROVA2021,
title = {Infrastructure and Population of the OpenBiodiv Biodiversity Knowledge Graph},
journal = {Biodiversity Data Journal},
volume = {9},
year = {2021},
issn = {1314-2836},
doi = {https://doi.org/10.3897/BDJ.9.e67671},
url = {https://www.sciencedirect.com/science/article/pii/S1314283621000622},
author = {Mariya Dimitrova and Viktor E Senderov and Teodor Georgiev and Georgi Zhelezov and Lyubomir Penev},
abstract = {Background
OpenBiodiv is a biodiversity knowledge graph containing a synthetic linked open dataset, OpenBiodiv-LOD, which combines knowledge extracted from academic literature with the taxonomic backbone used by the Global Biodiversity Information Facility. The linked open data is modelled according to the OpenBiodiv-O ontology integrating semantic resource types from recognised biodiversity and publishing ontologies with OpenBiodiv-O resource types, introduced to capture the semantics of resources not modelled before.
New information
We introduce the new release of the OpenBiodiv-LOD attained through information extraction and modelling of additional biodiversity entities. It was achieved by further developments to OpenBiodiv-O, the data storage infrastructure and the workflow and accompanying R software packages used for transformation of academic literature into Resource Description Framework (RDF). We discuss how to utilise the LOD in biodiversity informatics and give examples by providing solutions to several competency questions. We investigate performance issues that arise due to the large amount of inferred statements in the graph and conclude that OWL-full inference is impractical for the project and that unnecessary inference should be avoided.}
}
@article{MOON20184791,
title = {Document Management System Using Text Mining for Information Acquisition of International Construction},
journal = {KSCE Journal of Civil Engineering},
volume = {22},
number = {12},
pages = {4791-4798},
year = {2018},
issn = {1226-7988},
doi = {https://doi.org/10.1007/s12205-018-1528-y},
url = {https://www.sciencedirect.com/science/article/pii/S1226798824019755},
author = {Seonghyeon Moon and Yoonjung Shin and Bon-Gang Hwang and Seokho Chi},
keywords = {international construction, information acquisition, document management, text mining, web crawling, natural language processing},
abstract = {Acquiring timely and proper information of host countries is a crucial element to lead successful and lucrative delivery of international construction projects. This information, however, commonly exists in forms of unstructured text data such as news articles and reports, which calls for the need of text mining. The aim of this research is to develop a prototype of construction document management system for global contract, which provides the user-needed information in a timely manner. The system named UNI (User Needed Information)-Tacit collects text data containing the recent information of the global construction market by using the web crawling algorithm, automatically allocates tags for each document with the most representative keywords based on Natural Language Processing, and eventually visualizes the results in the form of word clouds. The developed system, the survey validated its usefulness, is expected to contribute to collecting and organizing the latest issues on the global construction market, which provides better understanding of the target countries for decision makers.}
}
@article{GONG2025112754,
title = {Multi-domain dialogue state tracking via dual dynamic graph with hierarchical slot selector},
journal = {Knowledge-Based Systems},
volume = {308},
pages = {112754},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112754},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124013881},
author = {Yeseul Gong and Heeseon Kim and Seokju Hwang and Donghyun Kim and Kyong-Ho Lee},
keywords = {Dialogue state tracking (DST), Hierarchical slot selection, Multi-domain, Dual dynamic graph, Co-reference},
abstract = {Dialogue state tracking aims to maintain user intent as a consistent state across multi-domains to accomplish natural dialogue systems. However, previous researches often fall short in capturing the difference of multiple slot types and fail to adequately consider the selection of discerning information. The increase in unnecessary information correlates with a decrease in predictive performance. Therefore, the careful selection of high-quality information is imperative. Moreover, considering that the types of essential and available information vary for each slot, the process of selecting appropriate information may also differ. To address these issues, we propose HS2DG-DST, a Hierarchical Slot Selector and Dual Dynamic Graph-based DST. Our model is designed to provide maximum information for optimal value prediction by clearly exploiting the need for differentiated information for each slot. First, we hierarchically classify slot types based on the multiple properties. Then, two dynamic graphs provide highly relevant information to each slot. Experimental results on MultiWOZ datasets demonstrate that our model outperforms state-of-the-art models.}
}
@article{LIBORIUSSEN2021516,
title = {A learning method of trust building: beyond the performance management of artistic events},
journal = {Qualitative Research in Accounting & Management},
volume = {18},
number = {45},
pages = {516-544},
year = {2021},
issn = {1176-6093},
doi = {https://doi.org/10.1108/QRAM-09-2019-0093},
url = {https://www.sciencedirect.com/science/article/pii/S1176609321000036},
author = {Jakob Mathias Liboriussen and Hanne Nørreklit and Mihaela Trenca},
keywords = {Performance management, Creativity, Pragmatic constructivism, Learning method of trust building},
abstract = {Purpose
This paper aims to address a dilemma raised in the accounting literature on how managers of creative practices can produce and use accounting measurements that support employees’ self-determination to create whilst also building trust in them to work for the interests of the organisation.
Design/methodology/approach
Using pragmatic constructivism as a paradigmatic setting, the paper develops a learning method of trust building as a way for organisations to produce and use accounting measurements. Empirical analysis of the European Capital of Culture Aarhus 2017 demonstrates the method in action.
Findings
The study displays a learning method of trust building as an effective way for organisations to account for their creative practices without intruding on the creative process of the people involved. The method involves proactive judgement and pragmatic observation of the trustworthiness of the actors’ language games, construction of quality in the conceptual structures of management narratives and measurement models, and learning that narrows the gap between the actors’ proactive judgement and the pragmatic observation of trustworthiness. Through such processes, including principles of truth, dialogical interactions, ongoing reflections and co-authorship, trust can be built in self-determining, creative actors to drive intentional results.
Research limitations/implications
The learning method of trust building extends the literature on trust building and on knowledge processes of performance measurement of actors in creative practices.
Originality/value
This is the first attempt in the accounting literature to develop a learning method of trust building.}
}
@article{SUFI20223631,
title = {AI-based Automated Extraction of Location-Oriented COVID-19 Sentiments},
journal = {Computers, Materials and Continua},
volume = {72},
number = {2},
pages = {3631-3649},
year = {2022},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2022.026272},
url = {https://www.sciencedirect.com/science/article/pii/S1546221822014941},
author = {Fahim K. Sufi and Musleh Alsulami},
keywords = {Entity recognition, AI-based social media monitoring, sentiment analysis, decision support system, COVID-19},
abstract = {The coronavirus disease (COVID-19) pandemic has affected the lives of social media users in an unprecedented manner. They are constantly posting their satisfaction or dissatisfaction over the COVID-19 situation at their location of interest. Therefore, understanding location-oriented sentiments about this situation is of prime importance for political leaders, and strategic decision-makers. To this end, we present a new fully automated algorithm based on artificial intelligence (AI), for extraction of location-oriented public sentiments on the COVID-19 situation. We designed the proposed system to obtain exhaustive knowledge and insights on social media feeds related to COVID-19 in 110 languages through AI-based translation, sentiment analysis, location entity detection, and decomposition tree analysis. We deployed fully automated algorithm on live Twitter feed from July 15, 2021 and it is still running as of 12 January, 2022. The system was evaluated on a limited dataset between July 15, 2021 to August 10, 2021. During this evaluation timeframe 150,000 tweets were analyzed and our algorithm found that 9,900 tweets contained one or more location entities. In total, 13,220 location entities were detected during the evaluation period, and the rates of average precision and recall rate were 0.901 and 0.967, respectively. As of 12 January, 2022, the proposed solution has detected 43,169 locations using entity recognition. According to the best of our knowledge, this study is the first to report location intelligence with entity detection, sentiment analysis, and decomposition tree analysis on social media messages related to COVID-19 and has covered the largest set of languages.}
}
@article{KUMARA2021106593,
title = {The do’s and don’ts of infrastructure code: A systematic gray literature review},
journal = {Information and Software Technology},
volume = {137},
pages = {106593},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106593},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921000720},
author = {Indika Kumara and Martín Garriga and Angel Urbano Romeu and Dario {Di Nucci} and Fabio Palomba and Damian Andrew Tamburri and Willem-Jan {van den Heuvel}},
keywords = {Infrastructure-as-code, DevOps, Gray literature review},
abstract = {Context:
Infrastructure-as-code (IaC) is the DevOps tactic of managing and provisioning software infrastructures through machine-readable definition files, rather than manual hardware configuration or interactive configuration tools.
Objective:
From a maintenance and evolution perspective, the topic has picked the interest of practitioners and academics alike, given the relative scarcity of supporting patterns and practices in the academic literature. At the same time, a considerable amount of gray literature exists on IaC. Thus we aim to characterize IaC and compile a catalog of best and bad practices for widely used IaC languages, all using gray literature materials.
Method:
In this paper, we systematically analyze the industrial gray literature on IaC, such as blog posts, tutorials, white papers using qualitative analysis techniques.
Results:
We proposed a definition for IaC and distilled a broad catalog summarized in a taxonomy consisting of 10 and 4 primary categories for best practices and bad practices, respectively, both language-agnostic and language-specific ones, for three IaC languages, namely Ansible, Puppet, and Chef. The practices reflect implementation issues, design issues, and the violation of/adherence to the essential principles of IaC.
Conclusion:
Our findings reveal critical insights concerning the top languages as well as the best practices adopted by practitioners to address (some of) those challenges. We evidence that the field of development and maintenance IaC is in its infancy and deserves further attention.}
}
@article{ANDERIES2022109769,
title = {A framework for conceptualizing and modeling social-ecological systems for conservation research},
journal = {Biological Conservation},
volume = {275},
pages = {109769},
year = {2022},
issn = {0006-3207},
doi = {https://doi.org/10.1016/j.biocon.2022.109769},
url = {https://www.sciencedirect.com/science/article/pii/S0006320722003226},
author = {John M. Anderies and Graeme S. Cumming and Hayley S. Clements and Steven J. Lade and Ralf Seppelt and Sivee Chawla and Birgit Müller},
keywords = {Dynamic modeling, Social ecological systems, Coupled infrastructure systems, Conservation policy, Governance},
abstract = {As conservation biology has matured, its scope has expanded from a primarily ecological focus to recognition that nearly all conservation problems involve people. At the same time, conservation actions have been increasingly informed by ever more sophisticated quantitative models. These models have focused primarily on ecological and geographic elements of conservation problems, such as mark-recapture methods, predicting species occurrences, and optimizing the placement of protected areas. There are many off-the-shelf ecological models for conservation managers to draw upon, but very few that describe human-nature interactions in a generalizable manner. We address this gap by proposing a minimalistic modeling framework for human-nature interactions, combining well-established ideas in economics and social sciences (grounded in Ostrom's social-ecological systems framework) and accepted ecological models. Our approach begins with a systems breakdown consisting of an ecosystem, resource users, public infrastructure, and infrastructure providers; and interactions between these system elements, which bring together the biophysical context, the relevant attributes of the human society, and the rules (institutions, such as protected areas) currently in use. We briefly review the different disciplinary building blocks that the framework could incorporate and then illustrate our approach with two examples: a detailed analysis of the social-ecological dynamics involved in managing South African protected areas and a more theoretical analysis of a general system. We conclude with further discussion of the urgent need in conservation biology for models that are genuinely designed to capture the complexities of human socioeconomic behavior, rather than the more typical approach of trying to adapt an ecological model or a stochastic process to simulate human agency and decision-making. Our framework offers a relatively simple but highly versatile way of specifying social-ecological models that will help conservation biologists better represent critical linkages between social and ecological processes when modeling social-ecological dynamics.}
}
@article{SCHREYER2024100350,
title = {Treatment resistance in schizophrenia and depression as an interactive kind: Mapping the development of a classification through Meta-Narrative review},
journal = {SSM - Mental Health},
volume = {6},
pages = {100350},
year = {2024},
issn = {2666-5603},
doi = {https://doi.org/10.1016/j.ssmmh.2024.100350},
url = {https://www.sciencedirect.com/science/article/pii/S2666560324000550},
author = {Leighton Schreyer and Csilla Kalocsai and Oshan Fernando and Melanie Anderson and Vanessa Lockwood and Sophie Soklaridis and Gary Remington and Araba Chintoh and Suze Berkhout},
abstract = {Despite ongoing attempts to delineate and name treatment resistance (TR) in psychiatry, the term is increasingly deployed across diagnostic categories. Still, what it is that constitutes TR remains unclear and in flux. Through a meta-narrative review, we construct a sociohistorical map of the concept of TR as it is employed in schizophrenia (TRS) and major depressive disorders (TRD). We track debates about TR, identify underlying assumptions and influencing factors that shape how the concept has evolved over time, and consider the intended and unintended consequences of its conceptualization. We develop our findings as three unique threads that, braided together, offer insight into TR as an interactive kind. Each thread analyzes and plays with the notion of heterogeneity, which arises in the literature as both a theme and a problem to be solved. Thread one looks at prevailing controversies surrounding the definition of TR. Here, heterogeneity arises in relation to how TR is delineated. We also consider the notion of “pseudoresistance,” a novel concept that functions to manage and contain heterogeneity, defining the boundaries of TR through its exclusions. Thread two explores the range of actors whose interests and practices are coordinated to shape TR as a concept: the pharmaceutical industry, academic psychiatry, clinicians, and health systems. Each group has its own interests and orientations: a heterogenous range of actors contributing to the thing that TR is. Thread three examines the intended and unintended consequences that attempts to conceptualize TR have yielded, including a reification of the biomedical paradigm and the personification of TR. This paper offers a systematic approach to thinking about similarities, differences, particularities and tensions embedded within TR to understand the politics and possibilities of the concept.}
}
@article{HOGENBOOM2021115568,
title = {The impact of word sense disambiguation on stock price prediction},
journal = {Expert Systems with Applications},
volume = {184},
pages = {115568},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115568},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421009738},
author = {Alexander Hogenboom and Alex Brojba-Micu and Flavius Frasincar},
keywords = {Stock price prediction, Event detection, Word sense disambiguation, Natural language processing},
abstract = {State-of-the-art decision support systems for stock price prediction incorporate pattern-based event detection in text into their predictions. These systems typically fail to account for word meaning, even though word sense disambiguation is crucial for text understanding. Therefore, we propose an advanced natural language processing pipeline for event-based stock price prediction, that allows for word sense disambiguation to be incorporated in the event detection process. We identify events in natural language news messages and subsequently weight these events for their historical impact on stock prices. We assess the merit of word sense disambiguation in event-based stock price prediction in two evaluation scenarios for NASDAQ-100 companies, based on historical stock prices and news articles retrieved from Dow Jones Newswires over a 2-year period. We evaluate the precision of generated buy and sell signals based on our predicted stock price movements, as well as the excess returns generated by a trading strategy that acts upon these signals. Event-based stock price predictions seem most reliable about 2 days into the future. The number of detected events tends to reduce with over 30% when graph-based word sense disambiguation using a degree centrality measure is applied in the event detection process, thus reducing the noise introduced into the stock price movement predictions by high-impact ambiguous events. As a result, modest improvements in the precision of buy and sell signals generated based on these predictions tend to lead to vast improvements of on average about 70% in the associated excess returns.}
}
@article{REHM2022100391,
title = {Researching digitalized work arrangements: A Laws of Form perspective},
journal = {Information and Organization},
volume = {32},
number = {2},
pages = {100391},
year = {2022},
issn = {1471-7727},
doi = {https://doi.org/10.1016/j.infoandorg.2022.100391},
url = {https://www.sciencedirect.com/science/article/pii/S1471772722000045},
author = {Sven-V. Rehm and Lakshmi Goel and Iris Junglas},
keywords = {Qualitative research, Interpretive research, Qualitative method, Ontology, Laws of Form, Digital, Agency, Identity, Artifact, Entanglement, Sociomaterial},
abstract = {Advances in digitalization have changed our apprehension of technology from discrete devices and application software as bounded artifacts, to dynamically evolving social-material entanglements in Digitalized Work Arrangements (DWA). This development makes studying DWAs increasingly difficult and challenges us to advance our methods that define how we can study, observe, and conceptualize DWAs. In this essay, we draw on the mathematical-logical formalism of the Laws of Form (LoF) (Spencer-Brown, 1969) to analyze how six illustrative IS studies conceptualize the social and the material to arrive at distinct perspectives on DWAs. Our analysis reveals three archetypes that capture these studies' conceptualizations and that inform a discussion of how IS research can extend qualitative methods beyond those six works. We offer two contributions. First, we provide novel insights and explanations to key conceptualizations in the study of DWAs. Second, we present the LoF as a pre-ontological and pre-theoretical formalism that enables commensurability of methods and development of novel qualitative empirical methods. Specifically, we demonstrate how the formalism helps articulating the distinctions we draw to refine our object of study and to critically examine and reconstruct other researchers' reasoning.}
}
@article{CARDINALE202264,
title = {Semantic framework of event detection in emergency situations for smart buildings},
journal = {Digital Communications and Networks},
volume = {8},
number = {1},
pages = {64-79},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2021.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S235286482100033X},
author = {Yudith Cardinale and Gabriel Freites and Edgar Valderrama and Ana Aguilera and Chinnapong Angsuchotmetee},
keywords = {Multimedia sensor network, Semantic web, Event processing, Ontology, Geolocalisation},
abstract = {Multimedia Sensor Networks (MSNs) have enhanced the ability to analyze the environment and provide responses based on its current status. Generally, MSNs are composed of scalar and multimedia sensors that have fixed locations. However, given the advancement of smart mobile device technologies, it is currently possible to dynamically integrate mobile sensors into MSNs. In this paper, we propose a formal platform to manage MSNs and the data gathered from them to detect complex events. Our main contributions include: M2SSN ​− ​Onto, a Mobile and Multimedia Semantic Sensor Networks Ontology; Py-CEMiD, an engine for detecting complex events and generate reactions to them; a mobile device location engine to locate mobile sensors; and a proof-of-concept in the context of detecting emergency situations in smart buildings. Several scenarios are validated for emergency events, combining simulated sensor measurements with real measurements of mobile devices. Results show complex events can be detected in near real time (less than 1 ​s).}
}
@article{STEURER20181067,
title = {SysML-based Profile for Dependable UAV Design},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {24},
pages = {1067-1074},
year = {2018},
note = {10th IFAC Symposium on Fault Detection, Supervision and Safety for Technical Processes SAFEPROCESS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.09.722},
url = {https://www.sciencedirect.com/science/article/pii/S240589631832442X},
author = {Mikael Steurer and Andrey Morozov and Klaus Janschek and Klaus-Peter Neitzke},
keywords = {Modeling, Systems Engineering, Robotics, Fault Tolerance, MBSE, SysML, UML, Dependability, Profile, Error Propagation, Stochastic Models},
abstract = {Model-based systems engineering approaches are commonly used for the development of various heterogeneous mechatronic systems including nowadays popular Unmanned Aerial Vehicles (UAV). As a rule, the dependability analysis is carried out during the system integration phase of the UAV development. Therefore, fundamental design drawbacks might be uncovered too late leading to serious and costly rework. This paper presents a new methodology for the early dependability analysis of a UAV system applicable during the design phase. The proposed dependability analysis is based on the Dual-graph Error Propagation Model (DEPM), a stochastic model of system design aspects that influence error propagation processes: control flow, data flow, and component-level reliability properties. A new domain-specific Systems Modeling Language (SysML) profile and the transformation algorithm from the SysML model to the DEPM are introduced as two key parts of this methodology as well as the main research results of this paper. The relevant technical details of the modeling workflow are demonstrated with a case study UAV that explains how to design a UAV using the introduced SysML profile, transform the SysML model into the DEPM, and evaluate dependability properties.}
}
@article{MITTAL2023400,
title = {Advancing chemical carcinogenicity prediction modeling: opportunities and challenges},
journal = {Trends in Pharmacological Sciences},
volume = {44},
number = {7},
pages = {400-410},
year = {2023},
issn = {0165-6147},
doi = {https://doi.org/10.1016/j.tips.2023.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0165614723000858},
author = {Aayushi Mittal and Gaurav Ahuja},
keywords = {bioactivity, carcinogenicity, mechanistic artificial intelligence, descriptors, knowledge graphs},
abstract = {Carcinogenicity assessment of any compound is a laborious and expensive exercise with several associated ethical and practical concerns. While artificial intelligence (AI) offers promising solutions, unfortunately, it is contingent on several challenges concerning the inadequacy of available experimentally validated (non)carcinogen datasets and variabilities within bioassays, which contribute to the compromised model training. Existing AI solutions that leverage classical chemistry-driven descriptors do not provide adequate biological interpretability involved in imparting carcinogenicity. This highlights the urgency to devise alternative AI strategies. We propose multiple strategies, including implementing data-driven (integrated databases) and known carcinogen-characteristic-derived features to overcome these apparent shortcomings. In summary, these next-generation approaches will continue facilitating robust chemical carcinogenicity prediction, concomitant with deeper mechanistic insights.}
}
@article{BARRA2021325,
title = {Visual question answering: Which investigated applications?},
journal = {Pattern Recognition Letters},
volume = {151},
pages = {325-331},
year = {2021},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2021.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167865521003147},
author = {Silvio Barra and Carmen Bisogni and Maria {De Marsico} and Stefano Ricciardi},
keywords = {Visual question answering, Real-world VQA, VQA for medical applicatons, VQA for assistive applications, VQA for context awareness, VQA in cultural heritage and education},
abstract = {Visual Question Answering (VQA) is an extremely stimulating and challenging research area where Computer Vision (CV) and Natural Language Processig (NLP) have recently met. In image captioning and video summarization, the semantic information is completely contained in still images or video dynamics, and it has only to be mined and expressed in a human-consistent way. Differently from this, in VQA semantic information in the same media must be compared with the semantics implied by a question expressed in natural language, doubling the artificial intelligence-related effort. Some recent surveys about VQA approaches have focused on methods underlying either the image-related processing or the verbal-related one, or on the way to consistently fuse the conveyed information. Possible applications are only suggested, and, in fact, most cited works rely on general-purpose datasets that are used to assess the building blocks of a VQA system. This paper rather considers the proposals that focus on real-world applications, possibly using as benchmarks suitable data bound to the application domain. The paper also reports about some recent challenges in VQA research.}
}
@article{SCHNEIDER2019189,
title = {Knowledge-based Conversion of Finite State Machines in Manufacturing Automation},
journal = {Procedia Manufacturing},
volume = {28},
pages = {189-194},
year = {2019},
note = {7th International conference on Changeable, Agile, Reconfigurable and Virtual Production (CARV2018)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.12.031},
url = {https://www.sciencedirect.com/science/article/pii/S235197891831374X},
author = {Georg Ferdinand Schneider and Georg Ambrosius Peßler and Walter Terkaj},
keywords = {Automation, Control, Finite State Machines, Ontology, Knowledge-based Method},
abstract = {More and more information and communication technologies originating from the web are introduced in industrial automation systems. The vision for future automation systems includes intelligent self-descriptive components, which exchange information and potentially reason by themselves through knowledge-assisted methods. Formal domain descriptions are required to enable this vision, including knowledge related to mechanical, electrical and control domains. This work focuses on formalizing knowledge of the automation and control domain and investigates how knowledge-based methods can support the automated conversion between different formalisms for modelling discrete behaviour in manufacturing automation: finite state machines. We detail our approach by deploying the presented method in a use case related to the automation of a pick and place unit available from the literature.}
}
@article{KULUS202076,
title = {The processes of cellular growth, aging, and programmed cell death are involved in lifespan of ovarian granulosa cells during short-term IVC – Study based on animal model},
journal = {Theriogenology},
volume = {148},
pages = {76-88},
year = {2020},
issn = {0093-691X},
doi = {https://doi.org/10.1016/j.theriogenology.2020.02.044},
url = {https://www.sciencedirect.com/science/article/pii/S0093691X20301564},
author = {Magdalena Kulus and Wiesława Kranc and Patrycja Sujka-Kordowska and Paul Mozdziak and Maurycy Jankowski and Aneta Konwerska and Jakub Kulus and Dorota Bukowska and Mariusz Skowroński and Hanna Piotrowska-Kempisty and Michał Nowicki and Bartosz Kempisty and Paweł Antosik},
keywords = {Pig, Ovarian follicle, Granulosa cells, Primary culture, Microarray},
abstract = {The oogenesis and folliculogenesis are closely linked and occur simultaneously in the growing ovarian follicles. Biochemical and morphological changes in oocytes (OC) and surrounding granulosa cells (GCs) are highly complex and depend on many factors, including intercellular communication. GCs are cells with many functions, often crucial for the proper viability of the oocyte and subsequent positive fertilization. The purpose of this study was to analyze gene expression in porcine GCs, to define differentially expressed genes belongs to the “cell growth”, “aging”, “positive regulation of cell death”, “apoptotic process”, “regulation of cell death”, "cell death" and "negative regulation of cell death" ontology groups during the short – term primary in vitro culture. Microarrays were employed to study the transcriptome contained in the total RNA of the cultured GCs. Ovaries were obtained after slaughter, from 40 gilts of swine aged 170 days. The cells were obtained through puncture of the ovaries, collection of follicular fluid, removal of the cumulus - oocyte complexes and centrifugation. The cells were then cultured in vitro. The RNA material was obtained before the culture was established (0h) and then after 48h, 96h and 144h of its course. From 182 differently expressed genes belonging to the these ontology groups, we have selected POSTN, FN1, FMOD, ITGB3, DCN, SERPINB2, SFRP2, IGFBP5, EMP1, and CCL2 which were upregulated, as well as DAPL1, ESR1, IHH, TGFBR3, PPARD, PDK4, TXNIP, IFIT3, CSRNP3, and TNFSF10 genes whose expression was downregulated during the time of in vitro culture of the GCs. The significance of the differential gene expression is to provide new information on the molecular aspects of in vitro granulosa cell culture.}
}
@article{XU2019245,
title = {A prediction method of building seismic loss based on BIM and FEMA P-58},
journal = {Automation in Construction},
volume = {102},
pages = {245-257},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S092658051830760X},
author = {Zhen Xu and Huazhen Zhang and Xinzheng Lu and Yongjia Xu and Zongcai Zhang and Yi Li},
keywords = {Seismic loss, BIM, FEMA P-58, Component level, Ontology},
abstract = {Predicting the seismic loss of a building is critical for its resilience. A prediction method for building seismic loss based on the building information model (BIM) and FEMA P-58 is proposed in this study. First, a component-level damage prediction algorithm is designed to establish the mapping from BIM components to the performance groups (PGs) in FEMA P-58, and to predict the component damage using the BIM-based time-history analysis (THA) and the fragility curves of PGs. Subsequently, an ontology-based model considering the deduction rules in the local unit-repair-cost database is created for obtaining exact measurement data of components in a BIM. Meanwhile, a component-level loss prediction algorithm is developed using the measurement data and the unit repair costs corresponding to damage states, by which the predicted seismic losses can agree with the actual situation of the specific region. Finally, a component-level visualization algorithm is designed to display the seismic damage and loss in a virtual reality (VR) environment. A six-story office building in Beijing is used as a pilot test to demonstrate the advantages of the proposed method. The outcome of this study produces a component-level and visual loss prediction result that agrees with the actual situation of the specific region, which can be used to evaluate the post-earthquake economic resilience of different buildings.}
}
@article{ZHAO2021779,
title = {Enabling situational awareness of business processes},
journal = {Business Process Management Journal},
volume = {27},
number = {3},
pages = {779-795},
year = {2021},
issn = {1463-7154},
doi = {https://doi.org/10.1108/BPMJ-07-2020-0331},
url = {https://www.sciencedirect.com/science/article/pii/S1463715421000273},
author = {Xiaohui Zhao and Sira Yongchareon and Nam-Wook Cho},
keywords = {Business process modelling, Context awareness, Event processing},
abstract = {Purpose
The purpose of this research is to explore the ways of integrating situational awareness into business process management for the purpose of realising hyper automated business processes. Such business processes will help improve their customer experiences, enhance the reliability of service delivery and lower the operational cost for a more competitive and sustainable business.
Design/methodology/approach
Ontology has been deployed to establish the context modelling method, and the event handling mechanisms are developed on the basis of event calculus. An approach on performance of the proposed approach has been evaluation by checking the cost savings from the simulation of a large number of business processes.
Findings
In this research, the authors have formalised the context presentation for a business process with a focus on rules and entities to support context perception; proposed a system architecture to illustrate the structure and constitution of a supporting system for intelligent and situation aware business process management; developed real-time event elicitation and interpretation mechanisms to operationalise the perception of contextual dynamics and real-time responses; and evaluated the applicability of the proposed approaches and the performance improvement to business processes.
Originality/value
This paper presents a framework covering process context modelling, system architecture and real-time event handling mechanisms to support situational awareness of business processes. The reported research is based on our previous work on radio frequency identification-enabled applications and context-aware business process management with substantial extension to process context modelling and process simulation.}
}
@article{BAO2022107864,
title = {A node2vec-based graph embedding approach for unified assembly process information modeling and workstep execution time prediction},
journal = {Computers & Industrial Engineering},
volume = {163},
pages = {107864},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107864},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221007683},
author = {Qiangwei Bao and Gang Zhao and Yong Yu and Pai Zheng},
keywords = {Assembly process modeling, Assembly constraint, Parameterized representation, Node2vec, Workstep prediction},
abstract = {The trend of customized production results in the demand for higher level of automation, in which artificial intelligence decision-making dominates. As the core of smart manufacturing systems, intelligent services stand a significant role in the analysis, prediction, and adjustment of production process, which is inseparable from the effective semantic modeling of the procedures and elements involved. However, there is an absence of unified modeling of assembly process, including both geometric and non-geometric information, leading to the incomprehensiveness when providing data support for intelligent services. To fill this gap, a generic node2vec-based parameterized representation of geometric elements and assembly constraints approach is proposed. Firstly, the information structure of assembly process is established, in which the geometric elements and topological relationships of the product are abstracted into a network. Secondly, node2vec is adopted for the graph embedding to generate preset dimension vectors corresponding to the nodes in the geometric network. As the edges in the network, the vectors corresponding to the assembly constraints, which are regarded as the parameterized representations, can be obtained through node vector calculation. Moreover, an assembly workstep execution time prediction method based on historical data is introduced with the parameterized representations of assembly constraints as the carriers of geometric topological information. At last, an industrial case study is illustrated to show the entire process of constraint parameterized representations and workstep execution time prediction, indicating the feasibility and availability of the method proposed.}
}
@article{SCHRODER20208276,
title = {Formal Definition of the Term “Semantics” as a Foundation for Semantic Interoperability in the Industrial Internet of Things},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {8276-8282},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.1957},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320325854},
author = {Tizian Schröder and Christian Diedrich},
keywords = {Semantics, Semantic Interoperability, Network of Interacting Systems, Industrial Internet of Things (IIoT), Knowledge Pyramid, Levels of Conceptual Interoperability Model (LCIM), Ontologies},
abstract = {Semantic interoperability is seen as the key to realize the ideas of the Industrial Internet of Things (IIoT). In order to equip technical systems with such a capability, a precise definition of the term “semantics” is needed. Complex IIoT devices can only be developed properly on a formal foundation. Existing approaches that intend to specify the term “semantics” are often more intuitively motivated. These include, for example, the knowledge pyramid or the Levels of Conceptual Interoperability Model (LCIM). The paper provides a formal definition of the term “semantics” and relates these existing approaches critically to the proposed definition.}
}
@article{ANTONIOMOURINOGARCIA201837,
title = {Leveraging Wikipedia knowledge to classify multilingual biomedical documents},
journal = {Artificial Intelligence in Medicine},
volume = {88},
pages = {37-57},
year = {2018},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2018.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0933365717304153},
author = {Marcos {Antonio Mouriño García} and Roberto {Pérez Rodríguez} and Luis {Anido Rifón}},
keywords = {Biomedical document classification, Hybrid word-concept document representation, Multilingual text classification, Wikipedia-based bag of concepts document representation, Wikipedia Miner semantic annotator},
abstract = {This article presents a classifier that leverages Wikipedia knowledge to represent documents as vectors of concepts weights, and analyses its suitability for classifying biomedical documents written in any language when it is trained only with English documents. We propose the cross-language concept matching technique, which relies on Wikipedia interlanguage links to convert concept vectors between languages. The performance of the classifier is compared to a classifier based on machine translation, and two classifiers based on MetaMap. To perform the experiments, we created two multilingual corpus. The first one, Multi-Lingual UVigoMED (ML-UVigoMED) is composed of 23,647 Wikipedia documents about biomedical topics written in English, German, French, Spanish, Italian, Galician, Romanian, and Icelandic. The second one, English-French-Spanish-German UVigoMED (EFSG-UVigoMED) is composed of 19,210 biomedical abstract extracted from MEDLINE written in English, French, Spanish, and German. The performance of the approach proposed is superior to any of the state-of-the art classifier in the benchmark. We conclude that leveraging Wikipedia knowledge is of great advantage in tasks of multilingual classification of biomedical documents.}
}
@article{DAO2024100367,
title = {Interlinking BIM and GIS data for a semantic pedestrian network and applications in high-density cities},
journal = {Developments in the Built Environment},
volume = {17},
pages = {100367},
year = {2024},
issn = {2666-1659},
doi = {https://doi.org/10.1016/j.dibe.2024.100367},
url = {https://www.sciencedirect.com/science/article/pii/S2666165924000486},
author = {Jicao Dao and S. Thomas Ng and Chung Yee Kwok},
keywords = {BIM, GIS, Linked data, Data integration, Pedestrian network},
abstract = {In high-density cities, pedestrians frequently traverse many publicly accessible indoor spaces, such as metro stations and footbridges, which seamlessly connect with outdoor sidewalks, forming indoor-outdoor combined pedestrian networks. However, the information island phenomenon hinders these connections in the digital world because outdoor sidewalks and indoor spaces are modelled using Geographical Information Systems (GIS) and Building Information Modelling (BIM) technologies, respectively, which challenges obtaining integrated information for intelligent pedestrian services. This study presents an approach for interlinking BIM and GIS data for a semantic pedestrian network using semantic web technologies. The proposed approach automatically converts BIM and GIS data into linked data and establishes interlinkages between BIM and GIS data through semantic queries and inferences. The resulting semantic pedestrian network based on integrated linked data graphs forms a knowledge base, including topological and geometrical information and abundant semantic information from BIM and GIS datasets. The application potential of the semantic pedestrian network is demonstrated through information query and semantic route planning. This research contributes to establishing indoor-outdoor combined semantic pedestrian networks ensuring seamless data integration.}
}
@article{DUI2024110402,
title = {Reliability model and emergency maintenance strategies for smart home systems},
journal = {Reliability Engineering & System Safety},
volume = {251},
pages = {110402},
year = {2024},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2024.110402},
url = {https://www.sciencedirect.com/science/article/pii/S0951832024004745},
author = {Hongyan Dui and Xinyue Wang and Xinghui Dong and Tianmeng Zhu and Yunkai Zhai},
keywords = {Maintenance strategies, Reliability, Smart home systems, Recovery prioritization},
abstract = {After the smart home system is impacted by malicious attacks or other factors, the system reliability drops drastically, so the user's living environment is greatly disturbed. To determine the emergency maintenance strategy, firstly, we study the failure mechanism of the smart home system by the fault tree theory, and distinguish the components into two types: critical components and non-critical components. Secondly, a recovery prioritization is proposed to measure the extent to which changes in the reliability of each component affect the reliability of the system. Then, three scenarios of smart home system failure are proposed. Under the constraints of time and cost, the maintenance strategies for the three scenarios are determined to achieve emergency recovery. Finally, through the multi-objective particle swarm optimization algorithm, the system reliability is maximized, the maintenance cost is minimized, and the optimal reliability recovery level of each component is obtained, so that the reliability of the smart home system is further improved. The effectiveness and practicality of the emergency maintenance strategies proposed in this paper are demonstrated by a smart home system example.}
}
@article{VOORHEIS2023,
title = {Making Sense of Theories, Models, and Frameworks in Digital Health Behavior Change Design: Qualitative Descriptive Study},
journal = {Journal of Medical Internet Research},
volume = {25},
year = {2023},
issn = {1438-8871},
doi = {https://doi.org/10.2196/45095},
url = {https://www.sciencedirect.com/science/article/pii/S1438887123001723},
author = {Paula Voorheis and Aunima R Bhuiya and Kerry Kuluski and Quynh Pham and Jeremy Petch},
keywords = {behavioral science, behavior change, health behavior, digital health, mobile health, theories, models, frameworks},
abstract = {Background
Digital health interventions are increasingly being designed to support health behaviors. Although digital health interventions informed by behavioral science theories, models, and frameworks (TMFs) are more likely to be effective than those designed without them, design teams often struggle to use these evidence-informed tools. Until now, little work has been done to clarify the ways in which behavioral science TMFs can add value to digital health design.
Objective
The aim of this study was to better understand how digital health design leaders select and use TMFs in design practice. The questions that were addressed included how do design leaders perceive the value of TMFs in digital health design, what considerations do design leaders make when selecting and applying TMFs, and what do design leaders think is needed in the future to advance the utility of TMFs in digital health design?
Methods
This study used a qualitative description design to understand the experiences and perspectives of digital health design leaders. The participants were identified through purposive and snowball sampling. Semistructured interviews were conducted via Zoom software. Interviews were audio-recorded and transcribed using Otter.ai software. Furthermore, 3 researchers coded a sample of interview transcripts and confirmed the coding strategy. One researcher completed the qualitative analysis using a codebook thematic analysis approach.
Results
Design leaders had mixed opinions on the value of behavioral science TMFs in digital health design. Leaders suggested that TMFs added the most value when viewed as a starting point rather than the final destination for evidence-informed design. Specifically, these tools added value when they acted as a gateway drug to behavioral science, supported health behavior conceptualization, were balanced with expert knowledge and user-centered design principles, were complementary to existing design methods, and supported both individual- and systems-level thinking. Design leaders also felt that there was a considerable nuance in selecting the most value-adding TMFs. Considerations should be made regarding their source, appropriateness, complexity, accessibility, adaptability, evidence base, purpose, influence, audience, fit with team expertise, fit with team culture, and fit with external pressures. Design leaders suggested multiple opportunities to advance the use of TMFs. These included improving TMF reporting, design, and accessibility, as well as improving design teams' capacity to use TMFs appropriately in practice.
Conclusions
When designing a digital health behavior change intervention, using TMFs can help design teams to systematically integrate behavioral insights. The future of digital health behavior change design demands an easier way for designers to integrate evidence-based TMFs into practice.}
}
@article{LU2025107098,
title = {Identity Model Transformation for boosting performance and efficiency in object detection network},
journal = {Neural Networks},
volume = {184},
pages = {107098},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.107098},
url = {https://www.sciencedirect.com/science/article/pii/S089360802401027X},
author = {Zhongyuan Lu and Jin Liu and Miaozhong Xu},
keywords = {Object detection, Model transfer, Deep learning, Identity transformation},
abstract = {Modifying the structure of an existing network is a common method to further improve the performance of the network. However, modifying some layers in network often results in pre-trained weight mismatch, and fine-tune process is time-consuming and resource-inefficient. To address this issue, we propose a novel technique called Identity Model Transformation (IMT), which keep the output before and after transformation in an equal form by rigorous algebraic transformations. This approach ensures the preservation of the original model’s performance when modifying layers. Additionally, IMT significantly reduces the total training time required to achieve optimal results while further enhancing network performance. IMT has established a bridge for rapid transformation between model architectures, enabling a model to quickly perform analytic continuation and derive a family of tree-like models with better performance. This model family possesses a greater potential for optimization improvements compared to a single model. Extensive experiments across various object detection tasks validated the effectiveness and efficiency of our proposed IMT solution, which saved 94.76% time in fine-tuning the basic model YOLOv4-Rot on DOTA 1.5 dataset, and by using the IMT method, we saw stable performance improvements of 9.89%, 6.94%, 2.36%, and 4.86% on the four datasets: AI-TOD, DOTA1.5, coco2017, and MRSAText, respectively.}
}
@article{PALCHUNOV2021S11,
title = {Model-Theoretical Methods of Knowledge Generation Based on Medical Record Analysis},
journal = {International Journal of Psychophysiology},
volume = {168},
pages = {S11},
year = {2021},
note = {Proceedings of the 20th World Congress of Psychophysiology (IOP 2021) of the International Organization of Psychophysiology (IOP)},
issn = {0167-8760},
doi = {https://doi.org/10.1016/j.ijpsycho.2021.07.031},
url = {https://www.sciencedirect.com/science/article/pii/S0167876021002373},
author = {Dmitry Palchunov}
}
@article{PHUA2024105708,
title = {Fostering urban resilience and accessibility in cities: A dynamic knowledge graph approach},
journal = {Sustainable Cities and Society},
volume = {113},
pages = {105708},
year = {2024},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2024.105708},
url = {https://www.sciencedirect.com/science/article/pii/S221067072400533X},
author = {Shin Zert Phua and Markus Hofmeister and Yi-Kai Tsai and Oisín Peppard and Kok Foong Lee and Seán Courtney and Sebastian Mosbach and Jethro Akroyd and Markus Kraft},
keywords = {Disaster resilience, 15-minute city, Accessibility, Isochrone analysis, Knowledge graph},
abstract = {This paper explores the utilisation of knowledge graphs and an agent-based implementation to enhance urban resilience and accessibility in city planning. We expand The World Avatar (TWA) dynamic knowledge graph to support decision-making in disaster response and urban planning. By employing a set of connected agents and integrating diverse data sources — including flood data, geospatial building information, land plots, and open-source data — through sets of ontologies, we demonstrate disaster response in a coastal town in the UK and various aspects relevant to city planning for a mid-sized town in Germany using TWA. In King’s Lynn, our agent-based approach facilitates holistic disaster response by calculating optimal routes, avoiding flooded segments dynamically, assessing infrastructure accessibility before and during a flood using isochrones, identifying inaccessible population areas, guiding infrastructure restoration, and conducting critical path analysis. In Pirmasens, for city planning purposes, the knowledge graph-driven isochrone generation provides evidence-based insights into current amenity coverage and enables scenario planning for future amenities while adhering to land regulations. The implementation of agents and knowledge graphs achieves interoperability and enhances urban resilience and accessibility by enabling cross-domain correlation analysis that extends various areas including geospatial buildings, population demographics, accessibility coverage, and land use regulations.}
}
@article{GAO2019414,
title = {An equine disease diagnosis expert system based on improved reasoning of evidence credibility},
journal = {Information Processing in Agriculture},
volume = {6},
number = {3},
pages = {414-423},
year = {2019},
issn = {2214-3173},
doi = {https://doi.org/10.1016/j.inpa.2018.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S2214317318302385},
author = {Hongyan Gao and Guimiao Jiang and Xiang Gao and Jianhua Xiao and Hongbin Wang},
keywords = {Equine disease, Diagnosis, Expert system, Object-based ontology, Evidence credibility},
abstract = {In China, there is a troubling shortage of well-trained equine veterinarians, leaving the needs of many equine farmers unmet. This is especially true with respect to the diagnosis of equine diseases. To solve this shortcoming, an equine disease diagnosis expert system was developed. For the aspect of knowledge representation, the structure of equine disease diagnosis knowledge was analyzed using an ontology system. Next, the clinical signs were described using an object-attribute-value (O-A-V) format, and the knowledge representation was then expressed using production rules. With respect to the reasoning mechanism, the weights of the clinical signs and promoted confidence factors (PCF) were combined to express information and rules pertaining to clinical signs with an associated level of uncertainty. The model was established based on improved reasoning of evidence credibility. Finally, using the ASP.Net platform and the SQL Server 2008 database, the equine disease diagnosis expert system based on the B/S structure has been developed, and is capable of reliably diagnosing 40 of the most common equine diseases. A functional evaluation of the system was conducted, and the diagnostic accuracy was observed to be 88%. This study demonstrates a bright prospect for the popularization and application of the system through continuous system maintenance and knowledge-based updates.}
}
@article{WANG2025102352,
title = {Integrating Generative Artificial Intelligence techniques into technology function matrix analysis},
journal = {World Patent Information},
volume = {81},
pages = {102352},
year = {2025},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2025.102352},
url = {https://www.sciencedirect.com/science/article/pii/S0172219025000195},
author = {Huei-Yu Wang and Shu-Hao Chang and Chia-Yi Chuang},
keywords = {Technology-function matrices, Generative artificial intelligence, GAI, International patent classification, IPC},
abstract = {This study proposes a novel method for automating the construction of technology-function matrices using generative artificial intelligence (GAI), specifically focusing on quantum technologies. By leveraging GAI to analyze International Patent Classification (IPC) definitions and benchmark reports, we developed a system that rapidly generates technology-function matrices, significantly reducing the time required for manual analysis. The method was applied to 2,399 quantum technology patents from 2023 to March 2024, covering four key areas: secure communications, computing, quantum simulators, and sensors. This approach not only aids government agencies in identifying new technological opportunities but also facilitates the industrialization of potential technologies. By combining GAI with established analytical frameworks, this study contributes to both the theoretical understanding and practical application of patent analysis in emerging fields.}
}
@article{BIRDSEY2021110610,
title = {Plurality of perspective: Doctor-parents of deaf children in a low-to middle-income country},
journal = {International Journal of Pediatric Otorhinolaryngology},
volume = {142},
pages = {110610},
year = {2021},
issn = {0165-5876},
doi = {https://doi.org/10.1016/j.ijporl.2021.110610},
url = {https://www.sciencedirect.com/science/article/pii/S0165587621000033},
author = {Bianca Birdsey and Lavanithum Joseph},
keywords = {Hearing loss, Lived experience, Universal newborn hearing screening, Experts by experience, Family-centeredness},
abstract = {Objectives
Pediatric deafness is an important consideration in neurodevelopment. Early identification and intervention are major factors in seeing that deaf children reach their full potential. Often, it is the medical professionals who themselves have limited knowledge about hearing loss or the consequences of delayed language acquisition. These knowledge gaps can negatively influence the timeous and holistic care that children with hearing loss require. With a dual experiential expertise gained through both parenting children with disabling hearing loss and being medical doctors, the purpose of this study was to better understand the field of pediatric hearing loss through doctors’ insights gained as parents.
Study design
Interpretative Phenomenological Analysis was the approach used for this qualitative enquiry. Five South African participants with children between the ages of two and ten years, were selected using purposive sampling and an in-depth semi-structured interview used as the data instrument. Thereafter, three levels of thematic analysis were conducted.
Results
Generally, doctors have limited knowledge of pediatric deafness. Perceptions towards deafness are typically through a medical-model lens, while appreciation of Universal Newborn Hearing Screening is lacking. Through exploring the various gains of parenting a deaf child, participants acknowledged the power of the lived experience in changing their own practice as both parents and practitioners.
Conclusion
Doctors need to know more about pediatric deafness. The pathway of care from identification to intervention in South Africa needs revision. Exposure to the lived experience is a powerful means of enabling expert insights to influence such change in a practical and meaningful way.}
}
@article{SCHLUTER2023135776,
title = {Sustainable business model innovation: Design guidelines for integrating systems thinking principles in tools for early-stage sustainability assessment},
journal = {Journal of Cleaner Production},
volume = {387},
pages = {135776},
year = {2023},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.135776},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622053501},
author = {Leonie Schlüter and Lone Kørnøv and Lucia Mortensen and Søren Løkke and Kasper Storrs and Ivar Lyhne and Belinda Nors},
keywords = {Sustainable business model innovation, Business model innovation, Systems thinking, Life cycle assessment, Sustainability assessment, Tool design},
abstract = {The need to develop sustainable business models, which have a positive effect on environment and society, has received increasing attention in research and practice in the last years. Describing the sustainability of these business models, however, often takes place without robust assessments and without consideration for the wider system within which they are embedded. Early in the innovation process, in particular, a lack of quantitative data, time, and competencies presents an issue. At the same time, Systems Thinking has long been described as necessary for innovating business models for sustainability, but it has not been made clear how exactly Systems Thinking can be used early in the innovation process to assess the sustainability of a business model innovation. This article develops guidelines for embedding Systems Thinking principles into tools for sustainability assessment for use in the early stages of sustainable business model (SBM) innovation. It does so by exemplifying Systems Thinking principles in the context of SBM innovation and analysing their integration in three selected tools for early-stage sustainability assessment. The article shows how, by embedding Systems Thinking into tools for the SBM innovation process, unintended consequences and negative trade-offs can be reduced and the sustainability of the innovation better understood. Eight design guidelines are proposed for effectively using Systems Thinking in tools for early-stage sustainability assessment of SBMs: (1) Define scope of application, (2) Design for collaboration, (3) Integrate the principles “Interconnections”, (4) “Causal relations & feedback loops”, and (5) “System change & adaptation”, (6) Consider sustainability dimensions, (7) Ensure flexibility of integration, and (8) Ensure compatibility with other assessment tools.}
}
@article{BROMBAL2024101634,
title = {Lexical indicators for Chinese language ecological discourse analysis: Design and testing of a novel framework},
journal = {Language Sciences},
volume = {104},
pages = {101634},
year = {2024},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2024.101634},
url = {https://www.sciencedirect.com/science/article/pii/S0388000124000238},
author = {Daniele Brombal and Sergio Conti and Pui Yiu Szeto},
keywords = {Ecolinguistics, Ecological discourse analysis, Sustainability transformations, Social-ecological values, Chinese vocabulary},
abstract = {This paper introduces the results of an exploratory study, aimed at designing and testing an ecological discourse analysis framework, applicable to texts in Chinese language. Focusing on vocabulary, the study followed an iterative deductive-inductive process. First, a preliminary framework of lexical indicators was identified through literature review, adapted to Chinese language by means of a pilot analysis, and validated based on experts’ judgement. The framework was then tested on a sample of documents relevant to an environmental justice case in China. Results indicate that our Chinese Language Ecological Discourse Analysis (C-LEDA) framework can assist a coherent ecolinguistic characterization of Chinese texts by highlighting lexical patterns that reflect diverse social-ecological values and worldviews. Our work is a meaningful contribution to developing reliable tools for ecolinguistic analysis and lays the ground for further advancements in the field. More specifically, it constitutes a first step to support the development of: (a) multi-criteria annotation schemes for quantitative, corpus-assisted ecological discourse analysis; (b) qualitative and quantitative comparative studies, broadening the scope of analysis to the wider Sinophone context; and (c) co-creative protocols to re-story documents used in environmental planning to address the extractivist bias inherent in such processes.}
}
@article{YU2024362,
title = {Integrating Knowledge and Augmented Reality: A Novel Framework for Intelligent Assembly Assistance},
journal = {Procedia CIRP},
volume = {130},
pages = {362-367},
year = {2024},
note = {57th CIRP Conference on Manufacturing Systems 2024 (CMS 2024)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.10.101},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124012587},
author = {Yang Yu and Zhanxi Wang and Shiqi Gao and Tengfei Sun and Chen Zheng},
keywords = {Assembly guidance, knowledge-based engineering, augmented reality, assembly semantic},
abstract = {In recent years, with the increasing manufacturing demand for multi-variety and small-batch personalized production, improving the efficiency and accuracy of the assembly process of complex products has become an essential issue for industrial output. Although researchers have developed many intelligent AR-assisted assembly systems to solve the problem, the generated assembly guidance has poor interpretability, which makes it difficult for operators to comprehend and execute the assembly procedures accurately. This paper proposes a novel system framework that integrates knowledge-based engineering (KBE) with augmented reality (AR) to solve the problems in the manual assembly of complex products. Firstly, Mask R-CNN is introduced to perform physical parts segmentation and classification. Secondly, an ontological knowledge model for the assembly field is developed to provide explicit semantic descriptions and establish a rule-based reasoning mechanism to infer implicit relationships. Assembly guidance can be developed using the semantic description and reasoning mechanisms mentioned above. Finally, the developed assembly guidance is visualized by displaying the assembly process and necessary industrial information to improve interactivity.}
}
@article{MASMOUDI2021720,
title = {Knowledge hypergraph-based approach for data integration and querying: Application to Earth Observation},
journal = {Future Generation Computer Systems},
volume = {115},
pages = {720-740},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.09.029},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20311961},
author = {Maroua Masmoudi and Sana Ben Abdallah Ben Lamine and Hajer Baazaoui Zghal and Bernard Archimede and Mohamed Hedi Karray},
keywords = {Semantic data integration, Knowledge graph, Ontology, Query processing, Knowledge hypergraph, Earth observation},
abstract = {According to the world economic forum report, around 70% of generated data are not used. This limitation of usage is mainly due to the lack of interoperability and linking of data that resides in isolated silos. Indeed, the overwhelming amount of data has worsened heterogeneity problems, as have the types of sources generating data in heterogeneous formats and different semantics. Those data related problematics are frequent in the domain of Earth Observation (EO). Earth observed data use different terminologies, which are difficult to reconcile because they reflect overlapped disciplines. These issues lead to misunderstandings and inefficient exchange and management of data in terms of access, pricing, and data rights, which can hamper environmental phenomena understanding. Virtual Knowledge Graph (VKG), allows semantic integration of existing data sources into a wide Knowledge Graph. In this work we propose a knowledge hypergraph-based approach for data integration and querying, with an application to Earth Observation data. Our proposal takes place in two phases (1) a knowledge hypergraph-based virtual data integration and (2) a hypergraph-based query processing. The first phase allows to generate a virtual knowledge hypergraph consisting of RML mappings between an ontology and the data. The second phase consists of enhancing the user’s query by extracting and consolidating a global view of data from different sources based on the generated knowledge hypergraph. The proposed approach is implemented in the Onto-KIT tool (Ontology-based Knowledge hypergraph data Integration and querying Tool) and evaluated through real use case studies. The obtained results show that our proposal enhances query processing in terms of accuracy, completeness, and semantic richness of response.}
}
@article{CUI2025103454,
title = {Beyond the images: Comprehensible unsafe behaviour recognition boosted by joint inference graph with multi-hop reasoning},
journal = {Advanced Engineering Informatics},
volume = {66},
pages = {103454},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103454},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625003477},
author = {Dongdong Cui and Sheng Xu and Shunyi Wang and Kaiqi Zhang},
keywords = {Knowledge graph, Computer vision, Network analysis, Construction scenario semantic molecule, Unsafe behaviour recognition},
abstract = {Computer vision (CV) technology has gained widespread adoption in monitoring unsafe behaviours, and efficiently improved the construction safety performance. However, current computer vision technologies focus primarily on image recognition while neglecting reasoning beyond the image by integrating multi-source knowledge graphs. In complex and dynamic construction environments, monitoring unsafe behaviours often requires deeper reasoning by integrating various external information such as regulatory texts and accident data. Therefore, this study proposed a comprehensible unsafe behaviour recognition framework based on a joint inference graph with multi-hop reasoning. The framework utilizes the combined reasoning of computer vision and an integrated knowledge graph to identify and reason about unsafe behaviours, providing corresponding mitigation measures. Specifically, regulatory texts and accident data were first aligned to build an integrated knowledge graph, which was demonstrated to be more in terms of network scale, information propagation connectivity, and node influence distribution, making it suitable for multi-hop reasoning. Then, the joint inference subgraph of the integrated knowledge graph was compared to the construction scene semantic molecules (CSSMs), derived from CV recognition results using the YOLOv10 model, to recognize unsafe behaviours and simultaneously provide related regulatory requirements, potential consequences, and possible mitigation measures. The proposed approach was tested by self-constructed dataset and the results showed an average accuracy of 94.10 %. Lastly, the feasibility and practicality of the method is verified by the implementation of unsafe behaviour recognition in five work scenarios.}
}
@article{ERKOYUNCU2020145,
title = {A design framework for adaptive digital twins},
journal = {CIRP Annals},
volume = {69},
number = {1},
pages = {145-148},
year = {2020},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2020.04.086},
url = {https://www.sciencedirect.com/science/article/pii/S0007850620301086},
author = {John Ahmet Erkoyuncu and Iñigo Fernández {del Amo} and Dedy Ariansyah and Dominik Bulka and Rok Vrabič and Rajkumar Roy},
keywords = {Digital twins, Design method, Ontology},
abstract = {Digital Twin (DT) is a ‘living’ entity that offers potential with monitoring and improving functionality of interconnected complex engineering systems (CESs). However, lack of approaches for adaptively connecting the existing brownfield systems and their data limits the use of DTs. This paper develops a new DT design framework that uses ontologies to enable co-evolution with the CES by capturing data in terms of variety, velocity, and volume across the asset life-cycle. The framework has been tested successfully on a helicopter gearbox demonstrator and a mobile robotic system across their life cycles, illustrating DT adaptiveness without the data architecture needing to be modified.}
}
@article{LORKIEWICZ20214138,
title = {Response Stance in Query-Response Language Interaction between Cognitive Agents},
journal = {Procedia Computer Science},
volume = {192},
pages = {4138-4147},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.189},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921019281},
author = {Wojciech A. Lorkiewicz and Grzegorz Popek},
keywords = {response stance, natural language generation, cognitive agent},
abstract = {A modern cognitive agent should be capable of handling linguistic questions about its beliefs concerning states of the external world. Properly generated linguistic responses should represent empirically-verified bits of agent’s knowledge and inferred beliefs guided by current social stance. The former is captured in a hierarchical model of categorization. The latter represents proposed extension that aside from its knowledge, the responding agent’s response-generation mechanism is guided by two social meta-dimensions, that is, by its attitude towards a content of the enquiry (or towards the interacting agent, in general) and its willingness to elaborate on an answer by providing additional insights or by strengthening its point. In addition, we highlight how the introduced stances may affect the structure and content of the response.}
}
@article{CAIN2025103535,
title = {Caring for technologies, caring for Country},
journal = {Futures},
volume = {166},
pages = {103535},
year = {2025},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2024.103535},
url = {https://www.sciencedirect.com/science/article/pii/S0016328724002180},
author = {Anna Cain},
keywords = {Feminist care ethics, Caring for Country, Indigenous Australians, Energy social science, Sustainability transitions, Maintenance, Energy futures},
abstract = {Care is an emerging theoretical tool supporting analysis of socioecological equity impacts as energy systems are transformed to support more sustainable futures. Derived from feminist critiques of rationalist, market-led approaches, energy scholars use care to draw attention to the matters of care that are counted into energy system transitions and the care labour required to realise these transitions. Less attention has been applied to non-Western concepts of care and how they might provide alternative futures through energy. This paper draws on Tronto’s (2013) phases of care framework to investigate how care shapes, flows through and is enabled by renewable energy programs in remote Indigenous communities in Australia. Using data collected through multi-sited project ethnography, this analysis considers how care is defined and built into energy program design and implementation. Interrogating these care logics illustrates the importance of prioritising sociocultural alongside technical forms of care. Understanding energy in this way offers insights into the role of energy in underpinning Indigenous futures, by supporting Indigenous ontological imperatives to exist in and care for Country, as well as insights into what it means to care at scale with energy through sustainability transitions.}
}
@article{ALOBAID2022108092,
title = {Balancing coverage and specificity for semantic labelling of subject columns},
journal = {Knowledge-Based Systems},
volume = {240},
pages = {108092},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.108092},
url = {https://www.sciencedirect.com/science/article/pii/S095070512101159X},
author = {Ahmad Alobaid and Oscar Corcho},
keywords = {Semantic labelling, Semantic annotation, Knowledge graph},
abstract = {Many data are published on the Web using tabular data formats (e.g., spreadsheets). One of the main challenges for their effective (re)use is their generalized lack of semantics (e.g., column names are not usually standardized, and their meaning and content are not always clear). There is a common understanding that the reuse of tabular data may be improved by annotating them with the types used in knowledge graphs. In this paper, we present a novel approach to automatically type entity columns in tabular data with ontology classes. In contrast with existing proposals in the state-of-the-art, our approach does not require external linguistic resources, lookup services, model training, building a model of the knowledge graph beforehand, or having a human in the loop.}
}
@article{ELLIA2022103281,
title = {Consciousness and complexity: Neurobiological naturalism and integrated information theory},
journal = {Consciousness and Cognition},
volume = {100},
pages = {103281},
year = {2022},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2022.103281},
url = {https://www.sciencedirect.com/science/article/pii/S1053810022000137},
author = {Francesco Ellia and Robert Chis-Ciure},
keywords = {Consciousness, Neurobiological Naturalism, Integrated Information Theory, Artificial consciousness, Neuroscientific theories of consciousness},
abstract = {In this paper we take a meta-theoretical stance and compare and assess two conceptual frameworks that endeavor to explain phenomenal experience. In particular, we compare Feinberg & Mallatt’s Neurobiological Naturalism (NN) and Tononi’s and colleagues Integrated Information Theory (IIT), given that the former pointed out some similarities between the two theories (Feinberg & Mallatt 2016c-d). To probe their similarity, we first give a general introduction into both frameworks. Next, we expound a ground-plan for carrying out our analysis. We move on to articulate a philosophical profile of NN and IIT, addressing their ontological commitments and epistemological foundations. Finally, we compare the two point-by-point, also discussing how they stand on the issue of artificial consciousness.}
}
@article{TUNNICLIFFE2021101227,
title = {The predictive capabilities of mathematical models for the type-token relationship in English language corpora},
journal = {Computer Speech & Language},
volume = {70},
pages = {101227},
year = {2021},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2021.101227},
url = {https://www.sciencedirect.com/science/article/pii/S0885230821000346},
author = {Martin Tunnicliffe and Gordon Hunter},
keywords = {Types/token systems, Vocabulary size, Zipf's law, Heaps’ law},
abstract = {We investigate the predictive capability of mathematical models of the type-token relationship applied to the vocabulary growth profiles of selected English language documents. We compare the existing Good-Toulmin and Heaps formulae with an alternative approach based on Bernoulli trial word selection from a fixed finite vocabulary using the Zipf and Zipf-Mandelbrot probability distributions. We make two major observations: firstly, while the Zipf-Mandelbrot model makes better predictions of vocabulary growth than the Zipf model, the optimized parameters of the latter correlate better than those of the former with statistics gleaned independently from the data. Secondly, the mean of the Zipf-Mandelbrot, Good-Toulmin and Heaps models provides a more consistent and unbiased prediction of vocabulary than any individual model alone.}
}
@article{TIAN2025100711,
title = {Artificial intelligence in risk management within the realm of construction projects: A bibliometric analysis and systematic literature review},
journal = {Journal of Innovation & Knowledge},
volume = {10},
number = {3},
pages = {100711},
year = {2025},
issn = {2444-569X},
doi = {https://doi.org/10.1016/j.jik.2025.100711},
url = {https://www.sciencedirect.com/science/article/pii/S2444569X25000617},
author = {Kun Tian and Zicheng Zhu and Jasper Mbachu and Amir Ghanbaripour and Matthew Moorhead},
keywords = {Artificial intelligence, Risk management, Construction projects},
abstract = {The construction industry faces risks across various domains, including cost, safety, schedule, quality, and supply chain management. Recent artificial intelligence (AI) advancements offer promising solutions to enhance risk management. This systematic literature review (SLR) explores the integration of AI in construction risk management, focusing on AI applications, risk categories, and key algorithms. A total of 84 peer-reviewed articles published between 2014 and 2024 were analysed. The SLR method involved rigorous identification, selection, and critical appraisal of studies, followed by bibliometric analysis to uncover research trends, influential authors, and thematic clusters. The bibliometric analysis, including keyword co-occurrence and author collaboration networks, provided insights into the structure of the research landscape. Findings revealed that AI methods such as machine learning (ML), natural language processing (NLP), knowledge-based reasoning (KBR), optimisation algorithm (OA), and computer vision (CV) play crucial roles in predicting and managing risks. ML is employed for predictive modelling, NLP for document and compliance risk management, KBR for decision support, OA for optimising resources and schedules, and CV for real-time safety monitoring. Despite advancements, challenges related to data quality, model interpretability, and workforce skills hinder full AI integration. Future research should explore AI’s intersection with emerging technologies such as blockchain and adaptive risk models for responsible adoption. This paper contributes to the growing knowledge of AI’s transformative impact on construction risk management.}
}
@article{HELLWEG2022245,
title = {Knowledge graph for manufacturing cost estimation of gear shafts - a case study on the availability of product and manufacturing information in practice},
journal = {Procedia CIRP},
volume = {109},
pages = {245-250},
year = {2022},
note = {32nd CIRP Design Conference (CIRP Design 2022) - Design in a changing world},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.244},
url = {https://www.sciencedirect.com/science/article/pii/S221282712200693X},
author = {Fynn Hellweg and Harry Brückmann and Thomas Beul and Constantin Mandel and Albert Albers},
keywords = {Manufacturing Cost Estimation, Product Development, Gear Hobbing, Ontology, Knowledge Graph, Semantic Technologies, Reference System Elements},
abstract = {Growing cost pressure forces companies to actively manage their product costs to secure profitability. Here, manufacturing cost estimation within product development estimates manufacturing and material costs. As most products are developed in generations, needed product and manufacturing information can origin from reference system elements (RSE), for example similar components of prior product generations. Problematically, this product and manufacturing information as well as the knowledge of its interrelation is often stored in an unstructured way, document based or at least not machine-readable. This makes manufacturing cost estimation an effortful, time consuming and mainly manual activity with low traceability, where a wide manufacturing knowledge is required. Trends in production, like new manufacturing processes and production systems further increase the need for manufacturing information and knowledge. Knowledge graphs as semantic technologies can improve the findability and reusability of reference system elements and enable automatic information processing. Within this research, cost estimation of research and development of a large automotive supplier was used as research environment. Guided by the model of PGE an ontology for the manufacturing cost estimation domain was developed. Then, a knowledge graph was instantiated based on product and manufacturing information from gear shafts of electric axles. A case study was carried out to evaluate process-specific cycle time calculation as exemplary use case of the knowledge graph. Process-specific cycle times are generally effortful estimated based on detailed manufacturing information and then used together with machine hourly rates to estimate manufacturing costs. Here, the structured and machine-readable manufacturing information of identified reference system elements is extracted from the knowledge graph to reduce the effort, increase the traceability and enable future automation. The case study shows exemplary, how a knowledge graph can support manufacturing cost estimation of gear shafts where product and manufacturing information is automatically identified using reference system elements.}
}
@article{GIMENEZSOLANO2021103747,
title = {Definition and validation of SNOMED CT subsets using the expression constraint language},
journal = {Journal of Biomedical Informatics},
volume = {117},
pages = {103747},
year = {2021},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2021.103747},
url = {https://www.sciencedirect.com/science/article/pii/S1532046421000769},
author = {V.M. Giménez-Solano and J.A. Maldonado and D. Boscá and S. Salas-García and M. Robles},
keywords = {SNOMED CT, Graph database, Expression constraint language, Expression constraint simplification, Subset visualization},
abstract = {Background
SNOMED CT Expression Constraint Language (ECL) is a declarative language developed by SNOMED International for the definition of SNOMED CT Expression Constraints (ECs). ECs are executable expressions that define intensional subsets of clinical meanings by stating constraints over the logic definition of concepts. The execution of an EC on some SNOMED CT substrate yields the intended subset, and it requires an execution engine able to receive an EC as input, execute it, and return the matching concepts. An important issue regarding subsets of clinical concepts is their use in terminology binding between clinical information models and terminologies for defining the set of valid values of codified data.
Objective
To define and implement methods for the simplification, semantic validation and execution of ECs over a graph-oriented SNOMED CT database, and to provide a method for the visual representation of subsets in order to explore, understand and validate its content, as well as to develop an EC execution platform, called SNQuery, which makes use of these methods.
Methods
Since SNOMED CT is a directed and acyclic graph, we have used a graph-oriented database to represent the content of SNOMED CT, where the schema and instances are represented as graphs and the data manipulation is expressed by graph-oriented operations. For the execution of ECs over the graph database, it is performed a translation process in which ECs are translated into a set of Cypher Query Language queries. We have defined some EC simplification methods that leverage the logic structure underlying SNOMED CT. The purpose of these methods is to reduce the complexity of ECs and, in turn, its execution time, as well as to validate them from a SNOMED CT Concept Model and logical definition points of view. We also have developed a graphic representation based on the circle packing geometrical concept, which allows validating subsets, as well as pre-defined refsets and the terminology itself.
Results
We have developed SNQuery, a platform for the definition of intensional subsets of SNOMED CT concepts by means of the execution of ECs over a graph-oriented SNOMED CT database. Additionally, we have incorporated methods for the simplification and semantic validation of ECs, as well as for the visualization of subsets as a mechanism to understand and validate them. SNQuery has been evaluated in terms of EC execution times.
Conclusion
In this paper, we provide methods to simplify, semantically validate and execute ECs over a graph-oriented database. We also offer a method to visualize the intensional subsets obtained by executing ECs to explore, understand and validate them, as well as refsets and the terminology itself. The definition of intensional subsets is useful to bind content between clinical information models and clinical terminologies, which is a necessary step to achieve semantic interoperability between EHR systems.}
}
@article{SIKOS201829,
title = {Representing network knowledge using provenance-aware formalisms for cyber-situational awareness},
journal = {Procedia Computer Science},
volume = {126},
pages = {29-38},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.07.206},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918311803},
author = {Leslie F. Sikos and Markus Stumptner and Wolfgang Mayer and Catherine Howard and Shaun Voigt and Dean Philp},
keywords = {Knowledge representation, ontology engineering, cybersecurity},
abstract = {Due to the volume, variety, and veracity of network data available, information fusion and reasoning techniques are needed to support network analysts’ cyber-situational awareness. These techniques rely on formal knowledge representation to define the network semantics with data provenance at various levels of granularity. To this end, this paper proposes the Communication Network Topology and Forwarding Ontology, a state-of-the-art ontology that enables the formal, unified representation of complex network concepts regardless of the type of the data source. The implementation of this ontology allows network analysts to represent expert knowledge and query network data fused from disparate data sources.}
}
@article{ZHANG2023101521,
title = {Construction of a fluvial facies knowledge graph and its application in sedimentary facies identification},
journal = {Geoscience Frontiers},
volume = {14},
number = {2},
pages = {101521},
year = {2023},
issn = {1674-9871},
doi = {https://doi.org/10.1016/j.gsf.2022.101521},
url = {https://www.sciencedirect.com/science/article/pii/S1674987122001748},
author = {Lei Zhang and Mingcai Hou and Anqing Chen and Hanting Zhong and James G. Ogg and Dongyu Zheng},
keywords = {Fluvial facies, Knowledge graph, Domain ontology, Sedimentary facies identification, Machine-assisted interpretation},
abstract = {Lithofacies paleogeography is a data-intensive discipline that involves the interpretation and compilation of sedimentary facies. Traditional sedimentary facies analysis is a labor-intensive task with the added complexity of using unstructured knowledge and unstandardized terminology. Therefore, it is very difficult for beginners or non-geology scholars who lack a systematic knowledge and experience in sedimentary facies analysis. These hurdles could be partly alleviated by having a standardized, structured, and systematic knowledge base coupled with an efficient automatic machine-assisted sedimentary facies identification system. To this end, this study constructed a knowledge system for fluvial facies and carried out knowledge representation. Components include a domain knowledge graph for types of fluvial facies (meandering, braided and other fluvial depositional environments) and their characteristic features (bedforms, grain size distribution, etc.) with visualization, a method for query and retrieval on a graph database platform, a hierarchical knowledge tree-structure, a data-mining clustering algorithm for machine-analysis of publication texts, and an algorithm model for this area of sedimentary facies reasoning. The underlying sedimentary facies identification and knowledge reasoning system is based on expert experience and synthesis of publications. For testing, 17 sets literature publications data that included details of sedimentary facies data (bedforms, grain sizes, etc.) were submitted to the artificial intelligence model, then compared and validated. This testing set of automated reasoning results yielded an interpretation accuracy of about 90% relative to the published interpretations in those papers. Therefore, the model and algorithm provide an efficient and automated reasoning technology, which provides a new approach and route for the rapid and intelligent identification of other types of sedimentary facies from literature data or direct use in the field.}
}
@article{CHAKRAVARTTY201910,
title = {Physics, metaphysics, dispositions, and symmetries – À la French},
journal = {Studies in History and Philosophy of Science Part A},
volume = {74},
pages = {10-15},
year = {2019},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2018.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0039368118303339},
author = {Anjan Chakravartty},
keywords = {Structural realism, Naturalized metaphysics, Metaphysics of science, Symmetries, Dispositions, Explanation},
abstract = {Recent philosophy has paid increasing attention to the nature of the relationship between the philosophy of science and metaphysics. In The Structure of the World: Metaphysics and Representation, Steven French offers many insights into this relationship (primarily) in the context of fundamental physics, and claims that a specific, structuralist conception of the ontology of the world exemplifies an optimal understanding of it. In this paper I contend that his messages regarding how best to think about the relationship are mixed, and in tension with one another. The tension is resolvable but at a cost: a weakening of the argument for French's structuralist ontology. I elaborate this claim in a specific case: his assertion of the superiority of a structuralist account of de re modality in terms of realism about laws and symmetries (conceived ontologically) over an account in terms of realism about dispositional properties. I suggest that these two accounts stem from different stances regarding how to theorize about scientific ontology, each of which is motivated by important aspects of physics.}
}
@article{QIN2020810,
title = {A novel machine natural language mediation for semantic document exchange in smart city},
journal = {Future Generation Computer Systems},
volume = {102},
pages = {810-826},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.07.028},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19305679},
author = {Peng Qin and Jingzhi Guo},
keywords = {Smart city, Semantic document exchange, Natural language processing, Universal grammar},
abstract = {Semantic information exchange is currently facing the heterogeneity and consistency across languages context in a smart city. Thus, this paper highlights the challenges that smart city is confronting in terms of semantic document exchange and proposes a novel Machine Natural Language Mediation (MNLM) framework which provides a sentence-based machine natural language (MNL) as a kind of mediation language, where each sentence as a compound concept is a set of atomic concepts to be compatible with all languages achieving a global semantic transformation. The MNL enables the sentence computer-readable and understandable through unique iid and eiid without semantic ambiguity, and simultaneously MNLM will eliminate the semantic inconsistency and improve the accurate meaning interpretation across conversational contexts. The overall aim of the framework is to offer a better solution to communicate and exchange information or document in a smart city.}
}
@article{TRIBOAN2019224,
title = {A semantics-based approach to sensor data segmentation in real-time Activity Recognition},
journal = {Future Generation Computer Systems},
volume = {93},
pages = {224-236},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.09.055},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18303947},
author = {Darpan Triboan and Liming Chen and Feng Chen and Zumin Wang},
keywords = {Sensor segmentation, User preferences, Activities of daily living (ADL), Composite activities, Ontology modelling, Activity recognition},
abstract = {Activity Recognition (AR) is key in context-aware assistive living systems. One challenge in AR is the segmentation of observed sensor events when interleaved or concurrent activities of daily living (ADLs) are performed. Several studies have proposed methods of separating and organising sensor observations and recognise generic ADLs performed in a simple or composite manner. However, little has been explored in semantically distinguishing individual sensor events directly and passing it to the relevant ongoing/new atomic activities. This paper proposes Semiotic theory inspired ontological model, capturing generic knowledge and inhabitant-specific preferences for conducting ADLs to support the segmentation process. A multithreaded decision algorithm and system prototype were developed and evaluated against 30 use case scenarios where each event was simulated at 10sec interval on a machine with i7 2.60GHz CPU, 2 cores and 8GB RAM. The result suggests that all sensor events were adequately segmented with 100% accuracy for single ADL scenarios and minor improvement of 97.8% accuracy for composite ADL scenario. However, the performance has suffered to segment each event with the average classification time of 3971ms and 62183ms for single and composite ADL scenarios, respectively.}
}
@article{MONER201871,
title = {Archetype modeling methodology},
journal = {Journal of Biomedical Informatics},
volume = {79},
pages = {71-81},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2018.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S1532046418300224},
author = {David Moner and José Alberto Maldonado and Montserrat Robles},
keywords = {Archetype, Methodology, Dual model, ISO 13606, Openehr},
abstract = {Clinical Information Models (CIMs) expressed as archetypes play an essential role in the design and development of current Electronic Health Record (EHR) information structures. Although there exist many experiences about using archetypes in the literature, a comprehensive and formal methodology for archetype modeling does not exist. Having a modeling methodology is essential to develop quality archetypes, in order to guide the development of EHR systems and to allow the semantic interoperability of health data. In this work, an archetype modeling methodology is proposed. This paper describes its phases, the inputs and outputs of each phase, and the involved participants and tools. It also includes the description of the possible strategies to organize the modeling process. The proposed methodology is inspired by existing best practices of CIMs, software and ontology development. The methodology has been applied and evaluated in regional and national EHR projects. The application of the methodology provided useful feedback and improvements, and confirmed its advantages. The conclusion of this work is that having a formal methodology for archetype development facilitates the definition and adoption of interoperable archetypes, improves their quality, and facilitates their reuse among different information systems and EHR projects. Moreover, the proposed methodology can be also a reference for CIMs development using any other formalism.}
}
@article{YU202513,
title = {METAseen: analyzing network traffic and privacy policies in Web 3.0 based Metaverse},
journal = {Digital Communications and Networks},
volume = {11},
number = {1},
pages = {13-25},
year = {2025},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2023.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S2352864823001694},
author = {Beiyuan Yu and Yizhong Liu and Shanyao Ren and Ziyu Zhou and Jianwei Liu},
keywords = {Metaverse, Privacy policy, Traffic analysis, Blockchain, Data ontology},
abstract = {Metaverse is a new emerging concept building up a virtual environment for the user using Virtual Reality (VR) and blockchain technology but introduces privacy risks. Now, a series of challenges arise in Metaverse security, including massive data traffic breaches, large-scale user tracking, analysis activities, unreliable Artificial Intelligence (AI) analysis results, and social engineering security for people. In this work, we concentrate on Decentraland and Sandbox, two well-known Metaverse applications in Web 3.0. Our experiments analyze, for the first time, the personal privacy data exposed by Metaverse applications and services from a combined perspective of network traffic and privacy policy. We develop a lightweight traffic processing approach suitable for the Web 3.0 environment, which does not rely on complex decryption or reverse engineering techniques. We propose a smart contract interaction traffic analysis method capable of retrieving user interactions with Metaverse applications and blockchain smart contracts. This method provides a new approach to de-anonymizing users' identities through Metaverse applications. Our system, METAseen, analyzes and compares network traffic with the privacy policies of Metaverse applications to identify controversial data collection practices. The consistency check experiment reveals that the data types exposed by Metaverse applications include Personal Identifiable Information (PII), device information, and Metaverse-related data. By comparing the data flows observed in the network traffic with assertions made in the privacy regulations of the Metaverse service provider, we discovered that far more than 49% of the Metaverse data flows needed to be disclosed appropriately.}
}
@article{ZHANG2024541,
title = {Comprehensive bioinformatics analysis of co-expressed genes of post-traumatic stress disorder and major depressive disorder},
journal = {Journal of Affective Disorders},
volume = {349},
pages = {541-551},
year = {2024},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2024.01.098},
url = {https://www.sciencedirect.com/science/article/pii/S0165032724001095},
author = {Haofuzi Zhang and Peng Luo and Xiaofan Jiang},
keywords = {Post-traumatic stress disorder, Major depressive disorder, Bioinformatics, Molecular mechanism, Immune},
abstract = {Background
Post-traumatic stress disorder (PTSD) is one of the most serious sequelae of trauma with serious impact worldwide. Studies have suggested an association between PTSD and major depressive disorder (MDD), but the underlying common mechanisms remain unclear. This study aimed to further explore the molecular mechanism between PTSD and MDD via comprehensive bioinformatics analysis.
Methods
The microarray data of PTSD and MDD were downloaded from the Gene Expression Omnibus (GEO) database. Differentially expressed genes (DEGs) analysis and weighted gene co-expression network analysis (WGCNA) were performed to identify the co-expressed genes associated with PTSD and MDD. Gene Set Enrichment Analysis (GSEA), enrichment analyses based on Disease Ontology (DO), Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) were performed using R software. Then, R software was used for single-sample gene set enrichment analysis (ssGSEA) and immune infiltration analysis on the co-expressed genes in the two datasets., Therefore, a logistic regression model was constructed to predict PTSD and MDD using the R language. Ultimately, this study employed PTSD and MDD models to assess alterations in the expression of target genes within the mouse hippocampus.
Results
Four core genes (GNAQ, DPEP3, ICAM2, PACSIN2) were obtained through different analyses, and these genes had predictive validity for PTSD and MDD, playing an important role in the common mechanism of PTSD and MDD. The study findings reveal decreased expression levels of DPEP3, GNAQ, and PACDIN2 in PTSD samples, accompanied by an increased expression of ICAM2. In MDD samples, the expression of DPEP3 and ICAM2 is reduced, whereas GNAQ and PACDIN2 show an increase in expression.
Conclusions
This study provides a new perspective on the common molecular mechanisms of PTSD and MDD. These common pathways and core genes may provide promising clues for further experimental studies.}
}
@article{FANG2020101060,
title = {Automated text classification of near-misses from safety reports: An improved deep learning approach},
journal = {Advanced Engineering Informatics},
volume = {44},
pages = {101060},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101060},
url = {https://www.sciencedirect.com/science/article/pii/S147403462030029X},
author = {Weili Fang and Hanbin Luo and Shuangjie Xu and Peter E.D. Love and Zhenchuan Lu and Cheng Ye},
keywords = {Deep learning, Near-miss, Safety, Text classification},
abstract = {Examining past near-miss reports can provide us with information that can be used to learn about how we can mitigate and control hazards that materialise on construction sites. Yet, the process of analysing near-miss reports can be a time-consuming and labour-intensive process. However, automatic text classification using machine learning and ontology-based approaches can be used to mine reports of this nature. Such approaches tend to suffer from the problem of weak generalisation, which can adversely affect the classification performance. To address this limitation and improve classification accuracy, we develop an improved deep learning-based approach to automatically classify near-miss information contained within safety reports using Bidirectional Transformers for Language Understanding (BERT). Our proposed approach is designed to pre-train deep bi-directional representations by jointly extracting context features in all layers. We validate the effectiveness and feasibility of our approach using a database of near-miss reports derived from actual construction projects that were used to train and test our model. The results demonstrate that our approach can accurately classify ‘near misses’, and outperform prevailing state-of-the-art automatic text classification approaches. Understanding the nature of near-misses can provide site managers with the ability to identify work-areas and instances where the likelihood of an accident may occur.}
}
@article{GUDWIN2018155,
title = {An Overview of the Multipurpose Enhanced Cognitive Architecture (MECA)},
journal = {Procedia Computer Science},
volume = {123},
pages = {155-160},
year = {2018},
note = {8th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2017 (Eighth Annual Meeting of the BICA Society), held August 1-6, 2017 in Moscow, Russia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.01.025},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918300267},
author = {Ricardo Gudwin and André Paraense and Suelen {de Paula} and Eduardo Fróes and Wandemberg Gibaut and Elisa Castro and Vera Figueiredo and Klaus Raizer},
keywords = {Cognitive Architecture, Dual-process Theory, Dynamic Subsumption, CST},
abstract = {In this paper, we present an overview of MECA, the Multipurpose Enhanced Cognitive Architecture, a cognitive architecture developed by our research group and implemented in the Java language. MECA was designed based on many ideas coming from Dual Process Theory, Dynamic Subsumption, Conceptual Spaces and Grounded Cognition, and constructed using CST, a toolkit for the construction of cognitive architectures in Java, also developed by our group. Basically MECA promotes an hybridism of SOAR, used to implement rule-based processing and space-state exploration in System 2 modules, with a Dynamic Subsumption Motivational System performing the role of System 1, using a representational system based on conceptual spaces and grounded cognition. We provide an overview of the many MECA sub-systems and an ontology of the concepts being represented within the architecture.}
}
@article{SU2023224,
title = {Characterisation and evaluation of identicality for digital twins for the manufacturing domain},
journal = {Journal of Manufacturing Systems},
volume = {71},
pages = {224-237},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523001784},
author = {Shuo Su and Aydin Nassehi and Ben Hicks and Joel Ross},
keywords = {Digital twin purpose, Identicality, Evaluation method, Material extrusion},
abstract = {This paper proposes a comprehensive characteristic for digital twins (DTs) for the manufacturing domain, called “identicality,” to evaluate the capability to represent their physical counterparts in terms of their specific purpose. It is characterised by four attributes: completeness, trueness, precision, and latency. Specifically, completeness refers to the proportion of physical information that has been represented in the DT. Trueness and precision are used to evaluate the agreement between representative information in the DT and the accepted reference value in the physical counterpart. Latency reflects the degree of bidirectional synchronisation in the twinning process. Regarded as the representative information, data, model(s), and parameters in the digital representation are considered. The evaluation method for identicality is then driven by the specific purpose of the DT for the manufacturing scenario. The approach involves two stages: manufacturing scenario analysis and identicality evaluation. Evolved from a proposed manufacturing ontology, an information model with purpose-based weights is developed in the first stage. Based on it, four attributes are evaluated by integrating metrology principles and human knowledge. The characteristic and its evaluation method are demonstrated using a specific use case of estimating the dimensional accuracy of a printed artefact during the material extrusion (MEX) process.}
}
@article{QI2022106420,
title = {A Decision-Making Framework to Support Urban Heat Mitigation by Local Governments},
journal = {Resources, Conservation and Recycling},
volume = {184},
pages = {106420},
year = {2022},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2022.106420},
url = {https://www.sciencedirect.com/science/article/pii/S0921344922002646},
author = {Jinda Qi and Lan Ding and Samsung Lim},
keywords = {Urban heat mitigation strategies, Sensitive planning and design variables, Urban contexts, Ontology, Multi-objective optimisation},
abstract = {Local governments have made extensive efforts to mitigate urban overheating, cool streetscapes and cities, and protect vulnerable people. However, there is uncertainty about which urban heat mitigation strategies (UHMSs) can provide better solutions for a specific urban context. There is a compelling need for local governments to automate the decision-making process and optimise the combination of UHMSs to maximise the mitigation outcomes for their cities. We develop a novel decision-making framework that incorporates artificial intelligence (AI) techniques into urban heat mitigation in the built environment to enable an automated process of decision making. The novel decision-making framework comprises: the ontology-based knowledge representation of UHMSs and their relationships with urban contexts and performance assessment to share knowledge in urban heat mitigation domain; sensitivity analysis of the environmental, social and economic performance of UHMSs to get key variables for UHMSs; and genetic algorithm-based multi-objective optimisation of UHMSs. The novel decision-making framework enables generating the context-based optimised UHMS combinations to support local governments’ decision-making. The research outcomes will advance interdisciplinary knowledge about using AI techniques in the decision-making process for urban heat mitigation.}
}
@incollection{SCHMITZ2025,
title = {Terminology and Terminology Databases},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00565-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041005652},
author = {Klaus-Dirk Schmitz},
keywords = {Terminology, Terminology science, Concept, Term, Object, Concept system, Terminology standardization, Terminology management, Termbase, Terminology database, Terminology management system},
abstract = {Terminology as the set of concepts and terms in a specific subject field is a major building block of special language texts and therefore a prerequisite for efficient knowledge transfer. Terminology science uses the terminology triangle as a model to explain and define concept, object and term as well as the relationships between concepts and terms on the one hand and concepts and concepts on the other hand. ISO standards can be used as a reference for the definition of important terminological concepts, as an introduction to the principles and methods of terminology management and as guidelines for the design and implementation of terminology databases.}
}
@article{TENACUCALA2021103518,
title = {Pay-as-you-go consequence-based reasoning for the description logic SROIQ},
journal = {Artificial Intelligence},
volume = {298},
pages = {103518},
year = {2021},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103518},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221000692},
author = {David {Tena Cucala} and Bernardo {Cuenca Grau} and Ian Horrocks},
keywords = {Knowledge representation and reasoning, Ontologies, Automated reasoning, Description logics},
abstract = {Consequence-based (CB) reasoners combine ideas from resolution and (hyper)tableau calculi for solving key reasoning problems in Description Logics (DLs), such as ontology classification. Existing CB reasoners, however, are only capable of handling DLs without nominals (such as ALCHIQ), or DLs without disjunction (such as Horn-ALCHOIQ). In this paper, we present a consequence-based calculus for concept subsumption and classification in the DL ALCHOIQ+, which extends ALC with role hierarchies, inverse roles, number restrictions, and nominals; to the best of our knowledge, ours is the first CB calculus for an NExpTime-complete DL. By using standard transformations, our calculus extends to SROIQ, which covers all of OWL 2 DL except for datatypes. A key feature of our calculus is its pay-as-you-go behaviour: our calculus is worst-case optimal for all the well-known proper fragments of ALCHOIQ+. Furthermore, our calculus can be applied to DL reasoning problems other than subsumption and ontology classification, such as instance retrieval and realisation. We have implemented our calculus as an extension of Sequoia, a CB reasoner which previously supported ontology classification in SRIQ. We have performed an empirical evaluation of our implementation, which shows that Sequoia offers competitive performance. Although there still remains plenty of room for further optimisation, the calculus presented in this paper and its implementation provide an important addition to the repertoire of reasoning techniques and practical systems for expressive DLs.}
}
@article{BIANCHINI20252257,
title = {SAMBA: A reference framework for Human-in-the-Loop in adaptive Smart Manufacturing},
journal = {Procedia Computer Science},
volume = {253},
pages = {2257-2267},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.01.286},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925002947},
author = {Filippo Bianchini and Marco Calamo and Francesca {De Luzi} and Mattia Macrì and Matteo Marinacci and Jerin George Mathew and Flavia Monti and Jacopo Rossi and Francesco Leotta and Massimo Mecella},
keywords = {Human-in-the-Loop, Artificial Intelligence, Large Language Models, Extended Reality},
abstract = {In modern smart manufacturing environments, human involvement remains critical for addressing complex tasks that require adaptability and decision-making, despite the growing presence of automation and artificial intelligence. This paper introduces SAMBA – Service-Augmented Manufacturing-Based Approach, an innovative framework designed to optimize human-in-the-loop processes in smart manufacturing. The problem addressed involves the challenge of effectively integrating human operators with advanced automation technologies to reduce errors and increase process adaptability. The framework employs Large Language Models to extract procedural specifications from unstructured sources and convert them into structured instructions. By integrating these instructions with Extended Reality technologies to assist human operators, SAMBA aims to minimize errors and adapt processes to complex, dynamic production environments. The framework proposes the enhancement of Manufacturing Execution Systems by incorporating Artificial Intelligence techniques to enable adaptability and automatic exception correction. Finally, an adaptive orchestrator models each resource as a service accessible via Industrial APIs, promoting interoperability and integration within the production ecosystem.}
}
@article{WARREN2019145,
title = {Improving comprehension of knowledge representation languages: A case study with Description Logics},
journal = {International Journal of Human-Computer Studies},
volume = {122},
pages = {145-167},
year = {2019},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2018.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S1071581918305068},
author = {Paul Warren and Paul Mulholland and Trevor Collins and Enrico Motta},
keywords = {Description Logics, Manchester OWL Syntax, User studies, Psychological theories of reasoning},
abstract = {Knowledge representation languages are frequently difficult to understand, particularly for those not trained in formal logic. This is the case for Description Logics, which have been adopted for knowledge representation on the Web and in a number of application areas. This work looks at the difficulties experienced with Description Logics; and in particular with the widely-used Manchester OWL Syntax, which employs natural language keywords. The work comprises three studies. The first two identify a number of difficulties which users experience, e.g. with negated intersection, functional properties, the use of subproperties and restrictions. Insights from cognitive psychology and the study of language are applied to understand these difficulties. Whilst these difficulties are in part inherent in reasoning about logic, and Description Logics in particular, they are made worse by the syntax. In the third study, alternative syntactic constructs are proposed which demonstrate some improvement in accuracy and efficiency of comprehension. In addition to proposing alternative syntactic constructs, the work makes some suggestions regarding training and support systems for Description Logics.}
}
@article{URSINI201950,
title = {Nouns for visual objects: A hypothesis of the vision-language interface},
journal = {Language Sciences},
volume = {72},
pages = {50-70},
year = {2019},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2019.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0388000118302262},
author = {Francesco-Alessio Ursini and Paolo Acquaviva},
keywords = {Visual objects, Nouns, Interface conditions, Concepts, Language and cognition, Infomorphism},
abstract = {We propose an interpretation of the vision process and a structural analysis of nouns and nominal reference which make it possible to relate the visual/cognitive and the linguistic encapsulation of objecthood in a rigorous way. The result of this integrated hypothesis is a predictive account of possible and impossible nouns lexicalizing visual objects. Visual objects are indexed relations between stimuli interpreted via visual properties, such as [round], and what we define as object concepts: a red ball is the relation between the red and spherical features and the object concept of a ball. In language, nouns identify object concepts, semantically modelled as kinds, and the noun phrases they head can refer to instances of those kinds. No aspect of grammatical structure links up to visual properties directly, so no noun in natural language can denote an arbitrary subset of visual properties; the interaction is only at the level of objects, whether an abstract concept or a fully specified referent (the latter expressed by a full noun phrase). We formalize the relation between the two by means of an infomorphism, a formal representation of information flow between systems. This translates the objects of the visual and linguistic systems in terms of information types and tokens, constraining the possible lexicalization of object concepts. For instance, a visual property cannot be identified by a choice of noun unless it is interpreted as instantiating an object concept, because nouns can denote object concepts but not directly properties.}
}
@article{NEMESHAEV2021643,
title = {Selection of experts for scientific and technical expertise based on semantic search},
journal = {Procedia Computer Science},
volume = {190},
pages = {643-646},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.06.102},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921013594},
author = {Sergey Nemeshaev and Leonid Barykin and Kazbek Dadteev},
keywords = {semantic search, experts, scientific expertise, ontology},
abstract = {This article discusses the possibility of using semantic search to solve the problem of appointing an expert in automation systems for the management of scientific and technical expertise. The characteristics of tag search and the problems that arise when using it are given, which is followed by a description of the domain ontology and the semantic search algorithm. The main problems to be dealt with during the implementation of this approach are also indicated. After that, the results achieved with the use of semantic search are examined. In the conclusion, we discuss the direction of the further development of the algorithm within the framework of the information system and its expediency.}
}
@article{EFFIONG2025,
title = {Exploring Research Methodology and Research Design:},
journal = {International Journal of Business Analytics},
volume = {12},
number = {1},
year = {2025},
issn = {2334-4547},
doi = {https://doi.org/10.4018/IJBAN.381677},
url = {https://www.sciencedirect.com/science/article/pii/S233445472500005X},
author = {Sokomba Hannah Effiong},
keywords = {Reflexivity Research Philosophy Epistemology Ontology Methodological Flexibility Research Practice Organizational Ethnography Autoethnography Thematic Template Analysis},
abstract = {ABSTRACT
This book review examines Exploring Research Methodology and Research Design: Doing Research Across the Business Disciplines, edited by Peter John Sandiford and Sabine Schührer. The book is a reflective and practice-based resource for researchers navigating the complexities of methodology and design in business disciplines. It is structured in three parts: foundational perspectives on research, considerations in design and planning, and the realities of conducting research. Through diverse voices and case-based insights, the text challenges linear, formulaic approaches to research and instead promotes critical engagement, methodological flexibility, and reflexivity. The book equips doctoral students and early-career researchers with tools for thoughtful and context-sensitive research practice by addressing philosophical, ethical, and social dimensions.}
}
@article{KAZANINA2023996,
title = {The neural ingredients for a language of thought are available},
journal = {Trends in Cognitive Sciences},
volume = {27},
number = {11},
pages = {996-1007},
year = {2023},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2023.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364661323001936},
author = {Nina Kazanina and David Poeppel},
keywords = {language-of-thought, symbolic representation, computational theory of mind, spatial navigation, compositionality},
abstract = {The classical notion of a ‘language of thought’ (LoT), advanced prominently by the philosopher Jerry Fodor, is an influential position in cognitive science whereby the mental representations underpinning thought are considered to be compositional and productive, enabling the construction of new complex thoughts from more primitive symbolic concepts. LoT theory has been challenged because a neural implementation has been deemed implausible. We disagree. Examples of critical computational ingredients needed for a neural implementation of a LoT have in fact been demonstrated, in particular in the hippocampal spatial navigation system of rodents. Here, we show that cell types found in spatial navigation (border cells, object cells, head-direction cells, etc.) provide key types of representation and computation required for the LoT, underscoring its neurobiological viability.}
}
@article{WANG2024112349,
title = {Col1a1 mediates the focal adhesion pathway affecting hearing in miR-29a mouse model by RNA-seq analysis},
journal = {Experimental Gerontology},
volume = {185},
pages = {112349},
year = {2024},
issn = {0531-5565},
doi = {https://doi.org/10.1016/j.exger.2023.112349},
url = {https://www.sciencedirect.com/science/article/pii/S053155652300270X},
author = {Shuli Wang and Mulan Li and Pengcheng Liu and Yaning Dong and Ruishuang Geng and Tihua Zheng and Qingyin Zheng and Bo Li and Peng Ma},
keywords = {Age-related hearing loss, RNA-seq, , , Focal adhesion pathway},
abstract = {Age-related hearing loss (ARHL) is a common neurodegenerative disease. Its molecular mechanisms have not been fully elucidated. In the present study, we obtained differential mRNA expression in the cochlea of 2-month-old miR-29a+/+ mice and miR-29a−/− mice by RNA-seq. Gene ontology (GO) analysis was used to identify molecular functions associated with hearing in miR-29a−/− mice, including being actin binding (GO: 0003779) and immune processes. We focused on the intersection of differential genes, miR-29a target genes and the sensory perception of sound (GO:0007605) genes, with six mRNA at this intersection, and we selected Col1a1 as our target gene. We validated Col1a1 as the direct target of miR-29a by molecular and cellular experiments. Total 6 pathways involved in Col1a1 were identified by through Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway enrichment analysis. We selected the focal adhesion pathway as our target pathway based. Their expression levels in miR-29a−/− mice were verified by qRT-PCR and Western blot. Compared with miR-29a+/+ mice, the expression levels of Col1a1, Itga4, Itga2, Itgb3, Itgb7, Pik3r3 and Ptk2 were different in miR-29a−/− mice. Immunofluorescence was used to locate genes in the cochlea. Col1a1, Itga4 and Itgb3 were differentially expressed in the basilar membranes and stria vascularis and spiral ganglion neurons compared to miR-29a+/+ mice. Pik3r3 and Ptk2 were differentially expressed in the basilar membranes and stria vascularis, but not at the s spiral ganglion neurons compared to miR-29a+/+ mice. Our results show that when miR-29a is knocked out, the Col1a1 mediates the focal adhesion pathway may affect the hearing of miR-29a−/− mice. These findings may provide a new direction for effective treatment of age-related hearing loss.}
}
@incollection{BOSWELL20231,
title = {Philosophical and theoretical underpinnings of qualitative research},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {1-13},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.11001-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305110012},
author = {Eileen Boswell and Wayne A. Babchuk},
keywords = {Qualitative research, Paradigm, Ontology, Epistemology, Axiology, Methods, Methodology, Reflexivity, Positionality, Ethnography, Phenomenology, Grounded theory, Critical theory, Postmodernism, Feminist theories},
abstract = {This chapter provides an overview of the philosophical assumptions that underly qualitative research, including the types of logics employed as well as the axiological, epistemological, and ontological perspectives of various designs, methods, and methodologies. Readers new to qualitative inquiry will gain an introductory understanding of the historical forces that enabled qualitative research to develop in response to positivist, quantitative designs in which the researcher was seen as an objective, impartial expert. The chapter centers participants as experts in a qualitative project and describes the specific design rationales and procedures that qualitative researchers employ in order to maintain this focus on participant perspectives.}
}
@incollection{SHANNONBAKER2023380,
title = {Philosophical underpinnings of mixed methods research in education},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {380-389},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.11037-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305110371},
author = {Peggy Shannon-Baker},
keywords = {Axiology, Constructivism, Critical realism, Dialectical pluralism, Epistemology, Indigenous philosophies, Methodology, Mixed methods research, Ontology, Philosophical paradigms, Research philosophy, Postmodernism, Postpositivism, Pragmatism, Transformative, Transformative-emancipation, Yinyang philosophy},
abstract = {Identifying one's philosophical underpinnings makes the mixed methods research process more credible, transparent, and trustworthy. A philosophical paradigm refers to a set of beliefs about the nature of reality (ontology); what is knowledge, who can create it, and how (epistemology); the values that relate to one's beliefs and practices (axiology); and one's research practices (methodology). Mixed methods educational researchers have many paradigmatic foundations: positivism, postpositivism, critical realism, constructivism, pragmatism, postmodernism, and transformative-emancipation as well as dialectical pluralism, yinyang philosophy, and Indigenous philosophies. Each paradigm has its own set of beliefs, values, and practices though many paradigms overlap with one another.}
}
@article{LAMY2021102074,
title = {A data science approach to drug safety: Semantic and visual mining of adverse drug events from clinical trials of pain treatments},
journal = {Artificial Intelligence in Medicine},
volume = {115},
pages = {102074},
year = {2021},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2021.102074},
url = {https://www.sciencedirect.com/science/article/pii/S0933365721000671},
author = {Jean-Baptiste Lamy},
keywords = {Data mining, Ontology, Visual analytics, Glyph, Drug safety, Adverse drug events, Pain treatments, Painkillers},
abstract = {Clinical trials are the basis of Evidence-Based Medicine. Trial results are reviewed by experts and consensus panels for producing meta-analyses and clinical practice guidelines. However, reviewing these results is a long and tedious task, hence the meta-analyses and guidelines are not updated each time a new trial is published. Moreover, the independence of experts may be difficult to appraise. On the contrary, in many other domains, including medical risk analysis, the advent of data science, big data and visual analytics allowed moving from expert-based to fact-based knowledge. Since 12 years, many trial results are publicly available online in trial registries. Nevertheless, data science methods have not yet been applied widely to trial data. In this paper, we present a platform for analyzing the safety events reported during clinical trials and published in trial registries. This platform is based on an ontological model including 582 trials on pain treatments, and uses semantic web technologies for querying this dataset at various levels of granularity. It also relies on a 26-dimensional flower glyph for the visualization of the Adverse Drug Events (ADE) rates in 13 categories and 2 levels of seriousness. We illustrate the interest of this platform through several use cases and we were able to find back conclusions that were initially found during meta-analyses. The platform was presented to four experts in drug safety, and is publicly available online, with the ontology of pain treatment ADE.}
}
@article{HOUSE2022132,
title = {Political Language in Contrast: An Introduction},
journal = {Journal of Pragmatics},
volume = {188},
pages = {132-137},
year = {2022},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2021.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S0378216621003933},
author = {Juliane House and Dániel Z. Kádár},
abstract = {This editorial is an introduction to the special issue Political Language in Contrast. We first provide a theoretical grounding for the field, by introducing the objectives and main analytic procedures of empirical cross-cultural pragmatic research. We then discuss why this area is particularly suitable for the study of political language use. Following this, we overview the contents of the special issue. Finally, we briefly discuss vistas for future research.}
}
@article{BRADLEY201981,
title = {Models on the move: Migration and imperialism},
journal = {Studies in History and Philosophy of Science Part A},
volume = {77},
pages = {81-92},
year = {2019},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2017.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0039368117303047},
author = {Seamus Bradley and Karim P.Y. Thébault},
keywords = {Models, Interdisciplinarity, Idealisation},
abstract = {We introduce ‘model migration’ as a species of cross-disciplinary knowledge transfer whereby the representational function of a model is radically changed to allow application to a new disciplinary context. Controversies and confusions that often derive from this phenomenon will be illustrated in the context of econophysics and phylogeographic linguistics. Migration can be usefully contrasted with the concept of ‘imperialism’, which has been influentially discussed in the context of geographical economics. In particular, imperialism, unlike migration, relies upon extension of the original model via an expansion of the domain of phenomena it is taken to adequately describe. The success of imperialism thus requires expansion of the justificatory sanctioning of the original idealising assumptions to a new disciplinary context. Contrastingly, successful migration involves the radical representational re-interpretation of the original model, rather than its extension. Migration thus requires ‘re-sanctioning’ of new ‘counterpart idealisations’ to allow application to an entirely different class of phenomena. Whereas legitimate scientific imperialism should be based on the pursuit of some form of ontological unification, no such requirement is needed to legitimate the practice of model migration. The distinction between migration and imperialism will thus be shown to have significant normative as well as descriptive value.}
}
@article{DEMEY2020101989,
title = {Search strategies at the European Patent Office},
journal = {World Patent Information},
volume = {63},
pages = {101989},
year = {2020},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2020.101989},
url = {https://www.sciencedirect.com/science/article/pii/S0172219020300818},
author = {Yan Tang Demey and Domenico Golzio},
keywords = {Ontologies, Search strategies, Patent search, Prior-art search, BPMN, CMMN},
abstract = {Prior-art search is a critical step towards determining whether a patent can be granted or not. In 2016, an internal project called Search Workflow Modelling (SWM) was launched at the European Patent Office (EPO) for building a search knowledgebase, which contains a set of models that record not only the current situation on how patent examiners deal with prior-art search (i.e. the as-is models), but also their requirements of being able to do a more efficient and effective search (i.e. the to-be models). We use the Fact-based Modelling (FBM) approach for formalizing search ontologies, which cover a common vocabulary, relations between concepts related to search, and constraints applicable to these relations. We use a hybrid modelling approach of Business Process Modelling Notations and Case Management Model and Notations (BPMN/CMMN) to model search work flows. A patent search strategy typically involves at least one FBM model and at least one BPMN/CMMN model. In this paper, we will illustrate 5 types of existing search strategies (including recursive flow patterns and FBM models for future search features), and future search strategies. The SWM empirical studies in this paper are being put into practice in the ongoing projects concerning search tools at the EPO.}
}
@incollection{SCARPINO202573,
title = {Text Mining: Topic Modeling},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {73-77},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00212-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027002128},
author = {Ileana Scarpino and Rosarina Vallelunga},
keywords = {Latent dirichlet allocation, Natural language processing, Non-negative matrix factorization, Topic modeling},
abstract = {Topic Modeling (TM) is an important aspect of Natural Language Processing (NLP) aimed at discovering underlying themes in textual data without predefined labels. It efficiently handles the increasing complexity and volume of textual data through advanced algorithms, automating theme extraction and categorization within a text corpus for quicker comprehension. Advancements in NLP technologies and access to robust computational resources have accelerated TM's development, establishing it as a crucial tool across various sectors like social media analysis, document classification, and scientific research. This chapter aims to clearly define and describe TM's significance in NLP. It introduces TM in the Introduction as a statistical method for revealing latent variables in textual data, emphasizing its reliance on Data Mining (DM) techniques to unveil hidden insights. The "Background and Fundamentals" section explores TM's pivotal role in analyzing scientific texts, articles, and abstracts, particularly in fields like medicine and biology. It delves into the Latent Dirichlet Allocation (LDA) model and Non-Negative Matrix Factorization (NMF), focusing on their applications and objectives. "Illustrative Examples and Case Studies" from medicine and biology are presented to elucidate TM's evolving applications. These examples demonstrate how TM enhances data analysis by identifying emerging patterns and extracting valuable knowledge from extensive datasets. The "Topic Modeling Steps" section outlines the core processes involved in TM, while the "Conclusions and Future Challenges" section emphasizes TM's growing significance in biology and scientific research. It discusses TM's potential in discovering new interdisciplinary connections while acknowledging challenges such as linguistic complexity and contextual considerations.}
}
@article{WANG2023107327,
title = {Application of knowledge graph in software engineering field: A systematic literature review},
journal = {Information and Software Technology},
volume = {164},
pages = {107327},
year = {2023},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2023.107327},
url = {https://www.sciencedirect.com/science/article/pii/S0950584923001829},
author = {Lu Wang and Chenhan Sun and Chongyang Zhang and Weikun Nie and Kaiyuan Huang},
keywords = {Knowledge graph, Software engineering, Code recommendation, API recommendation, Vulnerability mining and location, Intelligent development},
abstract = {Context:
Knowledge graphs describe knowledge resources and their carriers through visualization. Moreover, they mine, analyze, construct, draw, and display knowledge and their interrelationships to reveal the dynamic development law of the knowledge field. Furthermore, knowledge graphs provide practical and valuable references for subject research. With the development of software engineering, powerful semantic processing and organizational interconnection capabilities of knowledge graphs are gradually required. Current research suggests using knowledge graphs for code or API recommendation, vulnerability mining, and positioning to improve the efficiency and accuracy of development and design. However, software engineering lacks a systematic analysis of the knowledge graphs application.
Objective:
This paper explores the construction techniques and application status of knowledge graphs in the field of software engineering, broadens the application prospects of knowledge graphs in this field, and facilitates the subsequent research of researchers.
Methods:
We collected over 100 documents from 2017 to date and selected 55 directly related documents for systematic analysis. Then, we analyzed the organized knowledge mainly stored in software engineering knowledge graphs, including software architecture, code details, and security reports.
Results:
We studied the emerging research methods in ontology modeling, named entity recognition, and knowledge fusion in graph construction and found that current knowledge graphs are mainly used in intelligent software development, software vulnerability mining, security testing, and API recommendation.
Conclusion:
Our research on the innovation of knowledge graph in software engineering and the future construction of integrating open-source community software and developer recommendations with knowledge-driven microservice O&M aspects can inspire more scholars and knowledge workers to use knowledge graph technology, which is important to solve software engineering problems and promote the development of both fields.}
}
@article{CHEN2023146942,
title = {Five hub genes contributing to the oncogenesis and trastuzumab-resistance in gastric cancer},
journal = {Gene},
volume = {851},
pages = {146942},
year = {2023},
issn = {0378-1119},
doi = {https://doi.org/10.1016/j.gene.2022.146942},
url = {https://www.sciencedirect.com/science/article/pii/S0378111922007624},
author = {Fahai Chen and Yaping Wang and Xinling Zhang and Jianmin Fang},
keywords = {Gastric cancer, Trastuzumab-resistant, Hub genes, Gene set enrichment analysis, Gene ontology enrichment analysis},
abstract = {Background
Monoclonal antibodies, as the targeted therapeutic strategies, provide huge clinical benefits for tumor patients. However, after undergoing several times treatment, patients developed drug resistance which is a major bottleneck in clinical cancer therapy. In this study, we aimed to explore the potential molecular mechanism of trastuzumab-resistant and cancer progression, and identify valuable diagnosis biomarkers for gastric cancer.
Materials and methods
Gene expression profiles and RNA-sequencing dataset of gastric cancer were acquired from Gene Expression Omnibus (GEO) dataset and The Cancer Genome Atlas (TCGA) dataset, respectively. The Differently expressed genes (DEGs) were screened by R programing language, and Gene Set Enrichment Analysis (GSEA), Gene Ontology (GO), and Kyoto Encyclopedia of Genes and Genomes (KEGG) were adopted separately to analyze the function and pathway of DEGs. Subsequently, Search Tool for the Retrieval of Interacting (STRING) and Cytoscape was performed to establish the protein–protein interaction (PPI) network and to screen hub genes. The Receiver Operating Characteristic (ROC) curves were used to evaluate the diagnostic values of the hub genes.
Results
Integrated analysis of TCGA-STAD (Stomach adenocarcinoma) and GEO databases identified 310 common DEGs. GSEA, GO and KEGG enrichment analysis revealed several crucial enriched oncological signatures and trastuzumab-resistant signaling pathways, which may help to explain the potential modulating mechanisms of trastuzumab-resistant. Based on the PPI network, 10 hub genes were screened and five genes (GNGT1, KRT7, KRT16, SOX9, TIMP1) were identified with good performance in the diagnosis of gastric cancer by ROC analysis. Furthermore, Kaplan-Meier analysis and log-rank test suggested that upregulation of KRT16 was correlated with overall survival in gastric cancer.
Conclusion
Overall, our study identified five hub genes that may play a critical role in promoting trastuzumab-resistant in gastric cancer, and would be a promising diagnostic and therapeutic biomarker for trastuzumab-resistant gastric cancer.}
}
@article{FU2025101592,
title = {Using integrative bioinformatics approaches and machine–learning strategies to identify potential signatures for atrial fibrillation},
journal = {IJC Heart & Vasculature},
volume = {56},
pages = {101592},
year = {2025},
issn = {2352-9067},
doi = {https://doi.org/10.1016/j.ijcha.2024.101592},
url = {https://www.sciencedirect.com/science/article/pii/S2352906724002586},
author = {Shihao Fu and Zian Feng and Ao Li and Zhenxiao Ma and Haiyang Zhang and Zhiwei Zhao},
keywords = {Atrial fibrillation, WGCNA, Biomarkers, Machine–learning strategies},
abstract = {Atrial fibrillation (AF) is the most common tachyarrhythmia and seriously affects human health. Key targets of AF bioinformatics analysis can help to better understand the pathogenesis of AF and develop therapeutic targets. The left atrial appendage tissue of 20 patients with AF and 10 patients with sinus rhythm were collected for sequencing, and the expression data of the atrial tissue were obtained. Based on this, 2578 differentially expressed genes were obtained through differential analysis. Different express genes (DEGs) were functionally enriched on Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG), mainly focusing on neuroactive ligand-receptor interactions, neuronal cell body pathways, regulation of neurogenesis, and neuronal death, regulation of neuronal death, etc. Secondly, 14 significant module genes were obtained by analyzing the weighted gene co-expression network of DEGs. Next, LASSO and SVM analyzes were performed on the differential genes, and the results were in good agreement with the calibration curve of the nomogram model for predicting AF constructed by the weighted gene co-expression network key genes. The significant module genes obtained by the area under the ROC curve (AUC) analysis were analyzed. Through crossover, two key disease characteristic genes related to AF, HOXA2 and RND2, were screened out. RND2 was selected for further research, and qPCR verified the expression of RND2 in sinus rhythm patients and AF patients. Patients with sinus rhythm were significantly higher than those in AF patients. Our research indicates that RND2 is significantly associated with the onset of AF and can serve as a potential target for studying its pathogenesis.}
}
@article{SILVA2019100877,
title = {Using controlled vocabularies in anatomical terminology: A case study with Strumigenys (Hymenoptera: Formicidae)},
journal = {Arthropod Structure & Development},
volume = {52},
pages = {100877},
year = {2019},
issn = {1467-8039},
doi = {https://doi.org/10.1016/j.asd.2019.100877},
url = {https://www.sciencedirect.com/science/article/pii/S1467803919300234},
author = {Thiago S.R. Silva and Rodrigo M. Feitosa},
keywords = {Morphology, Ants, Ontology, Semantic annotation, Terminology},
abstract = {Morphological studies of insects can help us to understand the concomitant or sequential functionality of complex structures and may be used to hypothetize distinct levels of phylogenetic relationship among groups. Traditional morphological works, generally, have encompassed a set of elements, including descriptions of structures and their respective conditions, literature references and images, all combined in a single document. Fast forward to the digital era, it is now possible to release this information simultaneously but also independently as data sets linked to the original publication in an external environment. In order to link data from various fields of knowledge, disseminating morphological information in an open environment, it is important to use tools that enhance interoperability. For example, semantic annotations facilitate the dissemination and retrieval of phenotypic data in digital environments. The integration of semantic (i.e. web-based) components with anatomic treatments can be used to generate a traditional description in natural language along with a set of semantic annotations. The ant genus Strumigenys currently comprises about 840 described species distributed worldwide. In the Neotropical region, almost 200 species are currently known, but it is possible that much of the species' diversity there remains unexplored and undescribed. The morphological diversity in the genus is high, reflecting an extreme generic reclassification that occurred in the late 20th and early 21st centuries. Here we define the anatomical concepts in this highly diverse group of ants using semantic annotations to enrich the anatomical ontologies available online, focussing on the definition of terms through subjacent conceptualization.}
}
@article{MAI2025104368,
title = {Towards the next generation of Geospatial Artificial Intelligence},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {136},
pages = {104368},
year = {2025},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2025.104368},
url = {https://www.sciencedirect.com/science/article/pii/S1569843225000159},
author = {Gengchen Mai and Yiqun Xie and Xiaowei Jia and Ni Lao and Jinmeng Rao and Qing Zhu and Zeping Liu and Yao-Yi Chiang and Junfeng Jiao},
keywords = {Geospatial Artificial Intelligence, Heterogeneity-aware GeoAI, Knowledge-Guided GeoAI, Spatial representation learning, Geo-Foundation Models, Fairness-aware GeoAI, Privacy-aware GeoAI, Interpretable and explainable GeoAI},
abstract = {Geospatial Artificial Intelligence (GeoAI), as the integration of geospatial studies and AI, has become one of the fastest-developing research directions in spatial data science and geography. This rapid change in the field calls for a deeper understanding of the recent developments and envision where the field is going in the near future. In this work, we provide a quantitative analysis of the GeoAI literature from the spatial, temporal, and semantic aspects. We briefly discuss the history of AI and GeoAI by highlighting some pioneering work. Then we discuss the current landscape of GeoAI by selecting five representative subdomains including remote sensing, urban computing, Earth system science, cartography, and geospatial semantics. Finally, we highlight several unique future research directions of GeoAI which are classified into two groups: GeoAI method development challenges and GeoAI Ethics challenges. Topics include heterogeneity-aware GeoAI, knowledge-guided GeoAI, spatial representation learning, geo-foundation models, fairness-aware GeoAI, privacy-aware GeoAI, as well as interpretable and explainable GeoAI. We hope our review of GeoAI’s past, present, and future is comprehensive and can enlighten the next generation of GeoAI research.}
}
@article{STYLIANOU2022,
title = {Doc2KG:},
journal = {International Journal on Semantic Web and Information Systems},
volume = {18},
number = {1},
year = {2022},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.295552},
url = {https://www.sciencedirect.com/science/article/pii/S1552628322000424},
author = {Nikolaos Stylianou and Danai Vlachava and Ioannis Konstantinidis and Nick Bassiliades and Vassilios Peristeras},
keywords = {eGovernment, Government Portals, Linked Data, Machine Learning, Natural Language Processing, Open Data, Semantic Web},
abstract = {ABSTRACT
Document management systems (DMS) have been used for decades to store large amounts of information in textual form. Their technology paradigm is based on storing vast quantities of textual information enriched with metadata to support searchability. However, this exhibits limitations as it treats textual information as a black box and is based exclusively on user-created metadata, a process that suffers from quality and completeness shortcomings. The use of knowledge graphs in DMS can substantially improve searchability, providing the ability to link data and enabling semantic searching. Recent approaches focus on either creating knowledge graphs from document collections or updating existing ones. In this paper, the authors introduce Doc2KG (Document-to-Knowledge-Graph), an intelligent framework that handles both creation and real-time updating of a knowledge graph, while also exploiting domain-specific ontology standards. They use DIAVGEIA (clarity), an award-winning Greek open government portal, as the case-study and discuss new capabilities for the portal by implementing Doc2KG.}
}
@article{LAZZARIS2024743,
title = {Visualization of Information Through Complex Networks – An Applied Case of CMMI and OpenUp Alignment},
journal = {Procedia Computer Science},
volume = {239},
pages = {743-750},
year = {2024},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2023},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.231},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924014741},
author = {Joana Lazzaris and Miguel Silva and Tiago F. Pereira and Vítor Faria and Paulo Compadrinho and Ricardo J. Machado},
keywords = {Complex networks, Graph database, CMMI, OpenUP, Visualization of information},
abstract = {Increasingly technical advancements have created new hurdles for industrial software development, this article proposes the validation of an architecture for the visualization of information through complex networks, accomplished by the alignment of the practices of the Capability Maturity Model Integration (CMMI) with the OpenUp disciplines. Relying on ontology methods, CMMI and OpenUP data were crossed on Neo4j graph database management system, following the proposed architecture. This demonstration case started with tabled datasets that have low complexity at the data level, therefore, as defined, we followed for the graph database (Neo4j), then to the user interface finalizing at information visualization. The main contribution of this study is certainly the validation of the technological architecture, through the cross mapping of CMMI and OpenUP in graph database, allowing future contributions to the matter. For future work, we propose the alignment of the model in a study case applied in the real environment, through the development of an adequate replicable framework.}
}
@article{SCHUHEGGER2023227,
title = {Factory Data Model for Planning Variant Sequences, Expansion Stages and Reconfigurations},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {227-232},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.1573},
url = {https://www.sciencedirect.com/science/article/pii/S240589632301981X},
author = {Lukas Schuhegger and Thomas Armbruster and Stefan Galka},
keywords = {Factory data model, Factory planning, Variant sequence, Expansion stage, Reconfiguration},
abstract = {A factory has to adapt to a future production program and increasing volumes. The planning of new factories must therefore already take expansion stages and reconfigurations into account. This leads to an increase in the complexity of planning, as different variants have to be planned and put into a chronologically reasonable and cost-optimized sequence. A factory data model that covers expansion stages manages the planning data amount and reduces complexity. Based on existing models from the literature, a factory data model was developed and modeled in the Unified Modeling Language standard. The factory data model shows which classes are interdependent and how planning results are stored reusable for other variants. An example is used to explain the developed factory data model and highlighting its advantages. The aim is to enable factory planners to document variant sequences and expansion stages in a comprehensible way, which supports the interdisciplinarity of factory planning projects.}
}
@article{REYJOUANCHICOT2025101588,
title = {Adaptation in Smart Home Automation Systems: A systematic review of decision-making and interaction},
journal = {Internet of Things},
volume = {31},
pages = {101588},
year = {2025},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2025.101588},
url = {https://www.sciencedirect.com/science/article/pii/S2542660525001015},
author = {Jordan Rey-Jouanchicot and Eric Campo and Jean-Léon Bouraoui and André Bottaro and Nadine Vigouroux and Frédéric Vella},
keywords = {Artificial intelligence, Decision-making, Adaptivity, Interaction, Smart home, Automation system, Modelization},
abstract = {Smart home automation systems have gained in popularity recently due to the advent of the Internet of Things. These systems offer homeowners convenience, comfort, and energy efficiency using various devices such as thermostats and voice assistants. A key factor in the success of these systems in the future will be their ability to make effective decisions and seamlessly interact with users and their surroundings. However, as technology continues to advance, it is essential to investigate how these systems can adapt, especially in terms of decision-making and interaction. This work presents a systematic review of the literature on adaptivity in smart home automation systems, focusing on general-purpose and comfort use cases. The review explores and discusses various proposals and approaches to adaptation, with a specific emphasis on the use of artificial intelligence. The aim is to provide an overview of existing approaches and highlight recent research contributions. It also discusses limitations, challenges, and emerging trends in decision-making systems for smart homes. Finally, it suggests future research directions to improve their adaptivity.}
}
@article{MEDINI201989,
title = {Specifying a modelling language for PSS Engineering – A development method and an operational tool},
journal = {Computers in Industry},
volume = {108},
pages = {89-103},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.02.014},
url = {https://www.sciencedirect.com/science/article/pii/S0166361518306547},
author = {Khaled Medini and Xavier Boucher},
keywords = {PSS engineering, Conceptual modelling, Modelling language, Domain-specific modelling, Model-based system engineering},
abstract = {Although the literature is full of research on the transition of industry towards Product-Service Systems (PSS), the question of how to effectively support PSS engineering is poorly addressed. The compelling need for decision support throughout the various stages of the engineering process is particularly challenging due to the inherent complexity of PSS. In this sense, visualisation and modelling at large have been put forth as promising means for supporting PSS engineering. This paper proposes a method for specifying a modelling language for PSS engineering, putting together PSS domain-specific knowledge and modelling concepts inherited from conceptual modelling and model-based engineering. It relies on a recursive transformation process of the underlying PSS meta-model using knowledge from case studies and the literature. The method has proven to be a practical means for gradual enrichment of the modelling language leading to successful experimentations in the industrial context.}
}
@incollection{GAUTAM20233519,
title = {A Cloud-based Collaborative Interactive Platform for Education and Research in Dynamic Process Modelling},
editor = {Antonios C. Kokossis and Michael C. Georgiadis and Efstratios Pistikopoulos},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {52},
pages = {3519-3524},
year = {2023},
booktitle = {33rd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-15274-0.50562-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044315274050562X},
author = {Vinay Gautam and Alberto Rodríguez-Fernández and Heinz A. Preisig},
keywords = {Process modelling, computer-aided modelling, Simphony-remote, Jupyterhub},
abstract = {Process modelling is used in many disciplines for various purposes like simulation, process design, optimisation and control. Although there is an increasing demand to build large, complex process models involving multiple disciplines, a systematic approach to developing such models is largely missing from engineering education and research. The ontology-based methodology and ProcessModeller (ProMo) tool suite developed over the years (Preisig, 2021, 2020; Elve and Preisig, 2018) help users to build multi-disciplinary and multi-scale models systematically. This paper presents a cloud-based platform ProMo-Remote that uses Simphony-Remote, a free and open-source web service. In this platform, the graphical user interfaces of the ProMo tool suite are accessible using a regular web browser, freeing the user from installing the desktop tool locally. Users can save, download/upload modelling data and use it in computational workflows utilising cloud-based interactive computing tools like Jupyter notebooks. The user can also collaborate with others by screen sharing in real time. The platform can easily be configured and deployed with additional applications to facilitate further education, training and research on the systematic development of complex process models.}
}
@article{FUCHS2025103627,
title = {Exploring the potential of parallel drafting of building regulations},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103627},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103627},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625005208},
author = {Stefan Fuchs and Judith Fauth and Markus Boden and Robert Amor},
keywords = {Building regulations, Legislative drafting, Formal representation, Parallel drafting, Automated compliance checking, Building permit},
abstract = {The ability for computers to interpret and utilise computerised building regulations is crucial for automated compliance checking of buildings, enabling a faster and more objective building permit review. Since natural language regulations are often not designed to be represented formally, integrating formal representations into the regulation drafting process could drive this change. Through in-depth qualitative expert interviews with regulators worldwide, this study examines the drafting process of building regulations. It aims to establish a methodology for developing formal representations in parallel with natural language versions. To be able to do so, the first step is to understand the current workflow and its requirements. The interviewees in this study highlighted several benefits of parallel drafting, including improved regulation quality and reduced inconsistencies. However, they also acknowledged challenges such as the difficulty of modifying existing laws and the significant time and costs required for such changes. This study explores various strategies for integrating formal representations into the drafting process of building regulations. Notably, in many countries, significant progress has been made towards automated compliance checking. However, much of this progress has taken place independently of legislative bodies, limiting the potential of automation in the building permit process. Nonetheless, early efforts by legislators to adopt formal representations are emerging. This study aims to raise awareness of these developments and contribute to the broader understanding of how formal representations can enhance the regulation drafting process.}
}
@article{IATROPOULOS201837,
title = {The language of smell: Connecting linguistic and psychophysical properties of odor descriptors},
journal = {Cognition},
volume = {178},
pages = {37-49},
year = {2018},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2018.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0010027718301276},
author = {Georgios Iatropoulos and Pawel Herman and Anders Lansner and Jussi Karlgren and Maria Larsson and Jonas K. Olofsson},
keywords = {Odour naming, Odour identification, Sensory lexicon, Sensory-semantic integration, Distributional semantics, Computational linguistics},
abstract = {The olfactory sense is a particularly challenging domain for cognitive science investigations of perception, memory, and language. Although many studies show that odors often are difficult to describe verbally, little is known about the associations between olfactory percepts and the words that describe them. Quantitative models of how odor experiences are described in natural language are therefore needed to understand how odors are perceived and communicated. In this study, we develop a computational method to characterize the olfaction-related semantic content of words in a large text corpus of internet sites in English. We introduce two new metrics: olfactory association index (OAI, how strongly a word is associated with olfaction) and olfactory specificity index (OSI, how specific a word is in its description of odors). We validate the OAI and OSI metrics using psychophysical datasets by showing that terms with high OAI have high ratings of perceived olfactory association and are used to describe highly familiar odors. In contrast, terms with high OSI have high inter-individual consistency in how they are applied to odors. Finally, we analyze Dravnieks’s (1985) dataset of odor ratings in terms of OAI and OSI. This analysis reveals that terms that are used broadly (applied often but with moderate ratings) tend to be olfaction-unrelated and abstract (e.g., “heavy” or “light”; low OAI and low OSI) while descriptors that are used selectively (applied seldom but with high ratings) tend to be olfaction-related (e.g., “vanilla” or “licorice”; high OAI). Thus, OAI and OSI provide behaviorally meaningful information about olfactory language. These statistical tools are useful for future studies of olfactory perception and cognition, and might help integrate research on odor perception, neuroimaging, and corpus-based linguistic models of semantic organization.}
}
@article{NI2023106047,
title = {Generating textual emergency plans for unconventional emergencies — A natural language processing approach},
journal = {Safety Science},
volume = {160},
pages = {106047},
year = {2023},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2022.106047},
url = {https://www.sciencedirect.com/science/article/pii/S0925753522003861},
author = {Weijian Ni and Quanle Shen and Tong Liu and Qingtian Zeng and Lingzhe Xu},
keywords = {Emergency plan, Unconventional emergency, Natural language processing, Deep learning, Natural language generation},
abstract = {An emergency plan is an emergency administrative document that specifies the course of actions taken to minimize the effects of a crisis or incident. Establishing high-quality emergency plans has been a fundamental task for various emergency administrative agencies. Traditionally, emergency plans are developed based on the experiences of handling past emergencies, thus may not be well applied to unconventional emergencies that arise in an unrepeatable and unpredictable manner. This work proposes a novel emergency plan generation approach to assist decision-making under unconventional emergent situations. This goal is achieved by leveraging deep-learning-based natural language techniques to explore the interrelationship between existing emergency plans developed for common emergencies and the target unconventional emergency. In particular, an emergency response knowledge base is constructed based on a large number of existing emergency plans, and the relevant part with respect to the target unconventional emergency is retrieved. Then the new emergency plan is formed by organizing the relevant knowledge guided by a pre-defined emergency plan template. Furthermore, a novel emergency plan evaluation approach is proposed to perform a comprehensive evaluation of the quality of generated emergency plans. Empirical results on a real-world unconventional emergency case verify the feasibility of our emergency plan generation approach.}
}
@article{LI2025544,
title = {Converting Relational Databases to Manufacturing Knowledge Graph for Product Quality Tracing in Additive Manufacturing},
journal = {Procedia CIRP},
volume = {134},
pages = {544-549},
year = {2025},
note = {58th CIRP Conference on Manufacturing Systems 2025},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2025.02.172},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125005414},
author = {Laiyi Li and Maolin Yang and Pingyu Jiang},
keywords = {Manufacturing Knowledge Graph, Additive Manufacturing, Product Quality Tracing, Relational Database, Swimlane Flowchart},
abstract = {Product quality tracing (PQT) in additive manufacturing (AM) is a significant and complex challenge crucial for fully understanding the processes, tracing quality issues, and optimizing process technologies. Typically, tracing relies on production data from historical relational databases, but these data lack semantic information and integration with the service context of manufacturing processes. A manufacturing knowledge graph (MKG) offers a solution by adding manufacturing semantic information to production data. This paper constructed a PQT model and an MKG database for AM. The PQT model and the schema layer of the MKG originate from the swimlane flowchart. Subsequently, a relational database was mapped to instantiate the MKG, and the natural language model was used to complete it. This integration constructs a PQT model and an MKG database centered on manufacturing resources, events, data, and states. The tracing subgraphs extracted from the MKG database demonstrate that this method effectively traces manufacturing process issues and anomalies. The proposed method provides a viable database and method for PQT in AM, enabling comprehensive tracing of manufacturing processes, identifying quality anomalies, and providing potential cause references for AM experts.}
}
@article{BAZANMUNOZ2024101160,
title = {Taxonomy and software architecture for real-time context-aware collaborative smart environments},
journal = {Internet of Things},
volume = {26},
pages = {101160},
year = {2024},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2024.101160},
url = {https://www.sciencedirect.com/science/article/pii/S254266052400101X},
author = {Adrian Bazan-Muñoz and Guadalupe Ortiz and Juan C. Augusto and Alfonso Garcia-de-Prado},
keywords = {Taxonomy, Context awareness, Internet of Things, Smart environment, Complex event processing},
abstract = {The widespread of Internet of Things (IoT) and the price reduction and ubiquity of telecommunications has led to the emergence of smart environments where devices are becoming increasingly smarter and everything is connected and from which society aims to benefit. The data obtained from IoT is rapidly processed in various domains for the achievement of smart cities and societies. However, in many cases, applications are not contextualized by using data from outside the domain but are only contextualized using data from the domain itself, missing the opportunity for further contextualization. The lack of common criteria for the integration of data from different application domains is one of the main reasons that significantly hinders the integration of third-party data into real-time processing and decision-making systems and thus, the context awareness of developed applications. Although the use of several taxonomies and ontologies for context awareness in various application domains have been proposed, in many cases they are highly domain specific and/or difficult to integrate with other systems, which makes it challenging to facilitate data sharing between different systems and their processing to achieve enhanced context awareness. We aim to contribute to the addressing of these limitations through a reusable and extensible multi-domain taxonomy targeted to collaborative IoT and smart environments, which is also automatically integrated into a software architecture with real-time complex event processing technologies. The proposed solution has been illustrated through a case study and performance tests have been carried out in different computing capacity scenarios, showing its feasibility and usefulness.}
}
@article{PRINSLOO201887,
title = {Students’ intrinsic perspectives on the diverse functions of short stories beyond language learning},
journal = {System},
volume = {74},
pages = {87-97},
year = {2018},
issn = {0346-251X},
doi = {https://doi.org/10.1016/j.system.2018.02.019},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X17307236},
author = {Christiaan Prinsloo},
keywords = {Short stories, English language teaching, English as a Foreign Language, Literature-based actualization, Non-interventionist teaching methodology},
abstract = {The short story as literary genre has been used productively for language learning, and much pedagogic research has emphasized the language learning functions of short stories in English foreign language (EFL) contexts. While the language learning function appears to be a natural extension of reading short stories, they may also perform other functions in English language teaching (ELT). The objective of this study was to establish what functions EFL students intrinsically (i.e., without pedagogic intervention) attributed to short stories when the stories were assigned as supplementary reading to the main language-teaching syllabus. To support the objective, a qualitative survey was conducted to collect rich data from a total population purposive sample (N = 55). Through a thematic analysis, the following four principal themes were identified that account for the functions of short stories: language, thinking, gratification, and pedagogy. Subthemes illuminated the nature of the main themes and provided possible causes and effects of their intrinsic recognition by EFL students. The main contributions include the expansion of existing theory regarding the use of short stories for ELT and a conceptualization of literature-based actualization as plausible pedagogic paradigm. The study concludes with a reflection on methodological lucidity and future research directions.}
}
@article{LUONG20241416,
title = {System Architecture for Microservice-Based Data Exchange in the Manufacturing Plant Design Process},
journal = {Procedia CIRP},
volume = {130},
pages = {1416-1421},
year = {2024},
note = {57th CIRP Conference on Manufacturing Systems 2024 (CMS 2024)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.10.260},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124014173},
author = {Tommy Luong and David Hoffmann and Tobias Drees and Alfred Hypki and Bernd Kuhlenkötter},
keywords = {Data Exchange, Microservices, Engineering Process, Scalable Architectures},
abstract = {The data exchange between numerous sub-processes and involved suppliers in the manufacturing plant design process is complex due to the use of proprietary data formats from a variety of vendor-specific tools. Consequently, the current data transfer in the plant design process is characterized by data inconsistencies and increased error rates, requiring additional revisions when incorporating multidisciplinary proprietary data formats. Therefore, this publication proposes a microservice-based system architecture approach that aims to simplify data exchange between cross-manufacturer sub-processes in the manufacturing plant design process. For this purpose, the required components of the system architecture are specified, namely the microservices, an ontology database, and a backbone. The open standard AutomationML is used as the data exchange format, as it converts the information into an object-oriented data structure that summarizes the structure, topology, attributes, and roles of the objects being described. An exemplary processing with an initial set of six microservices is presented to demonstrate the functional overview for a simplified data exchange, covering basic data management functionalities and the conversion from AML to JSON. In order to validate ideas at an early stage of the development process, Python is selected for initial development. For the selection of a suitable software framework, a list of criteria is created to evaluate different solutions to build microservice architectures. A comparison of Flask, Django, and FastAPI, three well-established Python libraries, indicates that FastAPI meets the criteria to cover database integration, security and scalability with its built-in features. The resulting system architecture shows the potential to speed up the manufacturing plant design processes and indicates flexibility and scalability through the use of microservices.}
}
@incollection{HAN2019561,
title = {Natural Language Processing Approaches in Bioinformatics},
editor = {Shoba Ranganathan and Michael Gribskov and Kenta Nakai and Christian Schönbach},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology},
publisher = {Academic Press},
address = {Oxford},
pages = {561-574},
year = {2019},
isbn = {978-0-12-811432-2},
doi = {https://doi.org/10.1016/B978-0-12-809633-8.20463-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128096338204639},
author = {Xu Han and Chee K. Kwoh},
keywords = {Active learning, Biomedical natural language processing, Semantical analysis, Syntactical analysis},
abstract = {In this article, we provide an overview of the natural language processing and its applications to bioinformatics. We describe the historical evolution of NLP, and summarize the common NLP sub-problems in the field, as well as their research progress in the biomedical domain. In addition, we discuss an advanced topic of applying active learning methods into the NLP systems, in order to solve the practical issue of lacking training data in the field.}
}
@article{NGUYEN2025103949,
title = {Retrieve–Revise–Refine: A novel framework for retrieval of concise entailing legal article set},
journal = {Information Processing & Management},
volume = {62},
number = {1},
pages = {103949},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103949},
url = {https://www.sciencedirect.com/science/article/pii/S030645732400308X},
author = {Chau Nguyen and Phuong Nguyen and Le-Minh Nguyen},
keywords = {Retrieval–Revise–Refine framework, Legal article set retrieval, Information retrieval, COLIEE competition, Large language models},
abstract = {The retrieval of entailing legal article sets aims to identify a concise set of legal articles that holds an entailment relationship with a legal query or its negation. Unlike traditional information retrieval that focuses on relevance ranking, this task demands conciseness. However, prior research has inadequately addressed this need by employing traditional methods. To bridge this gap, we propose a three-stage Retrieve–Revise–Refine framework which explicitly addresses the need for conciseness by utilizing both small and large language models (LMs) in distinct yet complementary roles. Empirical evaluations on the COLIEE 2022 and 2023 datasets demonstrate that our framework significantly enhances performance, achieving absolute increases in the macro F2 score by 3.17% and 4.24% over previous state-of-the-art methods, respectively. Specifically, our Retrieve stage, employing various tailored fine-tuning strategies for small LMs, achieved a recall rate exceeding 0.90 in the top-5 results alone—ensuring comprehensive coverage of entailing articles. In the subsequent Revise stage, large LMs narrow this set, improving precision while sacrificing minimal coverage. The Refine stage further enhances precision by leveraging specialized insights from small LMs, resulting in a relative improvement of up to 19.15% in the number of concise article sets retrieved compared to previous methods. Our framework offers a promising direction for further research on specialized methods for retrieving concise sets of entailing legal articles, thereby more effectively meeting the task’s demands.}
}
@article{TRAPPEY2020101027,
title = {Intelligent compilation of patent summaries using machine learning and natural language processing techniques},
journal = {Advanced Engineering Informatics},
volume = {43},
pages = {101027},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2019.101027},
url = {https://www.sciencedirect.com/science/article/pii/S1474034619306007},
author = {Amy J.C. Trappey and Charles V. Trappey and Jheng-Long Wu and Jack W.C. Wang},
keywords = {Artificial intelligence, Machine learning, Natural language processing, Deep learning, Patent analysis},
abstract = {Patents are a type of intellectual property with ownership and monopolistic rights that are publicly accessible published documents, often with illustrations, registered by governments and international organizations. The registration allows people familiar with the domain to understand how to re-create the new and useful invention but restricts the manufacturing unless the owner licenses or enters into a legal agreement to sell ownership of the patent. Patents reward the costly research and development efforts of inventors while spreading new knowledge and accelerating innovation. This research uses artificial intelligence natural language processing, deep learning techniques and machine learning algorithms to extract the essential knowledge of patent documents within a given domain as a means to evaluate their worth and technical advantage. Manual patent abstraction is a time consuming, labor intensive, and subjective process which becomes cost and outcome ineffective as the size of the patent knowledge domain increases. This research develops an intelligent patent summarization methodology using artificial intelligence machine learning approaches to allow patent domains of extremely large sizes to be effectively and objectively summarized, especially for cases where the cost and time requirements of manual summarization is infeasible. The system learns to automatically summarize patent documents with natural language texts for any given technical domain. The machine learning solution identifies technical key terminologies (words, phrases, and sentences) in the context of the semantic relationships among training patents and corresponding summaries as the core of the summarization system. To ensure the high performance of the proposed methodology, ROUGE metrics are used to evaluate precision, recall, accuracy, and consistency of knowledge generated by the summarization system. The Smart machinery technologies domain, under the sub-domains of control intelligence, sensor intelligence and intelligent decision-making provide the case studies for the patent summarization system training. The cases use 1708 training pairs of patents and summaries while testing uses 30 randomly selected patents. The case implementation and verification have shown the summary reports achieve 90% and 84% average precision and recall ratios respectively.}
}
@article{RIVERAZAVALA2020,
title = {The Impact of Pretrained Language Models on Negation and Speculation Detection in Cross-Lingual Medical Text: Comparative Study},
journal = {JMIR Medical Informatics},
volume = {8},
number = {12},
year = {2020},
issn = {2291-9694},
doi = {https://doi.org/10.2196/18953},
url = {https://www.sciencedirect.com/science/article/pii/S2291969420000575},
author = {Renzo {Rivera Zavala} and Paloma Martinez},
keywords = {natural language processing, clinical text, deep learning, long short-term memory, contextual information},
abstract = {Background
Negation and speculation are critical elements in natural language processing (NLP)-related tasks, such as information extraction, as these phenomena change the truth value of a proposition. In the clinical narrative that is informal, these linguistic facts are used extensively with the objective of indicating hypotheses, impressions, or negative findings. Previous state-of-the-art approaches addressed negation and speculation detection tasks using rule-based methods, but in the last few years, models based on machine learning and deep learning exploiting morphological, syntactic, and semantic features represented as spare and dense vectors have emerged. However, although such methods of named entity recognition (NER) employ a broad set of features, they are limited to existing pretrained models for a specific domain or language.
Objective
As a fundamental subsystem of any information extraction pipeline, a system for cross-lingual and domain-independent negation and speculation detection was introduced with special focus on the biomedical scientific literature and clinical narrative. In this work, detection of negation and speculation was considered as a sequence-labeling task where cues and the scopes of both phenomena are recognized as a sequence of nested labels recognized in a single step.
Methods
We proposed the following two approaches for negation and speculation detection: (1) bidirectional long short-term memory (Bi-LSTM) and conditional random field using character, word, and sense embeddings to deal with the extraction of semantic, syntactic, and contextual patterns and (2) bidirectional encoder representations for transformers (BERT) with fine tuning for NER.
Results
The approach was evaluated for English and Spanish languages on biomedical and review text, particularly with the BioScope corpus, IULA corpus, and SFU Spanish Review corpus, with F-measures of 86.6%, 85.0%, and 88.1%, respectively, for NeuroNER and 86.4%, 80.8%, and 91.7%, respectively, for BERT.
Conclusions
These results show that these architectures perform considerably better than the previous rule-based and conventional machine learning–based systems. Moreover, our analysis results show that pretrained word embedding and particularly contextualized embedding for biomedical corpora help to understand complexities inherent to biomedical text.}
}
@article{ZHONG20181707,
title = {Equipment selection knowledge base system for industrial styrene process},
journal = {Chinese Journal of Chemical Engineering},
volume = {26},
number = {8},
pages = {1707-1712},
year = {2018},
note = {Special Issue for the 2017 Process Systems Engineering Annual Meeting AND Special Issue for the 28th Chinese Process Control Conference.},
issn = {1004-9541},
doi = {https://doi.org/10.1016/j.cjche.2017.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S1004954117313897},
author = {Weimin Zhong and Shuming Liu and Feng Wan and Zhi Li},
keywords = {Equipment selection, Ontology technology, Knowledge base system, Styrene process},
abstract = {Equipment selection for industrial process usually requires the extensive participation of industrial experts and technologists, which causes a serious waste of resources. This work presents an equipment selection knowledge base system for industrial styrene process (S-ESKBS) based on the ontology technology. This structure includes a low-level knowledge base and a top-level interactive application. As the core part of the S-ESKBS, the low-level knowledge base consists of the equipment selection ontology library, equipment selection rule set and Pellet inference engine. The top-level interactive application is implemented using S-ESKBS, including the parsing storage layer, inference query layer and client application layer. Case studies for the industrial styrene process equipment selection of an analytical column and an alkylation reactor are demonstrated to show the characteristics and implementability of the S-ESKBS.}
}
@article{WOLFF2025,
title = {Personalized Support in Hereditary Breast and Ovarian Cancer After Genetic Counseling by the Chatbot-Based GENIE Mobile App: Proof-of-Concept Wizard of Oz Study},
journal = {JMIR Formative Research},
volume = {9},
year = {2025},
issn = {2561-326X},
doi = {https://doi.org/10.2196/69115},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X25003828},
author = {Dominik Wolff and Thomas Kupka and Chiara Reichert and Nils Ammon and Steffen Oeltze-Jafra and Beate Vajen},
keywords = {HBOC, hereditary breast and ovarian cancer, Wizard of Oz study, mobile health, evaluation, hereditary diseases},
abstract = {Background
The primary aim of genetic counseling at a human genetics center is to empower individuals at risk for hereditary diseases to make informed decisions regarding their health. In Germany, genetic counseling sessions typically last approximately 1 hour and provide highly personalized information by a specialist in human genetics. Despite this, many counselees report a need for additional support following the counseling session.
Objective
This study introduces GENIE, a chatbot-based mobile app designed to assist individuals in the postcounseling phase, with a focus on hereditary breast and ovarian cancer. GENIE delivers expert-curated, personalized information tailored to the user’s health and family circumstances. The content is presented through predefined dialogs between the user and the mobile assistant, aiming to extend the benefits of genetic counseling beyond the initial session.
Methods
A Wizard of Oz study was conducted to evaluate a functional prototype of GENIE. A total of 6 patients with breast cancer, at least 2 years postdiagnosis, participated in the study. Participants were given access to the app for a minimum of 1 week. The evaluation was based on their interaction with GENIE, which was personalized using the details of a fictitious patient. Data collection included semistructured interviews and a 45-item questionnaire to assess usability and content quality.
Results
The analysis of the interview and questionnaire data indicated high usability for GENIE, with a mean System Usability Score of 75.33 (SD 4.13). In total, 5 of the 6 participants used the app daily; 3 participants were willing to pay between US $5 and US $45 as a single purchase, while the other 3 participants agreed that the app should be free for the user and the costs should be directly covered by health insurance. Still, opinions on the app’s appeal were divided. The layout was seen as moderately professional, a bit crowded, and slightly uninspiring. Nevertheless, participants highlighted the credibility and relevance of the content, noting its alignment with the fictitious patient’s scenario. However, areas for improvement were identified, particularly concerning the app’s design. All participants would recommend the app to other affected persons.
Conclusions
The findings suggest that a mobile app like GENIE can provide valuable support to individuals in the postcounseling phase of genetic services. GENIE offers distinct advantages over large language models, as the information it provides is carefully curated by human experts, minimizing the risk of inaccuracies or hallucinations and significantly enhancing the system’s credibility. This study highlights the need to involve the user group as early as possible in the development of a digital health app. Future work will focus on the implementation of a comprehensive personalization engine, redesign of the user interface, and the execution of a large-scale, 2-arm randomized intervention study to validate GENIE’s effectiveness.}
}
@article{NASER2025100057,
title = {Intuitive tests to validate machine learning models against physics and domain knowledge},
journal = {Digital Engineering},
volume = {7},
pages = {100057},
year = {2025},
issn = {2950-550X},
doi = {https://doi.org/10.1016/j.dte.2025.100057},
url = {https://www.sciencedirect.com/science/article/pii/S2950550X25000238},
author = {M.Z. Naser},
keywords = {Engineering, Artificial intelligence, Explainability},
abstract = {The integration of machine learning (ML) faces a fundamental challenge because current explainability methods provide statistical attribution and lack systematic frameworks for validating explanations against physical laws, constitutive relationships, and engineering principles. From this lens, this work aims to address the following two questions: when physics-informed models are inapplicable/infeasible, how can we relate ML model predictions to established physics principles and engineering domain knowledge, and when are such relationships reliable enough? This review introduces an intuitive hierarchical framework comprising ten validation levels, from fundamental physics compliance to operational context alignment. Our framework addresses critical gaps, including conservation law validation, multiscale physics consistency, temporal dependency verification, and uncertainty quantification in physics-constrained explanations. We also provide comprehensive implementation guidelines for both engineers and ML developers, including decision matrices, risk assessment protocols, and domain-specific validation procedures. This work concludes by identifying fundamental research questions and proposes standardized benchmarks for evaluating physics-informed explainability.}
}
@article{KUIPER2022194768,
title = {The gene regulation knowledge commons: the action area of GREEKC},
journal = {Biochimica et Biophysica Acta (BBA) - Gene Regulatory Mechanisms},
volume = {1865},
number = {1},
pages = {194768},
year = {2022},
issn = {1874-9399},
doi = {https://doi.org/10.1016/j.bbagrm.2021.194768},
url = {https://www.sciencedirect.com/science/article/pii/S1874939921000869},
author = {Martin Kuiper and Joseph Bonello and Jesualdo T. Fernández-Breis and Philipp Bucher and Matthias E. Futschik and Pascale Gaudet and Ivan V. Kulakovskiy and Luana Licata and Colin Logie and Ruth C. Lovering and Vsevolod J. Makeev and Sandra Orchard and Simona Panni and Livia Perfetto and David Sant and Stefan Schulz and Steven Vercruysse and Daniel R. Zerbino and Astrid Lægreid},
keywords = {Knowledge Commons, Biocuration, Ontologies, Computational biology, Text mining, Web services},
abstract = {As computational modeling becomes more essential to analyze and understand biological regulatory mechanisms, governance of the many databases and knowledge bases that support this domain is crucial to guarantee reliability and interoperability of resources. To address this, the COST Action Gene Regulation Ensemble Effort for the Knowledge Commons (GREEKC, CA15205, www.greekc.org) organized nine workshops in a four-year period, starting September 2016. The workshops brought together a wide range of experts from all over the world working on various steps in the knowledge management process that focuses on understanding gene regulatory mechanisms. The discussions between ontologists, curators, text miners, biologists, bioinformaticians, philosophers and computational scientists spawned a host of activities aimed to standardize and update existing knowledge management workflows and involve end-users in the process of designing the Gene Regulation Knowledge Commons (GRKC). Here the GREEKC consortium describes its main achievements in improving this GRKC.}
}
@article{MCGLINN2019235,
title = {Interlinking geospatial and building geometry with existing and developing standards on the web},
journal = {Automation in Construction},
volume = {103},
pages = {235-250},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2018.12.026},
url = {https://www.sciencedirect.com/science/article/pii/S0926580518303078},
author = {Kris McGlinn and Anna Wagner and Pieter Pauwels and Peter Bonsma and Philip Kelly and Declan O’Sullivan},
keywords = {Building information modeling, Geometry, Ontology, Linked Data, Industry foundation classes, Web technologies, Geospatial data},
abstract = {Geometric data plays a central role in the geospatial domain, architectural design and construction industry. For upcoming, new approaches to store building data, such as the Semantic Web, no universal common agreement exists on the combination of geometric and non-geometric data. It can therefore be unclear to users on how to represent their geometries, leading to a decelerated application and advancement of making building data available over the web. This gap can only be bridged if a common approach on the representation of geometries on the web is achieved. To first generate a common understanding of geometry representations, an overview of existing and developing geometry (web) standards needs to be given and discussed, i.e., the Industry Foundation Classes (IFC), CityGML, GeoSPARQL, and the OntoBREP and GEOM ontologies. This discussion needs to consider general contexts, e.g., 2D, 3D, detailed, or tessellated geometries, and specific use cases of the construction industry. Based on these discussions, this paper aims to propose a general recommendation for web-based geometry representations to enhance future applications of building data on the web. Due to the variety of use cases and their requirements, as well as technical constraints based on deviant interpretations of geometry descriptions from different geometry kernels, it became clear, that no approach or standard is generally superior to others. The biggest distinction identified in this paper is posed between the context of visualizing, where simplified, tessellated geometry holds the highest advantage, and (parametric) modeling, which requires semantically detailed geometry representations. Hence, we recommend to interlink non-geometric data with multiple geometry representations, to address all relevant contexts and use cases appropriately. The individual geometry representations should be chosen based on the relevant use cases for an optimal experience when using and exchanging geometry on the web. With this recommendation, the benefits of all discussed approaches can be exploited while avoiding their respective challenges.}
}
@article{MAO2020107094,
title = {Development of process safety knowledge graph: A Case study on delayed coking process},
journal = {Computers & Chemical Engineering},
volume = {143},
pages = {107094},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.107094},
url = {https://www.sciencedirect.com/science/article/pii/S0098135420304786},
author = {Shuai Mao and Yunmeng Zhao and Jinhe Chen and Bing Wang and Yang Tang},
keywords = {Process safety, Knowledge graph, Delayed coking process},
abstract = {Process safety is one of the essential preconditions for the achievement of green manufacturing. The improvement of process safety management requires a comprehensive risk analysis based on the collection of almost all safety related information, which are usually unstructured knowledge and experience. To handle the information and support the risk analysis, a process safety knowledge graph is prompted and the development of domain ontology on delayed coking process is elaborated. The combined top-down and bottom-up approaches are used in defining the process safety schema on ontology level. Several multi-structured data sources are introduced in establishing the process safety knowledge, in which the hazard and operability analysis (HAZOP) reports and process diagrams are most important. The ontology design and data extraction are demonstrated in the manuscript while various related applications are discussed. This process safety knowledge graph might empower the knowledge-based analysis abilities in discovering the hidden relationships between possible risk causes and consequences in an emergency situation, and could provide a foundation for more application related to process safety.}
}
@article{BASSAM2024662,
title = {A Model-Based Methodology for Life Cycle Assessment from Cradle-to-Grave Early in Product Design},
journal = {Procedia CIRP},
volume = {128},
pages = {662-667},
year = {2024},
note = {34th CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.07.058},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124007571},
author = {Hamza Bassam and Pascal Lünnemann and Theresa Riedelsheimer and Kai Lindow},
keywords = {Model-Based Systems Engineering, Sustainable Product Design, Life Cycle Assessment, Systems Modeling Language},
abstract = {The International Council for Systems Engineering (INCOSE) Vision 2035 states that future systems engineers will be required to routinely assess the environmental and societal impact of engineered systems. This paper addresses this need and presents a model-based methodology that expands the V-Model through the integration of Life Cycle Assessment (LCA) and the consideration of all life cycle stages from cradle-to-grave. The methodology is demonstrated using a Systems Modeling Language (SysML) model of a humanoid service robot. The case study demonstrates that Model-Based Systems Engineering (MBSE) can support in assessing the environmental impact of a product from cradle-to-grave in early design stages. It, therefore, enables systems engineers to reduce environmental impact by identifying harmful components early on and adapting the product life cycle, when design changes are easy and incurred costs are low. This can support in verifying sustainability requirements and decision-making between different product variants to find the optimal sustainable solution. Finally, the case study is evaluated, limitations are discussed and recommendations for future research are provided.}
}
@article{HOSSAIN202025,
title = {Knowledge-driven machine learning based framework for early-stage disease risk prediction in edge environment},
journal = {Journal of Parallel and Distributed Computing},
volume = {146},
pages = {25-34},
year = {2020},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2020.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0743731520303324},
author = {M. Anwar Hossain and Rahatara Ferdousi and Mohammed F. Alhamid},
keywords = {Machine learning, Epidemiology, Self-screening, Healthcare, Disease likelihood},
abstract = {Early-stage disease risk prediction can be beneficial to improve the health of the mass and can reduce the economic burden of late treatment. Machine learning has played a pivotal role in predictive systems, which requires achieving a specific degree of accuracy for healthcare systems. Most recently researchers have found the necessity of bridging between epidemiology and machine learning classifications toward health risk prediction. This work proposes an epidemiology knowledge-driven unique model that follows the principle of association rule-based ontology to select features and classification techniques. The goal of this approach is to generalize a framework for future robust systems to predict the likelihood of diseases, which can be executed in the edge computing environment. The framework introduces epidemiological library and structured attribute set along with the library of precaution to derive the disease risk-prediction process. To investigate the adoption of the epidemiology knowledge-driven model, we considered a real dataset of early-stage likelihood prediction of diabetes and carried out a set of experiments for highlighting the significance of several epidemiological factors. The classification aspect of the framework is further compared with widely accepted approaches for machine learning based healthcare, which shows the novelty of the proposed model.}
}
@article{RIAZI2023100065,
title = {Trustworthiness in L2 writing research: A review and analysis of qualitative articles in the Journal of Second Language Writing},
journal = {Research Methods in Applied Linguistics},
volume = {2},
number = {3},
pages = {100065},
year = {2023},
issn = {2772-7661},
doi = {https://doi.org/10.1016/j.rmal.2023.100065},
url = {https://www.sciencedirect.com/science/article/pii/S2772766123000253},
author = {A. Mehdi Riazi and Reza Rezvani and Hessameddin Ghanbar},
keywords = {Research methodology, Qualitative research, L2 writing, Trustworthiness, Quality and rigor in research},
abstract = {Researchers in applied linguistics have long been concerned with issues of quality and rigor. Because of various paradigmatic tensions, many qualitative researchers have sought to move away from the entrenched concepts of reliability, validity, and objectivity traditionally associated with quantitative research methodologies. Given the significance of quality in research methodology and the wide variation in evaluative nomenclature, we set out to investigate how “trustworthiness” is addressed by L2 writing researchers in their published articles in the Journal of Second Language Writing. We first identified all empirical articles in the JSLW since its inception (June 1992) up to volume 56 (June 2022), assigning them to one of three research approaches: quantitative, qualitative, or mixed methods. We found 389 articles that used primary data for analysis. From this pool, 152 (39%) were coded as qualitative in terms of their data and methods. Using AntConc (Anthony, 2014), 43 articles were found to have used quality-related terms or techniques. We then developed a coding and annotation scheme to code and annotate all 43 articles. Findings show that although researchers addressed research quality in different ways, there is a widespread lack of systematic attention to quality criteria in L2 writing qualitative studies. We hope the study findings and discussion of this review advance Applied Linguistics qualitative research and understandings of trustworthiness. In particular, we hope both early career and experienced researchers will find the insights generated by this study useful when designing, conducting, and publishing qualitative research.}
}
@article{LASSILA20181,
title = {Mapping mineral resources in a living land: Sami mining resistance in Ohcejohka, northern Finland},
journal = {Geoforum},
volume = {96},
pages = {1-9},
year = {2018},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2018.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0016718518302082},
author = {Maija M. Lassila},
keywords = {Mining, Extractivism, Indigenous peoples, Counter-narrative, Counter-mapping, Ontological conflict},
abstract = {During the past decade, Finland has been the target of a global boom in the quest for untapped mineral resources. Based on the mapped information of mineral potential provided by the state, multinational mining corporations are making reservations for and conducting mineral explorations particularly in Finland’s peripheral regions. This paper investigates the emergence of an anti-mining movement in Ohcejohka, in northernmost Finland, in 2014–2015, and the ontological conflict manifested in the outside mapping of the land as “mineral rich” as well as the local people's various knowledges of the land as a lived place. By producing a holistic counter-mapping of their social, ancestral and meaningful landscape, the movement questioned the state’s and the company’s homogenising knowledge in the production of land and resources. While the reality-making effects of modern maps have previously been studied, the entanglements of such mappings in environmental conflicts with local ontological realities and knowledge spheres have not been extensively studied. This paper argues that rather than imposing a “one world ontology”, maps and mappings of land and resources are culmination points in environmental conflicts, where they become renegotiated, challenged and redefined in the local and dynamic enactments of reality.}
}
@article{PENG2023119901,
title = {Building a knowledge graph for operational hazard management of utility tunnels},
journal = {Expert Systems with Applications},
volume = {223},
pages = {119901},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.119901},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423004025},
author = {Fang-Le Peng and Yong-Kang Qiao and Chao Yang},
keywords = {Knowledge graph, Utility tunnels, Operational hazard management, Decision support},
abstract = {The operational hazard management of the utility tunnel infrastructure is crucial for sustainable and resilient urban development. It not only involves various domain expertise such as safety, pipeline, structure, equipment, and electricity, but also requires timely and efficient access to and decision support from the relevant information of critical resources. However, current operational hazard management practices rely mainly on the experience of engineers, rendering it difficult to ensure management quality, thus increasing the risk of utility tunnel accidents. Hence, a knowledge graph-based decision support approach is proposed herein for the operational hazard management of utility tunnels, which illustrates the ontology design and information extraction from structured field inspection data and unstructured normative text files. The experimental case study demonstrates that this approach can improve the hazard management capabilities of utility tunnels by providing decision aids such as query, analysis, and control measure feedback.}
}
@article{KPODO2025426,
title = {Navigating challenges/opportunities in developing smart agricultural extension platforms: Multi-media data mining techniques},
journal = {Artificial Intelligence in Agriculture},
volume = {15},
number = {3},
pages = {426-448},
year = {2025},
issn = {2589-7217},
doi = {https://doi.org/10.1016/j.aiia.2025.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2589721725000418},
author = {Josué Kpodo and A. Pouyan Nejadhashemi},
keywords = {Agriculture extension, Artificial intelligence, Multi-media data mining, Decision-making, Large language models},
abstract = {Agricultural Extension (AE) research faces significant challenges in producing relevant and practical knowledge due to rapid advancements in artificial intelligence (AI). AE struggles to keep pace with these advancements, complicating the development of actionable information. One major challenge is the absence of intelligent platforms that enable efficient information retrieval and quick decision-making. Investigations have shown a shortage of AI-assisted solutions that effectively use AE materials across various media formats while preserving scientific accuracy and contextual relevance. Although mainstream AI systems can potentially reduce decision-making risks, their usage remains limited. This limitation arises primarily from the lack of standardized datasets and concerns regarding user data privacy. For AE datasets to be standardized, they must satisfy four key criteria: inclusion of critical domain-specific knowledge, expert curation, consistent structure, and acceptance by peers. Addressing data privacy issues involves adhering to open-access principles and enforcing strict data encryption and anonymization standards. To address these gaps, a conceptual framework is introduced. This framework extends beyond typical user-oriented platforms and comprises five core modules. It features a neurosymbolic pipeline integrating large language models with physically based agricultural modeling software, further enhanced by Reinforcement Learning from Human Feedback. Notable aspects of the framework include a dedicated human-in-the-loop process and a governance structure consisting of three primary bodies focused on data standardization, ethics and security, and accountability and transparency. Overall, this work represents a significant advancement in agricultural knowledge systems, potentially transforming how AE services deliver critical information to farmers and other stakeholders.}
}
@article{DASHTBANMOGHADAM2024148620,
title = {Hippocampal tandem mass tag (TMT) proteomics analysis during kindling epileptogenesis in rat},
journal = {Brain Research},
volume = {1822},
pages = {148620},
year = {2024},
issn = {0006-8993},
doi = {https://doi.org/10.1016/j.brainres.2023.148620},
url = {https://www.sciencedirect.com/science/article/pii/S0006899323003918},
author = {Elahe Dashtban-Moghadam and Shima Khodaverdian and Bahareh Dabirmanesh and Javad Mirnajafi-Zadeh and Amir Shojaei and Mehdi Mirzaie and Peyman Choopanian and Mona Atabakhshi-Kashi and Yaghoub Fatholahi and Khosro Khajeh},
keywords = {Proteomics, Epileptogenesis, LC-MS/MS, Electrical kindling, Hippocampus},
abstract = {Epilepsy is a neurological disorder that remains difficult to treat due to the lack of a clear molecular mechanism and incomplete understanding of involved proteins. To identify potential therapeutic targets, it is important to gain insight into changes in protein expression patterns related to epileptogenesis. One promising approach is to analyze proteomic data, which can provide valuable information about these changes. In this study, to evaluate the changes in gene expression during epileptogenesis, LC-MC2 analysis was carried out on hippocampus during stages of electrical kindling in rat models. Subsequently, progressive changes in the expression of proteins were detected as a result of epileptogenesis development. In line with behavioral kindled seizure stages and according to the proteomics data, we described epileptogenesis phases by comparing Stage3 versus Control (S3/C0), Stage5 versus Stage3 (S5/S3), and Stage5 versus Control group (S5/C0). Gene ontology analysis on differentially expressed proteins (DEPs) showed significant changes of proteins involved in immune responses like Csf1R, Aif1 and Stat1 during S3/C0, regulation of synaptic plasticity like Bdnf, Rac1, CaMK, Cdc42 and P38 during S5/S3, and nervous system development throughout S5/C0 like Bdnd, Kcc2 and Slc1a3.There were also proteins like Cox2, which were altered commonly among all three phases. The pathway enrichment analysis of DEPs was also done to discover molecular connections between phases and we have found that the targets like Csf1R, Bdnf and Cox2 were analyzed throughout all three phases were highly involved in the PPI network analysis as hub nodes. Additionally, these same targets underwent changes which were confirmed through Western blotting. Our results have identified proteomic patterns that could shed light on the molecular mechanisms underlying epileptogenesis which may allow for novel targeted therapeutic strategies.}
}
@article{ZGHEIB2023160,
title = {Towards an ML-based semantic IoT for pandemic management: A survey of enabling technologies for COVID-19},
journal = {Neurocomputing},
volume = {528},
pages = {160-177},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223000139},
author = {Rita Zgheib and Ghazar Chahbandarian and Firuz Kamalov and Haythem El Messiry and Ahmed Al-Gindy},
keywords = {COVID-19, Machine learning, Ontologies, Internet of things, Cloud architecture, Survey},
abstract = {The connection between humans and digital technologies has been documented extensively in the past decades but needs to be evaluated through the current global pandemic. Artificial Intelligence(AI), with its two strands, Machine Learning (ML) and Semantic Reasoning, has proven to be a great solution to provide efficient ways to prevent, diagnose and limit the spread of COVID-19. IoT solutions have been widely proposed for COVID-19 disease monitoring, infection geolocation, and social applications. In this paper, we investigate the usage of the three technologies for handling the COVID-19 pandemic. For this purpose, we surveyed the existing ML applications and algorithms proposed during the pandemic to detect COVID-19 disease using symptom factors and image processing. The survey includes existing approaches including semantic technologies and IoT systems for COVID-19. Based on the survey result, we classified the main challenges and the solutions that could solve them. The study proposes a conceptual framework for pandemic management and discusses challenges and trends for future research.}
}
@article{RODRIGUEZTORREALBA20251683,
title = {Joint Generation of Distractors for Multiple-Choice Questions: A Text-to-Text Approach},
journal = {Computers, Materials and Continua},
volume = {83},
number = {2},
pages = {1683-1705},
year = {2025},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2025.062004},
url = {https://www.sciencedirect.com/science/article/pii/S1546221825003200},
author = {Ricardo Rodriguez-Torrealba and Eva Garcia-Lopez and Antonio Garcia-Cabot},
keywords = {Text-to-text, distractor generation, fine-tuning, FlanT5, LongT5, multiple-choice, questionnaire},
abstract = {Generation of good-quality distractors is a key and time-consuming task associated with multiple-choice questions (MCQs), one of the assessment items that have dominated the educational field for years. Recent advances in language models and architectures present an opportunity for helping teachers to generate and update these elements to the required speed and scale of widespread increase in online education. This study focuses on a text-to-text approach for joints generation of distractors for MCQs, where the context, question and correct answer are used as input, while the set of distractors corresponds to the output, allowing the generation of three distractors in a single model inference. By fine-tuning FlanT5 models and LongT5 with TGlobal attention using a RACE-based dataset, the potential of this approach is explored, demonstrating an improvement in the BLEU and ROUGE-L metrics when compared to previous works and a GPT-3.5 baseline. Additionally, BERTScore is introduced in the evaluation, showing that the fine-tuned models generate distractors semantically close to the reference, but the GPT-3.5 baseline still outperforms in this area. A tendency toward duplicating distractors is noted, although models fine-tuned with Low-Rank Adaptation (LoRA) and 4-bit quantization showcased a significant reduction in duplicated distractors.}
}
@article{NIE2023,
title = {Research on the Construction and Application of Knowledge Graph in the Ceramic Field Based on Natural Language Processing},
journal = {International Journal on Semantic Web and Information Systems},
volume = {19},
number = {1},
year = {2023},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.327352},
url = {https://www.sciencedirect.com/science/article/pii/S1552628323000340},
author = {Yu Nie and Na Huang and Junjie Peng and Guanghua Song and Yilai Zhang and Yongkang Peng and Chenglin Ni},
keywords = {Ceramics Field, Entity Recognition, Knowledge Graph, Natural Language Processing, Relationship Recognition},
abstract = {ABSTRACT
There are problems of knowledge deficiency and effective unified expression of knowledge in the process of relevant knowledge data acquired by workers in the ceramic domain. In this study, the authors designed relevant experiments to construct ceramic field knowledge graphs to solve these problems. In the experiments of named entity recognition and relationship recognition, the authors compared the performance of several models in OwnThink and ceramics field datasets. The experimental results showed that the BiLSTM-CRF model is the best for named entity recognition and the TextCNN model is the best for relationship recognition in ceramics field datasets. Therefore, the first used the BiLSTM-CRF model to complete the naming entity recognition and then combined with the TextCNN model to complete the relationship recognition to construct the ceramic field knowledge graph. Then, they applied the constructed graph to the ceramic knowledge Q&A service to provide accurate data retrieval service for ceramic domain workers.}
}
@article{TORRESSILVA2020,
title = {XML Data and Knowledge-Encoding Structure for a Web-Based and Mobile Antenatal Clinical Decision Support System: Development Study},
journal = {JMIR Formative Research},
volume = {4},
number = {10},
year = {2020},
issn = {2561-326X},
doi = {https://doi.org/10.2196/17512},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X20000219},
author = {Ever Augusto {Torres Silva} and Sebastian Uribe and Jack Smith and Ivan Felipe {Luna Gomez} and Jose Fernando Florez-Arango},
keywords = {clinical decision support systems, computer-interpretable guidelines, knowledge representation, state machine, system design, XML},
abstract = {Background
Displeasure with the functionality of clinical decision support systems (CDSSs) is considered the primary challenge in CDSS development. A major difficulty in CDSS design is matching the functionality to the desired and actual clinical workflow. Computer-interpretable guidelines (CIGs) are used to formalize medical knowledge in clinical practice guidelines (CPGs) in a computable language. However, existing CIG frameworks require a specific interpreter for each CIG language, hindering the ease of implementation and interoperability.
Objective
This paper aims to describe a different approach to the representation of clinical knowledge and data. We intended to change the clinician’s perception of a CDSS with sufficient expressivity of the representation while maintaining a small communication and software footprint for both a web application and a mobile app. This approach was originally intended to create a readable and minimal syntax for a web CDSS and future mobile app for antenatal care guidelines with improved human-computer interaction and enhanced usability by aligning the system behavior with clinical workflow.
Methods
We designed and implemented an architecture design for our CDSS, which uses the model-view-controller (MVC) architecture and a knowledge engine in the MVC architecture based on XML. The knowledge engine design also integrated the requirement of matching clinical care workflow that was desired in the CDSS. For this component of the design task, we used a work ontology analysis of the CPGs for antenatal care in our particular target clinical settings.
Results
In comparison to other common CIGs used for CDSSs, our XML approach can be used to take advantage of the flexible format of XML to facilitate the electronic sharing of structured data. More importantly, we can take advantage of its flexibility to standardize CIG structure design in a low-level specification language that is ubiquitous, universal, computationally efficient, integrable with web technologies, and human readable.
Conclusions
Our knowledge representation framework incorporates fundamental elements of other CIGs used in CDSSs in medicine and proved adequate to encode a number of antenatal health care CPGs and their associated clinical workflows. The framework appears general enough to be used with other CPGs in medicine. XML proved to be a language expressive enough to describe planning problems in a computable form and restrictive and expressive enough to implement in a clinical system. It can also be effective for mobile apps, where intermittent communication requires a small footprint and an autonomous app. This approach can be used to incorporate overlapping capabilities of more specialized CIGs in medicine.}
}
@article{CARDINALE202264,
title = {Semantic framework of event detection in emergency situations for smart buildings},
journal = {Digital Communications and Networks},
volume = {8},
number = {1},
pages = {64-79},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2021.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S235286482100033X},
author = {Yudith Cardinale and Gabriel Freites and Edgar Valderrama and Ana Aguilera and Chinnapong Angsuchotmetee},
keywords = {Multimedia sensor network, Semantic web, Event processing, Ontology, Geolocalisation},
abstract = {Multimedia Sensor Networks (MSNs) have enhanced the ability to analyze the environment and provide responses based on its current status. Generally, MSNs are composed of scalar and multimedia sensors that have fixed locations. However, given the advancement of smart mobile device technologies, it is currently possible to dynamically integrate mobile sensors into MSNs. In this paper, we propose a formal platform to manage MSNs and the data gathered from them to detect complex events. Our main contributions include: M2SSN ​− ​Onto, a Mobile and Multimedia Semantic Sensor Networks Ontology; Py-CEMiD, an engine for detecting complex events and generate reactions to them; a mobile device location engine to locate mobile sensors; and a proof-of-concept in the context of detecting emergency situations in smart buildings. Several scenarios are validated for emergency events, combining simulated sensor measurements with real measurements of mobile devices. Results show complex events can be detected in near real time (less than 1 ​s).}
}
@article{PEREZVEREDA2022101718,
title = {Digital Avatars: A programming framework for personalized human interactions through virtual profiles},
journal = {Pervasive and Mobile Computing},
volume = {87},
pages = {101718},
year = {2022},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2022.101718},
url = {https://www.sciencedirect.com/science/article/pii/S1574119222001316},
author = {Alejandro Perez-Vereda and Ramon Hervas and Carlos Canal},
keywords = {Social computing, Mild cognitive impairment, People as a service, Digital Avatars, Independent living, Virtual profiles},
abstract = {Technology is evolving in the direction of making the users a key part in the model, developing services that adapt to their preferences and needs in a seamless way. So, information and knowledge about people has become one of the main actives for the IT industry. However, only a few major companies are presently able to take advantage of the digital footprint everyone leaves while performing their daily activities. Moreover, people cannot even decide when, how, and by whom the contents and information they are producing are used. In this work, we present the Digital Avatars framework for mobile-based collaborative social computing applications as a realization of People as a Service, a mobile computing model that empowers smartphones with the capability of learning virtual profiles or digital avatars of their owners and sharing them as a service in a controlled and seamless way. Digital Avatars offers services for third party applications, including an inference engine and a module for run-time scripts execution for interacting with the digital avatar in the smartphone assuring the privacy and full control of the users over their data. In this paper, we describe the architecture of this framework and the structure of the digital avatars focusing on an independent living scenario for Mild Cognitive Impairment patients.}
}
@article{JURIC2021563,
title = {A Platform and Algorithms for Interoperability Between Clinical Coding Systems},
journal = {Procedia Computer Science},
volume = {192},
pages = {563-572},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.058},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921015453},
author = {Damir Juric and David Geleta and Gregory McKay and Giorgos Stoilos},
keywords = {clinical codes, knowledge base, ontologies, ontology alignment},
abstract = {A large number of conceptually different medical coding and classification systems are in use in medical practice, of which the most popular are ICD-9, ICD-10, and Read Codes. To achieve interoperability, a platform that enables translation between them is needed. In this paper we report on the progress of Babylon Health in developing such an interoperability platform. Based on state-of-the-art partial mapping approaches, we propose a methodology for integrating coding systems into Babylon’s medical Knowledge Graph and propose a code translation algorithm in which this Knowledge Graph acts as a mediator.}
}
@article{CAPOCCHI2020102122,
title = {Discrete-Event Simulation Model Generation based on Activity Metrics},
journal = {Simulation Modelling Practice and Theory},
volume = {103},
pages = {102122},
year = {2020},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2020.102122},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X20300617},
author = {Laurent Capocchi and Jean François Santucci and Thorsten Pawletta and Hendrik Folkerts and Bernard P. Zeigler},
keywords = {Activity, Complexity, DEVS, Discrete-event, Model generation, Modeling, Simulation, System entity structure},
abstract = {System entity structure has been used since the 1970s as a formal ontology framework, axiomatically defined, to represent the elements of a system of systems and their hierarchical relationships resulting in a family of hierarchical models. One challenge with this approach is the process of exploring a family of hierarchical models, and selecting a particular composition from which a fit-for-purpose discrete-event simulation model can be automatically synthesized and executed. This paper deals with the definition of performance metrics which are used to guide the Modeler to select the most practical model of a real-world system from among a potentially large set. The discrete-event simulation model is selected among a set of system entity structure family models by using model level activity metrics in order to be able to have a prediction of the accuracy and the performance of the models. A case study is presented to illustrate and validate the relation between activity metrics and system entity structure concepts.}
}
@article{OCHOA20241503,
title = {Enhancing Flexibility in Industry 4.0 Workflows: A Context-Aware Component for Dynamic Service Orchestration},
journal = {Procedia Computer Science},
volume = {232},
pages = {1503-1512},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.01.148},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924001480},
author = {William Ochoa and Felix Larrinaga and Alain Perez and Javier Cuenca},
keywords = {Context-Awareness, Workflow Management, Smart Manufacturing, Service Re-selection, Semantic Web},
abstract = {Manufacturing processes of the future will rely on standards for asset interoperability and service orchestration. The Asset Administration Shell (AAS) facilitates information exchange among Industry 4.0 assets, while standardized Business Processes enable workflow execution in manufacturing systems. Combining these technologies provides agility and scalability to manufacturing systems by incorporating asset services within business processes. Service orchestration involves coordinating multiple services, which must be dynamic during runtime to manage unforeseen situations that may arise during the manufacturing process. Context information plays a crucial role in identifying such scenarios and selecting the most suitable devices/services in response, and the Semantic Web accurately represents this information. This paper proposes a context-aware approach for service orchestration using industrial asset services. Our contributions include (1) a component for Context-Aware Service Re-Selection. (2) a domain-specific ontology (DeviceServiceOnt) for Semantic Web-based context representation. And, (3) validation of our proposal in a manufacturing setting where robots are responsible for dispatching and distributing materials within a warehouse. Opportunities for future work are also highlighted, with a primary focus on enhancing workflow dynamicity with context-aware capabilities.}
}
@article{FERNANDES2024102426,
title = {Why traditional firms from the same industry reject digital transformation: Structural constraints of perception and attention},
journal = {Long Range Planning},
volume = {57},
number = {2},
pages = {102426},
year = {2024},
issn = {0024-6301},
doi = {https://doi.org/10.1016/j.lrp.2024.102426},
url = {https://www.sciencedirect.com/science/article/pii/S002463012400013X},
author = {Erik Fernandes and Ana Burcharth},
keywords = {Digital transformation, Attention-based view, Dynamic capabilities, Sensing capability, Cognition},
abstract = {We explain why some traditional companies fail to sense new digital technologies when facing an identical scenario of digital transformation. Our objective is to investigate situations where discontinuous changes steaming from digital transformation are actively rejected, in the sense that they are not perceived as a strategic issue, i.e., a threat or opportunity. We draw on a mixed-method research design comprising two sequential studies. The first study is based on Delphi's Technique, which uses a panel of specialists to build the most likely future scenarios in the medium term for the language education industry. The second one is a qualitative comparative study with eleven traditional firms. Their senior executives were first asked for their spontaneous sensing of emerging technologies and later asked to provide their assessment of the most likely future scenarios. Our contribution lies in developing a conceptual model that proposes a structural “schema-driven” explanation of why firm-level structures – concrete, contextual and knowledge – can hinder perception and attention. Active rejection is prompted not by the absence of attentional structures, but by their specific attributes. This expands the dominant ontology of issues, asserting their existence independently of an organization's epistemological experience, and adds to the theoretical understanding regarding the constraints of the sensing dynamic capability in digital transformation.}
}
@article{SHKEMBI2023100809,
title = {Semantic Web and blockchain technologies: Convergence, challenges and research trends},
journal = {Journal of Web Semantics},
volume = {79},
pages = {100809},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2023.100809},
url = {https://www.sciencedirect.com/science/article/pii/S1570826823000380},
author = {Klevis Shkembi and Petar Kochovski and Thanasis G. Papaioannou and Caroline Barelle and Vlado Stankovski},
keywords = {Semantic Web, Blockchain, Trust, Knowledge management, Ontology},
abstract = {In recent years, on the one hand, we have witnessed the rise of blockchain technology, which has led to better transparency, traceability, and therefore, trustworthy exchange of digital assets among different actors. On the other hand, achieving trustworthy content exchange has been one of the primary objectives of the Semantic Web, part of the World Wide Web Consortium. Semantic Web and blockchain technologies are the fundamental building blocks of Web3 (the third version of the Internet), which aims to link data through a decentralized approach. Blockchain provides a decentralized and secure framework for users to safeguard their data and take control over their data and Web3 experiences. However, developing trustworthy decentralized applications (Dapps) is a challenge because many blockchain-based functionalities must be developed from scratch, and combined with data semantics to open new innovative opportunities. In this survey paper, we explore the cross-cutting domain of the Semantic Web and blockchain and identify the critical building blocks required to achieve trust in the Next-Generation Internet. The application domains that could benefit from these technologies are also investigated. We developed a deep analysis of the published literature between 2015 and 2023. We performed our analysis in different digital libraries (e.g., Elsevier, IEEE, ACM), and as a result of our research, we retrieved 137 papers, of which 97 were retrieved as relevant to include in the paper. Furthermore, we studied several aspects (e.g., network type, transactions per second) of existing blockchain platforms. Semantic Web and blockchain technologies can be used to realize a verification and certification process for data quality. Examples of mechanisms to achieve this are the Decentralized Identities of the Semantic Web or the various blockchain consensus protocols that help achieve decentralization and realize democratic principles. Therefore, Semantic Web and blockchain technologies should be combined to achieve trust in the highly decentralized, semantically complex, and dynamic environments needed to build smart applications of the future.}
}
@article{BELLOMARINI2022101528,
title = {Vadalog: A modern architecture for automated reasoning with large knowledge graphs},
journal = {Information Systems},
volume = {105},
pages = {101528},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2020.101528},
url = {https://www.sciencedirect.com/science/article/pii/S0306437920300351},
author = {Luigi Bellomarini and Davide Benedetto and Georg Gottlob and Emanuel Sallinger},
keywords = {Knowledge graphs, Reasoning, Query answering, Datalog, Vadalog},
abstract = {The introduction of novel Datalog +/- fragments with good theoretical properties, together with the growing use of enterprise knowledge graphs motivated the development of Vadalog, a knowledge graph management system developed at the University of Oxford. It adopts Warded Datalog +/- as the core of its language for knowledge representation and reasoning, which exhibits a very good tradeoff between computational complexity of reasoning and expressive power, capturing PTIME data complexity while allowing ontological reasoning and full recursion. In this paper, we provide a detailed illustration of the Vadalog system, presenting: the essentials of the first implementation of Warded Datalog +/-; a comprehensive overview of the architecture with specific focus on runtime execution model, memory management, graph traversal strategies and join algorithms; and a detailed experimental evaluation. This paper is a substantially expanded version of the AMW 2019 paper “Datalog-based reasoning for Knowledge Graphs”. To stand apart from previous works on the topic, our focus in this work shall be a comprehensive presentation of the architecture of the Vadalog system and showing how our techniques work together to provide a full-fledged KGMS. In particular, roughly half of this paper is new material created particularly for this comprehensive architectural view. This includes a new series of experiments designed to shed light on architectural choices and alternatives.}
}
@article{ALI2021105973,
title = {Traffic accident detection and condition analysis based on social networking data},
journal = {Accident Analysis & Prevention},
volume = {151},
pages = {105973},
year = {2021},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2021.105973},
url = {https://www.sciencedirect.com/science/article/pii/S000145752100004X},
author = {Farman Ali and Amjad Ali and Muhammad Imran and Rizwan Ali Naqvi and Muhammad Hameed Siddiqi and Kyung-Sup Kwak},
keywords = {Traffic accident detection, Traffic accident analysis, Traffic monitoring system, Ontology, Bi-LSTM},
abstract = {Accurate detection of traffic accidents as well as condition analysis are essential to effectively restoring traffic flow and reducing serious injuries and fatalities. This goal can be obtained using an advanced data classification model with a rich source of traffic information. Several systems based on sensors and social networking platforms have been presented recently to detect traffic events and monitor traffic conditions. However, sensor-based systems provide limited information, and may fail owing to the long detection times and high false-alarm rates. In addition, social networking data are unstructured, unpredictable, and contain idioms, jargon, and dynamic topics. The machine learning algorithms utilized for traffic event detection might not extract valuable information from social networking data. In this paper, a social network–based, real-time monitoring framework is proposed for traffic accident detection and condition analysis using ontology and latent Dirichlet allocation (OLDA) and bidirectional long short-term memory (Bi-LSTM). First, the query-based search engine effectively collects traffic information from social networks, and the data preprocessing module transforms it into structured form. Second, the proposed OLDA-based topic modeling method automatically labels each sentence (e.g., traffic or non-traffic) to identify the exact traffic information. In addition, the ontology-based event recognition approach detects traffic events from traffic-related data. Next, the sentiment analysis technique identifies the polarity of traffic events employing user’s opinions, which helps determine accurate conditions of traffic events. Finally, the FastText model and Bi-LSTM with softmax regression are trained for traffic event detection and condition analysis. The proposed framework is evaluated using traffic-related data, comparing OLDA and Bi-LSTM with existing topic modeling methods and traditional classifiers using word embedding models, respectively. Our system outperforms state-of-the-art methods and achieves accuracy of 97 %. This finding demonstrates that the proposed system is more efficient for traffic event detection and condition analysis, in comparison to other existing systems.}
}
@incollection{YUAN2020345,
title = {Space–Time Modeling 2: Geographic Information System Science Approaches},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {345-353},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10617-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955106171},
author = {May Yuan},
keywords = {Change, Conceptualization, Data modeling, Dynamics, Event, Lifeline, Movement, Process, Representation, Spatiotemporal, Temporal GIS, Trajectory},
abstract = {Distinguished from mathematical modeling of space and time, a Geographic Information Science (GIScience) approach addresses conceptualization, representation, and database modeling of reality to support spatiotemporal query and analysis in Geographic Information Systems (GISs). Much GIScience progress in space–time modeling parallels to the development of database management; however, the added complexity of geographic properties and structures presents a wide range of challenges to properly and sufficiently conceptualize space and time in geographic worlds and to implement the conceptualization in a GIS environment. The fundamental question of what constitutes a geographic object and how to recognize and handle the persistency of the same object in GIS databases is nontrivial in GIScience. Each approach to space–time modeling of geographic information has strengths and weaknesses. Similar to the handling of time in database systems, space–time modeling in GIScience employs the concepts of valid time, transaction time, and user-defined time to support historical queries about geographic worlds or database versioning. Furthermore, much research progress contributes to the ontological foundation of space–time and capture events, processes, and dynamics in geographic domains. The existing GIScience developments in space–time modeling provide a fruitful ground for the recent popularity of temporal GIS applications and subserve a sound foundation for future research directions.}
}
@article{BALANDINA201877,
title = {Dependency Parsing of Natural Russian Language with Usage of Semantic Mapping Approach},
journal = {Procedia Computer Science},
volume = {145},
pages = {77-83},
year = {2018},
note = {Postproceedings of the 9th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2018 (Ninth Annual Meeting of the BICA Society), held August 22-24, 2018 in Prague, Czech Republic},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918322981},
author = {Anita Balandina and Anastasiya Kostkina and Artem Chernyshov and Valentin V. Klimov},
keywords = {dependency parsing, natural language processing, natural language understanding, semantic map},
abstract = {This article discusses the practical implementation of a linguistic processor that solves the task of parsing dependencies. Within this paper, we investigated various modern developments on the ability to adequately parse natural language sentences in Russian. As a result, we suggest the new method of dependency parsing based on BiLSTM neural networks. The comparative analysis showed that suggested method shows the best results than other parsers. We are going to improve our algorithm by appending the semantic analysis with the usage of semantic mapping for better understanding the intentions of sentences.}
}
@article{LI2022101884,
title = {Performance benchmark on semantic web repositories for spatially explicit knowledge graph applications},
journal = {Computers, Environment and Urban Systems},
volume = {98},
pages = {101884},
year = {2022},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2022.101884},
url = {https://www.sciencedirect.com/science/article/pii/S0198971522001284},
author = {Wenwen Li and Sizhe Wang and Sheng Wu and Zhining Gu and Yuanyuan Tian},
keywords = {Triple store, Property graph databases, Ontology, Knowledge graph, Relational database, Ontology-based Data Access (OBDA)},
abstract = {Knowledge graph has become a cutting-edge technology for linking and integrating heterogeneous, cross-domain datasets to address critical scientific questions. As big data has become prevalent in today's scientific analysis, semantic data repositories that can store and manage large knowledge graph data have become critical in successfully deploying spatially explicit knowledge graph applications. This paper provides a comprehensive evaluation of the popular semantic data repositories and their computational performance in managing and providing semantic support for spatial queries. There are three types of semantic data repositories: (1) triple store solutions (RDF4j, Fuseki, GraphDB, Virtuoso), (2) property graph databases (Neo4j), and (3) an Ontology-Based Data Access (OBDA) approach (Ontop). Experiments were conducted to compare each repository's efficiency (e.g., query response time) in handling geometric, topological, and spatial-semantic related queries. The results show that Virtuoso achieves the overall best performance in both non-spatial and spatial-semantic queries. The OBDA solution, Ontop, has the second-best query performance in spatial and complex queries and the best storage efficiency, requiring the least data-to-RDF conversion efforts. Other triple store solutions suffer from various issues that cause performance bottlenecks in handling spatial queries, such as inefficient memory management and lack of proper query optimization.}
}
@article{PENG2024102752,
title = {Knowledge graph-based mapping and recommendation to automate life cycle assessment},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102752},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102752},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624004002},
author = {Tao Peng and Lu Gao and Reuben S.K. Agbozo and Yuming Xu and Kateryna Svynarenko and Qi Wu and Changpeng Li and Renzhong Tang},
keywords = {Life cycle assessment, Life cycle inventory, Automated LCA, Knowledge graph, Environmental impact},
abstract = {The increasing global attention on environmental issues has heightened the demand for a quantitative evaluation of environmental impacts, which primarily relies on using Life Cycle Assessment (LCA). Despite the availability of foreground data from enterprises and the use of existing LCA software to streamline the executing LCA process, LCA practitioners still grapple with the time-consuming task of querying domain knowledge, selecting background data, and entering numerous parameters into the LCA software. To enhance the efficiency and overall performance of LCA-based evaluations, this paper introduces a knowledge graph-based method towards automated LCA. This method aims to recommend background datasets, encompassing flows and processes, and to automate life cycle modeling and LCA calculations. The proposed approach demonstrates significant improvements, with a flow recommendation Precision@10 of 79.52%, surpassing current search engines by 4.26 times, and a corresponding 2.45 times reduction in response time within the Top 10 results. Furthermore, processes are ranked based on the knowledge graph and geographical inclusion relationships. This aids in extracting system boundaries and functional units, facilitating process recommendations and sup-porting decision-making. After obtaining the life cycle inventory (LCI), an open-source LCA software, OpenLCA, is utilized to extend and refine the automated life cycle modeling and calculations. The proposed method is validated through a case study on an electrical product, and a prototype system is designed to ensure straightforward result interpretation. In conclusion, this method can efficiently select background dataset, automate life cycle modeling and LCA calculation, and improve the readability of LCA results.}
}
@article{LUO2021111224,
title = {An overview of data tools for representing and managing building information and performance data},
journal = {Renewable and Sustainable Energy Reviews},
volume = {147},
pages = {111224},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2021.111224},
url = {https://www.sciencedirect.com/science/article/pii/S1364032121005116},
author = {Na Luo and Marco Pritoni and Tianzhen Hong},
keywords = {Building information modeling, Ontology, Data schema, Metadata, Building performance data},
abstract = {Building information modeling (BIM) has been widely adopted for representing and exchanging building data across disciplines during building design and construction. However, BIM's use in the building operation phase is limited. With the increasing deployment of low-cost sensors and meters, as well as affordable digital storage and computing technologies, growing volumes of data have been collected from buildings, their energy services systems, and occupants. Such data are crucial to help decision makers understand what, how, and when energy is consumed in buildings—a critical step to improving building performance for energy efficiency, demand flexibility, and resilience. However, practical analyses and use of the collected data are very limited due to various reasons, including poor data quality, ad-hoc representation of data, and lack of data science skills. To unlock value from building data, there is a strong need for a toolchain to curate and represent building information and performance data in common standardized terminologies and schemas, to enable interoperability between tools and applications. This study selected and reviewed 24 data tools based on common use cases of data across the building life cycle, from design to construction, commissioning, operation, and retrofits. The selected data tools are grouped into three categories: (1) data dictionary or terminology, (2) data ontology and schemas, and (3) data platforms. The data are grouped into ten typologies covering most types of data collected in buildings. This study resulted in five main findings: (1) most data representation tools can represent their intended data typologies well, such as Green Button for smart meter data and Brick schema for metadata of sensors in buildings and HVAC systems, but none of the tools cover all ten types of data; (2) there is a need for data schemas to represent the basis of design data and metadata of occupant data; (3) standard terminologies such as those defined in BEDES are only adopted in a few data tools; (4) integrating data across various stages in the building life cycle remains a challenge; and (5) most data tools were developed and maintained by different parties for different purposes, their flexibility and interoperability can be improved to support broader use cases. Finally, recommendations for future research on building data tools are provided for the data and buildings community based on the FAIR principles to make data Findable, Accessible, Interoperable, and Reusable.}
}
@article{FAUTH2024102312,
title = {Taxonomy for building permit system - organizing knowledge for building permit digitalization},
journal = {Advanced Engineering Informatics},
volume = {59},
pages = {102312},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102312},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623004408},
author = {Judith Fauth and Tanya Bloch and Francesca Noardo and Nicholas Nisbet and Stefanie-Brigitte Kaiser and Peter {Nørkjær Gade} and Jernej Tekavec},
keywords = {Building permit, Taxonomy, Knowledge organization, Digitalization},
abstract = {The building permit process is a crucial aspect of the construction industry, as it ensures the safety and compliance of buildings with local regulations. A significant improvement in terms of accuracy, transparency and efficiency would be brought from digitalization, therefore several projects are being developed on the topic. However, to reach high maturity levels, it is necessary to address the several parts of building permit systems in a structured way, by considering the several subsystems composing the main issue thoroughly (e.g., legislative, organizational, technological, procedural). Therefore, this article proposes a taxonomy of building permit systems that can be the reference to guide and assess related developments across diverse countries, allowing their interpretation in a common framework. Different methodologies for taxonomy developments were applied in this study, including a combination of committee approaches and empirical methods, with a final validation against a use case. The obtained high-level taxonomy of building permit systems can serve as a basis for future improvements in the building permit digitalization and could be the basis for an extended ontology.}
}
@article{UGLANOV2024728,
title = {An NLP-based framework for early identification of design reliability issues from heterogeneous automotive lifecycle data},
journal = {Procedia CIRP},
volume = {128},
pages = {728-733},
year = {2024},
note = {34th CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.05.098},
url = {https://www.sciencedirect.com/science/article/pii/S221282712400773X},
author = {Alexey Uglanov and Felician Campean and Amr Abdullatiff and Daniel Neagu and Aleksandr Doikin and David Delaux and Pascal Bonnaud},
keywords = {automotive warranty data, natural language processing, design informatics},
abstract = {Natural Language Processing is increasingly used in different areas of design and product development with varied objectives, from enhancing productivity to embedding resilience into systems. In this paper, we introduce a framework that draws on NLP algorithms and expert knowledge for the automotive engineering domain, to extract actionable insight for system reliability improvement from data available from the operational phase of the system. Specifically, we are looking at the systematic exploration and exploitation of automotive heterogeneous data sources, including both closed-source (such as warranty records) and open-source (e.g., social networks, chatrooms, recall systems) data, to extract and classify information about faults, with predictive capability for early detection of issues. We present a preliminary NLP-based framework for enhancing system knowledge representation to increase the effectiveness and robustness of information extraction from data, and discuss the temporal alignment of data sources and insight to improve prediction ability. We demonstrate the effectiveness of the proposed framework using real-world automotive data in a recall study for a vehicle lighting system and a particular manufacturer: four recall campaigns were identified leading to corrective actions by the warranty experts.}
}
@article{DANG2023104460,
title = {GENA: A knowledge graph for nutrition and mental health},
journal = {Journal of Biomedical Informatics},
volume = {145},
pages = {104460},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104460},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423001818},
author = {Linh D. Dang and Uyen T.P. Phan and Nhung T.H. Nguyen},
keywords = {Knowledge graph, Information extraction, Mental health, Nutrition, Deep syntax, Dependency tree},
abstract = {While a large number of knowledge graphs have previously been developed by automatically extracting and structuring knowledge from literature, there is currently no such knowledge graph that encodes relationships between food, biochemicals and mental illnesses, even though a large amount of knowledge about these relationships is available in the form of unstructured text in biomedical literature articles. To address this limitation, this article describes the development of GENA - (Graph of mEntal-health and Nutrition Association), a knowledge graph that represents relations between nutrition and mental health, extracted from biomedical abstracts. GENA is constructed from PubMed abstracts that contain keywords relating to chemicals, food, and health. A hybrid named entity recognition (NER) model is firstly applied to these abstracts to identify various entities of interest. Subsequently, a deep syntax-based relation extraction model is used to detect binary relations between the identified entities. Finally, the resulting relations are used to populate the GENA knowledge graph, whose relationships can be accessed in an intuitive and interpretable manner using the Neo4J Database Management System. To evaluate the reliability of GENA, two annotators manually assessed a subset of the extracted relations. The evaluation results show that our methods obtain high precision for the NER task and acceptable precision and relative recall for the relation extraction task. GENA consists of 43,367 relationships that encode information about nutrition and health, of which 94.04% are new relations that are not present in existing ontologies of food and diseases. GENA is constructed based on scientific principles, and has the potential to be used within further applications to contribute towards scientific research within the domain. It is a pioneering knowledge graph in nutrition and mental health, containing a diverse range of relationship types. All of our source code and results are publicly available at https://github.com/ddlinh/gena-db.}
}
@article{JOURNEAULT2021102145,
title = {Sustainability performance reporting: A technocratic shadowing and silencing},
journal = {Critical Perspectives on Accounting},
volume = {74},
pages = {102145},
year = {2021},
issn = {1045-2354},
doi = {https://doi.org/10.1016/j.cpa.2019.102145},
url = {https://www.sciencedirect.com/science/article/pii/S1045235419301248},
author = {Marc Journeault and Yves Levant and Claire-France Picard},
keywords = {Ideology of numbers, Ontological views, Silencing and shadowing, Sustainability performance reporting},
abstract = {Global Reporting Initiative (GRI) guidelines have become the most relevant institution in sustainability reporting standards, highly adopted by organizations worldwide. However, much criticism has been raised about these standards’ ability to further sustainable development within organizations and render their sustainability performance more accountable and transparent. Using a case study of Hydro-Québec, an important hydro-electricity provider in Quebec, Canada, and its relationship with the Cree, an Indigenous community, the purpose of this study is to provide theoretical and empirical insights on the subject by showing how GRI guidelines, legitimized and reinforced through their institutionalization, tend to create a limited scope and an incomplete picture of the organization’s sustainability performance reporting. More specifically, this paper highlights two main problems arising from the use of these standards. First, drawing on the ideology of numbers of Chelli and Gendron (2013), this paper examines how GRI technocratic guidelines frame the sustainability reporting discourse, thereby contributing to leaving aspects of organizational sustainability performance in the shadows. Second, drawing on the ontological typology of Descola (2013), this study examines how these guidelines are ontologically driven by a Western view of nature. This contributes to silencing alternative ontologies in organizational sustainability performance reporting.}
}
@article{PHUA2024105708,
title = {Fostering urban resilience and accessibility in cities: A dynamic knowledge graph approach},
journal = {Sustainable Cities and Society},
volume = {113},
pages = {105708},
year = {2024},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2024.105708},
url = {https://www.sciencedirect.com/science/article/pii/S221067072400533X},
author = {Shin Zert Phua and Markus Hofmeister and Yi-Kai Tsai and Oisín Peppard and Kok Foong Lee and Seán Courtney and Sebastian Mosbach and Jethro Akroyd and Markus Kraft},
keywords = {Disaster resilience, 15-minute city, Accessibility, Isochrone analysis, Knowledge graph},
abstract = {This paper explores the utilisation of knowledge graphs and an agent-based implementation to enhance urban resilience and accessibility in city planning. We expand The World Avatar (TWA) dynamic knowledge graph to support decision-making in disaster response and urban planning. By employing a set of connected agents and integrating diverse data sources — including flood data, geospatial building information, land plots, and open-source data — through sets of ontologies, we demonstrate disaster response in a coastal town in the UK and various aspects relevant to city planning for a mid-sized town in Germany using TWA. In King’s Lynn, our agent-based approach facilitates holistic disaster response by calculating optimal routes, avoiding flooded segments dynamically, assessing infrastructure accessibility before and during a flood using isochrones, identifying inaccessible population areas, guiding infrastructure restoration, and conducting critical path analysis. In Pirmasens, for city planning purposes, the knowledge graph-driven isochrone generation provides evidence-based insights into current amenity coverage and enables scenario planning for future amenities while adhering to land regulations. The implementation of agents and knowledge graphs achieves interoperability and enhances urban resilience and accessibility by enabling cross-domain correlation analysis that extends various areas including geospatial buildings, population demographics, accessibility coverage, and land use regulations.}
}
@article{SEBASTIAN2025104797,
title = {Network-based analysis of Alzheimer’s Disease genes using multi-omics network integration with graph diffusion},
journal = {Journal of Biomedical Informatics},
volume = {164},
pages = {104797},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2025.104797},
url = {https://www.sciencedirect.com/science/article/pii/S1532046425000267},
author = {Softya Sebastian and Swarup Roy and Jugal Kalita},
keywords = {Gene expression, Regulatory network, Multi-omics data integration, Network characteristics, Degree distribution, Alzheimer’s Disease},
abstract = {Alzheimer’s Disease (AD) is a complex neurodegenerative disorder affecting millions worldwide. Despite extensive research, the mechanisms behind AD remain elusive. Many studies suggest that disease-responsible genes often act as hub genes in biological networks. However, this assumption requires further investigation in the context of AD. To examine the network characteristics of known AD genes, it is crucial to construct a highly confident network, which is challenging to achieve using a single data source. This work integrates multi-omics networks inferred from microarray, single-cell RNA sequencing, and single-nuclei RNA sequencing expression data, weighted with protein interaction and gene ontology information. We generate a high-quality integrated network by utilizing various inference methods and combining them through a graph diffusion-based integration approach. This network is then analyzed to investigate the properties of known AD-specific genes. Our findings reveal that AD genes are not always high-degree or central hub nodes in the network. Instead, these genes are distributed across different quartiles of degree centrality while maintaining significant interconnections for effective regulation. Furthermore, our study highlights that peripheral genes, often overlooked, also play crucial roles by connecting to relevant disease nodes and hub genes. These findings challenge the conventional understanding that AD-responsible genes are primarily the hub genes in the network, offering new insights into the complex regulatory mechanisms of AD and suggesting novel directions for future research.}
}
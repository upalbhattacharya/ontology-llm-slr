@article{WILK2020104075,
title = {An ontology-driven framework to support the dynamic formation of an interdisciplinary healthcare team},
journal = {International Journal of Medical Informatics},
volume = {136},
pages = {104075},
year = {2020},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2020.104075},
url = {https://www.sciencedirect.com/science/article/pii/S1386505619311360},
author = {Szymon Wilk and Mounira Kezadri-Hamiaz and Daniel Amyot and Wojtek Michalowski and Craig Kuziemsky and Nihan Catal and Daniela Rosu and Marc Carrier and Randy Giffen},
keywords = {Team-based care delivery, Interdisciplinary healthcare team, Dynamic team formation, Clinical workflow, Workflow execution engine},
abstract = {Background and purpose
Teamwork has become a modus operandi in healthcare and delivery of patient care by an interdisciplinary healthcare team (IHT) is now a prevailing modality of care. We argue that a formal and automated support framework is needed for an IHT to properly leverage information technology resources. Such a framework should allow for patient preferences and expand a representation of a clinical workflow with a formal model of dynamic formation of a team, especially with regards to team leader- and membership, and the assignment of tasks to team members. Our goal was to develop such a support framework, present its prototype software implementation and verify the implementation using a proof-of-concept use case. Specifically, we focused on clinical workflows for in-patient tertiary care and on patient preferences with regards to selecting team members and team leaders.
Materials and methods
Drawing on the research on clinical teamwork we defined the conceptual foundations for the proposed framework. Then, we designed its architecture and used ontology-driven design and first-order logic with associated reasoning methods to create and operationalize architectural elements. Finally, we incorporated existing solutions for business workflow modeling and execution as a backend for implementing the proposed framework.
Results
We developed a Team and Workflow Management Framework (TWMF) with semantic components that allow for formalizing and operationalizing team formation in in-patient tertiary care setting and support provider-related patient preferences. We also created a prototype software implementation of TWMF using the IBM Business Process Manager platform. This implementation was evaluated in several simulated patient scenarios.
Conclusions
TWMF integrates existing workflow technologies and extends them with the capabilities to support dynamic formation of an IHT. Results of this research can be used to support real-time execution of clinical workflows, or to simulate their execution in order to assess the impact of various conditions (e.g., patterns of work shifts, staffing) on IHT operations.}
}
@article{PARK20201,
title = {The ontology of digital asset after death: policy complexities, suggestions and critique of digital platforms},
journal = {Digital Policy, Regulation and Governance},
volume = {22},
number = {1},
pages = {1-14},
year = {2020},
issn = {2398-5038},
doi = {https://doi.org/10.1108/DPRG-04-2019-0030},
url = {https://www.sciencedirect.com/science/article/pii/S2398503820000077},
author = {Yong Jin Park and Yoonmo Sang and Hoon Lee and S. Mo Jones-Jang},
keywords = {Privacy, Data ethics, Digital remain, Privacy and property rights, Postlife digitalization, Data ethics},
abstract = {Purpose
The digitization of the life has brought complexities associated with addressing digital life after one’s death. This paper aims to investigate the two related issues of the privacy and property of postlife digital assets.
Design/methodology/approach
The understanding of digital assets has not been fully unpacked largely due to the current policy complexities in accessing and obtaining digital assets at death. This paper calls critical attention to the importance of respecting user rights in digital environments that currently favor service providers’ interests.
Findings
It is argued that there are ethical blind spots when protecting users’ rights, given no ontological difference between a person’s digital beings and physical existence. These derive from the restrictive corporate terms and ambiguous conditions drafted by digital service providers.
Originality/value
Fundamentally, the transition to the big data era, in which the collection, use and dissemination of digital activities became integral part of the ontology, poses new challenges to privacy and property rights after death.}
}
@incollection{PREISIG20221021,
title = {Documenting Models Comprehensively Using a Minimal Graphical Language},
editor = {Yoshiyuki Yamashita and Manabu Kano},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {49},
pages = {1021-1026},
year = {2022},
booktitle = {14th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-85159-6.50170-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851596501706},
author = {Heinz A. Preisig},
keywords = {Modelling ontologies, simulation, control, design, operations},
abstract = {A small graphical language provides the means to document and discuss process models in details without engaging into a programming environment. It proved to be a powerful tool to discuss model on the back of an envelope as well as for defining a graphical user interface for ontology-based modelling suite ProMo.}
}
@article{BIBI2024101865,
title = {Enhancing source code retrieval with joint Bi-LSTM-GNN architecture: A comparative study with ChatGPT-LLM},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {36},
number = {2},
pages = {101865},
year = {2024},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101865},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823004196},
author = {Nazia Bibi and Ayesha Maqbool and Tauseef Rana},
keywords = {Code reuse, Recommendation systems, Code recommendation, Joint model, Source code retrieval, Deep learning, LSTM, GNN, Bi-directional LSTM},
abstract = {Retrieving relevant source code from large repositories is a significant and ongoing challenge in the field of software engineering, primarily due to the vast and ever-expanding amount of available code. Existing deep learning methods, although effective to some extent, exhibit limitations in capturing the intricate and complex structural information embedded within source code, which hinders their ability to provide highly accurate retrieval results. This study endeavors to tackle this prominent issue by introducing a novel and innovative approach known as the Joint Bi-directional LSTM and Graph Neural Networks (JBLG) model for source code retrieval. The central aim is to harness the combined strengths and capabilities of Bi-directional Long Short-Term Memory (LSTM) networks and Graph Neural Networks (GNNs) to significantly enhance the model’s capacity to capture and interpret the complex structural characteristics intrinsic to source code. The proposed JBLG model employs a unique fusion of Bi-directional LSTM, which excels in capturing sequential and temporal dependencies within code, and GNN, which is adept at modeling the intricate graph structure of the code. By leveraging this hybrid architecture, the model aims to provide a comprehensive and highly effective solution for source code retrieval tasks. To assess the efficacy of the JBLG model, extensive experiments are conducted, and the model’s performance is evaluated against well-established benchmarks, including LSTM, GNN, and ChatGPT, using two diverse datasets: CodeSearchNet and CosBench datasets. These evaluations span multiple programming languages, ensuring a comprehensive and robust assessment of the model’s capabilities. The experimental results indicate that the JBLG model consistently outperforms its counterparts, including Bi-LSTM, GNN, ChatGPT, and DGMS, across various evaluation metrics. the JBLG model showcases an exceptional ability to handle and extract the intricate structural information inherent in source code, resulting in significantly enhanced retrieval accuracy. The JBLG model emerges as a highly promising solution for real-world source code retrieval applications, with the potential to revolutionize the field. The success of this model underscores the importance of combining deep learning techniques like Bi-directional LSTM and GNNs for tackling complex software engineering challenges. Furthermore, future research directions could involve exploring advanced techniques such as attention mechanisms and extending the model’s applicability to other software engineering tasks like code summarization and code completion. The findings of this study are expected to have a lasting impact on the advancement of source code retrieval methodologies.}
}
@article{XIA2024,
title = {Semiology Extraction and Machine Learning–Based Classification of Electronic Health Records for Patients With Epilepsy: Retrospective Analysis},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/57727},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424001443},
author = {Yilin Xia and Mengqiao He and Sijia Basang and Leihao Sha and Zijie Huang and Ling Jin and Yifei Duan and Yusha Tang and Hua Li and Wanlin Lai and Lei Chen},
keywords = {epilepsy, natural language processing, machine learning, electronic health record, unstructured text, semiology, health records, retrospective analysis, diagnosis, treatment, decision support tools, symptom, ontology, China, Chinese, seizure},
abstract = {Background
Obtaining and describing semiology efficiently and classifying seizure types correctly are crucial for the diagnosis and treatment of epilepsy. Nevertheless, there exists an inadequacy in related informatics resources and decision support tools.
Objective
We developed a symptom entity extraction tool and an epilepsy semiology ontology (ESO) and used machine learning to achieve an automated binary classification of epilepsy in this study.
Methods
Using present history data of electronic health records from the Southwest Epilepsy Center in China, we constructed an ESO and a symptom-entity extraction tool to extract seizure duration, seizure symptoms, and seizure frequency from the unstructured text by combining manual annotation with natural language processing techniques. In addition, we achieved automatic classification of patients in the study cohort with high accuracy based on the extracted seizure feature data using multiple machine learning methods.
Results
Data included present history from 10,925 cases between 2010 and 2020. Six annotators labeled a total of 2500 texts to obtain 5844 words of semiology and construct an ESO with 702 terms. Based on the ontology, the extraction tool achieved an accuracy rate of 85% in symptom extraction. Furthermore, we trained a stacking ensemble learning model combining XGBoost and random forest with an F1-score of 75.03%. The random forest model had the highest area under the curve (0.985).
Conclusions
This work demonstrated the feasibility of natural language processing–assisted structural extraction of epilepsy medical record texts and downstream tasks, providing open ontology resources for subsequent related work.}
}
@article{TANG2023102129,
title = {Automatic schema construction of electrical graph data platform based on multi-source relational data models},
journal = {Data & Knowledge Engineering},
volume = {145},
pages = {102129},
year = {2023},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2022.102129},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X22001203},
author = {Yachen Tang and Xingping Wu and Chunlei Zhou and Guangxin Zhu and Jinwei Song and Guangyi Liu and Zhihong Li},
keywords = {Automated extraction, Electrical graph data platform, Ontology model, Schema construction, Relational data models},
abstract = {Data storage and management in power systems usually adopt relational databases. However, the relational database requires ample storage space and has low data retrieval and query efficiency. An electrical graph data platform can describe the complicated relationships between concepts and entities involved in power systems with the form of an association graph, which provides a better ability to organize, manage, and apply massive amounts of information. Since the construction of the top-level ontology model or schema for a specific field graph data platform is cumbersome, complex, and generally requires lots of association analysis and expert system intervention, it is insufficiently automated, time-consuming, and unable to cope with large-scale electric power knowledge. This paper proposes a method for automatically constructing the schema of an electrical graph data platform, which uses the diverse table structure information from the relational database and SQL language descriptions to extract ontologies to form the ontology candidate set automatically. Then the method utilizes ontology clustering and disambiguation to initial an ontology graph model and automatically update ontology and relationship expressions. Meanwhile, the model layering is used to construct a hierarchical model based on different business needs, and the schema optimization is applied according to expert comments.}
}
@article{YIN2023104902,
title = {Two-stage Text-to-BIMQL semantic parsing for building information model extraction using graph neural networks},
journal = {Automation in Construction},
volume = {152},
pages = {104902},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.104902},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523001620},
author = {Mengtian Yin and Llewellyn Tang and Chris Webster and Jinyang Li and Haotian Li and Zhuoquan Wu and Reynold C.K. Cheng},
abstract = {With the increasing complexity of the building process, it is difficult for project stakeholders to retrieve large and multi-disciplinary building information models (BIMs). A natural language interface (NLI) is beneficial for users to query BIM models using natural language. However, parsing natural language queries (NLQs) is challenging due to ambiguous name descriptions and intricate relationships between entities. To address these issues, this study proposes a graph neural network (GNN)-based semantic parsing method that automatically maps NLQs into executable queries. Firstly, ambiguous mentions are collectively linked to referent ontological entities via a GNN-based entity linking model. Secondly, the logical forms of NLQs are interpreted through a GNN-based relation extraction model, which predicts links between mentioned entities in a heterogeneous graph fusing ontology and NLQ texts. The experiment based on 786 queries shows its outstanding performance. Moreover, a real-world case verifies the practicability of the proposed method for BIM model retrieval.}
}
@article{GIUFFRE2025S-1780,
title = {Su1752: OPTIMIZING LARGE LANGUAGE MODELS FOR HEPATOLOGY: A COMPREHENSIVE EVALUATION OF RETRIEVAL-AUGMENTED GENERATION AND SUPERVISED FINE-TUNING ON EASL GUIDELINES},
journal = {Gastroenterology},
volume = {169},
number = {1, Supplement },
pages = {S-1780},
year = {2025},
issn = {0016-5085},
doi = {https://doi.org/10.1016/S0016-5085(25)04959-5},
url = {https://www.sciencedirect.com/science/article/pii/S0016508525049595},
author = {Mauro Giuffrè and Simone Kresevic and Alessia Distefano and Marco Gulotta and Francesca Orbosuè and Milos Ajcevic and Lory S. Crocè and Dennis Shung}
}
@article{ADHIKARY2024,
title = {Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: Benchmark Study},
journal = {JMIR Mental Health},
volume = {11},
year = {2024},
issn = {2368-7959},
doi = {https://doi.org/10.2196/57306},
url = {https://www.sciencedirect.com/science/article/pii/S2368795924000775},
author = {Prottay Kumar Adhikary and Aseem Srivastava and Shivani Kumar and Salam Michael Singh and Puneet Manuja and Jini K Gopinath and Vijay Krishnan and Swati Kedia Gupta and Koushik Sinha Deb and Tanmoy Chakraborty},
keywords = {mental health, counseling summarization, large language models, digital health, artificial intelligence, AI},
abstract = {Background
Comprehensive session summaries enable effective continuity in mental health counseling, facilitating informed therapy planning. However, manual summarization presents a significant challenge, diverting experts’ attention from the core counseling process. Leveraging advances in automatic summarization to streamline the summarization process addresses this issue because this enables mental health professionals to access concise summaries of lengthy therapy sessions, thereby increasing their efficiency. However, existing approaches often overlook the nuanced intricacies inherent in counseling interactions.
Objective
This study evaluates the effectiveness of state-of-the-art large language models (LLMs) in selectively summarizing various components of therapy sessions through aspect-based summarization, aiming to benchmark their performance.
Methods
We first created Mental Health Counseling-Component–Guided Dialogue Summaries, a benchmarking data set that consists of 191 counseling sessions with summaries focused on 3 distinct counseling components (also known as counseling aspects). Next, we assessed the capabilities of 11 state-of-the-art LLMs in addressing the task of counseling-component–guided summarization. The generated summaries were evaluated quantitatively using standard summarization metrics and verified qualitatively by mental health professionals.
Results
Our findings demonstrated the superior performance of task-specific LLMs such as MentalLlama, Mistral, and MentalBART evaluated using standard quantitative metrics such as Recall-Oriented Understudy for Gisting Evaluation (ROUGE)-1, ROUGE-2, ROUGE-L, and Bidirectional Encoder Representations from Transformers Score across all aspects of the counseling components. Furthermore, expert evaluation revealed that Mistral superseded both MentalLlama and MentalBART across 6 parameters: affective attitude, burden, ethicality, coherence, opportunity costs, and perceived effectiveness. However, these models exhibit a common weakness in terms of room for improvement in the opportunity costs and perceived effectiveness metrics.
Conclusions
While LLMs fine-tuned specifically on mental health domain data display better performance based on automatic evaluation scores, expert assessments indicate that these models are not yet reliable for clinical application. Further refinement and validation are necessary before their implementation in practice.}
}
@article{YACOUBIAYADI2024100484,
title = {A unified approach to publish semantic annotations of agricultural documents as knowledge graphs},
journal = {Smart Agricultural Technology},
volume = {8},
pages = {100484},
year = {2024},
issn = {2772-3755},
doi = {https://doi.org/10.1016/j.atech.2024.100484},
url = {https://www.sciencedirect.com/science/article/pii/S2772375524000893},
author = {Nadia {Yacoubi Ayadi} and Stephan Bernard and Robert Bossy and Marine Courtin and Bill Gates {Happi Happi} and Pierre Larmande and Franck Michel and Claire Nédellec and Catherine Roussey and Catherine Faron},
keywords = {Agriculture, Knowledge graphs, Semantic modelling, RDF transformation, Natural language processing, Annotations, Semantic resources, Named entity recognition and linking},
abstract = {The research results presented in this paper were obtained as part of the D2KAB project (Data to Knowledge in Agriculture and Biodiversity) which aims to develop semantic web-based tools to describe and make agronomical data actionable and accessible following the FAIR principles. We focus on constructing domain-specific Knowledge Graphs (KGs) from textual data sources, using Natural Language Processing (NLP) techniques to extract and structure relevant entities. Our approach is based on the formalization of a semantic data model using common linked open vocabularies such as the Web Annotation Ontology (OA) and the Provenance Ontology (PROV). The model was developed by formulating motivating scenarios and competency questions from domain experts. This model has been used to construct three different KGs from three distinct corpora: PubMed scientific publications on wheat and rice genetics and phenotyping, and French agricultural alert bulletins. The named entities to be recognized include genes, phenotypes, traits, genetic markers, taxa and phenological stages normalized using semantic resources such as the Wheat Trait and Phenotype Ontology (WTO), the French Crop Usage (FCU) thesaurus and the Plant Phenological Description Ontology (PPDO). Named entities were extracted using different NLP approaches and tools. The relevance of the semantic model was validated by implementing experts questions as SPARQL queries to be answered on the constructed RDF knowledge graphs. Our work demonstrates how domain-specific vocabularies and systematic querying of KGs can reveal hidden interactions and support agronomists in navigating vast amounts of data. The resources and transformation pipelines developed are publicly available in Git repositories.}
}
@article{ROY2025105882,
title = {Unified modeling language for patient-centered telerehabilitation: A comprehensive framework integrating medical and biopsychosocial pathways},
journal = {International Journal of Medical Informatics},
volume = {199},
pages = {105882},
year = {2025},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2025.105882},
url = {https://www.sciencedirect.com/science/article/pii/S1386505625000991},
author = {Thomas Roy and Aurélie Bertaux and Ouassila {Labbani Narsis} and Jean-Pierre Didier and Davy Laroche},
keywords = {Rehabilitation, Telerehabilitation, Unified modeling language, Design methodology, Care pathway, Patient-centered care, Data integration},
abstract = {Background: Effective telerehabilitation requires robust, standardized models to ensure comprehensive and continuous patient monitoring. However, existing rehabilitation models often lack integration, failing to cover the entire care continuum and its interdisciplinary aspects. This gap limits their applicability in real-world settings. Objective: This study introduces a semi-formal, Unified Modeling Language (UML)-based framework that provides a holistic, patient-centered representation of the rehabilitation pathway. The model is designed to bridge gaps in care coordination, aligning with recent scientific advances and healthcare policies emphasizing patient empowerment and interdisciplinary collaboration. Methods: Using a professional didactics approach, we conducted a literature review, field observations, and expert consultations (questionnaires, interviews) to map rehabilitation pathways across diverse conditions and settings. The model was iteratively refined based on expert feedback to ensure its accuracy and usability. Results: Our findings reveal significant fragmentation in rehabilitation pathways, driven by diverse clinical practices and discontinuities in care. To address this, the proposed UML-based model integrates medical, functional, psychosocial, and organizational data, ensuring a cohesive, capability-driven approach. The structured design enhances communication between stakeholders and improves interoperability across healthcare systems. Conclusion: The proposed model provides a scalable foundation for digital telerehabilitation solutions, adaptable to various healthcare environments. By facilitating data integration and standardization, it supports better patient monitoring, decision-making, and personalized rehabilitation strategies. Future research will focus on refining the model to incorporate specialized rehabilitation fields and enhance interoperability with existing medical information systems.}
}
@article{BILENCHI2025101439,
title = {Cowl: Pushing OWL 2 over the Edge},
journal = {Internet of Things},
volume = {29},
pages = {101439},
year = {2025},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2024.101439},
url = {https://www.sciencedirect.com/science/article/pii/S2542660524003809},
author = {Ivano Bilenchi and Filippo Gramegna and Giuseppe Loseto and Saverio Ieva and Floriano Scioscia and Michele Ruta},
keywords = {Knowledge graphs, Semantic Web, Web Ontology Language, Embedded systems},
abstract = {The ever-complex information environments and rapidly expanding data volumes of the modern digital infrastructure demand efficient knowledge organization and retrieval techniques. The Semantic Web initiative has defined principles and technologies, such as the Resource Description Framework (RDF) and the Web Ontology Language (OWL), to create structured and semantically rich Knowledge Graphs. Current OWL toolkits, however, are largely unsuitable for resource-constrained platforms, hindering development of truly ubiquitous knowledge-enabled frameworks and applications. This paper introduces Cowl, an OWL manipulation software designed for a wide spectrum of devices, ranging from workstations to embedded systems with stringent resource limitations. Its architecture, optimizations, and novel processing techniques are detailed, emphasizing computation efficiency and minimal memory use, and providing actionable design principles for future toolkit developers. Comparative experiments reveal state-of-the-art performance and memory efficiency, and its versatility is demonstrated through a comprehensive evaluation on a popular microcontroller platform. Finally, a case study illustrates its usefulness in a knowledge-enabled smart city context.}
}
@article{IJEBU2025112551,
title = {Soft cosine and extended cosine adaptation for pre-trained language model semantic vector analysis},
journal = {Applied Soft Computing},
volume = {169},
pages = {112551},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.112551},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624013255},
author = {Funebi Francis Ijebu and Yuanchao Liu and Chengjie Sun and Patience Usoro Usip},
keywords = {Transformer-based model, Large language model, Vector space model, Transfer learning, Semantic textual similarity},
abstract = {Semantic textual analysis is a natural language processing task that has enjoyed several research contributions towards solving diverse real-life problems. Vector comparison is a core subtask in semantic textual similarity analysis. A plethora of solutions including recent state-of-the-art transformer-based pre-trained language models for transfer learning have focused on using only cosine similarity for embedding evaluation in downstream tasks and ignored other vector comparison methods. To investigate the relative performance of some such ignored measures, this work proposes novel adaptations for soft cosine and extended cosine vector measures. We investigate their performance against the conventional cosine measure, distance-weighted cosine, vector similarity measure, negative Manhattan, and Euclidean distances on downstream semantic textual similarity tasks, under same conditions, for the first time in literature. Adopting transformer-based Universal sentence encoder, SBERT, SRoBERTa, SimCSE, and ST5 for text encoding; the performances of the adapted measures are evaluated on diverse real world datasets using Pearson, Spearman, accuracy and F1 evaluation metrics. Results obtained show that the adapted measures significantly surpass previously reported state-of-the-art cosine similarity-based correlations in several test cases considered.}
}
@article{NTOUMANIS2025102879,
title = {Self-determination theory informed research for promoting physical activity: Contributions, debates, and future directions},
journal = {Psychology of Sport and Exercise},
volume = {80},
pages = {102879},
year = {2025},
issn = {1469-0292},
doi = {https://doi.org/10.1016/j.psychsport.2025.102879},
url = {https://www.sciencedirect.com/science/article/pii/S1469029225000780},
author = {Nikos Ntoumanis and Arlen C. Moller},
keywords = {Narrative review, Tripartite model, Behavioral science, Motivation, Financial incentives, Competition},
abstract = {In this review we evaluate the applications of self-determination theory (SDT) research to promote motivation for physical activity (PA) and exercise. The evidence suggests that SDT-informed interventions are often effective at changing health behaviors, including PA/exercise, and associated health outcomes. The effect sizes are small to moderate and are often mediated by increases in autonomous motivation (primarily), interpersonal support for basic psychological needs, and competence need satisfaction. We also identify conceptual debates within the SDT literature and between SDT and other literatures, and discuss their relevance with respect to PA. We particularly focus on tripartite conceptualizations of interpersonal styles and psychological needs, whether there are more than three basic psychological needs, and the use of financial incentives and competition to promote PA. Our review also provides future conceptual and methodological directions for SDT-based research, building on advances in technology (e.g., generative Artificial Intelligence and Large Language Models) and the broader field of behavioral science (e.g., optimization designs, system-level interventions, behavior change intervention ontologies).}
}
@article{HEUSINKVELD2021160,
title = {An ontology for developmental processes and toxicities of neural tube closure},
journal = {Reproductive Toxicology},
volume = {99},
pages = {160-167},
year = {2021},
note = {48th Conference of the European Teratology Society (ETS)},
issn = {0890-6238},
doi = {https://doi.org/10.1016/j.reprotox.2020.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0890623820302069},
author = {Harm J. Heusinkveld and Yvonne C.M. Staal and Nancy C. Baker and George Daston and Thomas B. Knudsen and Aldert Piersma},
keywords = {Developmental neurotoxicology, Retinoic acid, Neural tube defect, Systems biology},
abstract = {In recent years, the development and implementation of animal-free approaches to chemical and pharmaceutical hazard and risk assessment has taken off. Alternative approaches are being developed starting from the perspective of human biology and physiology. Neural tube closure is a vital step that occurs early in human development. Correct closure of the neural tube depends on a complex interplay between proteins along a number of protein concentration gradients. The sensitivity of neural tube closure to chemical disturbance of signalling pathways such as the retinoid pathway, is well known. To map the pathways underlying neural tube closure, literature data on the molecular regulation of neural tube closure were collected. As the process of neural tube closure is highly conserved in vertebrates, the extensive literature available for the mouse was used whilst considering its relevance for humans. Thus, important cell compartments, regulatory pathways, and protein interactions essential for neural tube closure under physiological circumstances were identified and mapped. An understanding of aberrant processes leading to neural tube defects (NTDs) requires detailed maps of neural tube embryology, including the complex genetic signals and responses underlying critical cellular dynamical and biomechanical processes. The retinoid signaling pathway serves as a case study for this ontology because of well-defined crosstalk with the genetic control of neural tube patterning and morphogenesis. It is a known target for mechanistically-diverse chemical structures that disrupt neural tube closure The data presented in this manuscript will set the stage for constructing mathematical models and computer simulation of neural tube closure for human-relevant AOPs and predictive toxicology.}
}
@article{CHEN2025100824,
title = {Knowledge sharing-enabled low-code program for collaborative robots in mix-model assembly},
journal = {Journal of Industrial Information Integration},
volume = {45},
pages = {100824},
year = {2025},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2025.100824},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X25000482},
author = {Baotong Chen and Xin Tong and Jiafu Wan and Lei Wang and Xianyin Duan and Zhaohui Wang and Xuhui Xia},
keywords = {Knowledge sharing, Low-code program, Collaborative robots, Skill migration, Mixed-model assembly},
abstract = {Multi-robot collaboration is a crucial execution tool for mixed-model assembly lines. The rapid reconfiguration of the robots with impaired skills to maintain the robustness of the assembly line remains a significant challenge. With a focus on knowledge-driven faster transition technologies for collaborative robots, this paper proposes a Knowledge Sharing-enabled Low-code Program (KSLC) method to address the deficient skill migration and the limited scalability caused by programs written statically in open-loop control. First, considering collaborative robots' functional requirements and environmental constraints, the parameterized action primitive library of assembly skills is developed with properties across multiple perspectives, levels, and granularities. Complex assembly skills are then formally expressed using the Web Ontology Language (OWL). Besides, digraph network model is created to represent action sequences and the corresponding parameters relevant to complex assembly tasks for the execution content. Finally, the DQN algorithm is utilized to learn low-dimensional vectors within the knowledge graph. The GraphSAGE algorithm is employed to facilitate skill search and matching, enabling the effective acquisition and transmission of robot skills. Experimental results demonstrate that the proposed KSLC-enabled collaborative robots achieve 90 % average success rate in the TwoArmPegInHole task, significantly outperforming the traditional experience transfer strategies that only attain 58 % success rate. This finding indicates that KSLC can substantially enhance robot learning efficiency and task performance.}
}
@article{MCSHANE2025101335,
title = {A neurosymbolic approach to authorship anonymization},
journal = {Cognitive Systems Research},
volume = {92},
pages = {101335},
year = {2025},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2025.101335},
url = {https://www.sciencedirect.com/science/article/pii/S1389041725000154},
author = {Marjorie McShane and Sergei Nirenburg and Christian Arndt and Sanjay Oruganti and Jesse English},
abstract = {We report a neurosymbolic approach to authorship anonymization that combines knowledge-based paraphrasing, grounded in cognitive modeling, with support functions provided by a large language model (LLM). The cognitive model accounts for four things: what it means to faithfully retain meaning and discourse coherence in a paraphrase, how do deal with polysemy given that full semantic analysis of open text is beyond the state of the art, how to define and characterize an author’s style, and how to leverage human linguistic capabilities when preparing systems to automatically anonymize texts. LLMs augment the knowledge-based paraphrases in three ways: by filtering out atypical formulations, by selecting the best from multiple candidate paraphrases, and by offering additional paraphrases in case the knowledge-based paraphrasing fails to adequately anonymize the text. This neurosymbolic architecture favors knowledge-based processing for being reliable and explainable, while exploiting LLMs for what they do best: manipulate regularities in the surface form of language.}
}
@article{IVANOVA2019145,
title = {Visualization and interaction for ontologies and linked data—Editorial},
journal = {Journal of Web Semantics},
volume = {55},
pages = {145-149},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300490},
author = {Valentina Ivanova and Patrick Lambrix and Steffen Lohmann and Catia Pesquita}
}
@article{ALMAGROHERNANDEZ2025113283,
title = {Evaluation of alignment methods to support the assessment of similarity between e-commerce knowledge graphs},
journal = {Knowledge-Based Systems},
volume = {315},
pages = {113283},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113283},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125003302},
author = {Ginés Almagro-Hernández and Juan Mulero-Hernández and Prashant Deshmukh and José Antonio Bernabé-Díaz and José Luis Sánchez-Fernández and Paola Espinoza-Arias and Juergen Mueller and Jesualdo Tomás Fernández-Breis},
keywords = {Knowledge graphs, Graph alignment, Semantic similarity, E-commerce},
abstract = {Enterprise knowledge graphs combine data from multiple sources, and the structure and granularity of the resulting graphs can vary widely. The data in knowledge graphs is typically structured by schemas, mainly ontologies. Finding the alignments between content from different graphs allows to reduce redundancy, detect inconsistencies, and improve the use of the data. Recently, many graph alignment methods have been proposed, usually tested on general knowledge bases, so there is very little knowledge about the effectiveness of the methods in specific domains. Moreover, we are also interested in studying how these methods capture the semantic similarity between the schemas of the knowledge graphs. In this article, we study both aspects using 25 graph alignment methods on datasets related to e-commerce activities of organizations, such as product sales or customer satisfaction. We have developed a pipeline for generating and evaluating the results. The results show that the alignment methods can be used as a semantic similarity system between the ontology-dataset pairs that generate the knowledge graphs, that AttrE and BootEA are the most effective and robust methods across the different datasets, that the complexity of the structure of the ontology has a clear impact on the effectiveness of the methods, and that the results of the alignment experiments generate information about the similarity of the ontologies and reveal which parts of them should be better modeled to increase the performance of the methods.}
}
@article{DENNIS2020102771,
title = {More-than-harm reduction: Engaging with alternative ontologies of ‘movement’ in UK drug services},
journal = {International Journal of Drug Policy},
volume = {82},
pages = {102771},
year = {2020},
issn = {0955-3959},
doi = {https://doi.org/10.1016/j.drugpo.2020.102771},
url = {https://www.sciencedirect.com/science/article/pii/S0955395920301122},
author = {Fay Dennis and Tim Rhodes and Magdalena Harris},
keywords = {Drug treatment services, ‘more-than-harm reduction’, movement, ontology, recovery},
abstract = {Over the last ten years, UK drug policy has moved towards making abstinence-based recovery rather than harm reduction its primary focus. Drawing on ethnographic fieldwork involving participant observations and interviews at two London drug services, we explore how this shift towards recovery materialises through the practices of drug service delivery as an ‘evidence-making intervention’. We understand recovery's making in terms of ‘movement’. Where previous policies performed harm reduction through ‘getting people into treatment’ and ‘keeping them safe in treatment’, new policies were said to be about ‘moving people through treatment’. Approaching movement as a sociomaterial process, we observe how movement is enacted in both narrow ways, towards abstinence from drugs, and more open ways, in what we call ‘more-than-harm reduction’. We think of the latter as a speculative practice of doing or ‘tinkering with’ recovery to afford a care for clients not bound to abstinence-based outcomes. This is important given the limits associated with a recovery-orientated policy impetus. By engaging with these alternative ontologies of movement, we highlight an approach to intervening that both subverts and adheres to perceptions of recovery, embracing its movement, while remaining critical to its vision of abstinence.}
}
@article{DEROSARIO2023,
title = {Applications of Natural Language Processing for the Management of Stroke Disorders: Scoping Review},
journal = {JMIR Medical Informatics},
volume = {11},
year = {2023},
issn = {2291-9694},
doi = {https://doi.org/10.2196/48693},
url = {https://www.sciencedirect.com/science/article/pii/S2291969423000248},
author = {Helios {De Rosario} and Salvador Pitarch-Corresa and Ignacio Pedrosa and Marina Vidal-Pedrós and Beatriz {de Otto-López} and Helena García-Mieres and Lydia Álvarez-Rodríguez},
keywords = {stroke, natural language processing, artificial intelligence, scoping review, scoping, review methods, review methodology, NLP, cardiovascular, machine learning, deep learning},
abstract = {Background
Recent advances in natural language processing (NLP) have heightened the interest of the medical community in its application to health care in general, in particular to stroke, a medical emergency of great impact. In this rapidly evolving context, it is necessary to learn and understand the experience already accumulated by the medical and scientific community.
Objective
The aim of this scoping review was to explore the studies conducted in the last 10 years using NLP to assist the management of stroke emergencies so as to gain insight on the state of the art, its main contexts of application, and the software tools that are used.
Methods
Data were extracted from Scopus and Medline through PubMed, using the keywords “natural language processing” and “stroke.” Primary research questions were related to the phases, contexts, and types of textual data used in the studies. Secondary research questions were related to the numerical and statistical methods and the software used to process the data. The extracted data were structured in tables and their relative frequencies were calculated. The relationships between categories were analyzed through multiple correspondence analysis.
Results
Twenty-nine papers were included in the review, with the majority being cohort studies of ischemic stroke published in the last 2 years. The majority of papers focused on the use of NLP to assist in the diagnostic phase, followed by the outcome prognosis, using text data from diagnostic reports and in many cases annotations on medical images. The most frequent approach was based on general machine learning techniques applied to the results of relatively simple NLP methods with the support of ontologies and standard vocabularies. Although smaller in number, there has been an increasing body of studies using deep learning techniques on numerical and vectorized representations of the texts obtained with more sophisticated NLP tools.
Conclusions
Studies focused on NLP applied to stroke show specific trends that can be compared to the more general application of artificial intelligence to stroke. The purpose of using NLP is often to improve processes in a clinical context rather than to assist in the rehabilitation process. The state of the art in NLP is represented by deep learning architectures, among which Bidirectional Encoder Representations from Transformers has been found to be especially widely used in the medical field in general, and for stroke in particular, with an increasing focus on the processing of annotations on medical images.}
}
@article{YU2025108773,
title = {GDReCo: Fine-grained gene-disease relationship extraction corpus},
journal = {Computer Methods and Programs in Biomedicine},
volume = {266},
pages = {108773},
year = {2025},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2025.108773},
url = {https://www.sciencedirect.com/science/article/pii/S0169260725001907},
author = {Hui Yu and Jing Wu and Suyan Bian and Sheng Zhang and Yibin Wu and Ziyan Zhou and Qian Jia and Yuan Ni and Zhengxing Huang and Huiyu Yan and Weidong Wang and Kunlun He and Jinlong Shi},
keywords = {Gene-disease relationships extraction, Systematic ontology framework, Natural language processing, ChatGPT},
abstract = {Background and objective
Understanding gene-disease relationships is crucial for medical research, drug discovery, clinical diagnosis, and other fields. However, there is currently no high-quality, fine-grained corpus available for training Natural Language Processing (NLP) models, which have proven to be effective in knowledge extraction.
Methods
This study introduces a novel ontology framework for gene-disease associations, addressing the absence of a formal descriptive system and training corpus for NLP models.
Results
We developed the Gene Disease Relationship Extraction Corpus (GDReCo), a refined dataset of over 24,000+ cases, including 2300+ manually annotated and 22,000+ model-predicted instances. BERT-based models trained on this data achieved high F1-scores for "event" and "rel" relationships, validating its effectiveness for Gene-Disease Relationship Extraction (GDRE) tasks.
Conclusions
GDReCo serves as a valuable resource for biomedical research, though ChatGPT's limitations in fine-grained relation extraction are noted.}
}
@article{GILANI2025110745,
title = {CDE-Mapper: Using retrieval-augmented language models for linking clinical data elements to controlled vocabularies},
journal = {Computers in Biology and Medicine},
volume = {196},
pages = {110745},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.110745},
url = {https://www.sciencedirect.com/science/article/pii/S0010482525010960},
author = {Komal Gilani and Marlo Verket and Christof Peters and Michel Dumontier and Hans-Peter Brunner-La Rocca and Visara Urovi},
keywords = {Clinical data elements, Retrieval-Augmented Generation, Metadata standardization, Tabular data annotation, Controlled vocabularies},
abstract = {The standardization of clinical data elements (CDEs) aims to ensure consistent and comprehensive patient information across various healthcare systems. Existing methods often falter when standardizing CDEs of varying representation and complex structure, impeding data integration and interoperability in clinical research. This paper presents CDE-Mapper, a framework that combines a retrieval-augmented generation strategy with large language models to automate the alignment of CDEs with controlled vocabularies. Our modular approach features query decomposition to manage varying levels of CDEs complexity, integrates expert-defined rules within prompt engineering, and employs in-context learning alongside multiple retriever components to resolve terminological ambiguities. In addition, we propose a knowledge reservoir validated by a human-in-loop approach, achieving accurate concept linking for future applications while minimizing computational costs. For four diverse datasets, CDE-Mapper achieved an average of 7.2% higher accuracy improvement compared to baseline methods. This work highlights the potential of advanced language models in improving data harmonization and significantly advancing capabilities in clinical decision support systems and research.}
}
@article{OCHIENG2021114712,
title = {PAROT: Translating natural language to SPARQL},
journal = {Expert Systems with Applications},
volume = {176},
pages = {114712},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114712},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421001536},
author = {Peter Ochieng},
keywords = {, Natural language processing, Ontologies, Query},
abstract = {This paper provides a dependency based framework for converting natural language to SPARQL. We present a tool known as PAROT (which echos answers from ontologies) which is able to handle user’s queries that contain compound sentences, negation, scalar adjectives and numbered list. PAROT employs a number of dependency based heuristics to convert user’s queries to user’s triples. The user’s triples are then processed by the lexicon into ontology triples. It is these ontology triples that are used to construct SPARQL queries. From the experiments conducted, PAROT provides state of the art results.}
}
@incollection{TEKINERDOGAN202145,
title = {Chapter 3 - A feature-based ontology for cyber-physical systems},
editor = {Bedir Tekinerdogan and Dominique Blouin and Hans Vangheluwe and Miguel Goulão and Paulo Carreira and Vasco Amaral},
booktitle = {Multi-Paradigm Modelling Approaches for Cyber-Physical Systems},
publisher = {Academic Press},
pages = {45-65},
year = {2021},
isbn = {978-0-12-819105-7},
doi = {https://doi.org/10.1016/B978-0-12-819105-7.00008-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128191057000088},
author = {Bedir Tekinerdogan and Rakshit Mittal and Rima Al-Ali and Mauro Iacono and Eva Navarro-López and Soumyadip Bandyopadhyay and Ken Vanherpen and Ankica Barišić and Kuldar Taveter},
keywords = {Cyber-physical systems, feature modelling, ontology},
abstract = {In this chapter a feature-based ontology of cyber-physical systems is provided. We have adopted feature modelling to represent the common and variant features of a CPS. The CPS feature model has been developed after a thorough domain analysis on CPS. The resulting feature model shows the configuration space for developing CPSs. Two different case studies on CPS have been used to illustrate how to derive a concrete CPS configuration.}
}
@article{MOFFITT2021105029,
title = {Undergraduate nursing students' perceptions about creating culturally safe classrooms: Living the epistemology, ontology and pedagogy},
journal = {Nurse Education Today},
volume = {105},
pages = {105029},
year = {2021},
issn = {0260-6917},
doi = {https://doi.org/10.1016/j.nedt.2021.105029},
url = {https://www.sciencedirect.com/science/article/pii/S0260691721002860},
author = {Pertice Moffitt and Kerry Lynn Durnford},
keywords = {Cultural Safety, Relational practice, Brave space, Indigenous, Nursing education},
abstract = {Cultural Safety is a means of empowerment through antiracist actions, decolonising praxis, an understanding of the effects of a colonial history, and reconciliation to an equitable and inclusive place. The purpose of this paper is to share the perceptions of undergraduate students in northern Canada about the creation of culturally safe classrooms. A secondary analysis was conducted on archived data from undergraduate nursing student-led research. The findings illuminated four themes: sharing with genuineness, disrupting dissonance, addressing history and transforming through relationality. The themes inform a model that offers a beginning understanding of how culturally safe classrooms transpired in the Canadian north to enable students to become inclusive, relational, critical, and informed.}
}
@article{YUAN2025103905,
title = {Research on the construction and mapping model of knowledge organization system driven by standards},
journal = {Computer Standards & Interfaces},
volume = {92},
pages = {103905},
year = {2025},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2024.103905},
url = {https://www.sciencedirect.com/science/article/pii/S0920548924000746},
author = {Jingshu Yuan and Kexin Zhai and Hongxin Li and Man Yuan},
keywords = {Knowledge organization system, Multidisciplinary theory, Concept, Metadata, semantic, Standardization, Ontology},
abstract = {With the rapid development of artificial intelligence and enterprise digital transformation, the standardization organization, storage and management of semantic knowledge in computers have become the current research focus. As the core theory of knowledge system construction, knowledge organization (KO) provides theoretical support for the study of semantic knowledge organization and representation, among which knowledge organization system (KOS) is the important tool of semantic organization. At present, many scholars have carried out research from different perspectives of KOS based on theory, which provides the direction for the sustainable development of KOS. However, most of these studies focus on some aspects of KOS, which are in a "scattered" state, lacking systematic analysis of the basic principles of KOS construction and semantic organization based on theories and international standards. Therefore, this paper firstly constructs KOS theoretical models in the conceptual world and computer world respectively through a comprehensive study of multi-disciplinary basic theories such as semantics, logic, system theory, and international standards such as ISO 1087:2019, ISO 25964:2013, and ISO 11179:2023, and traces the iterative construction, organization and mapping process from "concept" in the conceptual world to "metadata" knowledge and semantics in the computer world. The semantic organization based on metadata is realized in computer. Secondly, on this basis, in order to realize ontology representation of domain knowledge, the ontology construction method based on MDR metadata is proposed. Finally, taking the semantic organization and ontology construction of Epicentre model in petroleum field as an example, the feasibility of the ideas and methods proposed in this paper is verified. The model and method proposed in this paper is independent of the specific type of KOS, so it is innovative and universal. The methodology is also applicable to other fields of conceptual system modeling, metadata standard construction, and data model modeling.}
}
@article{ZHU2023105074,
title = {Autonomous complex knowledge mining and graph representation through natural language processing and transfer learning},
journal = {Automation in Construction},
volume = {155},
pages = {105074},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105074},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523003345},
author = {Xiaofeng Zhu and Haijiang Li and Tengxiang Su},
keywords = {Knowledge mining, Natural language processing (NLP), Transfer learning, Knowledge modelling, Regulation document},
abstract = {Regulatory documents play a significant role in securing engineering project quality, standard process management and long-term sustainable developments. With the digitisation of knowledge in the AEC industry, the demand for automated knowledge mining has emerged when confronted with substantial regulations. However, the current interpretation approaches for regulatory documents are still mostly labour-intensive and flawed in complex knowledge. Based on transfer learning (BERT) and natural language processing (e.g., NLP-Syntactic Parsing), this paper proposes a fully automated knowledge mining framework to convert complex knowledge in textual regulations to graph-based knowledge representations. The framework uses a BERT-based engine to extract clauses from regulation documents through fine-tuning with the self-developed domain dataset. A constituent extractor is developed to process the provisions with complex knowledge and extract constituents. A knowledge modelling engine integrates the extracted constituents into a graph-based regulation knowledge model, which can be queried, visualised, and directly applied to downstream applications. The outcome has demonstrated promising performance in complex knowledge mining and knowledge graph modelling based on ISO 19650 case study. This research can effectively convert textual regulation documents to their counterpart regulatory knowledge base, contributing to automated knowledge acquisition and multi-domain knowledge fusion toward regulation digitalization.}
}
@article{FAN2025127434,
title = {TIME-UIE: Tourism-oriented figure information model and unified information extraction via large language models},
journal = {Expert Systems with Applications},
volume = {278},
pages = {127434},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127434},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425010565},
author = {Zhanling Fan and Chongcheng Chen and Haifeng Luo},
keywords = {Historical figure, Information model, Unified information extraction, Large language models, Cultural tourism, ChatGPT},
abstract = {Information extraction is crucial for building and updating the knowledge base of expert systems. Large language models face challenges with prompt sensitivity and model hallucinations during information extraction. This study introduces the TIME (Tourism, Individuals, Moments, Events) model, which organizes figure-related information into four main dimensions: attributes, relationships, events, and their linkage to tourism resources. Then present a unified information extraction framework for figures, termed TIME-UIE. This framework integrates a unified task definition, a format output constraint, carefully selected demonstrations, and knowledge injection to verify consistency across different inference chains. Experimental results show that TIME-UIE outperforms baseline models in deciphering complex relationships between historical figures by 26.2% and in extracting event triplets by 11.1%. The study also proposes a loose matching metric for model performance evaluation, which holds significant implications for the practical application of the research methods.}
}
@incollection{MCCAFFREY2020129,
title = {Chapter 10 - Ontologies, terminology mappings, and code sets},
editor = {Peter Mccaffrey},
booktitle = {An Introduction to Healthcare Informatics},
publisher = {Academic Press},
pages = {129-141},
year = {2020},
isbn = {978-0-12-814915-7},
doi = {https://doi.org/10.1016/B978-0-12-814915-7.00010-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128149157000107},
author = {Peter Mccaffrey},
keywords = {Ontology, Terminology, Code, ICD, CPT, DRG, LOINC, SNOMED, HCPCS},
abstract = {Healthcare is full of ontologies that seek to provide useful and conserved labels by which to summarize aspects of medical knowledge. Ontologies are useful both because many are widely used and because, when properly understood, they can be powerful tools for mapping attributes and relationships within medical datasets. This chapter focuses on the key ontologic systems encountered in healthcare data engineering and analysis and discusses their intended use, design, and relative merits in practice. After completing this chapter, the reader should have an understanding of how procedures, diagnoses, and medical goods are described in many systems and how different levels of summarization serve both healthcare operations and healthcare analysis. In combination with the previous chapter on interoperability standards such as HL7 and FHIR, this chapter completes a description of the representative form and content through which healthcare data are most commonly encountered and most effectively used.}
}
@article{ZHANG2025110607,
title = {Boost Protein Language Model with Injected Structure Information Through Parameter Efficient Fine-tuning},
journal = {Computers in Biology and Medicine},
volume = {195},
pages = {110607},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.110607},
url = {https://www.sciencedirect.com/science/article/pii/S0010482525009588},
author = {Zixun Zhang and Yuzhe Zhou and Jiayou Zheng and Chunmei Feng and Shuguang Cui and Sheng Wang and Zhen Li},
keywords = {Protein language model, Structure information injection, Angle embedding injection, Distance map attention injection, Parameter-efficient fine-tuning},
abstract = {Large-scale Protein Language Models (PLMs), such as the Evolutionary Scale Modeling (ESM) family, have significantly advanced our understanding of protein structures and functions. These models have shown immense potential in biomedical applications, including drug discovery, protein design, and understanding disease mechanisms at the molecular level. However, PLMs are typically pre-trained on residue sequences alone, with limited incorporation of structural information, presenting opportunities for further enhancement. In this paper, we propose Structure Information Injecting Tuning (SI-Tuning), a parameter-efficient fine-tuning method, to integrate structural information into PLMs. SI-Tuning maintains the original model parameters in a frozen state while optimizing task-specific vectors for input embedding and attention maps. Structural features, including dihedral angles and distance maps, are used to derive this vector, injecting the structural information that improves model performance in downstream tasks. Extensive experiments on 650M ESM-2 demonstrate the effectiveness of our SI-Tuning across multiple downstream tasks. Specifically, our SI-Tuning achieves an accuracy of 93.95% on DeepLoc binary classification, and 76.05% on Metal Ion Binding, outperforming SaProt, a large-scale pre-trained PLM with structural modeling. SI-Tuning effectively enhances the performance of PLMs by incorporating structural information in a parameter-efficient manner. Our method not only advances downstream task performance, but also offers significant computational efficiency, making it a valuable strategy for applying large-scale PLM to broad biomedical downstream applications. Code is available at https://github.com/Nocturne0256/SI-tuning.}
}
@article{ZHANG201826,
title = {An ontology-based approach supporting holistic structural design with the consideration of safety, environmental impact and cost},
journal = {Advances in Engineering Software},
volume = {115},
pages = {26-39},
year = {2018},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2017.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0965997817303319},
author = {Jisong Zhang and Haijiang Li and Yinghua Zhao and Guoqian Ren},
keywords = {Holistic structural design, Ontology, Environmental impact, Lifecycle cost, Multi-criteria decision support},
abstract = {Early stage decision-making for structural design critically influences the overall cost and environmental performance of buildings and infrastructure. However, the current approach often fails to consider the multi-perspectives of structural design, such as safety, environmental issues and cost in a comprehensive way. This paper presents a holistic approach based on knowledge processing (ontology) to facilitate a smarter decision-making process for early design stage by informing designers of the environmental impact and cost along with safety considerations. The approach can give a reasoning based quantitative understanding of how the design alternatives using different concrete materials can affect the ultimate overall performance. Embodied CO2 and cost are both considered along with safety criteria as indicative multi-perspectives to demonstrate the novelty of the approach. A case study of a concrete structural frame is used to explain how the proposed method can be used by structural designers when taking multi performance criteria into account. The major contribution of the paper lies on the creation of a holistic knowledge base which links through different knowledge across sectors to enable the structural engineer to come up with much more comprehensive decisions instead of individual single objective targeted delivery.}
}
@article{CHEN2024102,
title = {Computational screening of biomarkers and potential drugs for arthrofibrosis based on combination of sequencing and large nature language model},
journal = {Journal of Orthopaedic Translation},
volume = {44},
pages = {102-113},
year = {2024},
issn = {2214-031X},
doi = {https://doi.org/10.1016/j.jot.2023.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S2214031X23000918},
author = {Xi Chen and Cheng Li and Ziyuan Wang and Yixin Zhou and Ming Chu},
keywords = {AF, biomarker, GPR17, macrophage, Stiff knee},
abstract = {Background
Arthrofibrosis (AF) is a fibrotic joint disease resulting from excessive collagen production and fibrous scar formation after total knee arthroplasty (TKA). This devastating complication may cause consistent pain and dramatically reduction of functionality. Unfortunately, the conservative treatments to prevent the AF in the early stage are largely unknown due to the lack of specific biomarkers and reliable therapeutic targets.
Methods
In this study, we extracted1782 fibrosis related genes (FRGs) from 373,461published literature based on the large natural language processing models (ChatGPT) and intersected with the 2750 differential expressed genes (DEGs) from mRNA microarray (GSE135854). A total of 311 potential AF biomarker genes (PABGs) were obtained and functional analysis were performed including gene ontology (GO) annotation and the Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway enrichment analyses. Subsequently, we accomplished validation in AF animal models with immobilization of the unilateral knee joints of 16 rabbits for 1-week, 2-weeks, 3-weeks and 4-weeks. Finally, we tested the biomarkers in a retrospective cohort enrolled 35 AF patients and 35 control group patients.
Results
We identified G-protein-coupled receptor 17 (GPR17) as a reliable therapeutic biomarker for AF diagnosis with higher AUC (0.819) in the ROC curve. A total of 21 potential drugs targeted to GPR17 were screened. Among them, pranlukast and montelukast have achieved therapeutic effect in animal models. In addition, we established an online AF database for data integration (https://chenxi2023.shinyapps.io/afdbv1).
Conclusions
These results unveiling therapeutic biomarkers for AF diagnosis, and provide potential drugs for clinical treatment.
The translational potential of this article
Our study demonstrated that GPR17 holds significant promise as a potential biomarker and therapeutic target for arthrofibrosis. Moreover, pranlukast and montelukast targeted to GPR17 that could be instrumental in the treatment of AF.}
}
@article{AGRAWAL2025103778,
title = {Leveraging linked data for space constraints checking of mobile cranes in modular construction assembly lookahead planning},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103778},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103778},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625006718},
author = {Ajay Kumar Agrawal and Yang Zou and Long Chen and Mohammed Abdelmegid and Vicente A. González and Hongyu Jin},
keywords = {Linked data, Space constraint checking, Shapes Constraint Language, Lookahead schedule, Modular construction},
abstract = {Preparing constraint-free lookahead schedules (LAS) in the assembly stage of dynamic modular construction (MC) projects requires checking space availability for mobile crane operation using heterogeneous, distributed information sources. Current automated crane space evaluation methods rely on centralized information databases, whereas linked data based approaches are limited by insufficient geometric computation capabilities. This study proposes a framework to model and validate the space constraints for mobile crane operations using the semantic web. It starts with developing an ontology to represent crane lifting space requirements on the semantic web. Information sources, including construction site point clouds, 4D building information models, and crane specifications, are semantically interconnected using linked data. Shapes Constraint Language JavaScript Extension performs constraint validation through JavaScript-based mathematical computations utilizing the Separating Axis Theorem and a triangulation-based approach to check space for crane placement and rotation, respectively. Validation on two MC sites demonstrated the framework’s effectiveness in identifying space constraint violations.}
}
@article{KWON2024488,
title = {On knowing a gene: A distributional hypothesis of gene function},
journal = {Cell Systems},
volume = {15},
number = {6},
pages = {488-496},
year = {2024},
issn = {2405-4712},
doi = {https://doi.org/10.1016/j.cels.2024.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S2405471224001236},
author = {Jason J. Kwon and Joshua Pan and Guadalupe Gonzalez and William C. Hahn and Marinka Zitnik},
keywords = {lexical semantics, gene function, machine learning, artificial intelligence, distributed representations, word embeddings, large language models, transformers},
abstract = {Summary
As words can have multiple meanings that depend on sentence context, genes can have various functions that depend on the surrounding biological system. This pleiotropic nature of gene function is limited by ontologies, which annotate gene functions without considering biological contexts. We contend that the gene function problem in genetics may be informed by recent technological leaps in natural language processing, in which representations of word semantics can be automatically learned from diverse language contexts. In contrast to efforts to model semantics as “is-a” relationships in the 1990s, modern distributional semantics represents words as vectors in a learned semantic space and fuels current advances in transformer-based models such as large language models and generative pre-trained transformers. A similar shift in thinking of gene functions as distributions over cellular contexts may enable a similar breakthrough in data-driven learning from large biological datasets to inform gene function.}
}
@article{OZEN2025100679,
title = {Extracting chemical food safety hazards from the scientific literature automatically using large language models},
journal = {Applied Food Research},
volume = {5},
number = {1},
pages = {100679},
year = {2025},
issn = {2772-5022},
doi = {https://doi.org/10.1016/j.afres.2024.100679},
url = {https://www.sciencedirect.com/science/article/pii/S2772502224002890},
author = {Neris Özen and Wenjuan Mu and Esther D. {van Asselt} and Leonieke M. {van den Bulk}},
keywords = {Chemical contamination, Food safety, Information extraction, Prompt engineering, Natural language processing, Artificial intelligence},
abstract = {The number of scientific articles published in the domain of food safety has consistently been increasing over the last few decades. It has therefore become unfeasible for food safety experts to read all relevant literature related to food safety and the occurrence of hazards in the food chain. However, it is important that food safety experts are aware of the newest findings and can access this information in an easy and concise way. In this study, an approach is presented to automate the extraction of chemical hazards from the scientific literature through large language models. The large language model was used out-of-the-box and applied on scientific abstracts; no extra training of the models or a large computing cluster was required. Three different styles of prompting the model were tested to assess which was the most optimal for the task at hand. The prompts were optimized with two validation foods (leafy greens and shellfish) and the final performance of the best prompt was evaluated using three test foods (dairy, maize and salmon). The specific wording of the prompt was found to have a considerable effect on the results. A prompt breaking the task down into smaller steps performed best overall. This prompt reached an average accuracy of 93 % and contained many chemical contaminants already included in food monitoring programs, validating the successful retrieval of relevant hazards for the food safety domain. The results showcase how valuable large language models can be for the task of automatic information extraction from the scientific literature.}
}
@article{IVANIUK2024100842,
title = {Natural language processing and expert follow-up establishes tachycardia association with CDKL5 deficiency disorder},
journal = {Genetics in Medicine Open},
volume = {2},
pages = {100842},
year = {2024},
issn = {2949-7744},
doi = {https://doi.org/10.1016/j.gimo.2023.100842},
url = {https://www.sciencedirect.com/science/article/pii/S2949774423008518},
author = {Alina Ivaniuk and Christian M. Boßelmann and Xiaoming Zhang and Mark {St. John} and Sara C. Taylor and Gokul Krishnaswamy and Alex Milinovich and Peter F. Aziz and Elia Pestana-Knight and Dennis Lal},
keywords = {CDKL5 deficiency disorder, Electronic health records, Genotype-phenotype correlation, Natural language processing, Phenotyping},
abstract = {Purpose
CDKL5 deficiency disorder (CDD) is a developmental and epileptic encephalopathy with multisystemic comorbidities. Cardiovascular involvement in CDD was shown in animal models but is yet poorly described in CDD cohorts.
Methods
We identified 38 individuals with genetically confirmed CDD through the Cleveland Clinic CDD specialty clinic and matched 190 individuals with non-genetic epilepsy to them as a comparison group. Natural language processing was applied to yield Human Phenotype Ontology (HPO) terms from medical records. We conducted HPO association testing and manual chart review to explore cardiovascular comorbidities associated with CDD.
Results
We extracted 243,541 HPO terms from 30,512 medical encounters. Phenome-wide analysis confirmed well-established CDD phenotypes and identified association of tachycardia with CDD (Odds ratio 4.2, 95% confidence interval (CI) 1.75-9.93, Padj < .001). We found a 99.6-fold enrichment of supraventricular tachycardia (SVT) in CDD encounter notes (Padj < .001), which led to identification of 2 cases of fetal/neonatal onset SVT previously undescribed in CDD. Tachycardia in CDD individuals was associated with the presence of other autonomic symptoms (Odds ratio 5.63, 95% CI 1.08-40.3, P = .038).
Conclusion
CDD is associated with tachycardia, potentially including early-onset SVT. Alongside prospective validation studies, semiautomated genotype-phenotype analysis with matched controls is a scalable, rapid, and efficient approach for validating known and identifying novel phenotype associations.}
}
@article{JIMENEZMOLINA2018106,
title = {ProFUSO: Business process and ontology-based framework to develop ubiquitous computing support systems for chronic patients’ management},
journal = {Journal of Biomedical Informatics},
volume = {82},
pages = {106-127},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2018.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1532046418300650},
author = {Angel Jimenez-Molina and Jorge Gaete-Villegas and Javier Fuentes},
keywords = {Data architecture, Chronic disease management, Ubiquitous health (u-health) services, Clinical decision support systems (CDSS), Ubiquitous computing, Interoperability},
abstract = {New advances in telemedicine, ubiquitous computing, and artificial intelligence have supported the emergence of more advanced applications and support systems for chronic patients. This trend addresses the important problem of chronic illnesses, highlighted by multiple international organizations as a core issue in future healthcare. Despite the myriad of exciting new developments, each application and system is designed and implemented for specific purposes and lacks the flexibility to support different healthcare concerns. Some of the known problems of such developments are the integration issues between applications and existing healthcare systems, the reusability of technical knowledge in the creation of new and more sophisticated systems and the usage of data gathered from multiple sources in the generation of new knowledge. This paper proposes a framework for the development of chronic disease support systems and applications as an answer to these shortcomings. Through this framework our pursuit is to create a common ground methodology upon which new developments can be created and easily integrated to provide better support to chronic patients, medical staff and other relevant participants. General requirements are inferred for any support system from the primary attention process of chronic patients by the Business Process Management Notation. Numerous technical approaches are proposed to design a general architecture that considers the medical organizational requirements in the treatment of a patient. A framework is presented for any application in support of chronic patients and evaluated by a case study to test the applicability and pertinence of the solution.}
}
@article{BRYANT2021315,
title = {Securitising uncertainty: Ontological security and cultural scripts in smart farming technology implementation},
journal = {Journal of Rural Studies},
volume = {81},
pages = {315-323},
year = {2021},
issn = {0743-0167},
doi = {https://doi.org/10.1016/j.jrurstud.2020.10.051},
url = {https://www.sciencedirect.com/science/article/pii/S0743016720311839},
author = {Melanie Bryant and Vaughan Higgins},
keywords = {Smart farming, Technology implementation, Uncertainty, Meso-scale actors, Ontological security, Cultural scripts},
abstract = {Smart farming technologies are primarily associated with the transformation of agricultural productivity. Despite this, empirical research focusing on farm-level application of smart farming reveals a more complex and nuanced picture characterised by considerable uncertainty over its implementation and use. In this paper we seek to extend farm-level research by investigating two questions: how do perceived uncertainties destabilise meso-scale actors' routines and practices that are critical for ‘supporting farmer learning about the nature of digital data and its interpretation’ (Eastwood et al., 2019: 8); and, in what ways do meso-scale actors seek to re-establish a sense of stability and, in doing so, manage the uncertainty associated with smart farming implementation, and technological change more broadly? To address these questions we investigate the findings from a qualitative study of 20 meso-scale actors involved in the planning and implementation of smart farming technology in the Australian rice industry through an ontological security lens. We refer to meso-scale actors as farm advisors and agronomists whom we argue play a critical role in the uptake of smart farming technology. In applying this lens we argue that the perceived uncertainties related to smart farming de-stabilise or de-securitise actors' day-to-day roles and routines, impacting on who they are and what they do. We then demonstrate that actors draw upon two specific cultural scripts as a way to re-securitise their uncertainty. The first script seeks to securitise resource uncertainty by drawing upon known discourses surrounding farmer adoption of technology, while the second reproduces the importance of technologies that are easy to adopt while downplaying the importance of smart farming technology. While at face value these scripts can appear to create barriers to smart farming adoption, we argue that they can be a catalyst for developing solutions to uncertainty in terms of making smart farming more workable at the farm-level.}
}
@article{LU2024110042,
title = {A semantic model-based systems engineering approach for assessing the operational performance of metal forming process},
journal = {Computers & Industrial Engineering},
volume = {190},
pages = {110042},
year = {2024},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2024.110042},
url = {https://www.sciencedirect.com/science/article/pii/S0360835224001633},
author = {Jinzhi Lu and George Tsinarakis and Nikolaos Sarantinoudis and George Arampatzis and Xiaochen Zheng and Dimitris Kiritsis},
keywords = {Metal forming process, Model-based systems engineering, Petri nets, KARMA language, Semantic modeling},
abstract = {Metal Forming is a basic and essential industrial process to provide materials for constructing complex products. To design an efficient metal forming process, the functional requirements and operational performance are two important aspects to be considered. In this paper, a semantic Model-based Systems Engineering (sMBSE) approach is proposed to support the design of the entire metal forming process. A multi-architecture modeling language KARMA is used to develop meta-models and architecture models of the metal forming process from the perspectives of mission, operation, function, logic flow and physical structure. Enabled by customized software, the KARMA models are transformed to ontology models, which are then converted to Petri Net (PN) models. Simulations are conducted based on the PN models to evaluate the operational performance of the created processes. A case study based on a real metal forming scenario is conducted to evaluate the proposed approach using quantitative and qualitative analysis. The result proves that the proposed approach enables to formalize the entire architecture modeling for metal forming process; and to provide an efficient approach for evaluating the operational performance of the designed solution for initial verification.}
}
@article{GANGEMI2025100859,
title = {Logic Augmented Generation},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100859},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100859},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000453},
author = {Aldo Gangemi and Andrea Giovanni Nuzzolese},
keywords = {Knowledge graphs, Large language models, Logic augmented generation},
abstract = {Semantic Knowledge Graphs (SKG) face challenges with scalability, flexibility, contextual understanding, and handling unstructured or ambiguous information. However, they offer formal and structured knowledge enabling highly interpretable and reliable results by means of reasoning and querying. Large Language Models (LLMs) may overcome those limitations, making them suitable in open-ended tasks and unstructured environments. Nevertheless, LLMs are hardly interpretable and often unreliable. To take the best out of LLMs and SKGs, we envision Logic Augmented Generation (LAG) to combine the benefits of the two worlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate potentially infinite relations and tacit knowledge on-demand. LAG uses SKGs to inject a discrete heuristic dimension with clear logical and factual boundaries. We exemplify LAG in two tasks of collective intelligence, i.e., medical diagnostics and climate projections. Understanding the properties and limitations of LAG, which are still mostly unknown, is of utmost importance for enabling a variety of tasks involving tacit knowledge in order to provide interpretable and effective results.}
}
@article{KALYAN2024100048,
title = {A survey of GPT-3 family large language models including ChatGPT and GPT-4},
journal = {Natural Language Processing Journal},
volume = {6},
pages = {100048},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2023.100048},
url = {https://www.sciencedirect.com/science/article/pii/S2949719123000456},
author = {Katikapalli Subramanyam Kalyan},
keywords = {Large language models, LLMs, GPT-3, ChatGPT, GPT-4, Transformers, LLM survey},
abstract = {Large language models (LLMs) are a special class of pretrained language models (PLMs) obtained by scaling model size, pretraining corpus and computation. LLMs, because of their large size and pretraining on large volumes of text data, exhibit special abilities which allow them to achieve remarkable performances without any task-specific training in many of the natural language processing tasks. The era of LLMs started with OpenAI’s GPT-3 model, and the popularity of LLMs has increased exponentially after the introduction of models like ChatGPT and GPT4. We refer to GPT-3 and its successor OpenAI models, including ChatGPT and GPT4, as GPT-3 family large language models (GLLMs). With the ever-rising popularity of GLLMs, especially in the research community, there is a strong need for a comprehensive survey which summarizes the recent research progress in multiple dimensions and can guide the research community with insightful future research directions. We start the survey paper with foundation concepts like transformers, transfer learning, self-supervised learning, pretrained language models and large language models. We then present a brief overview of GLLMs and discuss the performances of GLLMs in various downstream tasks, specific domains and multiple languages. We also discuss the data labelling and data augmentation abilities of GLLMs, the robustness of GLLMs, the effectiveness of GLLMs as evaluators, and finally, conclude with multiple insightful future research directions. To summarize, this comprehensive survey paper will serve as a good resource for both academic and industry people to stay updated with the latest research related to GLLMs.}
}
@article{JAGATHEESAPERUMAL2025110885,
title = {A review on recent advancements of ChatGPT and datafication in healthcare applications},
journal = {Computers in Biology and Medicine},
volume = {197},
pages = {110885},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.110885},
url = {https://www.sciencedirect.com/science/article/pii/S0010482525012363},
author = {Senthil Kumar Jagatheesaperumal and Abinaya Pandiyarajan and Prabadevi Boopathy and N. Deepa and Artur Gomes Barreto and Victor Hugo C. {de Albuquerque}},
keywords = {ChatGPT, Datafication, Healthcare, Language modeling, Generative AI},
abstract = {Integrating ChatGPT, a large language model (LLM), with datafication strategies presents the capacity to provide reliable and user-friendly healthcare applications. This paper provides a structured and critical review of how ChatGPT, supported by datafication and big data frameworks, shapes areas such as clinical decision support, telemedicine, and personalized healthcare. It examines the cognitive capabilities of ChatGPT in interpreting complex medical language while addressing key ethical concerns, such as algorithmic bias, data privacy, and explainability, that arise in its deployment. Moreover, the study highlights challenges related to data quality, provenance, and interoperability, which are vital for real-world implementation. The review synthesizes recent literature, identifies research gaps, and explores policy and technical considerations necessary for responsible adoption. Rather than asserting promising outcomes, the paper contextualizes the capabilities and limitations of ChatGPT in current healthcare settings, aiming to inform future developments in AI-driven healthcare systems by bridging theoretical perspectives with early-stage implementations. The paper aims to guide future developments in AI-driven healthcare systems by bridging theoretical perspectives with emerging use cases.}
}
@article{OZGER2023102574,
title = {A robust protein language model for SARS-CoV-2 protein–protein interaction network prediction},
journal = {Artificial Intelligence in Medicine},
volume = {142},
pages = {102574},
year = {2023},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2023.102574},
url = {https://www.sciencedirect.com/science/article/pii/S093336572300088X},
author = {Zeynep Banu Ozger},
keywords = {Protein–protein interaction, Protein language model, SARS-CoV-2, Virus–host interaction, Natural language processing},
abstract = {Protein-protein interaction is one of the ways viruses interact with their hosts. Therefore, identifying protein interactions between viruses and hosts helps explain how virus proteins work, how they replicate, and how they cause disease. SARS-CoV-2 is a new type of virus that emerged from the coronavirus family in 2019 and caused a worldwide pandemic. Detection of human proteins interacting with this novel virus strain plays an important role in monitoring the cellular process of virus-associated infection. Within the scope of the study, a natural language processing-based collective learning method is proposed for the prediction of potential SARS-CoV-2-human PPIs. Protein language models were obtained with the prediction-based word2Vec and doc2Vec embedding methods and the frequency-based tf-idf method. Known interactions were represented by proposed language models and traditional feature extraction methods (conjoint triad and repeat pattern), and their performances were compared. The interaction data were trained with support vector machine, artificial neural network (ANN), k-nearest neighbor (KNN), naive Bayes (NB), decision tree (DT), and ensemble algorithms. Experimental results show that protein language models are a promising protein representation method for protein-protein interaction prediction. The term frequency-inverse document frequency-based language model performed the SARS-CoV-2 protein-protein interaction estimation with an error of 1.4%. Additionally, the decisions of high-performing learning models for different feature extraction methods were combined with a collective voting approach to make new interaction predictions. For 10,000 human proteins, 285 new potential interactions were predicted, with models combining decisions.}
}
@article{PADILHA2019105688,
title = {Experimental correlation analysis of bicluster coherence measures and gene ontology information},
journal = {Applied Soft Computing},
volume = {85},
pages = {105688},
year = {2019},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2019.105688},
url = {https://www.sciencedirect.com/science/article/pii/S1568494619304697},
author = {Victor Alexandre Padilha and André Carlos Ponce de Leon Ferreira de Carvalho},
keywords = {Biclustering, Coherence measures, Gene ontology, Gene expression data},
abstract = {Biclustering algorithms have become popular tools for gene expression data analysis. They can identify local patterns defined by subsets of genes and subsets of samples, which cannot be detected by traditional clustering algorithms. In spite of being useful, biclustering is an NP-hard problem. Therefore, the majority of biclustering algorithms look for biclusters optimizing a pre-established coherence measure. Many heuristics and validation measures have been proposed for biclustering over the last 20 years. However, there is a lack of an extensive comparison of bicluster coherence measures on practical scenarios. To deal with this lack, this paper experimentally analyzes 17 bicluster coherence measures and external measures calculated from information obtained in the gene ontologies. In this analysis, results were produced by 10 algorithms from the literature in 19 gene expression datasets. According to the experimental results, a few pairs of strongly correlated coherence measures could be identified, which suggests redundancy. Moreover, the pairs of strongly correlated measures might change when dealing with normalized or non-normalized data and biclusters enriched by different ontologies. Finally, there was no clear relation between coherence measures and assessment using information from gene ontology.}
}
@incollection{GALITSKY2025167,
title = {Chapter 6 - Large language model-based personalized recommendations in health},
editor = {Boris Galitsky},
booktitle = {Healthcare Applications of Neuro-Symbolic Artificial Intelligence},
publisher = {Academic Press},
pages = {167-220},
year = {2025},
isbn = {978-0-443-30046-2},
doi = {https://doi.org/10.1016/B978-0-443-30046-2.00008-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443300462000089},
author = {Boris Galitsky},
keywords = {Large language models, personalization, recommendation, metaprompts, abductive reasoning},
abstract = {Today, large language models (LLMs) are not good at personalization providing recommendation. They advise physicians and financial advisors to ask professionals in respective fields for help, even having user information available. Answering questions of software professionals, LLM needs to deliver in-depth answers with codes or algorithms, whereas for professionals in other fields would need definitions and main concepts. The intent of this chapter is to make LLM answers tailored to the needs of users, taking into account available information about them. To do that, we need to generalize available information about a person like her health record, maintaining the privacy of this person. We rely on metalearning techniques to design an LLM prompt to produce a personalization prompt to obtain suitable relevant information. Such “metaprompt” is produced by generalization operation applied to available documents for the user. These documents need to be de-identified so that they are sufficient for personalization on one hand and will maintain user privacy on the other hand. The second neuro-symbolic technique to support personalization is abductive reasoning, acting in parallel to LLM fine-tuning. Traditional recommendation and personalization techniques as well as modern, deep learning—based are presented, and the comparison is drawn to the proposed approach. We also share the evaluation and comparative analyses of these approaches. We consider an example for how to build personalization LLM systems coming from the Langchain platform. We will explore how to construct chains to form a personalization profile for a user and apply it to user search and recommendation requests. https://github.com/bgalitsky/LLM-personalization.}
}
@article{SUKHOVERKHOV2024101662,
title = {Lost and found language: From fuzzy logic to yūgen},
journal = {Language Sciences},
volume = {106},
pages = {101662},
year = {2024},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2024.101662},
url = {https://www.sciencedirect.com/science/article/pii/S0388000124000512},
author = {Anton Vladimirovich Sukhoverkhov and Alla Gennadievna Karipidi},
keywords = {Fuzziness of language, Theories of language, Ontology of language, Research methodology in linguistics, Sustainable ecolinguistics, Common-sense realism},
abstract = {On the one side of the language studies, we have linguists who support the practical and theoretical autonomy of language and linguistics and argue that “linguistics must attempt to grasp language, not as a conglomerate of non-linguistic (e.g., physical, physio-logical, psychological, logical, sociological) phenomena, but as a self-sufficient totality, a structure sui generis” (Hjelmslev, 1961, 5–6). On the other side, there are researchers who declare that ‘‘linguistics does not need to postulate the existence of languages as part of its theoretical apparatus’’ (Harris, 2003, 46) or “if you want to learn about language, forget about language!” (Steffensen, 2011, 204). By resorting to the methodology of fuzzy logic and fuzzy sets, ideas of Greek and Eastern philosophy, the research suggests moving away from theoretical binarisation and exploring gradients between extreme positions (autonomous vs heteronomous, universal vs situated, real vs constructed). First of all, the article extends further the ideas of Harris and Steffensen and introduces a new thesis: ‘if you want to lose the language, study it!’. Secondly, the research demonstrates the need for the practical and aesthetical acknowledgment of the reality of language (e.g., in education). To prove the first statement, the emptiness of theoretical efforts to find the entity of language, the research brings into play the ideas of fuzzy logic and critically revises realism, conceptualism and nominalism in language studies. The work provides evidences that neither ‘language’ nor ‘dialects’ or ‘idiolects’ can be found in practice due to the inherent fuzziness of the linguistic facts (systems) ‘described’ by these clear-cut categories. It is argued that theories and concepts designed for the description of the language-related phenomena are theoretical constructions that do not fully capture the stochastic and dynamic reality of language. Instead, they merely construct or declare it, similar to how we create star constellations (Steffensen and Fill, 2014). It resonates with the idea that can be found in Zen Buddhism and Taoism: “name it and you will lose it”. The research also holds that even if ‘language’ is an “ensemble of idiolects, sociolects, dialects and so on – rather than an entity per se” (Hazan, 2015, 11), we cannot find a token of its existence in either the entity or in the elements (ensemble) constituting that hypothetical entity. The article concludes that those researchers who focus on the particular nature of language lose its complexity; conversely, those who embrace all aspects (e.g., integrational approaches) lose its entity. However, if we are not able to grasp theoretically the reality of language does it mean it has no reality whatsoever and researchers and learners cannot have any positive knowledge about the language? The article offers some analogies in favour of the reality of language (comparison with music, road traffic, star constellations) and methods for its understanding based on intuition, metaphorical thinking and aesthetic comprehension. An example of such intuitive observation can be found in Japanese culture through such aesthetic categories as shibui 渋い, wabi-sabi 侘寂 and yūgen 幽玄. As a result, the article proposes the idea of sustainable ecolinguistics that includes in the body of integrational approaches the phronesis of common-sense realism in language teaching/learning and “unscientific impressionism in language studies” (quoted from Steffensen, 2011, 204).}
}
@article{BARUAH2025242,
title = {Named Entity Recognition in Assamese Language using two separate models: BiLSTM and BERT},
journal = {Procedia Computer Science},
volume = {258},
pages = {242-251},
year = {2025},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.04.262},
url = {https://www.sciencedirect.com/science/article/pii/S187705092501364X},
author = {Plabita Baruah and Bandana Dutta and Shikhar Kumar Sarma and Kuwali Talukdar},
keywords = {NLP, Named Entity Recognition, Assamese Language, BiLSTM, NER dataset, NER Models, CRF, BERT},
abstract = {Named Entity Recognition (NER) is a tool based on principles of Artificial Intelligence (AI) and Natural Language Processing (NLP) for automatically tagging Named Entities from unstructured text. In the realm of Natural Language Processing (NLP) applications, Named Entity Recognition (NER) holds significance as it involves the crucial task of identifying and categorizing proper nouns into classes such as person, location, organization, and miscellaneous. While considerable progress has been made in widely spoken languages like English and other European languages, resulting in higher accuracy rates, the task of NER in Indian languages prove to be challenging due to limited resources. This study explores the implementation of NER in Assamese using two separate approaches: BiLSTM and BERT. The proposed methodology achieves an accuracy of 31%in the BiLSTM model. While using BERT, which is a pretrained model, fine-tuned for Assamese, we achieved a precision of 81.5% and F1- score of 0.383. Our comparative analysis shows that both models are effective for NER in a resource-scarce language like Assamese, but BERT performs better overall in recognizing entities. This suggests that BERT could play a key role in improving NER techniques for underrepresented languages.}
}
@incollection{BERMAN201897,
title = {5 - Classifications and Ontologies},
editor = {Jules J. Berman},
booktitle = {Principles and Practice of Big Data (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {97-135},
year = {2018},
isbn = {978-0-12-815609-4},
doi = {https://doi.org/10.1016/B978-0-12-815609-4.00005-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128156094000054},
author = {Jules J. Berman},
keywords = {Ontology, Classification, Class, Subclass, Superclass, Class hierarchy, Ontologic competence, Instances, Class object},
abstract = {Information has limited value unless it can take its place within our general understanding of the world. “How does this thing relate to that thing?” is often the central question of scientific efforts. Ontologies are formal systems that relate different information objects into classes and relate classes of information objects to other classes, often as a hierarchical lineage (i.e., classes that have superclasses and subclasses). Scientific analyses of large information resources can be greatly enhanced if every data object in the resource is positioned somewhere within a formal ontology. Using ontologies, scientists can determine whether observations on a single object will apply to other objects in the same class. Similarly, scientists can begin to ask whether observations that hold true for a class of objects will relate to other classes of objects. Basically, ontologies help scientists complete one of their most important tasks; determining how things relate to each other. This chapter will describe how ontologies are constructed, and how they are used for scientific discovery in Big Data resources.}
}
@article{BIGAJ2018145,
title = {Are field quanta real objects? Some remarks on the ontology of quantum field theory},
journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
volume = {62},
pages = {145-157},
year = {2018},
issn = {1355-2198},
doi = {https://doi.org/10.1016/j.shpsb.2017.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S1355219817300692},
author = {Tomasz Bigaj},
keywords = {Quantum field theory, Fock space, Harmonic oscillators, Field quantization, Particles, Ontology},
abstract = {One of the key philosophical questions regarding quantum field theory is whether it should be given a particle or field interpretation. The particle interpretation of QFT is commonly viewed as being undermined by the well-known no-go results, such as the Malament, Reeh-Schlieder and Hegerfeldt theorems. These theorems all focus on the localizability problem within the relativistic framework. In this paper I would like to go back to the basics and ask the simple-minded question of how the notion of quanta appears in the standard procedure of field quantization, starting with the elementary case of the finite numbers of harmonic oscillators, and proceeding to the more realistic scenario of continuous fields with infinitely many degrees of freedom. I will try to argue that the way the standard formalism introduces the talk of field quanta does not justify treating them as particle-like objects with well-defined properties.}
}
@article{LU2024,
title = {Enhancing Clinical Relevance of Pretrained Language Models Through Integration of External Knowledge: Case Study on Cardiovascular Diagnosis From Electronic Health Records},
journal = {JMIR AI},
volume = {3},
year = {2024},
issn = {2817-1705},
doi = {https://doi.org/10.2196/56932},
url = {https://www.sciencedirect.com/science/article/pii/S2817170524000437},
author = {Qiuhao Lu and Andrew Wen and Thien Nguyen and Hongfang Liu},
keywords = {knowledge integration, pre-trained language models, physician reasoning, adapters, physician, physicians, electronic health record, electronic health records, EHR, healthcare, heterogeneous, healthcare institution, healthcare institutions, proprietary information, healthcare data, methodology, text classification, data privacy, medical knowledge},
abstract = {Background
Despite their growing use in health care, pretrained language models (PLMs) often lack clinical relevance due to insufficient domain expertise and poor interpretability. A key strategy to overcome these challenges is integrating external knowledge into PLMs, enhancing their adaptability and clinical usefulness. Current biomedical knowledge graphs like UMLS (Unified Medical Language System), SNOMED CT (Systematized Medical Nomenclature for Medicine–Clinical Terminology), and HPO (Human Phenotype Ontology), while comprehensive, fail to effectively connect general biomedical knowledge with physician insights. There is an equally important need for a model that integrates diverse knowledge in a way that is both unified and compartmentalized. This approach not only addresses the heterogeneous nature of domain knowledge but also recognizes the unique data and knowledge repositories of individual health care institutions, necessitating careful and respectful management of proprietary information.
Objective
This study aimed to enhance the clinical relevance and interpretability of PLMs by integrating external knowledge in a manner that respects the diversity and proprietary nature of health care data. We hypothesize that domain knowledge, when captured and distributed as stand-alone modules, can be effectively reintegrated into PLMs to significantly improve their adaptability and utility in clinical settings.
Methods
We demonstrate that through adapters, small and lightweight neural networks that enable the integration of extra information without full model fine-tuning, we can inject diverse sources of external domain knowledge into language models and improve the overall performance with an increased level of interpretability. As a practical application of this methodology, we introduce a novel task, structured as a case study, that endeavors to capture physician knowledge in assigning cardiovascular diagnoses from clinical narratives, where we extract diagnosis-comment pairs from electronic health records (EHRs) and cast the problem as text classification.
Results
The study demonstrates that integrating domain knowledge into PLMs significantly improves their performance. While improvements with ClinicalBERT are more modest, likely due to its pretraining on clinical texts, BERT (bidirectional encoder representations from transformer) equipped with knowledge adapters surprisingly matches or exceeds ClinicalBERT in several metrics. This underscores the effectiveness of knowledge adapters and highlights their potential in settings with strict data privacy constraints. This approach also increases the level of interpretability of these models in a clinical context, which enhances our ability to precisely identify and apply the most relevant domain knowledge for specific tasks, thereby optimizing the model’s performance and tailoring it to meet specific clinical needs.
Conclusions
This research provides a basis for creating health knowledge graphs infused with physician knowledge, marking a significant step forward for PLMs in health care. Notably, the model balances integrating knowledge both comprehensively and selectively, addressing the heterogeneous nature of medical knowledge and the privacy needs of health care institutions.}
}
@article{HULTIN201991,
title = {On becoming a sociomaterial researcher: Exploring epistemological practices grounded in a relational, performative ontology},
journal = {Information and Organization},
volume = {29},
number = {2},
pages = {91-104},
year = {2019},
issn = {1471-7727},
doi = {https://doi.org/10.1016/j.infoandorg.2019.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1471772717302488},
author = {Lotta Hultin},
keywords = {Sociomateriality, Post-humanist methodology, Epistemology, Performativity, Ontology of becoming, Entanglement, Ethnography, Practice of research},
abstract = {What is the role of the researcher in a world that is continuously enacted and reconfigured in sociomaterial practices, a world in which subject and object, structure and agency, body and mind, knower and known, are assumed to be ontologically inseparable? In this article, I explore this question by drawing on my own experiences of reconsidering essentialist and representationalist assumptions, and becoming a sociomaterial researcher. My exploration draws on my experiences of conducting a qualitative longitudinal case study at the Swedish Migration Board. Specifically, I show what it can mean to ‘invite materiality’ into interviews, examine the conditions of possibility to become in certain ways by tracing the genealogy of practices, and engage with data relationally rather than categorically. By accounting for my experience of working through these practices, I aim to develop and articulate an understanding of what the ontological position underlying a sociomaterial approach implies for epistemology, and of how we can act (or, rather, intra-act) more creatively and responsibly as sociomaterial researchers. Moreover, I highlight differences in the kinds of knowledge that a sociomaterial approach grounded in relational and performative onto-epistemologies, as opposed to a socio-material approach, grounded in critical realism, produce about the unfolding of organizational practices—specifically, the practices unfolding in the reception area of the Swedish Migration Board. The paper contributes to the current debate on sociomaterial approaches, and in particular to the development of practices available to draw upon for researchers taking a sociomaterial approach.}
}
@article{NILASHI2018507,
title = {A recommender system based on collaborative filtering using ontology and dimensionality reduction techniques},
journal = {Expert Systems with Applications},
volume = {92},
pages = {507-520},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.09.058},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417306577},
author = {Mehrbakhsh Nilashi and Othman Ibrahim and Karamollah Bagherifard},
keywords = {Recommender systems, Ontology, Clustering, Dimensionality reduction, Scalability, Sparsity},
abstract = {Improving the efficiency of methods has been a big challenge in recommender systems. It has been also important to consider the trade-off between the accuracy and the computation time in recommending the items by the recommender systems as they need to produce the recommendations accurately and meanwhile in real-time. In this regard, this research develops a new hybrid recommendation method based on Collaborative Filtering (CF) approaches. Accordingly, in this research we solve two main drawbacks of recommender systems, sparsity and scalability, using dimensionality reduction and ontology techniques. Then, we use ontology to improve the accuracy of recommendations in CF part. In the CF part, we also use a dimensionality reduction technique, Singular Value Decomposition (SVD), to find the most similar items and users in each cluster of items and users which can significantly improve the scalability of the recommendation method. We evaluate the method on two real-world datasets to show its effectiveness and compare the results with the results of methods in the literature. The results showed that our method is effective in improving the sparsity and scalability problems in CF.}
}
@incollection{SINGLE20201783,
title = {Computer-Aided Hazop: Ontologies and Ai for Hazard Identification and Propagation},
editor = {Sauro Pierucci and Flavio Manenti and Giulia Luisa Bozzano and Davide Manca},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {48},
pages = {1783-1788},
year = {2020},
booktitle = {30th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-12-823377-1.50298-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128233771502986},
author = {Johannes I. Single and Jürgen Schmidt and Jens Denecke},
keywords = {Computer-aided HAZOP studies, safety engineering ontologies, inference-based hazard identification, case-based reasoning, support vector machine},
abstract = {The Hazard and Operability (HAZOP) study is an accepted hazard identification technique in the chemical process industry. It is a time and labor-intensive process and it can be prone to human error. Computer-aided HAZOP systems can be used to support human experts. In this research approach a semantically connected knowledge structure in the form of an ontology was developed that stores process safety engineering knowledge. Based on ontologies artificial intelligence methods and semantic reasoners are used to analyze the knowledge structure. Preliminary results show that an ontology-based reasoning algorithm in combination with case-based reasoning or a support vector machine algorithm is well-suited to infer hazards including their propagation.}
}
@article{ALAHMAR2020105559,
title = {Ontological framework for standardizing and digitizing clinical pathways in healthcare information systems},
journal = {Computer Methods and Programs in Biomedicine},
volume = {196},
pages = {105559},
year = {2020},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2020.105559},
url = {https://www.sciencedirect.com/science/article/pii/S0169260719324356},
author = {Ayman Alahmar and Matteo Ermando Crupi and Rachid Benlamri},
keywords = {Clinical pathway, Semantic web, Data analytics, SNOMED CT, Health information systems, Standardization and coding systems},
abstract = {Background and Objective: Most healthcare institutions are reorganizing their healthcare delivery systems based on Clinical Pathways (CPs). CPs are novel medical management plans to standardize medical activities, reduce cost, optimize resource usage, and improve the quality of service. However, most CPs are still paper-based and not fully integrated with Health Information Systems (HIS). More CP computerization research is therefore needed to fully benefit from CP’s practical potentials. A major contribution of this research is the vision that CP systems deserve to be placed at the centre of HIS, because within CPs lies the very heart of medical planning, treatment and impressions, including healthcare quality and cost factors. Methods: An important contribution to the realization of this vision is to fully standardize and digitize CPs so that they become machine-readable and smoothly linkable across various HIS. To achieve this goal, this research proposes a framework for (i) CP knowledge representation and sharing using ontologies, (ii) CP standardization based on SNOMED CT and HL7, and (iii) CP digitization based on a novel coding system to encode CP data. To show the feasibility of the proposed framework we developed a prototype clinical pathway management system (CPMS) based on CPs currently in use at hospitals. Results: The results show that CPs can be fully standardized and digitized using SNOMED CT terms and codes, and the CPMS can work as an independent system, performing novel CP-related functions, including useful data analytics. CPs can be compared easily for auditing and quality management. Furthermore, the CPMS was smoothly linked to a hospital EMR and CP data were captured in EMR without any loss. Conclusion: The proposed framework is promising and contributes toward solving major challenges related to CP standardization, digitization, and inclusion in today’s modern computerized hospitals.}
}
@incollection{SAHA2021177,
title = {Chapter 13 - Ontology-based intelligent decision support systems: A systematic approach},
editor = {Sarika Jain and Vishal Jain and Valentina Emilia Balas},
booktitle = {Web Semantics},
publisher = {Academic Press},
pages = {177-193},
year = {2021},
isbn = {978-0-12-822468-7},
doi = {https://doi.org/10.1016/B978-0-12-822468-7.00005-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128224687000055},
author = {Ramesh Saha and Sayani Sen and Jayita Saha and Asmita Nandy and Suparna Biswas and Chandreyee Chowdhury},
keywords = {Ontology, decision support system, web semantics, machine learning, internet of things},
abstract = {Intelligent and smart health monitoring is prevalent nowadays with the support of advancement in Internet of Things, machine learning, and ontology-based decision support systems. As a decision support system can analyze current patient vitals based on historical data, effective data representation from different data sources into a common knowledge base is essential. Web semantics has an increasingly important role to play here in terms of storing data following ontology for more usable knowledge repository. The findings of the decision support system can be fed to doctor’s smartphone as a message based on which the doctor may intervene in a specific scenario or may validate his own diagnosis with the one provided by the decision support system. As the comfort and convenience of the end-users of remote healthcare is important, in addition to quality of service, quality of experience is a matter of concern among other issues and challenges. This work emphasizes on several Machine Learning (ML) algorithms, ontology techniques to design and implement intelligent decision support system for effective healthcare support satisfying quality of service and quality of experience requirements.}
}
@incollection{GALITSKY202581,
title = {Chapter 4 - Extending large language model capabilities beyond reasoning},
editor = {Boris Galitsky},
booktitle = {Healthcare Applications of Neuro-Symbolic Artificial Intelligence},
publisher = {Academic Press},
pages = {81-106},
year = {2025},
isbn = {978-0-443-30046-2},
doi = {https://doi.org/10.1016/B978-0-443-30046-2.00005-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443300462000053},
author = {Boris Galitsky and Alexander Rybalov},
keywords = {LLMs and irrational decision-making, abstract meaning representation (AMR), semantic representation in health, ontology construction with LLMs, discourse-level reasoning accuracy},
abstract = {In this chapter, we explore how large language models (LLMs) can be extended to simulate irrational decision-making and its potential applications in the health domain. We also examine the role of abstract meaning representation (AMR) in enhancing semantic representation on top of LLMs. We assess the contribution of these components, as well as those introduced in Chapter 3, to the overall accuracy of reasoning and decision-making. A 1.0%–2.5% boost is observed in reasoning performance due to integration with a spectrum of reasoning components, such as ProbLog, ASP, AMR, and others.}
}
@article{QU2024100152,
title = {Semantically triggered qualitative simulation of a geological process},
journal = {Applied Computing and Geosciences},
volume = {21},
pages = {100152},
year = {2024},
issn = {2590-1974},
doi = {https://doi.org/10.1016/j.acags.2023.100152},
url = {https://www.sciencedirect.com/science/article/pii/S2590197423000411},
author = {Yuanwei Qu and Eduard Kamburjan and Anita Torabi and Martin Giese},
keywords = {Geological ontology modeling, Semantic lifting, Process simulation, Geological process modeling},
abstract = {The field of geology has been the subject of a range of research efforts aiming to formalize geological domain knowledge, notably through geological domain ontologies. The main focus of existing geological ontologies primarily lies in describing static geological objects and their properties, paying less attention to the knowledge concerning geological processes and events. Meanwhile, the geological process modeling and simulation predominantly rely on quantitative numerical approaches that necessitate comprehensive and abundant data as input. However, many geological processes took place on a million-year time scale with insufficient data and non-direct observations. Given the inherent incompleteness of geological data, geologists still rely on qualitative reasoning to validate their interpretations. There is currently a dearth of applicable methods to facilitate qualitative reasoning and simulate geological processes based on domain knowledge. We propose to model the effects of a geological process through an object-oriented program, while keeping an ontological representation of the situation at each instant. To combine the two models, we propose using semantically defined ‘process triggers.’ These process triggers are defined as part of the ontology, in accordance with the Basic Formal Ontology. They enable geologists to describe the precise moment when a geological process is triggered and initiated. On the computational program side, we employ the ‘Semantic Micro Object Language’ to embody the knowledge and rules provided by geologists, facilitating the simulation of geological processes. Through an evaluation experiment, our proposed approach demonstrates promising results within a reasonable timeframe. As proof of concept, we have applied our method to a real-world scenario of petroleum thermal maturation in Ekofisk Field and got a promising result. Our approach provides a formalism that allows a powerful code to interact with domain ontologies, which paves the path for future knowledge reasoning.}
}
@article{AKINYEMI2018111,
title = {An ontology-based data integration framework for construction information management},
journal = {Proceedings of the Institution of Civil Engineers - Management, Procurement and Law},
volume = {171},
number = {3},
pages = {111-125},
year = {2018},
issn = {1751-4304},
doi = {https://doi.org/10.1680/jmapl.17.00052},
url = {https://www.sciencedirect.com/science/article/pii/S1751430418000351},
author = {Abiodun Akinyemi and Ming Sun and Alasdair J G Gray},
keywords = {business, information technology, management},
abstract = {Information management during the construction phase of a built asset involves multiple stakeholders using multiple software applications to generate and store data. This is problematic as data come in different forms and are labour intensive to piece together. Existing solutions to this problem are predominantly in proprietary applications, which are sometimes cost prohibitive for small engineering firms, or conceptual studies with use cases that cannot be easily adapted. In view of these limitations, this research presents an ontology-based data integration framework that makes use of open-source tools that support Semantic Web technologies. The proposed framework enables rapid answering of queries over construction data integrated from heterogeneous sources, data quality checks and reuse of project software resources. The attributes and functionalities of the proposed solution align with the requirements common to small firms with limited information technology skill and budget. Consequently, this solution can be of great benefit for their data projects.}
}
@article{MEGHRAOUI2025100125,
title = {A new integrated neurosymbolic approach for crop-yield prediction using environmental data and satellite imagery at field scale},
journal = {Artificial Intelligence in Geosciences},
volume = {6},
number = {1},
pages = {100125},
year = {2025},
issn = {2666-5441},
doi = {https://doi.org/10.1016/j.aiig.2025.100125},
url = {https://www.sciencedirect.com/science/article/pii/S2666544125000218},
author = {Khadija Meghraoui and Teeradaj Racharak and Kenza {Ait El Kadi} and Saloua Bensiali and Imane Sebari},
keywords = {Crop-yield prediction, Neuro-symbolic AI, Ontology, Ontology embedding, Satellite imagery, Machine learning},
abstract = {Crop-yield is a crucial metric in agriculture, essential for effective sector management and improving the overall production process. This indicator is heavily influenced by numerous environmental factors, particularly those related to soil and climate, which present a challenging task due to the complex interactions involved. In this paper, we introduce a novel integrated neurosymbolic framework that combines knowledge-based approaches with sensor data for crop-yield prediction. This framework merges predictions from vectors generated by modeling environmental factors using a newly developed ontology focused on key elements and evaluates this ontology using quantitative methods, specifically representation learning techniques, along with predictions derived from remote sensing imagery. We tested our proposed methodology on a public dataset centered on corn, aiming to predict crop-yield. Our developed smart model achieved promising results in terms of crop-yield prediction, with a root mean squared error (RMSE) of 1.72, outperforming the baseline models. The ontology-based approach achieved an RMSE of 1.73, while the remote sensing-based method yielded an RMSE of 1.77. This confirms the superior performance of our proposed approach over those using single modalities. This integrated neurosymbolic approach demonstrates that the fusion of statistical and symbolic artificial intelligence (AI) represents a significant advancement in agricultural applications. It is particularly effective for crop-yield prediction at the field scale, thus facilitating more informed decision-making in advanced agricultural practices. Additionally, it is acknowledged that results might be further improved by incorporating more detailed ontological knowledge and testing the model with higher-resolution imagery to enhance prediction accuracy.}
}
@article{SHAW2025106282,
title = {Knowledge graph for policy- and practice-aligned life cycle analysis and reporting},
journal = {Automation in Construction},
volume = {176},
pages = {106282},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106282},
url = {https://www.sciencedirect.com/science/article/pii/S092658052500322X},
author = {Conor Shaw and Flávia {de Andrade Pereira} and Martijn {de Riet} and Cathal Hoare and Karim Farghaly and James O’Donnell},
keywords = {Environmental policy, Life cycle assessment, Asset management, Information management, Requirements engineering, Knowledge graph, Ontology engineering},
abstract = {The built environment is a key leverage point for policy intervention to combat climate change and the statutory reporting of financial and non-financial indicators over the asset lifecycle is increasingly required. This poses significant information management challenges in a sector characterised by complexity. Contributions to-date which address Life Cycle Asset Information Management (LCAIM) remain siloed and difficult to generalise, resulting in limited in-practice uptake, but domain literature identifies graph databases and ontologies as suitable strategies for addressing this information-intensive challenge. This paper provides a LCAIM ontology, co-developed with stakeholders, and verified technically through implementation in a case study by responding to end-user-defined storage, retrieval, and enrichment functions using a knowledge graph. The prototype is then validated qualitatively with experts who perceive it as addressing collective governance-practice requirements. Overall, the study suggests that addressing technical LCAIM challenges may be feasible using available technologies and recommends prioritising research towards socio-economic issues.}
}
@incollection{ADEL2019315,
title = {Chapter 13 - Ontology-based electronic health record semantic interoperability: A survey},
editor = {Nilanjan Dey and Amira S. Ashour and Simon James Fong and Surekha Borra},
booktitle = {U-Healthcare Monitoring Systems},
publisher = {Academic Press},
pages = {315-352},
year = {2019},
series = {Advances in Ubiquitous Sensing Applications for Healthcare},
issn = {25891014},
doi = {https://doi.org/10.1016/B978-0-12-815370-3.00013-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012815370300013X},
author = {Ebtsam Adel and Shaker El-Sappagh and Sherif Barakat and Mohammed Elmogy},
keywords = {Ontology, Semantic interoperability, EHR system, Healthcare domain, Electronic health record (HER), Ubiquitous healthcare (u-Health)},
abstract = {EHR semantic interoperability is urgently needed for systems to improve healthcare quality. However, its achievement has many challenges and barriers. The primary purpose of this chapter is to attract attention to the importance of semantic interoperability in the healthcare industry. We intensely surveyed the current literature for aspects of semantic interoperability, including its main definitions, standards, schemas, models, terminologies, barriers, and future challenges. We depended on the existing databases, including ScienceDirect, IEEE Xplore, PubMed, ELSEVIER, MEDLINE, Cochranelibrary, Informit, and Springer. The results include a comprehensive survey of healthcare semantic interoperability. We noted that the most intuitive EHR semantic interoperability approach is based on standards, but they have some challenges. The medical domain is characterized by its vagueness and uncertainty. All parties in the healthcare domain use imprecise concepts to describe their ideas. As a result, we recommend fuzzy ontology to achieve the target goal of global interoperability.}
}
@incollection{KUILER2018161,
title = {Chapter 10 - Federal Big Data Analytics in the Health Domain: An Ontological Approach to Data Interoperability},
editor = {Feras A. Batarseh and Ruixin Yang},
booktitle = {Federal Data Science},
publisher = {Academic Press},
pages = {161-176},
year = {2018},
isbn = {978-0-12-812443-7},
doi = {https://doi.org/10.1016/B978-0-12-812443-7.00010-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128124437000107},
author = {Erik W. Kuiler and Connie L. McNeely},
keywords = {Big data analytics, Data interoperability, Health data, Lexicon, Ontology},
abstract = {“Big data” in the health domain occupies a critical position on the federal policy and research agenda, with emphasis on leveraging large, complex data sets to manage population health, drive down disease rates, and control costs. The complexity of big data analytics requires new rules and algorithms to effect the interoperability of data derived from multiple sources. Accordingly, a lexicon and ontology-based approach to data interoperability is offered as a practical and adaptable framework to address challenges of data interoperability presented by big health data analytics and related issues. The use of ontologies as descriptive, heuristic, and normative instruments is presented as means for facilitating data interoperability by ensuring semantic congruity and syntactic conformance within and across large and complex data sets. A framework is provided for an ontological approach to health data interoperability, focusing on the importance of standards and considering implications for practice and policy in relevant federal agencies.}
}
@incollection{DEVANAND20182047,
title = {An Ontology Based Cyber-infrastructure for the Development of Smart Eco Industrial Parks},
editor = {Mario R. Eden and Marianthi G. Ierapetritou and Gavin P. Towler},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {44},
pages = {2047-2052},
year = {2018},
booktitle = {13th International Symposium on Process Systems Engineering (PSE 2018)},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-64241-7.50336-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780444642417503360},
author = {Aravind Devanand and Li Zhou and Iftekhar A. Karimi and Markus Kraft},
keywords = {ontology, knowledge base, modular nuclear power},
abstract = {This paper provides insight into the methodology employed for the development of a smart system called the J-Park simulator (JPS). JPS provides a virtual representation of an Eco-Industrial Park (EIP) and contains information pertaining to various aspects of an EIP. An ontology based approach is used to store this information in a structured and machine-readable form. Ontology represents data in a structured form and can be used for the creation of knowledge bases. These knowledge bases can then be utilised to conduct case studies on JPS. In one such case study the most suitable locations for modular nuclear plants in an EIP is identified. The architecture for the case study and how it can be implemented in JPS is demonstrated in the paper.}
}
@article{WEI2025104403,
title = {Enhancing vision–language contrastive representation learning using domain knowledge},
journal = {Computer Vision and Image Understanding},
volume = {259},
pages = {104403},
year = {2025},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2025.104403},
url = {https://www.sciencedirect.com/science/article/pii/S1077314225001262},
author = {Xiaoyang Wei and Camille Kurtz and Florence Cloppet},
keywords = {Visual representation, Knowledge graph, Vision–language contrastive learning},
abstract = {Visual representation learning plays a key role in solving medical computer vision tasks. Recent advances in the literature often rely on vision–language models aiming to learn the representation of medical images from the supervision of paired captions in a label-free manner. The training of such models is however very data/time intensive and the alignment strategies involved in the contrastive loss functions may not capture the full richness of information carried by inter-data relationships. We assume here that considering expert knowledge from the medical domain can provide solutions to these problems during model optimization. To this end, we propose a novel knowledge-augmented vision–language contrastive representation learning framework consisting of the following steps: (1) Modeling the hierarchical relationships between various medical concepts using expert knowledge and medical images in a dataset through a knowledge graph, followed by translating each node into a knowledge embedding; And (2) integrating knowledge embeddings into a vision–language contrastive learning framework, either by introducing an additional alignment loss between visual and knowledge embeddings or by relaxing binary constraints of vision–language alignment using knowledge embeddings. Our results demonstrate that the proposed solution achieves competitive performances against state-of-the-art approaches for downstream tasks while requiring significantly less training data. Our code is available at https://github.com/Wxy-24/KL-CVR.}
}
@article{HUITZIL2024100731,
title = {Semantic Building Information Modeling: An empirical evaluation of existing tools},
journal = {Journal of Industrial Information Integration},
volume = {42},
pages = {100731},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100731},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24001742},
author = {Ignacio Huitzil and Miguel Molina-Solana and Juan Gómez-Romero and Marco Schorlemmer and Pere Garcia-Calvés and Nardine Osman and Josep Coll and Fernando Bobillo},
keywords = {Building Information Modeling, Ontologies, RDF graphs, Semantic BIM software},
abstract = {Semantic Building Information Modeling (BIM) consists in translating data expressed using BIM formats (namely IFC) into Semantic Web files using RDF serializations (e.g., Turtle). This enables the inference of new knowledge and constraint checking, among other advantages. While several software tools for translating BIM models into Semantic Web languages have been proposed in the literature, they differ in the features exposed. This paper analyzes and empirically compares some of these tools (namely, IFC converters translating an input IFC model into an RDF graph), identifying their strengths and main limitations. Our methodology includes measuring computation times of common tasks (file conversion, query and inference over output files), assessing the retention of knowledge (particularly, geometric information) and examining reasoning capabilities (complexity and completeness of the resulting models). Our results show that IFCtoLBD is the best option in many cases. IFCtoRDF and IFC2LD are slower but better preserve geometric information, while KGG is faster at the expense of losing information in the translation.}
}
@incollection{SAXENA2022145,
title = {Chapter 9 - Gene Ontology: application and importance in functional annotation of the genomic data},
editor = {Dev Bukhsh Singh and Rajesh Kumar Pathak},
booktitle = {Bioinformatics},
publisher = {Academic Press},
pages = {145-157},
year = {2022},
isbn = {978-0-323-89775-4},
doi = {https://doi.org/10.1016/B978-0-323-89775-4.00015-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323897754000158},
author = {Reshu Saxena and Ritika Bishnoi and Deepak Singla},
keywords = {Gene Ontology, software, functional assignment, annotation, enrichment analysis, classification, machine learning, visualization},
abstract = {In the era of next-generation sequencing, high-throughput sequencing technologies are being continuously exploited to produce a massive amount of uncharacterized and novel sequencing data. Manual annotation of such data is a complex, time-consuming, and laborious task that can only be performed by experienced biocurators. Thus computational methods are evolving from time to time to act as the best alternative approach without any substantial drop in the quality of annotation. Based on this, some software has been developed for automatic Gene Ontology (GO) term assignment. In this chapter, we will discuss the GO-based annotation, their types, software developed for the annotation purpose, and their application in establishing the structural–functional relationship.}
}
@article{LIU2025110889,
title = {A blockchain-based LLM-driven energy-efficient scheduling system towards distributed multi-agent manufacturing scenario of new energy vehicles within the circular economy},
journal = {Computers & Industrial Engineering},
volume = {201},
pages = {110889},
year = {2025},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2025.110889},
url = {https://www.sciencedirect.com/science/article/pii/S0360835225000348},
author = {Changchun Liu and Qingwei Nie},
keywords = {Large language model, Energy-efficient scheduling, Blockchain, Distributed manufacturing, Multi-agent, Circular economy},
abstract = {As processing technology is becoming increasingly complex, a single enterprise is no longer able to satisfy all the customization needs, which also requires extra energy consumption and vast time to seek cooperation from other enterprises for processing, especially in the field of new energy vehicle manufacturing. To coincide with the circular economy principle, a blockchain-based Large Language Model (LLM)-driven energy-efficient scheduling solution is proposed towards distributed multi-agent manufacturing scenario of new energy vehicles in this work. Firstly, distributed manufacturing system has the potential to efficiently organize distributed manufacturing resources by abstracting various machines from different factory nodes into agents with corresponding processing capabilities. Additionally, energy-efficient scheduling in line with the circular economy principles helps to optimize production cycle and reduce energy consumption and delay time, thereby lowering production costs and enhancing competitiveness. Compared with the traditional methods that suffer from long training time and local optimization, LLMs offer innovative solutions by learning a wealth of experiential knowledge in advance from vast amounts of data to further support self-adaptive and real-time energy-efficient scheduling. It is also worth noting that untrusted production data in factories may mislead the learning process of LLM, which may generate incorrect decision results. Therefore, a credit evaluation-based consensus mechanism is proposed to provide a trustworthy data access in distributed manufacturing, which can improve the transparency and traceability of the whole production process. Finally, the proposed approach is validated in the distributed manufacturing scenario for new energy vehicles. Compared with common methods, experimental results demonstrate the superiority of the proposed method on the distributed multi-agent manufacturing scenario for new energy vehicles, highlighting its potential to enhance production efficiency and circular economy.}
}
@incollection{BERMAN2022155,
title = {4 - Coping with paradoxical or flawed classifications and ontologies},
editor = {Jules J. Berman},
booktitle = {Classification Made Relevant},
publisher = {Academic Press},
pages = {155-201},
year = {2022},
isbn = {978-0-323-91786-5},
doi = {https://doi.org/10.1016/B978-0-323-91786-5.00011-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323917865000112},
author = {Jules J. Berman},
keywords = {Composite data objects, Unclassifiable objects, Part of relationships, Molecular classification of disease, International Classification of Disease, ICD, Diagnostic and Statistical Manual of Mental Disorders, DSM},
abstract = {We know how to classify a horse, and we know how to classify a human, but we do not know how to classify a human riding on a horse. Classifications are always constructed to hold single objects having characteristic features of the class. When we combine objects to form composites, the resulting collection of objects will almost always defy simple classification. We run into this problem whenever we deal with biological entities that have multiple components, such as classifications of human disease and classifications of anatomy. Such collections tend to devolve into lists of named objects, without any unifying biological principles from which relationships can be perceived. Fortunately, we have several options by which we can establish sense and order among nonclassifiable objects. In this chapter, we will study several flawed or nonsensical classifications. We will discuss some of the features that signal when a collection of objects should not be classified. Lastly, we will look at the available semantic and computational methods that can be used to integrate unclassifiable objects into existing well-characterized classifications and ontologies.}
}
@article{ZHANG2025796,
title = {A comprehensive review of semantic web technologies supported life cycle management for road infrastructures},
journal = {Alexandria Engineering Journal},
volume = {128},
pages = {796-815},
year = {2025},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2025.07.037},
url = {https://www.sciencedirect.com/science/article/pii/S1110016825008592},
author = {Rujie Zhang and Jianwei Wang and Haijiang Li and Xinhua Mao},
keywords = {Road infrastructure, Semantic web technologies, Ontology, Knowledge graph, Data integration, Scientometric analysis},
abstract = {Aiming at a comprehensive understanding of semantic web technologies in enhancing digital intelligence of road infrastructure, 141 papers were selected for scientometric analysis and critical review. Research trends were visualized through co-authorship, co-citation, and co-word analyses, while critical reviews identified themes and limitations. Publication trends revealed growth peaks in 2016 and 2021 and shift toward journal-dominated outputs signaling maturation from exploratory methodologies to robust theoretical frameworks and practical validations. Co-authorship analysis revealed growing engineering-computer science collaborations, while co-citation analysis stressed foundational ontology methodologies. Keyword analysis identified essential themes including building information modeling, digital twins, and deep learning. Data exchange and semantic integration, knowledge management, and reasoning and simple querying were identified pivotal roles of semantic web technologies in road infrastructure. Subsequently, a preliminary framework was proposed synthesizing core components and key processes. Five limitations were identified: lack of comprehensive guiding framework and ontology development protocols; limited information integration and synchronization; insufficient automation; and weak capacity of logical inference and decision support. This paper contributes to the current knowledge body by providing insights into how semantic web technologies support the management of road infrastructures throughout life cycle and addressing concerns and limitations faced therein to offer suggestions for future advancement.}
}
@article{UNLU2025103017,
title = {Protein language models for predicting drug–target interactions: Novel approaches, emerging methods, and future directions},
journal = {Current Opinion in Structural Biology},
volume = {91},
pages = {103017},
year = {2025},
issn = {0959-440X},
doi = {https://doi.org/10.1016/j.sbi.2025.103017},
url = {https://www.sciencedirect.com/science/article/pii/S0959440X25000351},
author = {Atabey Ünlü and Erva Ulusoy and Melih Gökay Yiğit and Melih Darcan and Tunca Doğan},
abstract = {Identifying new drug candidates remains a critical and complex challenge in drug development. Recent advances in deep learning have demonstrated significant potential to accelerate this process, particularly through the use of protein language models (pLMs). These models aim to effectively capture the structural and functional properties of proteins by embedding them in high-dimensional spaces, thereby providing powerful tools for predictive tasks. This review examines the application of pLMs in drug-target interaction (DTI) prediction, addressing both small-molecule and protein-based therapeutics. We explore diverse methodologies, including end-to-end learning models and those that leverage pre-trained foundational pLMs. Furthermore, we highlight the role of heterogeneous data integration—ranging from protein structures to knowledge graphs—to improve the accuracy of DTI predictions. Despite notable progress, challenges persist in accurately identifying DTIs, mainly due to data-related limitations and algorithmic constraints. Future research directions include utilising multimodal learning approaches, incorporating temporal/dynamic interaction data into training, and employing novel deep learning architectures to refine protein representations, gain a deeper understanding of biological context regarding molecular interactions, and, thus, advance the DTI prediction field.}
}
@incollection{NGUYEN2025169,
title = {Chapter 11 - Towards human digital twins for healthcare agent-based modeling in the Metaverse☆☆Disclaimer: This book chapter and my related published materials reflect my personal views only, and do not necessarily reflect the views of the US HHS nor the FDA. The chapter contains a part of my earlier published paper at https://xmed.jmir.org/2022/2/e33502.},
editor = {Chang S. Nam and Donggil Song and Heejin Jeong},
booktitle = {Human-Centered Metaverse},
publisher = {Morgan Kaufmann},
pages = {169-194},
year = {2025},
isbn = {978-0-443-21996-2},
doi = {https://doi.org/10.1016/B978-0-443-21996-2.00013-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443219962000139},
author = {Tam N. Nguyen},
keywords = {Human digital twins, Metaverse, Agent-based modeling, Cognitive computing, Smart agents},
abstract = {Agent-based modeling (ABM) has been increasingly used to model complex real-life issues, such as informing prompt COVID-19 response policies. ABM represents subsystems and their entities as agents while employing flexible rules to describe complex relationships and interactions among the agents. The Metaverse, with its sophisticated agents like digital twins (DTs) and human digital twins (HDTs), can significantly boost ABM performance. However, current cognitive architectures are not ready for HDTs use in the Metaverse. Here we show that extending current digital cognitive architectures is a crucial first step towards building more robust HDTs. We introduce Cybonto, a novel ontology that packages 108 psychology constructs and thousands of related paths based on 20 time-tested psychology theories. Using 20 network science centrality algorithms, we rank the Cybonto psychology constructs by their influences, identifying the top 10 constructs: behavior, arousal, goals, perception, self-efficacy, circumstances, evaluating, behavior-controllability, knowledge, and intentional modality. These findings confirm the need for specific extensions of current digital cognitive architectures in preparation for future HDTs in the Metaverse. Additionally, Cybonto can be used to develop cognitive evaluation metrics for large language models, moving the field forward in terms of practical applications and theoretical advancements.}
}
@article{MANNS2025106012,
title = {Towards a decision support system for pediatric emergency telephone triage},
journal = {International Journal of Medical Informatics},
volume = {203},
pages = {106012},
year = {2025},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2025.106012},
url = {https://www.sciencedirect.com/science/article/pii/S1386505625002291},
author = {Aurélia Manns and Alix Millet and Florence Campeotto and Benoit Vivien and Laurent Dupic and Olivier Guillard and Christelle El Hage and Florence Castela and Stephanie Fogel and Anita Burgun and Fleur Mougin and Rosy Tsopra},
keywords = {Decision-support system, Telephone triage, Pediatric emergency},
abstract = {Background
Telephone triage could limit admissions to emergency departments. However, telephone triage is challenging in pediatrics due to nonspecific symptoms, reliance on parental description, and emotional distress. Clinical decision support systems (CDSSs) could improve the accuracy and quality of telephone triage. Despite proven benefits, current CDSSs are not well suited to the nuances of pediatrics. This study aims to develop a CDSS for pediatric emergency telephone triage.
Methods
We developed a formal knowledge base (KB) for pediatric telephone triage inspired by the ontology model and implemented a generic medical reasoning system that mimics the clinical reasoning used in pediatric emergency triage. The CDSS is built in three layers (a knowledge layer, a Python-based decision layer, and a web interface layer) and provides real-time recommendations. We assessed its accuracy on 96 fictitious clinical cases.
Results
The CDSS uses an ontology-oriented KB that includes 303 concepts and 1780 axioms and a generic algorithm that provides recommendations based on user input, exploring and updating decisions continuously. It demonstrated 100 % internal validity compared to written recommendations and 77.1 % accuracy compared to a trio of experts. The 22.9 % discrepancies were due to experts using additional elements not documented in the written recommendations (11.5 %) or experts making different decisions despite consistent rules in the textual recommendations (10.4 %), emphasizing the challenges of standardized guidelines in this narrow but complex field.
Discussion/conclusion
The CDSS provides explainable and interpretable recommendations designed to alleviate healthcare professionals’ cognitive load so that they can focus on complex clinical situations. Future improvements involve enriching the KB, enhancing user interaction with patient-friendly language, and combining this knowledge-based approach with data-driven approaches.}
}
@article{LING20242148,
title = {A new incremental pipeline for concept formation driven by prior knowledge: Application on the AI Act domain},
journal = {Procedia Computer Science},
volume = {246},
pages = {2148-2157},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.618},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924026711},
author = {Hongtao Ling and Mounira Harzallah and Margo Bernelin and Claudia Marinica and Patricia Serrano-Alvarado},
keywords = {Ontology learning, Concept formation, Clustering, Dimension reduction, Prior knowledge, Incremental pipeline, AI Act domain},
abstract = {In the Ontology Learning research domain, despite recent advancements, the performance of current non or semi-supervised approaches for concept formation remains sub-optimal, particularly from a single, small-sized corpus for a specialized domain. In order to answer the performance drawback, this paper introduces a novel pipeline, called CO-ISSC (Core Ontology-based Incremental Semi-Supervised Clustering), for concept formation towards ontology learning. This pipeline uses a PLM (Pre-trained Language Model) and combines in an incremental manner a semi-supervised dimension reduction technique and a clustering technique, guided by core concepts as prior knowledge to align results with the ontology domain. Its incremental nature enhances prior knowledge and boosts its performance. The CO-ISSC pipeline’s performance is evaluated on the recent and significant AI Act text established by the European Union, which aims to ensure the safety, transparency, and non-discrimination of AI systems. To this end, we manually built a benchmark terminology for the AI Act domain given that no reference model exists yet. The results demonstrate promising performance of the CO-ISSC pipeline, outperforming baseline non-supervised or semi-supervised approaches such as DBSCAN, similarity measure based approaches, support vector machines and ANN.}
}
@article{TRIPERINA2018366,
title = {Visual-aided ontology-based ranking on multidimensional data: a case study in academia},
journal = {Data Technologies and Applications},
volume = {52},
number = {3},
pages = {366-383},
year = {2018},
issn = {2514-9288},
doi = {https://doi.org/10.1108/DTA-03-2017-0014},
url = {https://www.sciencedirect.com/science/article/pii/S2514928818000093},
author = {Evangelia Triperina and Georgios Bardis and Cleo Sgouropoulou and Ioannis Xydas and Olivier Terraz and Georgios Miaoulis},
keywords = {Ranking, Ontology, Visual analytics, ELECTRE III, Semantic web, Multiple criteria decision making},
abstract = {Purpose
The purpose of this paper is to introduce a novel framework for visual-aided ontology-based multidimensional ranking and to demonstrate a case study in the academic domain.
Design/methodology/approach
The paper presents a method for adapting semantic web technologies on multiple criteria decision-making algorithms to endow to them dynamic characteristics. It also showcases the enhancement of the decision-making process by visual analytics.
Findings
The semantic enhanced ranking method enables the reproducibility and transparency of ranking results, while the visual representation of this information further benefits decision makers into making well-informed and insightful deductions about the problem.
Research limitations/implications
This approach is suitable for application domains that are ranked on the basis of multiple criteria.
Originality/value
The discussed approach provides a dynamic ranking methodology, instead of focusing only on one application field, or one multiple criteria decision-making method. It proposes a framework that allows integration of multidimensional, domain-specific information and produces complex ranking results in both textual and visual form.}
}
@article{WEI2025106168,
title = {Enhancing charge prediction through the collaboration of large and small models},
journal = {Computer Law & Security Review},
volume = {58},
pages = {106168},
year = {2025},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2025.106168},
url = {https://www.sciencedirect.com/science/article/pii/S2212473X25000410},
author = {Bin Wei and Yaoyao Yu and Jiawen Zhang and Yiquan Wu},
keywords = {Charge prediction, Large language models, Model collaboration},
abstract = {Charge prediction is a fundamental task in AI&Law, where the goal is to predict charges based on fact descriptions. Although various methods have been introduced to enhance performance, challenges remain. Specifically, small models (SMs)-based methods such as BERT struggle with hard cases involving low-frequency or confusing charges due to their limited capacity, whereas large language models (LLMs)-based approaches like GPT-4 exhibit difficulties in handling diverse charges owing to insufficient legal knowledge. To overcome these limitations, we propose a hybrid framework that collaborates both large and small models to improve charge prediction performance, based on the idea that combining the strengths of each can overcome their limitations. Initially, SMs provide an initial prediction along with a predicted probability distribution. If the maximum predicted probability falls below a threshold, LLMs step in to reflect and re-predict as needed. Additionally, we construct a confusing charges dictionary and design a two-stage legal inference prompt, which helps LLMs make the secondary prediction for the hard cases. Extensive experiments on two datasets from China and Italy demonstrate the effectiveness of this approach, yielding average F1 improvements of 7.94% and 11.46% respectively. Moreover, a fine-grained analysis demonstrates that our proposed framework is effective in identifying low-frequency and confusing charges.}
}
@article{WEI2025106166,
title = {A Comparative review of Symbolism and Connectionism in AI&Law: Tracing evolution and exploring integration},
journal = {Computer Law & Security Review},
volume = {58},
pages = {106166},
year = {2025},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2025.106166},
url = {https://www.sciencedirect.com/science/article/pii/S2212473X25000392},
author = {Bin Wei},
keywords = {AI&Law, Symbolism, Connectionism, Integration, Logic, Machine Learning, LLMs},
abstract = {AI&Law explores computational methods for automating legal reasoning and prediction, evolving in parallel with AI research and developing along two primary paths: symbolic and connectionist approaches. Symbolic AI&Law centers on the formal representation of legal concepts and performing reasoning based on statutes and case law. These methods have led to the development of rule-based and case-based reasoning systems, successfully implemented in legal expert systems. The primary advantage of symbolic approaches is their inherent explainability, although they face limitations due to the knowledge acquisition bottleneck. Connectionist AI&Law encourages legal professionals to adopt inductive inference and use “bottom-up” learning models to extract hidden features from large datasets. This paradigm incorporates machine learning and natural language processing (NLP) techniques to address legal information extraction, retrieval, text classification, summarization, and legal prediction tasks. The advent of large language models (LLMs) has further expanded the capabilities of connectionist models, enabling more sophisticated legal text analysis and predictive accuracy, though issues of model transparency and hallucination remain active areas of research. The interaction between symbolic and connectionist approaches can complement each other. Symbolic models can enhance the transparency and explainability of connectionist systems, while connectionist techniques can optimize the scalability and efficiency of symbolic reasoning processes. These two paradigms exhibit strong potential for collaboration, particularly in the domains of explainable dialogue systems, neuro-symbolic systems, legal knowledge embedding and legal argumentation mining, etc.}
}
@article{KELM20257,
title = {Enhancing Assembly Instruction Generation for Cognitive Assistance Systems with Large Language Models},
journal = {Procedia CIRP},
volume = {134},
pages = {7-12},
year = {2025},
note = {58th CIRP Conference on Manufacturing Systems 2025},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2025.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125004512},
author = {Benedikt Kelm and Paul Hubert Haas and Simon Jochum and Lennard Margies and Rainer Müller},
keywords = {Cognitive Assistance Systems, Large Language Models, Assembly Instruction Generation, Prompt Engineering, Fine-Tuning},
abstract = {Cognitive Assistance Systems enhance manual assembly by shortening learning cycles and allowing workers to handle a wider range of products. However, generating assembly instructions remains time-consuming, particularly in environments with high product variability. This paper presents a novel approach to automate and streamline this process using MTM-based standardized instruction texts and Large Language Models via the OpenAI API. By deriving instructions from MTM analyses, a unified syntax and structure can be realized, improving consistency and efficiency. The integration of GTP-4o further enables the automatic generation of context-specific warnings and error notifications. Model fine-tuning and prompt engineering play a pivotal role in this approach, allowing the generation of precise instructions. The evaluation, based on the BLEU and METEOR scores, focuses on the assessment of the technical functionality and the quality of the generated outputs and shows promising results that highlight the potential of this approach for improving the automated generation of standardized assembly instructions for Cognitive Assistance Systems.}
}
@article{HU2025110776,
title = {Product complexity management enabled by a model-based approach},
journal = {Computers & Industrial Engineering},
volume = {200},
pages = {110776},
year = {2025},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2024.110776},
url = {https://www.sciencedirect.com/science/article/pii/S0360835224008982},
author = {Zhenchao Hu and Jinwei Chen and Yuanfu Li and Jinzhi Lu and Huisheng Zhang and Dimitris Kiritsis},
keywords = {Complexity management, Complexity modeling, Complexity analysis, Model-based system engineering, Design structure matrix, Ontology modeling},
abstract = {As contemporary engineered systems become more interdependent, there is a growing need to manage their system complexity. However, system complexity only exists as implicit and heterogeneous information. This situation makes it difficult to obtain and analyze system complexity. This paper proposes a model-based approach to supporting system complexity management, called MBCM, where system complexity includes dynamic complexity and structural complexity. The proposed approach includes complexity modeling and complexity analysis. For complexity modeling, a graph-matrix hybrid complexity modeling approach is proposed to describe the dynamic and structural complexity information. Graph-based architecture models are developed by a unified meta-meta modeling approach. The connections among different architecture models are then described using a design structure matrix. For complexity analysis, a general complexity metric provides dynamic and structural complexity values for the designer to evaluate the system. Moreover, automatic ontology modeling is introduced to integrate complexity modeling and complexity analysis. Finally, a case study on aero-engine design was conducted. The complexity management processes of three different scenarios were compared in this case study. The comparison results show that our approach could sensitively represent increased product complexity from both dynamic and structural aspects. Thus, the proposed approach is general and applicable to any engineered system and can support the trade-off between complex product design schemes.}
}
@article{TURKI2019103292,
title = {Wikidata: A large-scale collaborative ontological medical database},
journal = {Journal of Biomedical Informatics},
volume = {99},
pages = {103292},
year = {2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103292},
url = {https://www.sciencedirect.com/science/article/pii/S1532046419302114},
author = {Houcemeddine Turki and Thomas Shafee and Mohamed Ali {Hadj Taieb} and Mohamed {Ben Aouicha} and Denny Vrandečić and Diptanshu Das and Helmi Hamdi},
keywords = {Wikidata, Biomedical ontology, Semantic resources, Multilingual resources, Medical databases},
abstract = {Created in October 2012, Wikidata is a large-scale, human-readable, machine-readable, multilingual, multidisciplinary, centralized, editable, structured, and linked knowledge-base with an increasing diversity of use cases. Here, we raise awareness of the potential use of Wikidata as a useful resource for biomedical data integration and semantic interoperability between biomedical computer systems. We show the data model and characteristics of Wikidata and explain how this database can be automatically processed by users as well as by computer methods and programs. Then, we give an overview of the medical entities and relations provided by the database and how they can be useful for various medical purposes such as clinical decision support.}
}
@article{SEN2024102344,
title = {Developing A Decision Support System for Healthcare Practices: A Design Science Research Approach},
journal = {Data & Knowledge Engineering},
volume = {154},
pages = {102344},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102344},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000685},
author = {Arun Sen and Atish P. Sinha and Cong Zhang},
keywords = {design science research, decision support system, patient-centered medical home, healthcare practice, practice transformation, domain ontology, task ontology},
abstract = {We propose a new approach for designing a decision support system (DSS) for the transformation of healthcare practices. Practice transformation helps practices transition from their current state to patient-centered medical home (PCMH) model of care. Our approach employs activity theory to derive the elements of practice transformation by designing and integrating two ontologies: a domain ontology and a task ontology. By incorporating both goal-oriented and task-oriented aspects of the practice transformation process and specifying how they interact, our integrated design model for the DSS provides prescriptive knowledge on assessing the current status of a practice with respect to PCMH recognition and navigating efficiently through a complex solution space. This knowledge, which is at a moderate level of abstraction and expressed in a language that practitioners understand, contributes to the literature by providing a formulation for a nascent design theory. We implement the integrated design model as a DSS prototype; results of validation tests conducted on the prototype indicate that it is superior to the existing PCMH readiness tracking tool with respect to effectiveness, usability, efficiency, and sustainability.}
}
@article{ARRIGONI201931,
title = {Exploring the “relational” link between responsibility and social ontology},
journal = {Journal of Global Responsibility},
volume = {10},
number = {1},
pages = {31-46},
year = {2019},
issn = {2041-2568},
doi = {https://doi.org/10.1108/JGR-10-2018-0047},
url = {https://www.sciencedirect.com/science/article/pii/S2041256819000097},
author = {Adalberto Arrigoni},
keywords = {Corporate social responsibility, Responsibility, Corporate agency, Dynamic ontology, Ontology of the firm, Social ontology},
abstract = {Purpose
This study aims to point out and try to describe the (missing) link between “responsible practises” (e.g. CSR – corporate social responsibility) and social ontology. This critical gap in the literature may conceivably be a stumbling block to responsible business/political/societal action and its theoretical/empirical understanding and effectiveness; therefore, we can legitimately ask ourselves whether a social ontology-focused approach can be considered relevant to this field of study.
Design/methodology/approach
As the role of social ontology has presumably been under-explored despite its foundational importance, a set of germane and adjoining themes has been identified, which can be possibly included in future research projects. An overview of relevant literature is provided, and further analysis and desk research can be drawn from the key notions identified.
Findings
It is argued that social ontology – especially the underlying debate in terms of shared agency, collective responsibility and collective intentionality – can be an innovative and promising perspective within business ethics studies. Potentially, CSR management and/or similar responsible practices can re-appraised in similar terms.
Research limitations/implications
This study specifically focuses on some selected key aspects related to the ontological status of social collectives (e.g. groups and organisations), trying to recall the main trajectories/directions of the relevant arguments and debates. More empirical research/pilot case studies validating the approach presented here will be required.
Practical implications
Building on the findings of this study, new emergent research methodologies/theoretical tools will make it possible to explore not so much the ways “responsible” practises are defined (indeed, there seems to be a broad consensus about it), but rather how they are socially constructed, implemented and carried out.
Social implications
This theoretical work can potentially facilitate a comprehensive inter-/multi-/pluri-disciplinary understanding of the novel links explored, namely, between responsibility, social ontology and the underlying longstanding philosophical issues.
Originality/value
The novel thematic approach outlined in this study can challenge and widen the mainstream approaches about CSR management, e.g. stakeholder management and engagement, social accounting and reporting, SRI (socially responsible investment).}
}
@article{OSTERMEYER2018302,
title = {An ontology-based framework for the management of machining information in a data mining perspective},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {302-307},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.300},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318314241},
author = {Emeric Ostermeyer and Christophe Danjou and Alexandre Durupt and Julien Le Duigou},
keywords = {Manufacturing, KBE, OntoSTEP-NC, CNC, Ontology},
abstract = {The advent and fast development of data mining techniques induced, in every field, interest for the data produced and stored. Machining process planning is no exception to this rule, and, with the important amount of related data that is stored in the enterprise information system, the application of data mining seems promising. However, the strong heterogeneity in data, and the distribution of information throughout several different documents, may hinder the application of data-mining methods and machine learning tools. This paper will introduce a knowledge-based engineering framework which can be used as an information system able to query consistent, correct and complete data from the heterogeneous corpus of documents used in process planning of machined parts.}
}
@article{NERELLA2024102900,
title = {Transformers and large language models in healthcare: A review},
journal = {Artificial Intelligence in Medicine},
volume = {154},
pages = {102900},
year = {2024},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.102900},
url = {https://www.sciencedirect.com/science/article/pii/S0933365724001428},
author = {Subhash Nerella and Sabyasachi Bandyopadhyay and Jiaqing Zhang and Miguel Contreras and Scott Siegel and Aysegul Bumin and Brandon Silva and Jessica Sena and Benjamin Shickel and Azra Bihorac and Kia Khezeli and Parisa Rashidi},
keywords = {Transformers, Healthcare, Electronic Health Records, Large Language Models, Medical Imaging, Natural Language Processing},
abstract = {With Artificial Intelligence (AI) increasingly permeating various aspects of society, including healthcare, the adoption of the Transformers neural network architecture is rapidly changing many applications. Transformer is a type of deep learning architecture initially developed to solve general-purpose Natural Language Processing (NLP) tasks and has subsequently been adapted in many fields, including healthcare. In this survey paper, we provide an overview of how this architecture has been adopted to analyze various forms of healthcare data, including clinical NLP, medical imaging, structured Electronic Health Records (EHR), social media, bio-physiological signals, biomolecular sequences. Furthermore, which have also include the articles that used the transformer architecture for generating surgical instructions and predicting adverse outcomes after surgeries under the umbrella of critical care. Under diverse settings, these models have been used for clinical diagnosis, report generation, data reconstruction, and drug/protein synthesis. Finally, we also discuss the benefits and limitations of using transformers in healthcare and examine issues such as computational cost, model interpretability, fairness, alignment with human values, ethical implications, and environmental impact.}
}
@article{GELLER2018106,
title = {Quality assurance of biomedical terminologies and ontologies},
journal = {Journal of Biomedical Informatics},
volume = {86},
pages = {106-108},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2018.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S1532046418301801},
author = {James Geller and Yehoshua Perl and Licong Cui and G.Q. Zhang}
}
@incollection{2018287,
title = {Appendix 6 - Ontologies},
editor = {Marius Fieschi},
booktitle = {Health Data Processing},
publisher = {Elsevier},
pages = {287-289},
year = {2018},
isbn = {978-1-78548-287-8},
doi = {https://doi.org/10.1016/B978-1-78548-287-8.50031-6},
url = {https://www.sciencedirect.com/science/article/pii/B9781785482878500316}
}
@article{ALKAHTANI20191027,
title = {A decision support system based on ontology and data mining to improve design using warranty data},
journal = {Computers & Industrial Engineering},
volume = {128},
pages = {1027-1039},
year = {2019},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2018.04.033},
url = {https://www.sciencedirect.com/science/article/pii/S0360835218301694},
author = {Mohammed Alkahtani and Alok Choudhary and Arijit De and Jennifer Anne Harding},
keywords = {Ontology, Self-Organizing Maps, Warranty data, Text mining, Decision support},
abstract = {Analysis of warranty based big data has gained considerable attention due to its potential for improving the quality of products whilst minimizing warranty costs. Similarly, customer feedback information and warranty claims, which are commonly stored in warranty databases might be analyzed to improve quality and reliability and reduce costs in areas, including product development processes, advanced product design, and manufacturing. However, three challenges exist, firstly to accurately identify manufacturing faults from these multiple sources of heterogeneous textual data. Secondly, accurately mapping the identified manufacturing faults with the appropriate design information and thirdly, using these mappings to simultaneously optimize costs, design parameters and tolerances. This paper proposes a Decision Support System (DSS) based on novel integrated stepwise methodologies including ontology-based text mining, self-organizing maps, reliability and cost optimization for identifying manufacturing faults, mapping them to design information and finally optimizing design parameters for maximum reliability and minimum cost respectively. The DSS analyses warranty databases which collect the warranty failure information from the customers in a textual format. To extract the hidden knowledge from this, an ontology-based text mining based approach is adopted. A data mining based approach using Self Organizing Maps (SOM) has been proposed to draw information from the warranty database and to relate it to the manufacturing data. The clusters obtained using SOM are analyzed to identify the critical regions, i.e., sections of the map where maximum defects occur. Finally, to facilitate the correct implementation of design parameter changes, the frequency and type of defects analyzed from warranty data are used to identify areas where improvements have resulted in the greatest reliability for the lowest cost.}
}
@article{PARK2018201,
title = {Toward ontology of designer-user interaction in the design process: a knowledge management foundation},
journal = {Journal of Knowledge Management},
volume = {22},
number = {1},
pages = {201-218},
year = {2018},
issn = {1367-3270},
doi = {https://doi.org/10.1108/JKM-06-2017-0220},
url = {https://www.sciencedirect.com/science/article/pii/S1367327018000376},
author = {Jaehyun Park and Arkalgud Ramaprasad},
keywords = {Ontology, Design process, Knowledge management, Designer-user interaction},
abstract = {Purpose
The purpose of this study is to explore an ontology of designer-user interaction with a knowledge management foundation. To address this research gap, the authors ask the following research question: what types of knowledge on designer-user interactions are associated with design function and approach in creating effective design outcomes in a collaborative design process?
Design/methodology/approach
Based on ontology of a knowledge management foundation and 99 design projects, the authors conceptualized the ontology of designer-user interaction, which considers design role, function, approach and outcome as a knowledge of designer-user interaction in the design process.
Findings
Based on this analysis, the authors theorize an ontology of designer-user interactions with five dimensions: participant, role, function, design approach and design outcome. Also, this study presents a case study of how this ontology could be applied into the actual projects.
Originality/value
In this study, the authors explore an ontology of designer-user interaction with a knowledge management foundation, because previous interdisciplinary design studies have not formalized the types of designer-user interaction. To address this research gap, the authors ask the following research question: What types of knowledge on designer-user interactions are associated with design function and approach in creating effective design outcomes in a collaborative design process?}
}
@article{OFER2024104650,
title = {Automated annotation of disease subtypes},
journal = {Journal of Biomedical Informatics},
volume = {154},
pages = {104650},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104650},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000686},
author = {Dan Ofer and Michal Linial},
keywords = {Disease subtypes, Disease ontology, Explainability, Machine learning, Medical language models, Ontology completion, Open Targets, Orphanet, Personalized medicine},
abstract = {Background
Distinguishing diseases into distinct subtypes is crucial for study and effective treatment strategies. The Open Targets Platform (OT) integrates biomedical, genetic, and biochemical datasets to empower disease ontologies, classifications, and potential gene targets. Nevertheless, many disease annotations are incomplete, requiring laborious expert medical input. This challenge is especially pronounced for rare and orphan diseases, where resources are scarce.
Methods
We present a machine learning approach to identifying diseases with potential subtypes, using the approximately 23,000 diseases documented in OT. We derive novel features for predicting diseases with subtypes using direct evidence. Machine learning models were applied to analyze feature importance and evaluate predictive performance for discovering both known and novel disease subtypes.
Results
Our model achieves a high (89.4%) ROC AUC (Area Under the Receiver Operating Characteristic Curve) in identifying known disease subtypes. We integrated pre-trained deep-learning language models and showed their benefits. Moreover, we identify 515 disease candidates predicted to possess previously unannotated subtypes.
Conclusions
Our models can partition diseases into distinct subtypes. This methodology enables a robust, scalable approach for improving knowledge-based annotations and a comprehensive assessment of disease ontology tiers. Our candidates are attractive targets for further study and personalized medicine, potentially aiding in the unveiling of new therapeutic indications for sought-after targets.}
}
@article{GAULD2020101328,
title = {Commentary on Lammers et al. “Diagnosis of central disorders of hypersomnolence: A reappraisal by European experts”: From clinic to clinic via ontology and semantic analysis on a bullet point path},
journal = {Sleep Medicine Reviews},
volume = {52},
pages = {101328},
year = {2020},
issn = {1087-0792},
doi = {https://doi.org/10.1016/j.smrv.2020.101328},
url = {https://www.sciencedirect.com/science/article/pii/S108707922030071X},
author = {Christophe Gauld and Kévin Ouazzani and Jean-Arthur Micoulaud-Franchi},
keywords = {Sleep, Classification, Diagnosis, Hypersomnolence, Theoretical, Ontology, Semantic analysis}
}
@article{2022,
title = {Hybrid Firefly-Ontology-Based Clustering Algorithm for Analyzing Tweets to Extract Causal Factors},
journal = {International Journal on Semantic Web and Information Systems},
volume = {18},
number = {1},
year = {2022},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.295550},
url = {https://www.sciencedirect.com/science/article/pii/S1552628322000412},
keywords = {Association Rules, Firefly, Social Media, Swarm Intelligence, Twitter, Wordnet},
abstract = {ABSTRACT
Social media, especially Twitter, has become ubiquitous among people where they express their opinions on various domains. This paper presents a hybrid firefly ontology-based clustering (FF-OC) algorithm that attempts to extract factors impacting a major public issue that is trending. In this research work, the issue of food price rise and disease, which was trending during the time of the investigation, is considered. The novelty of the algorithm lies in the fact that it clusters the association rules without any prior knowledge. The findings from the experimentation suggest different factors impacting the rise of price in food items and diseases such as diabetes, flu, zika virus. The empirical results show the significant improvement when compared with artificial bees colony, cuckoo search algorithm, particle swarm optimization, and ant colony optimization-based clustering algorithms. The proposed method gives an improvement of 81% in terms of DB index, 79% in terms of silhouette index, 85% in terms of C index when compared to other algorithms.}
}
@incollection{MALKI2018165,
title = {6 - An Ontologically-based Trajectory Modeling Approach for an Early Warning System},
editor = {Florence Sèdes},
booktitle = {How Information Systems Can Help in Alarm/Alert Detection},
publisher = {Elsevier},
pages = {165-198},
year = {2018},
isbn = {978-1-78548-302-8},
doi = {https://doi.org/10.1016/B978-1-78548-302-8.50006-X},
url = {https://www.sciencedirect.com/science/article/pii/B978178548302850006X},
author = {Jamal Malki and Alain Bouju},
keywords = {Design and methodology, Domain inference implementation, Domain trajectory ontology, Ontologically-based Trajectory Modeling, Semantic trajectory ontology, Temporal inference implementation, Time ontology, Trajectory ontology inference entailment},
abstract = {Abstract:
Thanks to advances in scientific research, communication and information studies, and sensor technologies, considerable progress has been made in the field of early warning systems, especially for animal zone tracking. A global early warning system is needed to inform us of pending threats. The basic idea behind early warning is that the earlier and more accurately we are able to predict short- and long-term potential risks associated with natural and human-induced hazards, the more likely we will be able to manage and mitigate a disaster’s impact on society, economies and the environment. Early warning is “the provision of timely and effective information, through identified institutions, that allows individuals exposed to hazard to take action to avoid or reduce their risk and prepare for effective response”.}
}
@article{OFOGHI2023110980,
title = {Knowledge representation of mathematical optimization problems and constructs for modeling},
journal = {Knowledge-Based Systems},
volume = {280},
pages = {110980},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110980},
url = {https://www.sciencedirect.com/science/article/pii/S095070512300730X},
author = {Bahadorreza Ofoghi and John Yearwood},
keywords = {Knowledge engineering, Mathematical models, Constraint optimization},
abstract = {We introduce two new knowledge bases that we have developed to categorize mixed-integer linear programming (MILP) problems and standardize the element definitions of MILP models. MILP is a commonly used mathematical programming technique for modeling and solving real-life scheduling, routing, planning, resource allocation, and timetabling optimization problems providing optimized business solutions for industry sectors such as manufacturing, agriculture, defense, healthcare, medicine, energy, finance, and transportation among others. Despite the numerous real-life combinatorial optimization problems (COPs) found and solved, and many yet to be discovered and formulated, the number of types of constraints (the building blocks of a MILP) is relatively small. In the search for a uniform categorization of MILP problems and a machine-readable knowledge representation structure for MILP models, we have developed an optimization modeling tree (OMT) and a MILP model ontology. The two knowledge structures can serve as a standardized, uniform representation in understanding MILP problems and developing MILP models for combinatorial business optimization problems. While there are several algebraic modeling languages (AMLs) for developing and solving MILP models, the semantic correctness of such models cannot be guaranteed using the syntactic grammar of such AMLs. The MILP model ontology will act as the main resource for semantic validation of MILP models through ontology instantiations and axiom assertions.}
}
@article{LIN2023119460,
title = {Knowledge representation and reuse model of civil aircraft structural maintenance cases},
journal = {Expert Systems with Applications},
volume = {216},
pages = {119460},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.119460},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422024794},
author = {Ruiguan Lin and Huawei Wang and Junzhou Wang and Ningyue Wang},
keywords = {Civil aircraft, Structural maintenance, Cases, Ontology, Similarity},
abstract = {Maintenance cases outside the Structural Repair Manual (OSRM) often occur in civil aircraft structural maintenance (CASM) work. At this time, maintenance personnel needs to learn from historical cases to formulate new maintenance plans. Since CASM cases are diverse, empirical, and heterogeneous, we propose a knowledge representation and reuse model for CASM cases. Firstly, the CASM cases domain ontology is established using Web Ontology Language (OWL), and the case similarity index system is proposed. Then, the proposed global similarity measure integrates three local similarity measures related to different attributes. In particular, considering the engineering significance of attributes, a semantic similarity measure based on ontology structure is proposed. Next, the historical case with the highest similarity with the target case is retrieved under the condition of similarity threshold. Finally, using CASM cases from actual engineering scenes, it is verified that the model has higher retrieval accuracy than traditional methods. Our study shows that case-based decisions have essential reference significance for designing OSRM structural maintenance plans.}
}
@article{LIU2025106500,
title = {Natural language-extracted and BIM-referenced knowledge base for construction quality inspection via augmented reality},
journal = {Automation in Construction},
volume = {179},
pages = {106500},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106500},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525005400},
author = {Han Liu and Donghai Liu and Junjie Chen},
keywords = {Construction quality inspection, Building information modeling, Knowledge mining, Knowledge database, Smart construction, Augmented reality},
abstract = {Construction quality is of upmost importance for delivering well-performed civil structures. Inspection offers a critical means to ensure construction quality. However, its effective implementation relies on an excess of domain knowledge that usually takes years to accumulate, making inspection expensive and challenging to conduct. This paper introduces a construction quality inspection knowledge base that empowers inspectors with easily accessible and intuitive construction requirement information. Natural language processing (NLP) is applied to automatically extract knowledge from construction documents such as specification and regulatory files. The extracted knowledge is linked to the building information model (BIM) using proposed association methods and semantic similarity matching. The natural language-extracted and BIM-referenced knowledge base (NLBIM-KB) is integrated into an augmented reality (AR) interface, which provides a freehand tool to assist inspectors' decision-making via on-demand construction knowledge extraction.}
}
@article{K2018137,
title = {Industrial information extraction through multi-phase classification using ontology for unstructured documents},
journal = {Computers in Industry},
volume = {100},
pages = {137-147},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0166361517306498},
author = {Rajbabu K. and Harshavardhan Srinivas and Sudha S.},
keywords = {Information extraction, Multi-phase classification, Ontology, Industrial unstructured documents, Feature transformation},
abstract = {The increased availability of unstructured text documents in industries such as e-mails, office documents, PDF files etc., has inspired many researchers towards Information Extraction. The objective of the proposal is to extract information from unstructured tender documents of power plant industries. The extraction efficiency of recent works depends on the linguistic structure and keyword taxonomy. Hence, these approaches are unsuitable for domain specific applications that demand semantic and contextual taxonomy together. In this paper, a two-phase classification approach for information extraction with feature weighing is proposed. The proposal performs sentence classification in first phase followed by word classification. As industries spans across multiple domains, a multi domain layered industrial ontology is used for knowledge representation. The unstructured documents are enhanced into DAG based semi-structured text with enriched features. A unique feature transformation approach based on the categorical data type of features is attempted to handle heterogeneous textual features. The proposal is evaluated with real time documents obtained from power plant tenders. The results showed minimal loss of precision which can be rectified by enriching the training data and customizing standard parser algorithms to suit the domain requirements.}
}
@article{ACHARYA2018341,
title = {Novel symmetry-based gene-gene dissimilarity measures utilizing Gene Ontology: Application in gene clustering},
journal = {Gene},
volume = {679},
pages = {341-351},
year = {2018},
issn = {0378-1119},
doi = {https://doi.org/10.1016/j.gene.2018.08.062},
url = {https://www.sciencedirect.com/science/article/pii/S0378111918309272},
author = {Sudipta Acharya and Sriparna Saha and Prasanna Pradhan},
keywords = {Gene Ontology(GO), Dissimilarity measure, Symmetry-based distance, Gene clustering, Gene-GO term annotation matrix, Multi-objective clustering},
abstract = {In recent years DNA microarray technology, leading to the generation of high-volume biological data, has gained significant attention. To analyze this high volume gene-expression data, one such powerful tool is Clustering. For any clustering algorithm, its efficiency majorly depends upon the underlying similarity/dissimilarity measure. During the analysis of such data often there is a need to further explore the similarity of genes not only with respect to their expression values but also with respect to their functional annotations, which can be obtained from Gene Ontology (GO) databases. In the existing literature, several novel clustering and bi-clustering approaches were proposed to identify co-regulated genes from gene-expression datasets. Identifying co-regulated genes from gene expression data misses some important biological information about functionalities of genes, which is necessary to identify semantically related genes. In this paper, we have proposed sixteen different semantic gene-gene dissimilarity measures utilizing biological information of genes retrieved from a global biological database namely Gene Ontology (GO). Four proximity measures, viz. Euclidean, Cosine, point symmetry and line symmetry are utilized along with four different representations of gene-GO-term annotation vectors to develop total sixteen gene-gene dissimilarity measures. In order to illustrate the profitability of developed dissimilarity measures, some multi-objective as well as single-objective clustering algorithms are applied utilizing proposed measures to identify functionally similar genes from Mouse genome and Yeast datasets. Furthermore, we have compared the performance of our proposed sixteen dissimilarity measures with three existing state-of-the-art semantic similarity and distance measures.}
}
@article{LEBENA2025101637,
title = {Large language models aided patient progression documentation according to the ICD standard},
journal = {Informatics in Medicine Unlocked},
volume = {55},
pages = {101637},
year = {2025},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2025.101637},
url = {https://www.sciencedirect.com/science/article/pii/S2352914825000255},
author = {Nuria Lebeña and Arantza Casillas and Alicia Pérez},
keywords = {Natural language processing, Electronic health records, International classification of diseases, Clinical documentation, Preventive medicine},
abstract = {Background and Objective
Healthcare documentation processing is becoming more and more efficient and effective as a result of advances in machine learning and natural language processing (NLP). One challenge in clinical practice is the early detection of future patient potential diagnoses, which is crucial for preventive medicine. Estimating the potential future diagnoses, helps to speed up the management of Electronic Health Records (EHRs) and opens a path towards clinical prevention. It is a challenging task, as there are thousands of possible diseases, and, in general, there is limited data available to train systems due to privacy concerns. The objective of his study is to infer future probable diagnoses given patients diagnosis history. In previous works, this task has been carried out using structured data, such as, ICD-coded diagnoses, overlooking unstructured textual information in EHRs. Unlike traditional methods, this study aims to enhance next-diagnosis prediction by integrating patient diagnosis information codified according to the International Classification of Diseases (ICD) with unstructured clinical text.
Methods:
We propose a multi-faceted model that integrates structured ICD-encoded patient histories with unstructured EHR text for future diagnosis prediction. Our approach consists of (1) a sequential model trained on structured diagnosis timelines, (2) a Clinical Longformer-based model trained on unstructured EHRs, and (3) an ensemble strategy to combine predictions from both components.
Results:
Our proposed ensemble strategy significantly outperforms current state-of-the-art approaches in predicting future diagnoses, achieving a Precision@5 of 72.34% and a Precision@20 of 77.49%. Additionally, it showed high robustness and reliability across different demographic groups and a varying scope of medical history.
Conclusion:
This research demonstrates that the integration of structured ICD diagnoses timelines with unstructured EHRs achieves improved results compared to just using structured diagnosis timelines. Notably, the proposed model also maintained high accuracy even with a short-term history of diagnoses.}
}
@article{XIAO2025100003,
title = {Optical image processing and applications empowered by vision-language models},
journal = {iOptics},
volume = {1},
number = {1},
pages = {100003},
year = {2025},
issn = {3051-1771},
doi = {https://doi.org/10.1016/j.iopt.2025.100003},
url = {https://www.sciencedirect.com/science/article/pii/S3051177125000031},
author = {Jiangong Xiao and Zhe Sun and Hongjun An and Haofei Zhao and Maosheng Qiu and Xuelong Li},
keywords = {Imaging processing, Vision-language models, Optical image},
abstract = {Optical images present significant analytical challenges due to their high-dimensional structures, complex modalities, and multi-scale characteristics. This review systematically examines the technological evolution in optical image processing and applications. It traces the developmental trajectory from traditional image processing methods to deep learning approaches, culminating in the emergence of Vision Language Models. The discussion is organized around four pillars, including high-dimensional data representation, cross-modal feature fusion, semantic alignment, and reasoning strategy design. It synthesizes recent advances while mapping persisting constraints in network architecture, data dependence, and inference efficiency. The review closes by outlining priority research avenues aimed at elevating efficiency, generalization, and deployability in optical image understanding.}
}
@article{GONZALEZ2020102257,
title = {The potential of Amazon indigenous agroforestry practices and ontologies for rethinking global forest governance},
journal = {Forest Policy and Economics},
volume = {118},
pages = {102257},
year = {2020},
issn = {1389-9341},
doi = {https://doi.org/10.1016/j.forpol.2020.102257},
url = {https://www.sciencedirect.com/science/article/pii/S1389934119305118},
author = {Nidia Catherine González and Markus Kröger},
keywords = {Forest definitions, Indigenous knowledge, Agroforestry practices, Political ontology, Global forest politics, Amazon},
abstract = {This article explores the potential of Amazon indigenous agroforestry practices and forest understandings for making global forest governance more nuanced and thus rethinking the value of forests in the context of multiple global crises. Indigenous forest practices and their inherent knowledge are included in current global governance in very limited ways. Onto-epistemological openings in forest policies are needed in the face of converging climate, food and health crises. The indigenous forest relations and practices analyzed here may offer possibilities for such onto-epistemological openings. The current FAO and UNFCCC forest definitions are contrasted with indigenous forest understandings. While the current national and global definitions of forests contain a wide range of discrepant definitions, making the application of a shared forest policy difficult and even impossible, most institutional definitions share a positivist and technical approach to forest defining and governance. National and global discrepancies in definitions exist within the politics-as-usual process of forest defining, politics that could be challenged by the political ontology of forests that questions the deeper level of how forests should be conceptualized, placing greater emphasis on care, reciprocity, and the type of relational approach present among Amazon indigenous communities.}
}
@article{SHAIK2024104566,
title = {On mining mobile emergency communication applications in Nordic countries},
journal = {International Journal of Disaster Risk Reduction},
volume = {108},
pages = {104566},
year = {2024},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2024.104566},
url = {https://www.sciencedirect.com/science/article/pii/S2212420924003285},
author = {Fuzel Ahamed Shaik and Mourad Oussalah},
keywords = {Social media, Emergency communication, Aspect Based Sentiment Analysis, BERT, Ontological vocabulary, Empath categorization},
abstract = {Nowadays, the use of mobile devices has become ubiquitous, allowing users to access and share information in almost real-time through various social media platforms. This has provided an edge in emergency communication and disaster handling. Mobile emergency apps have emerged as key technologies in emergency communication. Mining the content of users’ reviews of mobile emergency apps has the potential to learn about users’ behavior and uncover unforeseen events in the emergency management process. This paper focused on emergency apps present in Nordic countries (Finland, Sweden, and Norway). User feedback was collected for every app from the Google/Apple store, and appropriate text mining techniques were employed to mine the discussion content for a given emergency communication ontology. Next, we investigated the contexts that generate either positive or negative sentiment, highlighting the main factors that impact user behavior most by leveraging the Empath Categorization technique. Finally, we constructed a word association by considering different ontological vocabularies related to mobile applications and emergency response and management systems. The study’s findings can help develop early warning systems that trigger alarms whenever a critical event requiring special attention is identified. It also paves the way for developing a more tailored communication strategy that considers the identified community behavior concerning emergency apps.}
}
@article{OH2024123960,
title = {Language model-guided student performance prediction with multimodal auxiliary information},
journal = {Expert Systems with Applications},
volume = {250},
pages = {123960},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.123960},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424008261},
author = {Changdae Oh and Minhoi Park and Sungjun Lim and Kyungwoo Song},
keywords = {Student performance prediction, Multimodal auxiliary information, Large language model, Matrix factorization},
abstract = {Student Performance Prediction (SPP) has received a lot of attention due to its educational implications, such as personalized instruction. Among numerous attempts in SPP, recommendation-based approaches (e.g. matrix factorization; MF) are especially attractive because of the specialized nature of personalization. However, they commonly struggle with the cold-start problem and incorporating side information. While existing works cope with the problem by utilizing additional information, such as students’ personalities, none of them cover the multimodal auxiliary information that can unleash the full potential of the recommendation-based SPP. In this work, we leverage multimodal auxiliary information at SPP for the first time by adopting large language models (GPT-J and Llama 2) as information blenders to produce extra guidance signals for the MF method. Specifically, our language model-guided matrix factorization (LMgMF), consumes the verbalized multimodal information, produces semantically rich embeddings for educational interaction, and uses them as auxiliary signals for MF. While doing so, we harness a API-level black-box language model without requiring parameter accessibility to adapt them. We evaluate LMgMF on two real-world datasets: (1) a large-scale Korean dataset that contains 1.6M instances from 10K pre-high school students; (2) ASSISTments2009 covering 346K interactions between students and math questions. Throughout extensive validation, we demonstrate that LMgMF consistently outperforms baseline methods in various SPP scenarios, including the cold-start.}
}
@article{SAL2024100657,
title = {Domain-specific languages for the automated generation of datasets for industry 4.0 applications},
journal = {Journal of Industrial Information Integration},
volume = {41},
pages = {100657},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100657},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24001018},
author = {Brian Sal and Diego García-Saiz and Alfonso {de la Vega} and Pablo Sánchez},
keywords = {Data selection, Industry 4.0, Fishbone diagrams, Ishikawa diagrams, Domain specific languages},
abstract = {Data collected in Industry 4.0 applications must be converted into tabular datasets before they can be processed by analysis algorithms, as in any data analysis system. To perform this transformation, data scientists have to write complex and long scripts, which can be a cumbersome process. To overcome this limitation, a language called Lavoisier was recently created to facilitate the creation of datasets. This language provides high-level primitives to select data from an object-oriented data model describing data available in a context. However, industrial engineers might not be used to deal with this kind of model. So, this work introduces a new set of languages that adapt Lavoisier to work with fishbone diagrams, which might be more suitable in industrial settings. These new languages keep the benefits of Lavoisier, reducing dataset creation complexity by 40% and up to 80%, and outperforming Lavoisier in some cases.}
}
@article{HYVONEN2025100852,
title = {Serendipitous knowledge discovery on the Web of Wisdom based on searching and explaining interesting relations in knowledge graphs},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100852},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100852},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000386},
author = {Eero Hyvönen},
keywords = {Knowledge graphs, Relational search, Knowledge discovery, Information retrieval, Large Language Models, Generative AI},
abstract = {This paper maintains that the Semantic Web is changing into a kind of Web of Wisdom (WoW) where AI-based problem solving, based on symbolic search and sub-symbolic methods, and Information Retrieval (IR) merge: IR is seen as a process for solving information-related problems of the end user with explanations, a form of knowledge discovery. As a case of example, relational search is concerned, i.e., solving problems of the type “How are X1…Xn related to Y1…Ym?”. For example: how is Pablo Picasso related to Barcelona? The idea is to find explainable “interesting” or even serendipitous associations in Knowledge Graphs (KG) and textual web contents. It is argued that domain knowledge-based symbolic methods based of KGs are needed to complement domain-agnostic graph-based methods and Generative AI (GenAI) boosted by Large Language Models (LLM). By using domain specific knowledge, it is possible to find and explain meaningful reliable textual answers, answer quantitative questions, and use data analyses and visualizations for explaining and studying the relations.}
}
@article{FONOUDOMBEU2021,
title = {OntoCSA:},
journal = {International Journal of Agricultural and Environmental Information Systems},
volume = {12},
number = {4},
year = {2021},
issn = {1947-3192},
doi = {https://doi.org/10.4018/IJAEIS.292476},
url = {https://www.sciencedirect.com/science/article/pii/S1947319221000095},
author = {Jean Vincent Fonou-Dombeu and Nadia Naidoo and Micara Ramnanan and Rachan Gowda and Sahil Ramkaran Lawton},
keywords = {Climate Change, Climate Smart Agriculture, Description Logics, Ontology, OWL, Protégé, Semantic Web, Semantic Web Technologies},
abstract = {ABSTRACT
The modelling of agriculture with ontologies has been of interest to many authors in the past years. However, no research, currently, has focused on building a knowledge-based ontology for the climate smart agriculture (CSA) domain. This study attempts to fill this gap through the development of a climate smart agriculture ontology (OntoCSA). Information was gathered from secondary sources including websites, published research articles and reports, as well as related ontologies to formalize the OntoCSA ontology in description logics (DLs). The OntoCSA ontology was developed in web ontology language (OWL) with Protégé. Furthermore, the OntoCSA ontology was successfully validated with the HermiT reasoner within Protégé. The resulting OntoCSA ontology is a machine-readable model of CSA that can be leveraged in web-based applications for the storage, open and automated access, and sharing of CSA information/data for research and dissemination of best practices.}
}
@article{XIAO2025129963,
title = {Robot learning in the era of foundation models: a survey},
journal = {Neurocomputing},
volume = {638},
pages = {129963},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129963},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225006356},
author = {Xuan Xiao and Jiahang Liu and Zhipeng Wang and Yanmin Zhou and Yong Qi and Shuo Jiang and Bin He and Qian Cheng},
keywords = {Robot Learning, Foundation Models, Embodied Artificial Intelligence},
abstract = {The proliferation of large language models (LLMs) has fueled a shift in robot learning from automation towards general embodied artificial intelligence (AI). Adopting foundation models together with traditional learning methods for robot learning has increasingly gained interest in the research community and shown potential for real-life application. However, there is little literature that comprehensively reviews the relatively new technologies combined with robotics. The purpose of this review is to systematically assess the state-of-the-art foundation models in robot learning and to identify future potential areas. Specifically, we first summarized the technical evolution of robot learning and identified the necessary preliminary preparations for foundation models, including the simulators, datasets, and foundation model framework. In addition, we focused on the following four mainstream areas of robot learning, including manipulation, navigation, task planning, and reasoning, and demonstrated how the foundation model can be adopted in the above scenarios. Furthermore, critical issues that are neglected in the current literature, including robot hardware and software decoupling, dynamic data, generalization performance in the presence of humans, etc., were discussed. This review highlights the state-of-the-art progress of foundation models in robot learning. Future research should focus on multimodal interaction, especially dynamics data, robotics-specific foundation models, AI alignment, etc.}
}
@article{CHEN2025108,
title = {DFM: Dialogue foundation model for universal large-scale dialogue-oriented task learning},
journal = {AI Open},
volume = {6},
pages = {108-117},
year = {2025},
issn = {2666-6510},
doi = {https://doi.org/10.1016/j.aiopen.2025.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666651025000075},
author = {Zhi Chen and Da Ma and Hanqi Li and Lu Chen and Jiabao Ji and Yuncong Liu and Bei Chen and Mengyue Wu and Su Zhu and Xin Dong and Fujiang Ge and Qingliang Miao and Jian-Guang Lou and Shuai Fan and Kai Yu},
keywords = {Dialogue foundation model, Pre-trained language model, Multitask learning, Knowledge-grounded dialogue},
abstract = {Building a universal conversational agent has been a long-standing goal of the dialogue research community. Most previous works only focus on a small set of dialogue tasks. In this work, we aim to build a unified dialogue foundation model (DFM) which can be used to solve massive diverse dialogue tasks. To achieve this goal, a large-scale well-annotated dialogue dataset with rich task diversity (DialogZoo) is collected. We introduce a framework to unify all dialogue tasks and propose novel auxiliary self-supervised tasks to achieve stable training of DFM on the highly diverse large scale DialogZoo corpus. Experiments show that, compared with models of the same size, DFM can achieve competitive performance on very rich cross-domain downstream dialogue tasks. Furthermore, when scaling to large language models, DFM remains effective. This demonstrates that DFM largely extends the ability of unified dialogue pre-trained model.}
}
@article{KUMAR2025109011,
title = {An evolutionary study on technologies for polyethylene terephthalate waste recycling using natural language processing},
journal = {Computers & Chemical Engineering},
volume = {195},
pages = {109011},
year = {2025},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2025.109011},
url = {https://www.sciencedirect.com/science/article/pii/S0098135425000158},
author = {Avan Kumar and Harshitha Chandra Jami and Bhavik R. Bakshi and Manojkumar Ramteke and Hariprasad Kodamana},
keywords = {PET waste, Recycle technologies, NLP technique, Knowledge graph, Topic modeling, Popularity index},
abstract = {Polyethylene terephthalate (PET) is valued for its durability, tensile strength, low moisture absorption, and cost-effectiveness. However, its non-biodegradability poses an environmental threat, and plastic recycling is the sole remedy. This study proposes an NLP framework for concisely extracting and summarizing key information on recycling technologies and alternatives from relevant scientific literature. This NLP framework comprises three approaches: time-series knowledge graphs, dynamic transformer-based topic modeling, and estimating popularity indices for technologies. The framework aims to streamline the extraction of qualitative and quantitative insights for sustainable and economical PET waste recycling pathways. Key findings of the study show that there is a 406% rise in pyrolysis technology use, a 278% increase in chemical conversion, and a 1353% surge in waste PET utilization for electronic device-making. It is worth noting that some of the identified recycling pathways corroborate well with the actual implementation in the industries.}
}
@article{BORROTO2023120383,
title = {SPARQL-QA-v2 system for Knowledge Base Question Answering},
journal = {Expert Systems with Applications},
volume = {229},
pages = {120383},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120383},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423008850},
author = {Manuel A. Borroto and Francesco Ricca},
keywords = {Knowledge base, Question answering, Neural network},
abstract = {Accessing the large volumes of information available in public knowledge bases might be complicated for those users unfamiliar with formal languages, such as the SPARQL query language and the ontology definition languages. This issue can be overcome by providing systems able to answer questions posed in natural language on a knowledge base, a task that is called Knowledge Base Question Answering (KBQA) in the literature. More in detail, many KBQA systems aim at translating automatically questions into the corresponding SPARQL queries to be executed over the knowledge base to get the answers. Effective state-of-the-art KBQA systems are based on neural-machine translation but easily fail to recognize words that are Out Of the Vocabulary (OOV) of the training set. This is a serious issue while querying large ontologies where the list of entities is huge and easily evolves over time. In this paper, we present the SPARQL-QA-v2 system that combines in an innovative way Named Entity Linking, Named Entity Recognition, and Neural Machine Translation for addressing the problem of generating SPARQL queries from questions posed in natural language. We demonstrate empirically that SPARQL-QA-v2 is effective and resilient to OOV words and delivers state-of-the-art performance in well-known datasets for question answering over DBpedia and Wikidata knowledge bases.}
}
@article{CHEN2023104441,
title = {Self-prediction of relations in GO facilitates its quality auditing},
journal = {Journal of Biomedical Informatics},
volume = {144},
pages = {104441},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104441},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423001624},
author = {Cheng Chen and Lingyun Luo and Chunlei Zheng and Pingjian Ding and Huan Liu and Hanyu Luo},
keywords = {Gene ontology, Ontology auditing, Machine learning, Relation prediction},
abstract = {As applications of the gene ontology (GO) increase rapidly in the biomedical field, quality auditing of it is becoming more and more important. Existing auditing methods are mostly based on rules, observed patterns or hypotheses. In this study, we propose a machine-learning-based framework for GO to audit itself: we first predict the IS-A relations among concepts in GO, then use differences between predicted results and existing relations to uncover potential errors. Specifically, we transfer the taxonomy of GO 2020 January release into a dataset with concept pairs as items and relations between them as labels(pairs with no direct IS-A relation are labeled as ndrs). To fully obtain the representation of each pair, we integrate the embeddings for the concept name, concept definition, as well as concept node in a substring-based topological graph. We divide the dataset into 10 parts, and rotate over all the parts by choosing one part as the testing set and the remaining as the training set each time. After 10 rotations, the prediction model predicted 4,640 existing IS-A pairs as ndrs. In the GO 2022 March release, 340 of these predictions were validated, demonstrating significance with a p-value of 1.60e−46 when compared to the results of randomly selected pairs. On the other hand, the model predicted 2,840 out of 17,079 selected ndrs in GO to be IS-A’s relations. After deleting those that caused redundancies and circles, 924 predicted IS-A’s relations remained. Among 200 pairs randomly selected, 30 were validated as missing IS-A’s by domain experts. In conclusion, this study investigates a novel way of auditing biomedical ontologies by predicting the relations in it, which was shown to be useful for discovering potential errors.}
}
@article{WEN2024,
title = {A Case Demonstration of the Open Health Natural Language Processing Toolkit From the National COVID-19 Cohort Collaborative and the Researching COVID to Enhance Recovery Programs for a Natural Language Processing System for COVID-19 or Postacute Sequelae of SARS CoV-2 Infection: Algorithm Development and Validation},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/49997},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424001169},
author = {Andrew Wen and Liwei Wang and Huan He and Sunyang Fu and Sijia Liu and David A Hanauer and Daniel R Harris and Ramakanth Kavuluru and Rui Zhang and Karthik Natarajan and Nishanth P Pavinkurve and Janos Hajagos and Sritha Rajupet and Veena Lingam and Mary Saltz and Corey Elowsky and Richard A Moffitt and Farrukh M Koraishy and Matvey B Palchuk and Jordan Donovan and Lora Lingrey and Garo Stone-DerHagopian and Robert T Miller and Andrew E Williams and Peter J Leese and Paul I Kovach and Emily R Pfaff and Mikhail Zemmel and Robert D Pates and Nick Guthe and Melissa A Haendel and Christopher G Chute and Hongfang Liu},
keywords = {natural language processing, clinical information extraction, clinical phenotyping, extract, extraction, NLP, phenotype, phenotyping, narratives, unstructured, PASC, COVID, COVID-19, SARS-CoV-2, OHNLP, Open Health Natural Language Processing},
abstract = {Background
A wealth of clinically relevant information is only obtainable within unstructured clinical narratives, leading to great interest in clinical natural language processing (NLP). While a multitude of approaches to NLP exist, current algorithm development approaches have limitations that can slow the development process. These limitations are exacerbated when the task is emergent, as is the case currently for NLP extraction of signs and symptoms of COVID-19 and postacute sequelae of SARS-CoV-2 infection (PASC).
Objective
This study aims to highlight the current limitations of existing NLP algorithm development approaches that are exacerbated by NLP tasks surrounding emergent clinical concepts and to illustrate our approach to addressing these issues through the use case of developing an NLP system for the signs and symptoms of COVID-19 and PASC.
Methods
We used 2 preexisting studies on PASC as a baseline to determine a set of concepts that should be extracted by NLP. This concept list was then used in conjunction with the Unified Medical Language System to autonomously generate an expanded lexicon to weakly annotate a training set, which was then reviewed by a human expert to generate a fine-tuned NLP algorithm. The annotations from a fully human-annotated test set were then compared with NLP results from the fine-tuned algorithm. The NLP algorithm was then deployed to 10 additional sites that were also running our NLP infrastructure. Of these 10 sites, 5 were used to conduct a federated evaluation of the NLP algorithm.
Results
An NLP algorithm consisting of 12,234 unique normalized text strings corresponding to 2366 unique concepts was developed to extract COVID-19 or PASC signs and symptoms. An unweighted mean dictionary coverage of 77.8% was found for the 5 sites.
Conclusions
The evolutionary and time-critical nature of the PASC NLP task significantly complicates existing approaches to NLP algorithm development. In this work, we present a hybrid approach using the Open Health Natural Language Processing Toolkit aimed at addressing these needs with a dictionary-based weak labeling step that minimizes the need for additional expert annotation while still preserving the fine-tuning capabilities of expert involvement.}
}
@article{LIU2025108864,
title = {Investigating the interpretability of ChatGPT in mental health counseling: An analysis of artificial intelligence generated content differentiation},
journal = {Computer Methods and Programs in Biomedicine},
volume = {268},
pages = {108864},
year = {2025},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2025.108864},
url = {https://www.sciencedirect.com/science/article/pii/S0169260725002810},
author = {Yang Liu and Fan Wang},
keywords = {Interpretability analysis, Mental health, Machine learning, AIGC, Psychological counseling, Large language models, Topic modeling},
abstract = {The global impact of COVID-19 has caused a significant rise in the demand for psychological counseling services, creating pressure on existing mental health professionals. Large language models (LLM), like ChatGPT, are considered a novel solution for delivering online psychological counseling. However, performance evaluation, emotional expression, high levels of anthropomorphism, ethical issues, transparency, and privacy breaches need to be addressed before LLM can be widely adopted. This study aimed to evaluate ChatGPT’s effectiveness and emotional support capabilities in providing mental health counseling services from both macro and micro perspectives to examine whether it possesses psychological support abilities comparable to those of human experts. Building on the macro-level evaluation, we conducted a deeper comparison of the linguistic differences between ChatGPT and human experts at the micro-level. In addition, to respond to current policy requirements regarding the labeling, we further explored how to identify artificial intelligence generated content (AIGC) in counseling texts and which micro-level linguistic features can effectively distinguish AIGC from user-generated content (UGC). Finally, the study addressed transparency, privacy breaches, and ethical concerns. We utilized ChatGPT for psychological interventions, applying LLM to address various mental health issues. The BERTopic algorithm evaluated the content across multiple mental health problems. Deep learning techniques were employed to differentiate between AIGC and UGC in psychological counseling responses. Furthermore, Local Interpretable Model-agnostic Explanation (LIME) and SHapley Additive exPlanations (SHAP) evaluate interpretability, providing deeper insights into the decision-making process and enhancing transparency. At the macro level, ChatGPT demonstrated performance comparable to human experts, exhibiting professionalism, diversity, empathy, and a high degree of human likeness, making it highly effective in counseling services. At the micro level, deep learning models achieved accuracy rates of 99.12 % and 96.13 % in distinguishing content generated by ChatGPT 3.5 and ChatGPT 4.0 from UGC, respectively. Interpretability analysis revealed that context, sentence structure, and emotional expression were key factors differentiating AIGC from UGC. The findings highlight ChatGPT's potential to deliver effective online psychological counseling and demonstrate a reliable framework for distinguishing between artificial intelligence-generated and human-generated content. This study underscores the importance of leveraging large-scale language models to support mental health services while addressing high-level anthropomorphic issues and ethical and practical challenges.}
}
@article{MANNS2025110645,
title = {PED-IA, a CDSS to support decision in pediatrics telephone triage: a crossover evaluation},
journal = {Computers in Biology and Medicine},
volume = {195},
pages = {110645},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.110645},
url = {https://www.sciencedirect.com/science/article/pii/S0010482525009965},
author = {Aurélia Manns and Alix Millet and Fleur Mougin and Florence Campeotto and Benoît Vivien and Laurent Dupic and Anita Burgun and Jean-Philippe Jais and Rosy Tsopra},
keywords = {Decision support systems, Clinical, Artificial intelligence, Digital health, Triage, Emergency care, Prehospital},
abstract = {Background
Pediatric emergency departments face overcrowding, often driven by non-urgent consultations. Telephone triage, supported by clinical decision support systems (CDSSs), offers a potential solution to improve decision accuracy and reduce unnecessary visits. However, pediatric-specific CDSSs are scarce and underexplored.
Objective
This study aimed to evaluate the impact of a pediatric-specific CDSS, PED-IA, on decision-making accuracy, confidence, and response time.
Methods
PED-IA is an ontology-based CDSS featuring a rule-driven inference engine and a dynamic interface that guides practitioners through structured clinical reasoning. A crossover study was conducted with 51 practitioners who had to answer clinical cases with and without the CDSS. Decision accuracy, confidence, and response times were measured, and satisfaction was assessed through questionnaires.
Results
The CDSS significantly improved decision accuracy from 52.9 % to 76.0 % (+23.1 %, p < 0.01) and increased confidence levels by 0.65 points on a 10-point scale (p < 0.01). Residents benefited the most, with an improved accuracy (odds ratio of 3.70 [2.15, 6.36]). Response times increased by an average of 261.8 s per case (p < 0.01). Practitioners expressed high satisfaction, with 88.2 % finding the system useful for decision-making and 84.3 % believing it could reduce stress in clinical practice.
Conclusion
The PED-IA CDSS significantly enhances triage decision accuracy and user confidence, making it a promising system for clinical practice and medical education. Practitioners viewed the system positively and identified its long-term time-saving potential. Future works should focus on refining system ergonomics and exploring hybrid models that combine data-driven and logic-based approaches to improve usability and adaptability.}
}
@article{CLARKSON2025104804,
title = {Developing libraries of semantically-augmented graphics as visual standards for biomedical information systems},
journal = {Journal of Biomedical Informatics},
volume = {163},
pages = {104804},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2025.104804},
url = {https://www.sciencedirect.com/science/article/pii/S1532046425000334},
author = {Melissa D. Clarkson and Steven Roggenkamp and Landon T. Detwiler},
keywords = {Standards, Visual communication, Ontology, Semantic web},
abstract = {Objective
Visual representations generally serve as supplements to information, rather than as bearers of computable information themselves. Our objective is to develop a method for creating semantically-augmented graphic libraries that will serve as visual standards and can be implemented as visual assets in intelligent information systems.
Methods
Graphics were developed using a composable approach and specified using SVG. OWL was used to represent the entities of our system, which include elements, units, graphics, graphic libraries, and library collections. A graph database serves as our data management system. Semantics are applied at multiple levels: (a) each element is associated with a semantic style class to link visual style to semantic meaning, (b) graphics are described using object properties and data properties, (c) relationships are specified between graphics, and (d) mappings are made between the graphics and outside resources.
Results
The Graphic Library web application enables users to browse the libraries, view information pages for each graphic, and download individual graphics. We demonstrate how SPARQL can be employed to query the graphics database and the APIs can be used to retrieve the graphics and associated data for applications. In addition, this work shows that our method of designing composable graphics is well-suited to depicting variations in human anatomy.
Conclusion
This work provides a bridge between visual communication and the field of knowledge representation. We demonstrate a method for creating visual standards that are compatible with practices in biomedical ontology and implement a system for making them accessible to information systems.}
}
@article{KHALIL2021106988,
title = {Identification of trusted IoT devices for secure delegation},
journal = {Computers & Electrical Engineering},
volume = {90},
pages = {106988},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.106988},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621000197},
author = {Ushna Khalil and Adnan Ahmad and Abdel-Haleem Abdel-Aty and Mohamed Elhoseny and Mohamed W. Abo El-Soud and Furkh Zeshan},
keywords = {Delegation, IoT, Ontology, Semantic matching, Trust},
abstract = {In the era of ubiquitous computing, Internet of Things (IoT) devices have a remarkable impact on human lives. In addition to numerous services provided by IoT devices, there are some constraints and limitations in their service provision. For instance, due to their resource constraint nature, sometimes these devices have insufficient resources to perform assigned tasks. Therefore, a delegation mechanism is usually required to handle task requests assigned to devices with limited resources. However, at the same time, the data possessed by these devices can be sensitive, so a trust mechanism is needed to identify trusted devices for delegating data sensitive services. Ontologies provide a robust method to integrate models with their corresponding real-world scenarios. This paper proposes a model for the identification of trusted IoT devices for delegation, designed through ontologies to resolve the problems of incomplete task requests owned by resource-constrained IoT devices. The proposed ontology provides a unique view of domain knowledge from the operational knowledge of pervasive and ubiquitous systems, like IoT devices. The paper also proposes a semantic matching algorithm to rank highly favorable and trusted devices to handle resource and trust priorities. The proposed ontology is developed in protégé editor and evaluated through a web query language. The experimental results based on simulation and parametric evaluations reflect that the proposed system performs better than existing approaches.}
}
@article{OLICK2025102157,
title = {The Reality of Collective Memory},
journal = {Current Opinion in Psychology},
pages = {102157},
year = {2025},
issn = {2352-250X},
doi = {https://doi.org/10.1016/j.copsyc.2025.102157},
url = {https://www.sciencedirect.com/science/article/pii/S2352250X25001708},
author = {Jeffrey K. Olick},
keywords = {Collective memory, Distributed cognition, Transactive memory, Extended mind, Social Ontology},
abstract = {This essay reconsiders the concept of collective memory, long dismissed by psychologists as metaphorical or unscientific. Drawing on recent developments in psychology, philosophy, and systems theory, it argues that memory can be seen as genuinely collective, distributed across individuals, artifacts, and institutions, without abandoning scientific rigor.}
}
@article{ALMQUIST2025111543,
title = {Historical spatio-temporal data on North American radical environmental direct-action events},
journal = {Data in Brief},
volume = {60},
pages = {111543},
year = {2025},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2025.111543},
url = {https://www.sciencedirect.com/science/article/pii/S2352340925002756},
author = {Zack W. Almquist and Benjamin E. Bagozzi and Daria Blinova},
keywords = {Event data, Radical environmental activism, Animal liberation, Animal rights, Environmentalism, United States, Canada},
abstract = {Social and political event data are widely used in scientific research. However, event data concerning the direct actions of radical environmental groups is comparatively scarce, due in large part to inconsistent news coverage and the clandestine nature of the groups involved. Leveraging original reports maintained by radical environmental groups and their allies, this article codes historical spatio-temporal event data on radical environmental direct-action events in the United States and Canada during a period of heightened prominence in radical environmentalism: 1995-2007. The article's event level data include information on event type, date and geolocation, and the target of each event, as well as the original textual reports of each coded event. This data will facilitate a wide variety of qualitative and quantitative analyses of radical environmental activism, alongside validations of recently developed large language model (LLM) tools for event data extraction. We also offer a separate spatio-temporally aggregated version of these same data. This second dataset is aggregated to the 0.5 × 0.5 decimal-degree spatial grid-year level and adds additional environmental-, environmental group-, and social-correlates. Accordingly, this second dataset will readily enable spatio-temporal statistical analyses of radical environmental direct-action events, their causes, and their determinants—phenomena that have been previously under-explored in large N studies.}
}
@article{SERMET201851,
title = {An intelligent system on knowledge generation and communication about flooding},
journal = {Environmental Modelling & Software},
volume = {108},
pages = {51-60},
year = {2018},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2018.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S1364815217308368},
author = {Yusuf Sermet and Ibrahim Demir},
keywords = {Intelligent systems, Natural language processing, Knowledge generation, Ontology, Disaster preparedness, Information communication},
abstract = {Communities are at risk from extreme events and natural disasters that can lead to dangerous situations for residents. Improving resilience by helping people learn how to better prepare for, recover from, and adapt to disasters is critical to reduce the impacts of these extreme events. This project presents an intelligent system, Flood AI, designed to improve societal preparedness for flooding by providing a knowledge engine that uses voice recognition, artificial intelligence, and natural language processing based on a generalized ontology for disasters with a primary focus on flooding. The knowledge engine uses flood ontology to connect user input to relevant knowledge discovery channels on flooding by developing a data acquisition and processing framework using environmental observations, forecast models, and knowledge bases. The framework’s communication channels include web-based systems, agent-based chatbots, smartphone applications, automated web workflows, and smart home devices, opening the knowledge discovery for flooding to many unique use cases.}
}
@article{SELLERS2022101671,
title = {The narrative framework of psychological jurisprudence: Virtue ethics as criminal justice practice},
journal = {Aggression and Violent Behavior},
volume = {63},
pages = {101671},
year = {2022},
note = {Practice Frameworks in the Criminal Justice System},
issn = {1359-1789},
doi = {https://doi.org/10.1016/j.avb.2021.101671},
url = {https://www.sciencedirect.com/science/article/pii/S1359178921001257},
author = {Brian G. Sellers and Bruce A. Arrigo},
keywords = {Psychological jurisprudence, Practice frameworks, Virtue ethics, Ontology and epistemology, Forensic and correctional psychology},
abstract = {Recently, Ward and Durrant (2021) utilized the construct of “practice frameworks” to better organize the important theoretical aspects of two major correctional interventions (i.e., Good Lives Model and Restorative Justice). This organization included the configuration of three interlinked conceptual levels that bridge ontological and epistemological gaps between core functional values, causal assumptions, and practical applications. In this paper, we employ the practice framework strategy to highlight a different way to think about forensic and correctional psychology. We explain how this alternative way advances ethical principles that reconceive the nature of offending and redirect the epistemology of offender treatment. First, we elucidate the key conceptual principles of Psychological Jurisprudence (PJ). These include the values of reciprocal consciousness, inter-subjectivity, and mutual power as conceived of and developed within continental philosophy. Second, we examine the operating assumptions of PJ. These include the stipulations that: (1) the unconscious exists and it is structured much like a language; (2) subjectivity is always politicized because unconscious structured language pre-codes and prefigures conscious thought; and (3) power functions through the politics of discourse, producing circumscribed and deferred knowledge about human interrelatedness and interdependence. Third, we explain how PJ's core set of values and operating assumptions reframe the therapeutic practice of forensic and correctional psychology. This reframing radicalizes the ontology of offending and the epistemology of offender treatment. To substantiate this claim, we discuss several prevailing approaches to therapeutic practice, including the deficit-correcting (RNR), desistance-management (GLM), and normative-reconciliation (NRJ) models. Fourth, we propose alternative therapeutic practice guidelines for practitioners who work within treatment and therapy as well as recovery and reentry spaces of carceral coexistence.}
}
@article{GUI2024103767,
title = {Map retrieval intention recognition based on relevance feedback and geographic semantic guidance: For better understanding user retrieval demands},
journal = {Information Processing & Management},
volume = {61},
number = {4},
pages = {103767},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103767},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324001274},
author = {Zhipeng Gui and Xinjie Liu and Anqi Zhao and Yuhan Jiang and Zhipeng Ling and Xiaohui Hu and Fa Li and Zelong Yang and Huayi Wu and Shuangming Zhao},
keywords = {Map retrieval, User relevance feedback, Geographic ontology, Frequent itemset mining, Minimum description length principle},
abstract = {Effective retrieval is essential for finding resources in demand handily amidst extensive data records in data warehouse. Mainstream map retrieval methods suffer from intention gap problem and are incapable to describe sophisticated user demands precisely due to the limits of low- and middle-level text or visual feature matching, resulting in unsatisfactory retrieval results. Such limitations are more marked when map retrieval demands were characterized with joint constraints of geographic concepts. To address this issue, we propose a map retrieval intention recognition method to perceive user demands with relevance feedback samples and geographic semantics guidance. Specifically, we construct a hierarchical intention expression model to describe retrieval goals and their multi-dimensional attribute constrains; incorporate geographic ontologies to provide semantic guidance and facilitate recognition; utilize the frequent itemset mining (FIM) algorithm Apriori to generate intention candidates from relevance feedback samples, and search for the optimal intention set by adopting the minimum description length (MDL) principle. The experiments verify the effectiveness of Apriori algorithm and MDL principle on intention recognition. The proposed method outperforms the FIM algorithm Gene Ontology (RuleGO) and the Decision Tree algorithm with Hierarchical Features (DTHF) with higher recognition accuracy and noise tolerance. Furthermore, through our sample augmentation strategy, the method yields promising recognition accuracy even when the feedback sample size is as low as ten, substantially reducing the feedback burden in human-computer interactions. We envision that the application of our method in spatial data infrastructures (SDIs), such as geoportals and catalogue services, could enhance the quality of service and user experience in geospatial data discovery.}
}
@article{CHENG202495,
title = {Automated knowledge graphs for complex systems (AutoGraCS): Applications to management of bridge networks},
journal = {Resilient Cities and Structures},
volume = {3},
number = {4},
pages = {95-106},
year = {2024},
issn = {2772-7416},
doi = {https://doi.org/10.1016/j.rcns.2024.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S2772741624000607},
author = {Minghui Cheng and Syed M.H. Shah and Antonio Nanni and H. Oliver Gao},
keywords = {System digital twin, Bayesian network, Infrastructure systems, Knowledge Graph},
abstract = {With the ability to harness the power of big data, the digital twin (DT) technology has been increasingly applied to the modeling and management of structures and infrastructure systems, such as buildings, bridges, and power distribution systems. Supporting these applications, an important family of methods are based on graphs. For DT applications in modeling and managing smart cities, large-scale knowledge graphs (KGs) are necessary to represent the complex interdependencies and model the urban infrastructure as a system of systems. To this end, this paper develops a conceptual framework: Automated knowledge Graphs for Complex Systems (AutoGraCS). In contrast to existing KGs developed for DTs, AutoGraCS can support KGs to account for interdependencies and statistical correlations across complex systems. The established KGs from AutoGraCS can then be easily turned into Bayesian networks for probabilistic modeling, Bayesian analysis, and adaptive decision supports. Besides, AutoGraCS provides flexibility in support of users’ need to implement the ontology and rules when constructing the KG. With the user-defined ontology and rules, AutoGraCS can automatically generate a KG to represent a complex system consisting of multiple systems. The bridge network in Miami-Dade County, FL is used as an illustrative example to generate a KG that integrates multiple layers of data from the bridge network, traffic monitoring facilities, and flood water watch stations.}
}
@article{HE2025,
title = {Chinese relation extraction for constructing satellite frequency and orbit knowledge graph: a survey},
journal = {Digital Communications and Networks},
year = {2025},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2025.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352864825000665},
author = {Yuanzhi He and Zhiqiang Li and Zheng Dou},
keywords = {Relation extraction, Information extraction, Distant supervision, Parsing tree, Joint entity-relation extraction},
abstract = {As Satellite Frequency and Orbit (SFO) constitute scarce natural resources, constructing a Satellite Frequency and Orbit Knowledge Graph (SFO-KG) becomes crucial for optimizing their utilization. In the process of building the SFO-KG from Chinese unstructured data, extracting Chinese entity relations is the fundamental step. Although Relation Extraction (RE) methods in the English field have been extensively studied and developed earlier than their Chinese counterparts, their direct application to Chinese texts faces significant challenges due to linguistic distinctions such as unique grammar, pictographic characters, and prevalent polysemy. The absence of comprehensive reviews on Chinese RE research progress necessitates a systematic investigation. A thorough review of Chinese RE has been conducted from four methodological approaches: pipeline RE, joint entity-relation extraction, open domain RE, and multimodal RE techniques. In addition, we further analyze the essential research infrastructure, including specialized datasets, evaluation benchmarks, and competitions within Chinese RE research. Finally, the current research challenges and development trends in the field of Chinese RE were summarized and analyzed from the perspectives of ecological construction methods for datasets, open domain RE, N-ary RE, and RE based on large language models. This comprehensive review aims to facilitate SFO-KG construction and its practical applications in SFO resource management.}
}
@article{GHIO2024102687,
title = {Democratizing academic research with Artificial Intelligence: The misleading case of language},
journal = {Critical Perspectives on Accounting},
volume = {98},
pages = {102687},
year = {2024},
issn = {1045-2354},
doi = {https://doi.org/10.1016/j.cpa.2023.102687},
url = {https://www.sciencedirect.com/science/article/pii/S1045235423001430},
author = {Alessandro Ghio},
keywords = {Artificial Intelligence, ChatGPT, Communication model, Language, Posthuman, Technology, Translation},
abstract = {This essay questions the use of Artificial Intelligence (AI) models like ChatGPT to enable academics to work in multiple languages. ChatGPT has the potential to dismantle the dominance of English in research communication. Adapting Te Eni's model of communication complexity, I explore the implications of using ChatGPT for non-native English speakers in the development, inputs, process, and impact of research communication. I then relate these technological changes to broader reflections on the relationship between machines and humans and the implications for the future of academic research. I argue that far from democratizing research communication, the proliferation of AI models like ChatGPT is creating new power imbalances and hegemonic positions that raise important ethical concerns for the academic community.}
}
@article{JAEKEL2024100151,
title = {Moving beyond binary language status in research: Investigating early foreign language learning and linguistic distance},
journal = {Research Methods in Applied Linguistics},
volume = {3},
number = {3},
pages = {100151},
year = {2024},
issn = {2772-7661},
doi = {https://doi.org/10.1016/j.rmal.2024.100151},
url = {https://www.sciencedirect.com/science/article/pii/S2772766124000570},
author = {Nils Jaekel and Michael Schurig and Eliane Lorenz},
keywords = {Linguistic distance, Multilingual learners, Early language learning, Mixed-effects modeling, Germany},
abstract = {Globalization and migration continue to shape our societies, including educational contexts such as school classrooms. In response to young learners’ linguistic needs, particularly in the context of foreign language learning in Germany, educational approaches need to be adapted to meet the needs of multilingual students. Current, binary approaches accounting for diverse linguistic backgrounds of students in research assume a high degree of homogeneity among multilingual students. Linguistic distance measures may provide alternative, more fine-grained, continuous tools to account for linguistic diversity. This study employs lexical linguistic distance to account for young language learners’ linguistic diversity in a reanalysis of Jaekel et al. (2017). Additionally, mixed-effects modeling was employed to factor in within-class effects for within-class factors versus structural equation modeling, which was previously used. The results outline that linguistic distance provides additional information beyond binary language status. Mixed effects modeling renders comparable results with the same tendencies, but yields more nuanced perspectives on the data.}
}
@article{ALBUKHITAN2020989,
title = {Framework of Semantic Annotation of Arabic Document using Deep Learning},
journal = {Procedia Computer Science},
volume = {170},
pages = {989-994},
year = {2020},
note = {The 11th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 3rd International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.096},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920305342},
author = {Saeed Albukhitan and Ahmed Alnazer and Tarek Helmy},
keywords = {Semantic Annotation, Arabic Language, Deep Learning, Ontology},
abstract = {Semantic Web vision is to have machines interpret and understand the content of Web documents. There is a need to convert the existing Web of documents into an understandable format, which could be done by automatic semantic annotation. Annotation could be performed using a set of tools provided with general and domain-specific ontologies. The aim of this paper is to present a generic semantic annotation framework of Arabic text using deep learning models. The framework produces annotations using different output formats for a given set of Arabic documents and ontologies. With a prototype of the framework, the initial evaluation shows a promising performance using different public Arabic word embedding models with different vectorization and matching techniques.}
}
@article{BANTON2024100707,
title = {Relational clinical practice: A hermeneutic, enactive, intersubjective model of osteopathy},
journal = {International Journal of Osteopathic Medicine},
volume = {51},
pages = {100707},
year = {2024},
issn = {1746-0689},
doi = {https://doi.org/10.1016/j.ijosm.2023.100707},
url = {https://www.sciencedirect.com/science/article/pii/S1746068923000512},
author = {Amanda Banton and Steven Vogel},
keywords = {Osteopathic medicine, Medical and healthcare philosophy, Professional identity, Phenomenology, Hermeneutics, Enactivism},
abstract = {This commentary proposes a framework for considering the philosophical foundations that support osteopathic practice, using the metaphor of ‘tectonic plates’. It is argued that reflective osteopathic practitioners, and the organisations they make up, would benefit from engaging with the differing ontological and epistemological traditions that underpin healthcare theory, evidence and practice. It is proposed that having more insight into the philosophical traditions of reality (ontologies) and forms of knowledge (epistemologies), that underpin medicine and healthcare, will support ethical, reflective practice, informed by the concepts of ontic integrity and epistemic humility. We focus particularly on phenomenological ontology and epistemology, which we propose underpins the relational and embodied concept of osteopathy as hermeneutic, enactive, intersubjective healthcare, locating it within the sphere of phenomenological and enactivist theory and research.}
}
@article{MARCONI2025105543,
title = {Titian's mythological paintings: A pictorial personal code},
journal = {BioSystems},
volume = {255},
pages = {105543},
year = {2025},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2025.105543},
url = {https://www.sciencedirect.com/science/article/pii/S0303264725001534},
author = {Valerio Marconi},
keywords = {Person, Branding, Peirce, Connotation, Mythology, Painting, Titian},
abstract = {This paper aims to introduce pictorial personal codes that differ from linguistic ones. Since individual norms distinguish personal codes, the concept of norms serves as the starting point for extending their scope beyond language. Indeed, there is a strong case for nonverbal norms within the philosophy of normativity. Gestures and drawings can establish these nonverbal norms. Drawn norms, such as those enforced by traffic signals, are part of a pictorial code in our everyday experiences. However, the application of this code view has been challenged in the field of semiotics, particularly in art and visual semiotics. Conversely, the pictorial turn suggests that cultural history involves a struggle between words and pictures. Pictures differ from words, yet are not purely visual media. This distinction can be traced back to Marcello Barbieri's differentiation between the world of perceptual objects and the world of names. I hypothesize that pictorial personal codes lack an abstract system of rules but still present a set of norms that can be categorized into shared and individual. I exemplify this with a case study of the Venetian painter Tiziano Vecellio (Titian). By integrating research results from art history, I show that Titian transformed his workshop into a modern firm and, as a form of legitimization, defined his original style in terms of tragic painting, specifically a personal approach to painting that connotes a literary genre such as Greek tragedy. I also evaluate the role of a graphic code in Titian's productive system. Finally, I draw certain conclusions regarding the social ontology of the firm, suggesting that Titian's tragic painting illustrates how firms endure, thanks to personal or brand-specific codes, and propose that gestural personal codes, like pictorial ones and unlike linguistic ones, should include unsystematic legisigns and a set of norms that can be divided into shared and individual ones.}
}
@article{WEI2025106119,
title = {Text-to-structure interpretation of user requests in BIM interaction},
journal = {Automation in Construction},
volume = {174},
pages = {106119},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106119},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525001591},
author = {Yinyi Wei and Xiao Li and Frank Petzold},
keywords = {Building information modeling, User request understanding, BIM interaction, Natural language processing, Language models},
abstract = {Numerous efforts have been devoted to utilizing a natural language-based interface for BIM interaction. These interfaces require extracting user's intent (i.e., the operation type) and slots (i.e., the targeted elements and properties). However, there is a lack of a fine-grained approach for extracting intent and slot information simultaneously. This paper introduces a text-to-structure approach based on language models to interpret user requests for BIM interaction (T2S4BIM). It proposed a synthetic data generation method and a curated dataset as data support. Employing Transformer-based models, T2S4BIM converts unstructured user requests into a structured format with intent and slot information. Experiments demonstrated that T2S4BIM outperformed existing approaches, with encoder-decoder models like T5 and FLAN-T5 achieving performance comparable to larger, decoder-only models such as Llama3.1-8B and Qwen2.5-7B, while improving efficiency. The practical applicability of T2S4BIM was illustrated through a Revit plug-in that interprets user requests and executes corresponding actions (e.g., manipulating object properties).}
}
@article{SUKHOBOKOV2024101279,
title = {A universal knowledge model and cognitive architectures for prototyping AGI},
journal = {Cognitive Systems Research},
volume = {88},
pages = {101279},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101279},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724000731},
author = {Artem Sukhobokov and Evgeny Belousov and Danila Gromozdov and Anna Zenger and Ilya Popov},
keywords = {Cognitive architecture, AGI, Metagraph, Archigraph, Universal knowledge model, Machine consciousness, Machine subconsciousness, Machine reflection, Machine worldview},
abstract = {The article identified 56 cognitive architectures for creating general artificial intelligence (AGI) and proposed a set of interrelated functional blocks that an agent approaching AGI in its capabilities should possess. Since the required set of blocks is not found in any of the existing architectures, the article proposes a reference cognitive architecture for intelligent systems approaching AGI in their capabilities. As one of the key solutions within the framework of the architecture, a universal method of knowledge representation is proposed, which allows combining various non-formalized, partially and fully formalized methods of knowledge representation in a single knowledge base, such as texts in natural languages, images, audio and video recordings, graphs, algorithms, databases, neural networks, knowledge graphs, ontologies, frames, essence-property-relation models, production systems, predicate calculus models, conceptual models, and others. To combine and structure various fragments of knowledge, archigraph model are used, constructed as a development of annotated metagraphs. As other components, the reference cognitive architecture being developed includes following modules: machine consciousness, machine subconsciousness, interaction with the external environment, a goal management, an emotional control, social interaction, reflection, ethics, worldview, learning, monitoring, statement problems, solving problems, self-organization and meta learning. Based on the composition of the proposed reference architecture modules, existing cognitive architectures containing the following modules were analyzed: machine consciousness, machine subconsciousness, reflection, worldview.}
}
@article{YUE2024103748,
title = {Detecting APT attacks using an attack intent-driven and sequence-based learning approach},
journal = {Computers & Security},
volume = {140},
pages = {103748},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.103748},
url = {https://www.sciencedirect.com/science/article/pii/S016740482400049X},
author = {Hao Yue and Tong Li and Di Wu and Runzi Zhang and Zhen Yang},
keywords = {Attack detection, Network event ontology, Provenance graph, Attack intent, Tagging policy},
abstract = {Advanced persistent threats (APTs) are a significant threat to network security as they can disintegrate the security fortress of enterprises. Recent studies have focused on detecting APT attacks by matching typical tactics, techniques, and procedures (TTPs) associated with APT attacks. However, the lack of positive APT samples affects the performance of existing approaches. To address this challenge, we propose a novel attack intent-driven and sequence-based learning approach (AISL) for APT detection. AISL integrates heterogeneous audit data and creates corresponding security tags based on attack intent. Specifically, we investigate various data sources of attack detection and establish a dedicated network event ontology. Based on this ontology, we construct a provenance graph that integrates audit data from heterogeneous sources. During the construction of the provenance graph, we identify and tag potential attack behaviors based on attack intent to increase the number of positive samples in the dataset. Finally, we train a tag-sequence-based semantic model for APT detection. We evaluated AISL through ten realistic APT attacks and achieved an average precision of 93.05%, recall of 98.12%, and F1-score of 95.36%, outperforming state-of-the-art approaches.}
}
@article{GALLOWAY2024101358,
title = {Promoting humanizing, meaningful, and just language instruction for multilingual learners and their peers: A pedagogical vision illustrated by examples from practice},
journal = {Linguistics and Education},
volume = {84},
pages = {101358},
year = {2024},
issn = {0898-5898},
doi = {https://doi.org/10.1016/j.linged.2024.101358},
url = {https://www.sciencedirect.com/science/article/pii/S0898589824000913},
author = {Emily Phillips Galloway and Paola Uccelli},
keywords = {Academic registers, Language learning, Academic language, Humanizing pedagogies, Translanguaging, Multilingualism},
abstract = {In this article, we engage with the question of how to create just and humanizing instructional conditions for learning through and about language at school. Rather than an empirical study, this article invites readers to rethink the role of languages in education by introducing and illustrating what we call Pedagogies of Voices (POV). Informed by research and practice, POV instructional approaches acknowledge and leverage the reality of multilingual and multidialectal repertoires in linguistically diverse classrooms, counteracting the tendency towards prescriptivism, which constrains what counts as language learning and teaching in schools. Through vignettes from middle-school multilingual classrooms implementing the TRANSLATE literacy curriculum, we illustrate how POV-inspired instruction transforms the conventions of classroom interactions by scaffolding language learning through relational activities that affirm and expand students’ multilingual repertoires and metalinguistic strategies for learning through and about language. These POV-inspired practices shift the role of teachers, who become learners of their students’ ways with language and shift the instructional goal from a narrow focus on teaching the language of school literacy to a concerted effort to foster flexible, resourceful, critical, and creative student voices—what we call Critical Rhetorical Flexibility. These two shifts, we argue, contribute to creating the enabling conditions to foster inclusive, humanizing communities in which students and teachers experience the joyful challenge of learning through languages together. We conclude with thoughts and considerations for theory, future practice-embedded research, and evidence-based educational practice.}
}
@article{WANG2025100162,
title = {Life is chemistry plus information},
journal = {BBA Advances},
volume = {7},
pages = {100162},
year = {2025},
issn = {2667-1603},
doi = {https://doi.org/10.1016/j.bbadva.2025.100162},
url = {https://www.sciencedirect.com/science/article/pii/S2667160325000250},
author = {Richard Liangchen Wang},
keywords = {Biological information, Symmetry in physical laws, Irreversibility, Non-physicochemical process, Molecular machine},
abstract = {In contemporary biology, the predominant paradigm is that life is chemistry. This means that all biological processes could be reduced to physicochemical processes. Some scientists have argued that life is chemistry plus information, but this ontological claim has not yet been proven. This paper first defines biological information through the decoding process, in which molecular machines such as membrane receptors and ribosomes connect signs to objects. Experiments have shown that molecular machines provide specific mechanisms to unidirectionally connect extracellular signals to intracellular second messengers in signal transduction, and unidirectionally link mRNAs to proteins in genetic information. However, I point out that the unidirectionality or irreversibility of biological information violates the (microscopic) time-reversal symmetry in physical laws and the principle of microscopic reversibility in chemistry. Genetic information also breaks the translational symmetry in physical laws, since the role of nucleotide triplets in translation depends on their position in mRNA. Therefore, biological information is non-physicochemical and differs ontologically from chemistry. This means that the paradigm in biology should be that life is chemistry plus information, rather than that life is merely chemistry. The paradigm of “life is chemistry plus information” succinctly explains that biological information provides an arrow of time in living organisms and why living and nonliving things are so different.}
}
@article{BABAIHA2023100078,
title = {A natural language processing system for the efficient updating of highly curated pathophysiology mechanism knowledge graphs},
journal = {Artificial Intelligence in the Life Sciences},
volume = {4},
pages = {100078},
year = {2023},
issn = {2667-3185},
doi = {https://doi.org/10.1016/j.ailsci.2023.100078},
url = {https://www.sciencedirect.com/science/article/pii/S2667318523000223},
author = {Negin Sadat Babaiha and Hassan Elsayed and Bide Zhang and Abish Kaladharan and Priya Sethumadhavan and Bruce Schultz and Jürgen Klein and Bruno Freudensprung and Vanessa Lage-Rupprecht and Alpha Tom Kodamullil and Marc Jacobs and Stefan Geissler and Sumit Madan and Martin Hofmann-Apitius},
keywords = {Knowledge graphs, Relation extraction, Natural language processing, Biomedical text mining, Biological expression language (BEL), Human brain pharmacome (HBP)},
abstract = {Background
Biomedical knowledge graphs (KG) have become crucial for describing biological findings in a structured manner. To keep up with the constantly changing flow of knowledge, their embedded information must be regularly updated with the latest findings. Natural language processing (NLP) has created new possibilities for automating this upkeep by facilitating information extraction from free text. However, due to annotated and labeled biomedical data limitations, the development of completely autonomous information extraction systems remains a substantial scientific and technological hurdle. This study aims to explore methodologies best suited to support the automatic extraction of causal relationships from biomedical literature with the aim of regular and rapid updating of disease-specific pathophysiology mechanism KGs.
Methods
Our proposed approach first searches and retrieves PubMed abstracts using the desired terms and keywords. The extension corpora are then passed through the NLP pipeline for automatic information extraction. We then identify triples representing cause-and-effect relationships and encode this content using the Biological Expression Language (BEL). Finally, domain experts perform an analysis of the completeness, relevance, accuracy, and novelty of the extracted triples.
Results
In our test scenario, which is focused on the KG regarding the phosphorylation of the Tau protein, our pipeline successfully contributed novel data, which was then subsequently used to update the KG leading to the identification of six additional upstream regulators of Tau phosphorylation.
Conclusion
Here, it is demonstrated that the NLP-based workflow we created is capable of rapidly updating pathophysiology mechanism graphs. As a result, production-scale, semi-automated updating of pre-existing, curated mechanism graphs is enabled.}
}
@article{JING2021,
title = {The Unified Medical Language System at 30 Years and How It Is Used and Published: Systematic Review and Content Analysis},
journal = {JMIR Medical Informatics},
volume = {9},
number = {8},
year = {2021},
issn = {2291-9694},
doi = {https://doi.org/10.2196/20675},
url = {https://www.sciencedirect.com/science/article/pii/S2291969421002672},
author = {Xia Jing},
keywords = {Unified Medical Language System, systematic literature analysis, biomedical informatics, health informatics},
abstract = {Background
The Unified Medical Language System (UMLS) has been a critical tool in biomedical and health informatics, and the year 2021 marks its 30th anniversary. The UMLS brings together many broadly used vocabularies and standards in the biomedical field to facilitate interoperability among different computer systems and applications.
Objective
Despite its longevity, there is no comprehensive publication analysis of the use of the UMLS. Thus, this review and analysis is conducted to provide an overview of the UMLS and its use in English-language peer-reviewed publications, with the objective of providing a comprehensive understanding of how the UMLS has been used in English-language peer-reviewed publications over the last 30 years.
Methods
PubMed, ACM Digital Library, and the Nursing & Allied Health Database were used to search for studies. The primary search strategy was as follows: UMLS was used as a Medical Subject Headings term or a keyword or appeared in the title or abstract. Only English-language publications were considered. The publications were screened first, then coded and categorized iteratively, following the grounded theory. The review process followed the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines.
Results
A total of 943 publications were included in the final analysis. Moreover, 32 publications were categorized into 2 categories; hence the total number of publications before duplicates are removed is 975. After analysis and categorization of the publications, UMLS was found to be used in the following emerging themes or areas (the number of publications and their respective percentages are given in parentheses): natural language processing (230/975, 23.6%), information retrieval (125/975, 12.8%), terminology study (90/975, 9.2%), ontology and modeling (80/975, 8.2%), medical subdomains (76/975, 7.8%), other language studies (53/975, 5.4%), artificial intelligence tools and applications (46/975, 4.7%), patient care (35/975, 3.6%), data mining and knowledge discovery (25/975, 2.6%), medical education (20/975, 2.1%), degree-related theses (13/975, 1.3%), digital library (5/975, 0.5%), and the UMLS itself (150/975, 15.4%), as well as the UMLS for other purposes (27/975, 2.8%).
Conclusions
The UMLS has been used successfully in patient care, medical education, digital libraries, and software development, as originally planned, as well as in degree-related theses, the building of artificial intelligence tools, data mining and knowledge discovery, foundational work in methodology, and middle layers that may lead to advanced products. Natural language processing, the UMLS itself, and information retrieval are the 3 most common themes that emerged among the included publications. The results, although largely related to academia, demonstrate that UMLS achieves its intended uses successfully, in addition to achieving uses broadly beyond its original intentions.}
}
@article{GEORGES2025107889,
title = {Bridging the gap between user stories and feature models by leveraging version control systems: A step towards software product line migration},
journal = {Information and Software Technology},
pages = {107889},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107889},
url = {https://www.sciencedirect.com/science/article/pii/S0950584925002289},
author = {Thomas Georges and Marianne Huchard and Mélanie König and Clémentine Nebut and Chouki Tibermacine},
keywords = {Software product line, Agile process, User story, Reengineering, SPL domain engineering, Feature model, Natural language processing, Formal concept analysis},
abstract = {Context:
Throughout the software lifecycle, a significant amount of knowledge is accumulated around the source code. In our work, we focus on agile software requirements, particularly user stories, and on issues and merge requests in version control systems, that have been opened for implementing user stories.
Objective:
The objective of this paper is to present a method that leverages this knowledge to guide an SPL migration.
Methods:
We consider merge requests in version control systems as the link between user stories (requirements) and the source code (implementation). The method combines Natural Language Processing (NLP) and clustering to identify features from user stories and hierarchically organize them. Relational Concept Analysis (RCA) is then used to compute logical rules from the hierarchy of features, using their links with the products and the source code. The logical rules are finally transformed into constraints in the produced feature model.
Results:
The method was implemented and evaluated on a dataset from an industrial partner. The results showed the efficiency of our method in synthesizing feature models for an SPL migration of the partner’s code base.
Conclusion:
The proposed method synthesizes feature models to guide an SPL migration based on agile software development practices and demonstrates its effectiveness on a real industrial dataset.}
}
@article{CALAUTTI2022103772,
title = {Preference-based inconsistency-tolerant query answering under existential rules},
journal = {Artificial Intelligence},
volume = {312},
pages = {103772},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103772},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222001126},
author = {Marco Calautti and Sergio Greco and Cristian Molinaro and Irina Trubitsyna},
keywords = {Preference, Inconsistency, Existential rule},
abstract = {Ontology-mediated query answering (OMQA) emerged as a paradigm to enhance querying of data sources with an ontology that encodes background knowledge. In applications involving large amounts of data from multiple data sources, it might well be the case that inconsistency arises, making standard query answering useless, since everything is entailed by an inconsistent knowledge base. Being able to provide meaningful query answers in the presence of inconsistency is thus a critical issue to make OMQA systems successful in practice. The problem of querying inconsistent knowledge has attracted a great deal of interest over the years. Different inconsistency-tolerant semantics of query answering have been proposed, that is, approaches to answer queries in a meaningful way despite the knowledge at hand being inconsistent. Most of the semantics in the literature are based on the notion of repair, that is, a “maximal” consistent subset of the database. In general, there can be several repairs, so it is often natural and desirable to express preferences among them. In this paper, we propose a framework for querying inconsistent knowledge bases under user preferences for existential rule languages. Specifically, we introduce preference rules, a declarative formalism which enable users to express (i) preferences over both the database and the knowledge that can be derived from it via an ontology, and (ii) preconditions for preferences to hold. We then define two notions of preferred repairs which take preference rules into account. This naturally leads us to introducing preference-aware counterparts of popular inconsistency-tolerant semantics, where only preferred repairs are considered for query answering. We provide a thorough analysis of the data and combined complexity of different relevant problems for a wide range of existential rule languages.}
}
@article{ROQUE2025107796,
title = {Trust requirements in sociotechnical systems: A systematic literature review},
journal = {Information and Software Technology},
volume = {186},
pages = {107796},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107796},
url = {https://www.sciencedirect.com/science/article/pii/S0950584925001351},
author = {Geicianfran Roque and José Nascimento and Rafael Souza and Carina Alves and João Araújo},
keywords = {Trust, Sociotechnical systems, Requirements engineering, Systematic literature review},
abstract = {Context:
Trust in Sociotechnical Systems (STS) has emerged as an essential requirement in everyday interactions, with notable relevance in fostering user acceptance of novel technologies. The growing dependence on automated and interactive systems underlines the importance of understanding the trust requirements in these systems in depth.
Objective:
This study aims to synthesize primary studies that address trust requirements in sociotechnical systems to identify key definitions, approaches, tools, processes, and application domains.
Methods:
We conducted a Systematic Literature Review (SLR) to answer four research questions. We searched studies from four databases: IEEE Xplore, ACM Digital Library, Scopus, and Springer. In addition, we performed snowballing to complement the automatic search.
Results:
We reviewed 42 primary studies. Our analysis indicates that the definition of trust requirements in sociotechnical systems is multifaceted and context-dependent. We observed that trust is influenced by factors such as data security, transparency in interactions, and the reliability of systems. We identified different approaches to specify trust requirements in sociotechnical systems, such as agent-oriented modeling, goal-oriented modeling, and ontological modeling.
Conclusion:
The study highlights the crucial need for more research that adopts holistic and interdisciplinary approaches to address trust requirements in several domains and popular areas such as Artificial Intelligence (AI). The findings suggest the importance of developing approaches based on conceptual models to address users’ trust requirements effectively.}
}
@article{PLASEK2025109855,
title = {Comparative ranking of marginal confounding impact of natural language processing-derived versus structured features in pharmacoepidemiology},
journal = {Computers in Biology and Medicine},
volume = {188},
pages = {109855},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.109855},
url = {https://www.sciencedirect.com/science/article/pii/S0010482525002057},
author = {Joseph M. Plasek and Richard D. Wyss and Janick G. Weberpals and Jie Yang and Thomas Deramus and Theodore N. Tsacogianis and Kerry Ngan and Lily G. Bessette and Kueiyu Joshua Lin and Li Zhou},
keywords = {Confounding factors, Epidemiologic, Pharmacoepidemiology, Osteoarthritis, Peptic ulcer, Natural language processing},
abstract = {Objective
To explore the ability of natural language processing (NLP) methods to identify confounder information beyond what can be identified using claims codes alone for pharmacoepidemiology.
Methods
We developed a retrospective cohort for high vs low dose proton pump inhibitors from linked Medicare claims (2008–2017) and electronic health record data for patients with a history of peptic ulcer disease or osteoarthritis. Clinical notes authored one year before first dispensing date were processed by off-the-shelf tools: bag-of-n-grams, latent Dirichlet allocation, a linguistics-focused tool, BERT sentence embeddings, BioBERT word embeddings, and GloVe word embeddings. Candidate features were ranked using Bross formula, a simple way to rank the marginal confounding impact of binary features on estimated causal effects.
Results
The marginal confounding impact in the Bross rankings of NLP-derived features trended from 39 % in the top 100 to 77 % in the top 500 to 93 % in the top 5000 among patients with peptic ulcer disease. More specifically, the top 25 confounders are largely from factors identified by domain experts and structured fields, and the marginal impact of these confounders is stronger than others. Features 25 to 50 include features identified by a linguistics-focused tool and embeddings, whereas features 50 to 100 include more embeddings and bag-of-ngrams. After 100, the curve flattens, meaning that the marginal impact of those potential confounders gets smaller. Similarly, among patients with osteoarthritis, NLP-derived features trended from 66 % in the top 100 to 84 % in the top 500 to 95 % in the top 5000 when the outcome was gastrointestinal bleed and from 47 % in the top 100 to 81 % in the top 500 to 94 % in the top 5000 when the outcome was acute kidney injury. Similar trends were observed in the information gain data, though NLP-derived features had higher baselines.
Conclusions
NLP contributed to finding large numbers of features that can supplement claims data and prespecified variables to help provide additional confounder information. We found that unsupervised off-the-shelf NLP tools can scale to generate large numbers of features appropriate for high-dimensional proxy adjustment and pharmacoepidemiology use cases.}
}
@article{ROZOV2025105389,
title = {Stages and causes of the evolution of language and consciousness: A theoretical reconstruction},
journal = {BioSystems},
volume = {248},
pages = {105389},
year = {2025},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2024.105389},
url = {https://www.sciencedirect.com/science/article/pii/S0303264724002740},
author = {Nikolai S. Rozov},
keywords = {Origin of language, Origin of consciousness, Anthropogenesis, Glottogenesis, Self-domestication, Joint intentionality, Multilevel selection, Interactive ritual, Internalization, Cognitive evolution},
abstract = {This article presents a refinement of theoretical explanations of the main stages of linguistic and cognitive evolution in anthropogenesis. The concepts of language, consciousness, self-consciousness, the self, the unconscious, the subconscious, and the relation between free will and determinism remain at the center of active and complex debates in philosophy and neuroscience. A basic theoretical apparatus comprising the central concepts of "concern" and "providing structure" (an extension of the biological concept of "adaptation") develops the paradigm of the extended evolutionary synthesis. Challenge-threats and challenge-opportunities are invariably associated with concerns pertaining to sustenance, safety, sexuality, parenthood, status, and emotional support. The consolidation of successful behavioral tries (tries), in response to these challenges occurs through the formation of a variety of providing structures including practices, abilities, and attitudes. These structures are formed through mechanisms of interactive rituals and internalization. These novel practices facilitate the transformation of both techno-natural environmental niches and group niches. The emergence of new structures gives rise to new challenges and concerns, which in turn necessitate undertaking of new tries. In the context of African multiregionalism, hominin groups and populations that experienced favorable periods of demographic growth, active migration, genetic, technological, and skill exchange also underwent significant demographic disasters. During the most unfavorable bottleneck periods only the most advanced groups, populations and species survived. The achieved potential for these abilities was consolidated as complexes of innate assignments in gene pools through the Baldwin effect and the multilevel selection. This logic provides an explanation for the main stages of language and speech complication (from holophrases and articulation to complex syntax), as well as the emergence of new abilities of consciousness (from the expansion of attention field to self-consciousness and the "I"-structure).}
}
@article{BELEHAKI20253082,
title = {Integrating plasmasphere, ionosphere and thermosphere observations and models into a standardised open access research environment: The PITHIA-NRF international project},
journal = {Advances in Space Research},
volume = {75},
number = {3},
pages = {3082-3114},
year = {2025},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2024.11.065},
url = {https://www.sciencedirect.com/science/article/pii/S0273117724011967},
author = {Anna Belehaki and Ingemar Häggström and Tamas Kiss and Ivan Galkin and Anders Tjulin and Mária Miháliková and Carl-Fredrik Enell and Gabriel Pierantoni and Yin Chen and Gergely Sipos and Sean Bruinsma and Viviane Pierrard and David Altadill and Antoni Segarra and Víctor Navas-Portella and Emanuele Pica and Luca Spogli and Lucilla Alfonsi and Claudio Cesaroni and Vicenzo Romano and Sara Mainella and Pietro Vermicelli and Tobias Verhulst and Stefaan Poedts and Manuel Hernández-Pajares and Dalia Buresova and Jan Rusz and Jaroslav Chum and Fabien Darrouzet and Edith Botek and Hanna Rothkaehl and Barbara Matyjasiak and Mariusz Pożoga and Marcin Grzesiak and David {Chan You Fee} and Dimitris Kagialis and Ioanna Tsagouri and Angeliki Thanasou and Themistocles Herekakis and Jean-Marie Chevalier and Nicolas Bergeot and Alexandre Winant and Maaijke Mevius and Ben Witvliet and Victoria Graffigna and Aurélie Marchaudon and David Wenzel and Martin Kriegel and Jürgen Matzka and Guram Kervalishvili and Tero Raita and Reko Hynönen and Jurgen Watermann},
abstract = {The PITHIA-NRF project “Plasmasphere Ionosphere Thermosphere Integrated Research Environment and Access services: a Network of Research Facilities” aims at building a European distributed network that integrates observations from space and ground, data processing tools and models to support scientific research on the Plasmasphere-Ionosphere-Thermosphere system. PITHIA-NRF is designed to provide formalised open access to experimental facilities, data and models, standardised data products, and training services. Participating organisations that operate these facilities, formed twelve nodes in eleven European countries. These nodes work on optimising their observing facilities and offer trans-national access to scientists and engineers. The PITHIA-NRF e-Science Centre is a core element of the project. Its design and evolution are controlled by a systematic ontology which governs the collection of scientific observations and research models, jointly termed data collections, which are registered with the e-Science Centre. Several tens of data collections are being registered. Data collection registrations adhere to FAIR principles and transparent quality measures to a large extent. The e-Science Centre facilitates the execution of research projects proposed by researchers from inside and outside the PITHIA-NRF consortium which require trans-national access to and understanding of data collections (observations and models) residing at one or several PITHIA-NRF nodes. Upon completion of the project a comprehensive collection of observations and models will have been gathered by the e-Science Centre for the benefit of efficient scientific research which relies on Europe-wide collaboration.}
}
@article{YANG2025394,
title = {A digital twin-driven industrial context-aware system: A case study of overhead crane operation},
journal = {Journal of Manufacturing Systems},
volume = {78},
pages = {394-409},
year = {2025},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524002991},
author = {Chao Yang and Hao Yu and Yuan Zheng and Lei Feng and Riku Ala-Laurinaho and Kari Tammi},
keywords = {Augmented reality, Context-aware system, Digital twin, Human-centered, Semantic technology},
abstract = {With advancements in Information and Communication Technologies (ICT), traditional manufacturing industries are engaged in a digital transformation. This transformation enables the acquisition of vast amounts of data and information, enhancing decision-making capabilities. This, in turn, has raised the expectations of field operators who seek data and information management tailored to the dynamic working environment, thereby improving efficiency in their daily operations. However, there is a lack of a holistic approach to integrating diverse data sources, extracting valuable contextual information, and delivering real-time information to field operators. This paper addresses this gap by proposing an adaptive, interoperable, and user-centered Context-Aware System (CAS). Initially, the paper explores the challenges and requirements associated with CAS’s current practices while proposing potential solutions. Furthermore, it introduces a system framework of CAS that integrates Digital Twin (DT) and semantic technologies. This framework includes three primary technical solutions: (1) Integrating DT to create a comprehensive digital representation of physical entities, enabling real-time data integration and synchronization; (2) Providing an ontology-based approach to model manufacturing context, facilitating knowledge representation and reasoning; (3) Developing a user-centered information delivery system leveraging Augmented Reality (AR) for context-aware visualization. The system architecture has been implemented and tested in a laboratory-scale industrial environment, focusing on crane operations within logistics scenarios. Lastly, three use cases are presented to demonstrate the system’s practical applicability, showcasing its feasibility in furnishing informed contextual information to end-users within the dynamic manufacturing environment.}
}
@article{SAHIN2025102686,
title = {Investigating teachers’ perceived costs and benefits toward a knowledge generation environment},
journal = {International Journal of Educational Research},
volume = {133},
pages = {102686},
year = {2025},
issn = {0883-0355},
doi = {https://doi.org/10.1016/j.ijer.2025.102686},
url = {https://www.sciencedirect.com/science/article/pii/S0883035525001594},
author = {Ercin Sahin and Jee Kyung Suh and Jale Ercan Dursun and Brian Hand},
keywords = {Cost and benefit, Axiology, Professional development, Knowledge generation, Approaches, Epistemological and ontological orientations},
abstract = {In knowledge generation environments, students actively engage as classroom participants, tasked with creating and developing new concepts and materials both individually and collaboratively. This approach aims to deepen their understanding beyond the provided information and foster meaningful learning experiences. However, transitioning from traditional learning focused on knowledge replication to generative learning is complex. Building on previous research centered on teacher concerns, we recognize that change incurs costs for each teacher involved, necessitating an examination of the benefits compared to these costs. This study employs a cost-benefit framework to assess teachers’ perceptions of the value of professional development (PD) in aiding their understanding of knowledge generation. Conducted as a multiple-case study involving ten K-5 teachers, data were collected through semi-structured interviews, vignettes, written reflections, and classroom observations. A key contribution of this study is the introduction of axiological orientations—value frameworks that we argue influence the development and implementation of knowledge generation environments. The findings underscore the critical role of perceived costs and benefits in determining the effectiveness of PD initiatives aimed at promoting knowledge generation. The analysis reveals that teachers’ willingness to adopt and implement knowledge generation environments heavily depends on their perceived benefits versus costs in instructional practices, epistemic tools, and orientations.}
}
@article{YAN2023106798,
title = {Intelligent predictive maintenance of hydraulic systems based on virtual knowledge graph},
journal = {Engineering Applications of Artificial Intelligence},
volume = {126},
pages = {106798},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106798},
url = {https://www.sciencedirect.com/science/article/pii/S095219762300982X},
author = {Wei Yan and Yu Shi and Zengyan Ji and Yuan Sui and Zhenzhen Tian and Wanjing Wang and Qiushi Cao},
keywords = {Industry 4.0, Predictive maintenance, Virtual knowledge graph, Ontology, Ontology-based data access, Hydraulic systems},
abstract = {In the manufacturing industry, a hydraulic system harnesses liquid fluid power to create powerful machines. Under the trend of Industry 4.0, the predictive maintenance of hydraulic systems is transforming to more intelligent and automated approaches that leverage the strong power of artificial intelligence and data science technologies. However, due to the knowledge-intensive and heterogeneous nature of the manufacturing domain, the data and information required for predictive maintenance are normally collected from ubiquitous sensing networks. This leads to the gap between massive heterogeneous data/information resources in hydraulic system components and the limited cognitive ability of system users. Moreover, how to capture and structure useful domain knowledge (in a machine-readable way) for solving domain-specific tasks remains an open challenge for the predictive maintenance of hydraulic systems. To address these challenges, in this paper we propose a virtual knowledge graph-based approach for the digital modeling and intelligent predictive analytics of hydraulic systems. We evaluate the functionalities and effectiveness of the proposed approach on a predictive maintenance task under real-world industrial contexts. Results show that our proposed approach is capable and feasible to be implemented for digital modeling, data access, data integration, and predictive analytics.}
}
@article{CUI2024834,
title = {A Sentiment-driven SPSS Requirement Analysis Method Based on Online Reviews},
journal = {Procedia CIRP},
volume = {128},
pages = {834-839},
year = {2024},
note = {34th CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.07.066},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124007947},
author = {Haoran Cui and Jiahuan Huang and Yan Yan},
keywords = {Smart product-service system (SPSS), Requirement analysis, Sentiment analysis, Kano model},
abstract = {In recent years, the rapid development of information and communication technologies (ICT) has enabled the prevailing digital transformation, bridging the gap between the physical and virtual worlds. Meanwhile, a continuously growing demand for more personalized products leads to the fact that enterprises must diversify their business model from the traditional product range. To adapt to this change, enterprises are adopting a new business model to offer not only physical products, but also personalized services, called smart product-service system (SPSS). Among the SPSS development process, innovation design is one of the most critical steps. How to obtain and analyze user requirements could significantly improve the efficiency and quality of the design process. Therefore, a sentiment-driven requirement analysis method based on online reviews is proposed. Firstly, ontologies are constructed to describe SPSS design knowledge. Secondly, requirements are automatically elicited from user reviews with natural language processing (NLP) and clustered according to their similarity. Next, sentiment analysis is conducted, and a representation method of sentiment is proposed named sentiment vector. Thirdly, according to the Kano model, requirements could be classified to support the further design process with designers. Finally, a case study is carried out to demonstrate the feasibility and advantages of the proposed novel method.}
}
@article{WANG2025111360,
title = {A classification method for online consultation on civil disputes based on deep transfer learning with small datasets},
journal = {Engineering Applications of Artificial Intelligence},
volume = {157},
pages = {111360},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.111360},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625013624},
author = {Ning Wang and Shibo Cui and Enhui Zhao and Baiji Jin},
keywords = {Deep learning, Transfer learning, Civil dispute, Classification of online consultation, Intelligent system},
abstract = {With the widespread use of artificial intelligent systems to resolve civil disputes, the classification function of online consultation intelligence systems has become increasingly significant, providing accurate knowledge support for later decision-making. However, the informality of online consultation and cross-platform data decentralization, together with the small amount of data, cause typical machine learning approaches to perform poorly when dealing with such data. In light of this, this study provides a deep transfer learning-based method to solving the challenge of classifying online consultation for civil disputes with unbalanced and small datasets via cross-platform knowledge transfer. First, ontology design and modeling are integrated to normalize cross-domain feature representations and reduce the impact of linguistic ambiguity on transfer results. Second, improved marginal fisher analysis is applied to improve the model's performance of intra-class compactness and inter-class separation. Then, joint distribution adaptation with conditional feature mapping is proposed to alleviate the adverse impact of outliers on transfer learning using a conditional feature mapping mechanism. Finally, online datasets on Chinese civil disputes are used to validate the method. The results indicate that the method is more effective than other popular machine learning techniques with evaluation metrics showing accuracy 84.17 % and recall 84.40 %. The study can provide technical support for artificial intelligence systems that lack data deposition and has practical significance for improving the efficiency of civil dispute mediation.}
}
@article{BAGHDASARYAN2024124736,
title = {Knowledge retrieval and diagnostics in cloud services with large language models},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124736},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124736},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424016038},
author = {Ashot Baghdasaryan and Tigran Bunarjyan and Arnak Poghosyan and Ashot Harutyunyan and Jad El-Zein},
keywords = {Customer support, Large language models, Support request, Knowledge base article, Recommender system, Trending issue},
abstract = {Efficient customer support is the foundation for any service provider trying to improve customer relationships. An important measure of successful support is the mean time to resolve issues. The complexity and large scale of modern cloud environments make it unrealistic to reduce the resolution time without deploying intelligent solutions. The latest also provides an exceptional opportunity to leverage cross-customer product usage data for proactive solutions when the troubles of some users can be analyzed in advance to prevent similar issues of other users. We build a recommender system that matches customer support requests to other resolved support requests or knowledge base articles that contain valuable information for problem remediation. This system can be used by customers or support teams to quickly find problem-resolution tips or detect trending issues to warn vulnerable users. We utilize large language models, fine-tune for better performance, and discuss capabilities and possible improvements. During our research, we highlighted several evaluation metrics such as mean time to resolve issues and the accuracy of recommendations. However, estimating accuracy is challenging due to insufficient datasets with precise and comprehensive recommendations. Despite this, our support managers provided some estimates regarding the remediation durations. Typically, identifying and resolving an issue takes several days or weeks. With appropriate recommendations, this time can be significantly reduced to several hours and, in some simple cases, even lead to self-service capabilities.}
}
@article{AUSTIN2025100221,
title = {Ethics for researching language and education: What the discourse of professional guidelines reveals},
journal = {Research Methods in Applied Linguistics},
volume = {4},
number = {2},
pages = {100221},
year = {2025},
issn = {2772-7661},
doi = {https://doi.org/10.1016/j.rmal.2025.100221},
url = {https://www.sciencedirect.com/science/article/pii/S2772766125000424},
author = {Theresa Austin and Rosa Alejandra {Medina Riveros}},
keywords = {DA, Deontic, Education, Ethics, Language, Research ethics, Values},
abstract = {How do the guidelines provided by language and literacy professional organizations configure the ethical stances of their members, who are practitioners and researchers? As part of a larger project involving the lived experiences of teacher educators and researchers who work with multilingual populations, we present an initial critical discourse study of four professional organizations’ ethical codes of conduct. This article focuses on ethics as it materializes through contemporary textual analysis of public guidelines readily accessible to educational linguistics professionals during the 2021–2024 timeframe. It examines how ethical statements operate on the imagination of professionals desiring membership. We employ critical discourse analysis (CDA), where Van Dijk’s (1993) definition of discourse conceptualizes how language use creates power to dominate, resist, and build social hierarchies. We characterize each code as within deontic ethics by interpreting how the choice of modality guides members’ actions and values. During 2021–24 our corpus included documents detailing guidelines from AERA (2011), MLA (1992), LRA (2016), and AAAL (2017). Our findings highlight the characteristics of these ethical codes and their differences. These documents configure an assemblage that affects members through assumed values and obligations. Furthermore, we identify unexamined ethical needs that arise in actual lived realities of researchers that require more fluid, contingent, and responsive ethics; hence, we propose a critically and dialogically engaged stance open to periodic yet continual reformulations.}
}
@article{DENG2025103897,
title = {An automatic selective PDF table-extraction method for collecting materials data from literature},
journal = {Advances in Engineering Software},
volume = {204},
pages = {103897},
year = {2025},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2025.103897},
url = {https://www.sciencedirect.com/science/article/pii/S0965997825000353},
author = {Jianxin Deng and Gang Liu and Rui Tang and Xiusong Wu and Zheng Yin},
keywords = {Materials data, Data extraction, Table extraction, Table detection, Squeeze casting},
abstract = {Table data in scientific literature is an important and economic data source for constructing materials database. The existing PDF table-extraction method is mainly designed for the common table type, which has no difference in various disciplines and does not have the ability to automatically filter the tabular data and extract non-full-framed tables with high precision. In view of this, we propose herein the use of unique coordinates for each object in a PDF and a method of automated table extraction from scientific literature based on text-state characteristics including six stages. In this method, we analyze the special presentation of table content and decode the PDF content stream to detect tables by key words of the table caption, especially use data ontology to filter irrelevant table data, and restore the data structure of tables according to the certainty and uniqueness of character coordinates. The proposed method automatically and accurately extracts table data from scientific literature without relying on table grid lines, thereby overcoming the drawbacks of existing technology for extracting data from three-line tables. The validity and advantages of the proposed method are verified by applying it to squeeze casting literature. Experiments show that the recall rate and precision of the proposed method reach 0.891 and 0.861. The comprehensive performance outperforms the main tools in the market for scientific literature table extraction.}
}
@article{LV2024455,
title = {Design of a sustainable development path in Chenzhou based on a knowledge graph},
journal = {Chinese Journal of Population, Resources and Environment},
volume = {22},
number = {4},
pages = {455-468},
year = {2024},
issn = {2325-4262},
doi = {https://doi.org/10.1016/j.cjpre.2024.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S232542622400072X},
author = {Qiuli Lv and Lijie Gao and Longyu Shi and Houbo Zhou},
keywords = {Ontological modeling, Knowledge graph, Chenzhou, Sustainable urban development, Path design},
abstract = {Sustainable urban development involves many fields with complex data types and rich semantic relationships, such as the economic, societal, and ecological fields. Knowledge graphs provide a new means for sustainable urban development research by leveraging their strengths in the construction of knowledge networks and display of knowledge associations. Focusing on Chenzhou, a resource-based city serving as a China’s Innovation Demonstration Zone for Sustainable Development Agenda, this study adopted a top-down approach, applying the “seven-step” and “skeleton” methods to construct an ontology for sustainable urban development through manual editing. A knowledge graph was constructed for Chenzhou’s sustainable development, comprising 515 nodes, 3 209 relations, and 28 157 attributes. Sustainable measures and pathways were proposed based on this knowledge graph. The results showed that Chenzhou’s future sustainable development should focus on innovation, growth, emissions reduction, centering around high-quality and sustainable development. Promoting the transfer and transformation of scientific and technological achievements, accelerating the optimization and upgrading of industrial structures, and enhancing talent cultivation and recruitment will foster new quality productive forces, providing strong momentum and support for the high-quality and sustainable development of Chenzhou. To accelerate the green economy transition, Chenzhou should improve the market-oriented allocation system for resources and environmental factors, explore the “gross ecosystem product + eco-environment-oriented development” project implementation model, encourage enterprises to adopt environmental, social, and governance principles, and foster synergies between supply and demand. Furthermore, coordinating Chenzhou’s low-carbon city pilot projects and constructing carbon sequestration pathways that leverage nature-based solutions will help implement the “dual carbon” actions and enhance the city’s ability to respond to climate change.}
}
@article{LIU2024,
title = {Research on Traditional Chinese Medicine: Domain Knowledge Graph Completion and Quality Evaluation},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/55090},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424000954},
author = {Chang Liu and Zhan Li and Jianmin Li and Yiqian Qu and Ying Chang and Qing Han and Lingyong Cao and Shuyuan Lin},
keywords = {graph completion, traditional Chinese medicine, graph quality evaluation, graph representation, knowledge graph},
abstract = {Background
Knowledge graphs (KGs) can integrate domain knowledge into a traditional Chinese medicine (TCM) intelligent syndrome differentiation model. However, the quality of current KGs in the TCM domain varies greatly, related to the lack of knowledge graph completion (KGC) and evaluation methods.
Objective
This study aims to investigate KGC and evaluation methods tailored for TCM domain knowledge.
Methods
In the KGC phase, according to the characteristics of TCM domain knowledge, we proposed a 3-step “entity-ontology-path” completion approach. This approach uses path reasoning, ontology rule reasoning, and association rules. In the KGC quality evaluation phase, we proposed a 3-dimensional evaluation framework that encompasses completeness, accuracy, and usability, using quantitative metrics such as complex network analysis, ontology reasoning, and graph representation. Furthermore, we compared the impact of different graph representation models on KG usability.
Results
In the KGC phase, 52, 107, 27, and 479 triples were added by outlier analysis, rule-based reasoning, association rules, and path-based reasoning, respectively. In addition, rule-based reasoning identified 14 contradictory triples. In the KGC quality evaluation phase, in terms of completeness, KG had higher density and lower sparsity after completion, and there were no contradictory rules within the KG. In terms of accuracy, KG after completion was more consistent with prior knowledge. In terms of usability, the mean reciprocal ranking, mean rank, and hit rate of the first N tail entities predicted by the model (Hits@N) of the TransE, RotatE, DistMult, and ComplEx graph representation models all showed improvement after KGC. Among them, the RotatE model achieved the best representation.
Conclusions
The 3-step completion approach can effectively improve the completeness, accuracy, and availability of KGs, and the 3-dimensional evaluation framework can be used for comprehensive KGC evaluation. In the TCM field, the RotatE model performed better at KG representation.}
}
@article{CHOI2024115042,
title = {GPT-based data-driven urban building energy modeling (GPT-UBEM): Concept, methodology, and case studies},
journal = {Energy and Buildings},
volume = {325},
pages = {115042},
year = {2024},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.115042},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824011587},
author = {Sebin Choi and Sungmin Yoon},
keywords = {Urban building energy modeling (UBEM), Urban informatics, Top-down, Large language model (LLM), GPT, Technical exploration},
abstract = {Achieving carbon neutrality is a critical global goal, with urban building energy modeling (UBEM) playing a pivotal role by providing data-driven insights to optimize energy consumption and reduce emissions. This paper introduces GPT-based urban building energy modeling (GPT-UBEM), a novel approach utilizing GPT’s advanced capabilities to address key UBEM challenges using GPT-4o. The study aimed to demonstrate the effectiveness of GPT-UBEM in performing UBEM tasks and to explore its potential in overcoming traditional limitations. Specifically, (1) basic analytics of urban data, (2) data analysis and energy prediction, (3) building feature engineering and optimization, and (4) energy signature analysis were conducted in four case studies. These analyses were applied to 2,000 buildings in Seoul and 31 buildings in Gangwon-do, South Korea. Through case study, the findings highlighted the ability of GPT-UBEM to integrate diverse data sources, optimize building features for high accuracy in prediction models, and provide valuable insights for urban planners and policymakers through the use of expert domain knowledge and intervention. Additionally, based on the results derived from GPT-UBEM in this study, the current limitations of GPT-UBEM (L1 to L3) and future research directions (F1 to F4) have been outlined.}
}
@article{GRECO2023204,
title = {Transformer-based language models for mental health issues: A survey},
journal = {Pattern Recognition Letters},
volume = {167},
pages = {204-211},
year = {2023},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2023.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S0167865523000430},
author = {Candida M. Greco and Andrea Simeri and Andrea Tagarelli and Ester Zumpano},
keywords = {Transformers, Language models, Mental health, NLP, Deep learning, Benchmarks},
abstract = {Early identification and prevention of mental health stresses and their outcomes has become of urgent importance worldwide. To this purpose, artificial intelligence provides a body of advanced computational tools that can effectively support decision-making clinical processes by modeling and analyzing the presence of a variety of mental health issues, particularly when these can be detected in text data. In this regard, Transformer-based language models (TLMs) have demonstrated exceptional efficacy in a number of NLP tasks also in the health domain. To the best of our knowledge, the use of TLMs for specifically addressing mental health issues has not been deeply investigated so far. In this paper, we aim to fill this gap in the literature by providing the first survey of methods using TLMs for text-based identification of mental health issues.}
}
@article{MUSTOE2025125625,
title = {Quality by digital design to accelerate sustainable medicines development},
journal = {International Journal of Pharmaceutics},
volume = {681},
pages = {125625},
year = {2025},
issn = {0378-5173},
doi = {https://doi.org/10.1016/j.ijpharm.2025.125625},
url = {https://www.sciencedirect.com/science/article/pii/S0378517325004624},
author = {Chantal L. Mustoe and Alice J. Turner and Stephanie J. Urwin and Ian Houson and Helen Feilden and Daniel Markl and Mohammed M. {Al Qaraghuli} and Magdalene W.S. Chong and Murray Robertson and Alison Nordon and Blair F. Johnston and Cameron J. Brown and John Robertson and Claire Adjiman and Hannah Batchelor and Brahim Benyahia and Massimo Bresciani and Christopher L. Burcham and Javier Cardona and Ciro Cottini and Andrew S. Dunn and David Fradet and Gavin W. Halbert and Mark Henson and Pirmin Hidber and Marianne Langston and Ye Seol Lee and Wei Li and Jérôme Mantanus and John McGinty and Bhavik Mehta and Tabbasum Naz and Sara Ottoboni and Elke Prasad and Per-Ola Quist and Gavin K. Reynolds and Chris Rielly and Martin Rowland and Walkiria Schlindwein and Sven L.M. Schroeder and Jan Sefcik and Ettore Settanni and Humera Siddique and Kenneth Smith and Rachel Smith and Jagjit Singh Srai and Alpana A. Thorat and Antony Vassileiou and Alastair J. Florence},
abstract = {We present a shared industry-academic perspective on the principles and opportunities for Quality by Digital Design (QbDD) as a framework to accelerate medicines development and enable regulatory innovation for new medicines approvals. This approach exploits emerging capabilities in industrial digital technologies to achieve robust control strategies assuring product quality and patient safety whilst reducing development time/costs, improving research and development efficiency, embedding sustainability into new products and processes, and promoting supply chain resilience. Key QbDD drivers include the opportunity for new scientific understanding and advanced simulation and model-driven, automated experimental approaches. QbDD accelerates the identification and exploration of more robust design spaces. Opportunities to optimise multiple objectives emerge in route selection, manufacturability and sustainability whilst assuring product quality. Challenges to QbDD adoption include siloed data and information sources across development stages, gaps in predictive capabilities, and the current extensive reliance on empirical knowledge and judgement. These challenges can be addressed via QbDD workflows; model-driven experimental design to collect and structure findable, accessible, interoperable and reusable (FAIR) data; and chemistry, manufacturing and control ontologies for shareable and reusable knowledge. Additionally, improved product, process, and performance predictive tools must be developed and exploited to provide a holistic end-to-end development approach.}
}
@article{SZEJKA2024100661,
title = {Knowledge-based expert system to drive an informationally interoperable manufacturing system: An experimental application in the Aerospace Industry},
journal = {Journal of Industrial Information Integration},
volume = {41},
pages = {100661},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100661},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24001055},
author = {Anderson Luis Szejka and Osiris {Canciglieri Junior} and Fernando Mas},
keywords = {Manufacturing systems, Digital transformation, Semantic interoperability, Ontology, Information and knowledge formalization, Semantic rules},
abstract = {The industrial revolutions have challenged organisations to rethink their product design and manufacturing processes, making them faster and more connected to market demands and changes. Digital technologies have emerged with solutions to virtual represent physical objects, processes, systems, or assets to simulate and analyse the impact of manufacturing changes before actual implementation. However, the challenge is to deal with thousands of heterogeneous information sets which must be shared simultaneously by different groups within and across institutional boundaries. Each manufacturing industry has its format and model to represent the product in the development, manufacturing process, material features, etc. In this context, this paper explores a knowledge-based expert system to support the information exchange and inconsistency detection across the manufacturing process, specifically in an experimental application in the Aerospace Industry. The proposed framework was based on knowledge formalisation and semantic rules through ontologies, semantic reconciliation strategies and connectivity interfaces to manage information and knowledge and identify inconsistencies across the manufacturing system. It was mainly evaluated across the product and manufacturing design of sheet metal forming aluminium thin wall parts for the aerospace industry. Results demonstrate the capability of the approach to enhance data accuracy, coherence, and efficiency throughout the manufacturing of complex products. However, the solution presents challenges such as interdisciplinary collaboration in product design, specific information requirements for manufacturing planning, and the impact of production planning on manufacturing capacities.}
}
@article{CHAKHTOUNA2024428,
title = {Modeling Speech Emotion Recognition via ImageBind representations},
journal = {Procedia Computer Science},
volume = {236},
pages = {428-435},
year = {2024},
note = {International Symposium on Green Technologies and Applications (ISGTA’2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.05.050},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924010664},
author = {Adil CHAKHTOUNA and Sara SEKKATE and Abdellah ADIB},
keywords = {ImageBind, Speech Emotion Recognition, Embedding representations, IEMOCAP, Nu-SVM},
abstract = {Speech Emotion Recognition (SER) refers to the ability of Machine Learning (ML) and Deep Learning (DL) techniques to accurately predict people's emotional states from speech signals. significant progress has been achieved in the SER domain involving the incorporation of DL models to introduce novel features extraction processes. This paper introduces the use of deep representations learned from the multi-modal Large Language Model (LLM) called ImageBind. These representations were subsequently provided as input to the Nu-Support Vector Machine (Nu-SVM) with RBF kernel for the classification task. The experiments were executed using the IEMOCAP database within the context of a Speaker-Dependent (SD) scenario. The method achieved a noteworthy overall accuracy rate of 80.58% for the four emotions of IEMOCAP, representing a substantial improvement over well-established methods in the existing body of literature. Thus, affirming that the proposed methodology, founded upon ImageBind representations, introduces a novel perspective to the field of SER.}
}
@article{CHOUCHANI2022111082,
title = {Model-based safety engineering for autonomous train map},
journal = {Journal of Systems and Software},
volume = {183},
pages = {111082},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111082},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221001795},
author = {Nadia Chouchani and Sana Debbech and Matthieu Perin},
keywords = {Model-based safety engineering, Safety ontology, Model-driven engineering, Safety/assurance case, Railway infrastructure model, Autonomous train},
abstract = {As a part of the digital revolution of railway systems, an autonomous driving train will use a complete and precise map of railway infrastructure to conduct operational actions. Nevertheless, the full autonomy of trains depends on the safety decisions management capacity both on-board and track-side. These decisions must be refined into safety requirements in order to continuously check the consistency between the perceived infrastructure and safety related properties. However, traditionally, the integration of safety analysis requires the intervention of human agent skills. This may be error-prone and in interference with the embedded aspect of the train map. In this paper, we propose a model-based approach to match between safety concepts expressed as an ontology, a derived safety model and a safety-extended railway infrastructure map model for autonomous trains. This approach is validated by railway safety case studies for autonomous train map. The integration of this model-based safety solution from the early stages of the map system design improves the safety decisions management process.}
}
@article{COWLEY201846,
title = {Life and language: Is meaning biosemiotic?},
journal = {Language Sciences},
volume = {67},
pages = {46-58},
year = {2018},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2018.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0388000117302358},
author = {Stephen J. Cowley},
keywords = {Biosemiotics, Enactivism, Ecological psychology, Distributed language, Dialogism, Biology of cognition, Biology of meaning, Semiotics, Philosophy of language, Pragmatics, Ecolinguistics},
abstract = {Since the multi-scalarity of life encompasses bodies, language and human experience, Timo Järvilehto's (1998) ‘one-system’ view can be applied to acts of meaning, knowing and ethics. Here, I use Paul Cobley's Cultural Implications of Biosemiotics (2016) to explore a semiotic construal of such a position. Interpretation, he argues, shows symbolic, indexical and iconic ‘layers’ of living. While lauding Cobley's breadth of vision, as a linguist, I baulk at linking ‘knowing’ too closely with the ‘symbolic’ qua what can be said, diagrammed or signed. This is because, given first-order experience (which can be deemed indexical/iconic), humans use observations (by others and self) to self-construct as embodied individuals. While symbolic semiosis matters, I trace it to, not languaging, but the rise of literacy, graphics and pictorial art. Unlike Chomsky and Deely, I find no epigenic break between the symbolic and the iconic/indexical. The difference leads one to ontology. I invite the reader to consider, if, as Cobley suggests, meaning depends on modelling systems (with ententional powers) and/or if, as Gibson prefers, we depend on encounters with whatever is out-there. Whereas Cobley identifies the semiotic with the known, for others, living beings actively apprehend what is observable (for them). Wherever the reader stands, I claim that all one-system views fall in line with Cobley's ‘anti-humanist’ challenge. Ethics, he argues, can only arise from participating in the living. Knowing, and coming to know, use repression and selection that can only be captured by non-disciplinary views of meaning. As part of how life and language unfold, humans owe a duty of care to all of the living world: hence, action is needed now.}
}
@article{LIU2019103318,
title = {Ensembles of natural language processing systems for portable phenotyping solutions},
journal = {Journal of Biomedical Informatics},
volume = {100},
pages = {103318},
year = {2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103318},
url = {https://www.sciencedirect.com/science/article/pii/S1532046419302370},
author = {Cong Liu and Casey N. Ta and James R. Rogers and Ziran Li and Junghwan Lee and Alex M. Butler and Ning Shang and Fabricio Sampaio Peres Kury and Liwei Wang and Feichen Shen and Hongfang Liu and Lyudmila Ena and Carol Friedman and Chunhua Weng},
keywords = {Natural language processing, Human phenotype ontology, Concept recognition, Ensemble method, Evaluation, Reproducibility},
abstract = {Background
Manually curating standardized phenotypic concepts such as Human Phenotype Ontology (HPO) terms from narrative text in electronic health records (EHRs) is time consuming and error prone. Natural language processing (NLP) techniques can facilitate automated phenotype extraction and thus improve the efficiency of curating clinical phenotypes from clinical texts. While individual NLP systems can perform well for a single cohort, an ensemble-based method might shed light on increasing the portability of NLP pipelines across different cohorts.
Methods
We compared four NLP systems, MetaMapLite, MedLEE, ClinPhen and cTAKES, and four ensemble techniques, including intersection, union, majority-voting and machine learning, for extracting generic phenotypic concepts. We addressed two important research questions regarding automated phenotype recognition. First, we evaluated the performance of different approaches in identifying generic phenotypic concepts. Second, we compared the performance of different methods to identify patient-specific phenotypic concepts. To better quantify the effects caused by concept granularity differences on performance, we developed a novel evaluation metric that considered concept hierarchies and frequencies. Each of the approaches was evaluated on a gold standard set of clinical documents annotated by clinical experts. One dataset containing 1,609 concepts derived from 50 clinical notes from two different institutions was used in both evaluations, and an additional dataset of 608 concepts derived from 50 case report abstracts obtained from PubMed was used for evaluation of identifying generic phenotypic concepts only.
Results
For generic phenotypic concept recognition, the top three performers in the NYP/CUIMC dataset are union ensemble (F1, 0.634), training-based ensemble (F1, 0.632), and majority vote-based ensemble (F1, 0.622). In the Mayo dataset, the top three are majority vote-based ensemble (F1, 0.642), cTAKES (F1, 0.615), and MedLEE (F1, 0.559). In the PubMed dataset, the top three are majority vote-based ensemble (F1, 0.719), training-based (F1, 0.696) and MetaMapLite (F1, 0.694). For identifying patient specific phenotypes, the top three performers in the NYP/CUIMC dataset are majority vote-based ensemble (F1, 0.610), MedLEE (F1, 0.609), and training-based ensemble (F1, 0.585). In the Mayo dataset, the top three are majority vote-based ensemble (F1, 0.604), cTAKES (F1, 0.531) and MedLEE (F1, 0.527).
Conclusions
Our study demonstrates that ensembles of natural language processing can improve both generic phenotypic concept recognition and patient specific phenotypic concept identification over individual systems. Among the individual NLP systems, each individual system performed best when they were applied in the dataset that they were primary designed for. However, combining multiple NLP systems to create an ensemble can generally improve the performance. Specifically, the ensemble can increase the results reproducibility across different cohorts and tasks, and thus provide a more portable phenotyping solution compared to individual NLP systems.}
}
@article{BABAIHA2025100138,
title = {Unraveling the co-morbidity between COVID-19 and neurodegenerative diseases through multi-scale graph analysis: A systematic investigation of biological databases and text mining},
journal = {Artificial Intelligence in the Life Sciences},
volume = {8},
pages = {100138},
year = {2025},
issn = {2667-3185},
doi = {https://doi.org/10.1016/j.ailsci.2025.100138},
url = {https://www.sciencedirect.com/science/article/pii/S2667318525000145},
author = {Negin Sadat Babaiha and Stefan Geissler and Vincent Nibart and Heval {Atas Güvenilir} and Vinay Srinivas Bharadhwaj and Alpha {Tom Kodamullil} and Juergen Klein and Marc Jacobs and Martin Hofmann-Apitius},
keywords = {COVID-19, Neurodegenerative diseases, Co-morbidity analysis, Knowledge graphs (KG), Natural language processing (NLP), Biological databases},
abstract = {The COVID-19 pandemic has generated a vast volume of research, yet much of it focuses on individual diseases, overlooking complex comorbidity relationships. While extensive literature exists on both neurodegenerative diseases (NDDs), such as Alzheimer’s and Parkinson’s, and COVID-19, their intersection remains underexplored. Co-morbidity modeling is crucial, particularly for hospitalized patients often presenting with multiple conditions. This study investigates the interplay between COVID-19 and NDDs by integrating knowledge graphs (KGs) built from curated biomedical datasets and text mining tools. We performed comprehensive analyses—including path analysis, phenotype coverage, and mapping of cellular and genetic factors—across multiple KGs, such as PrimeKG, DrugBank, OpenTargets, and those generated via natural language processing (NLP) methods. Our findings reveal notable variability in graph density and connectivity, with each KG offering unique insights into molecular and phenotypic links between COVID-19 and NDDs. Key genetic and inflammatory markers, especially immune response genes, consistently appeared across graphs, suggesting a shared pathogenic basis. By unifying structured biological data with unstructured textual evidence, we enhance co-morbidity modeling and improve recall in identifying mechanisms underlying COVID-19–NDD interactions. This integrative framework supports the development of a co-morbidity hypothesis database aimed at facilitating therapeutic target discovery. All data, methods, and instructions for accessing the co-morbidity hypothesis database are publicly available at: https://github.com/SCAI-BIO/covid-NDD-comorbidity-NLP.}
}
@article{NAGARAJU2025S77,
title = {P03-16 AutoPIF – A platform to optimize cosmetic regulatory documentation: An AI-Enabled Approach},
journal = {Toxicology Letters},
volume = {411},
pages = {S77},
year = {2025},
note = {Abstracts of the 59th Congress of the European Societies of Toxicology (EUROTOX 2025) TOXICOLOGY ADDRESSES SOCIETY'S REAL LIFE RISKS FOR SUSTAINABLE HEALTH AND WELL BEING},
issn = {0378-4274},
doi = {https://doi.org/10.1016/j.toxlet.2025.07.211},
url = {https://www.sciencedirect.com/science/article/pii/S0378427425017941},
author = {V.R. Dasarahalli Nagaraju and A.I. Kapoor and A. Niznik and G.L. Viswanatha and N. Jaideep},
abstract = {A Product Information File (PIF) is a mandatory regulatory document to ensure a thorough safety assessment is completed before a product is launched, thus meeting regulatory expectations. A comprehensive PIF includes but not limited to product composition, safety assessments, manufacturing specifications, regulatory information, amongst others. With increasing regulatory complexity and pressure to launch product launches, there is a growing need to reduce the time to author high-quality PIFs. AutoPIF™, an innovative and advanced tool designed to optimize and automate the development and management of PIFs, addresses this by leveraging artificial intelligence (AI) to streamline, validate, and optimize PIF creation – ensuring faster, accurate, and compliant submissions. AutoPIF™ combines large language models (LLMs), a regulatory rules engine, and smart document structuring to transform raw technical files into submission-ready PIFs. It extracts and standardizes key data points from safety data sheets, formulation specifications, toxicological profiles, and certificates of analysis, amongst others. It uses automation and AI methodologies to process input data, enabling an automated generation of compliant PIFs that meet regional and client-specific requirements. They include: • NLP integration with the CosIng database for regulatory compliance. • Information extraction for calculation of exposure and margin of safety (MoS) • Assistance in the preparation of content in tables and sections • Record and store the artwork-related images • Automatically capture summaries related to toxicological profiles, claims, PMS, packaging, etc. • Automated versioning for audit readiness Every draft authored by AutoPIF™ is reviewed by qualified subject matter experts who validate and sign off the document (human-in-theloop). The feedback from the quality review process continuously trains the system at the machine-authoring stage. AutoPIF™ has demonstrated an 60% improvement in effort reduction and 95% accuracy in structured content generation, while consistently performing precise exposure and MoS calculations. AutoPIF™ combines the best of human expertise and the latest AI approaches to achieve high-precision efficient automation in the preparation of PIF. Through the optimization of data extraction, organization, and human-driven validation processes, AutoPIF™ facilitates faster and more accurate PIF submissions that minimize operational costs and accelerate product launch timelines, thereby accelerating market penetration and enhancing overall time-to-market efficiency.}
}
@article{SUHAG2023101029,
title = {ChatGPT: a pioneering approach to complex prenatal differential diagnosis},
journal = {American Journal of Obstetrics & Gynecology MFM},
volume = {5},
number = {8},
pages = {101029},
year = {2023},
issn = {2589-9333},
doi = {https://doi.org/10.1016/j.ajogmf.2023.101029},
url = {https://www.sciencedirect.com/science/article/pii/S2589933323001714},
author = {Anju Suhag and Jennifer Kidd and Meghan McGath and Raeshmma Rajesh and Joseph Gelfinbein and Nicole Cacace and Berrin Monteleone and Martin R. Chavez},
keywords = {artificial intelligence, ChatGPT-4, complex prenatal disorders, generative pre-trained transformer language model, human phenotype ontology, neonatal genetic disorders, online mendelian inheritance in man, prenatal diagnosis},
abstract = {This commentary examines how ChatGPT can assist healthcare teams in the prenatal diagnosis of rare and complex cases by creating a differential diagnoses based on deidentified clinical findings, while also acknowledging its limitations.}
}
@article{LI2022108548,
title = {A semantic model-based fault detection approach for building energy systems},
journal = {Building and Environment},
volume = {207},
pages = {108548},
year = {2022},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2021.108548},
url = {https://www.sciencedirect.com/science/article/pii/S0360132321009410},
author = {Tingting Li and Yang Zhao and Chaobo Zhang and Kai Zhou and Xuejun Zhang},
keywords = {Semantic model, Ontology, Rule, Fault detection, Building energy system},
abstract = {In this paper, a semantic model-based approach is proposed for building energy systems fault detection. Its basic idea is to mimic the general intelligence of human experts in understanding massive amounts of operational data of various buildings, and further proposing customized fault detection solutions. A domain ontology is developed to allow computers to understand the prior knowledge of building energy systems fault detection. Classes and properties are developed to formalize all possible configurations in this domain. Semantic rules are proposed to detect the operation problems, control problems, equipment malfunction and sensor failure in building energy systems. These rules are written in abstract syntax. They can be reused in various building energy systems. For a target system, the building data are mapped to the ontology to generate a customized knowledge graph. The knowledge graph captures the physics underlying the system operations. The semantic rules are activated based on the knowledge graph to detect the faults. The proposed approach is demonstrated using the historical data from an industrial building located in Wuhan, China. The results show that the approach is powerful in providing the customized fault detection solutions for different situations. It has high levels of interpretability, reliability and automation. The knowledge graph is automatically updated with new data step by step. The semantic rules are activated if the conditions are satisfied based on the knowledge graph. The fault action mechanisms are captured based on the inference chains of the rules. Experts can find the fault reasons and take actions for commissioning.}
}
@article{JAYANTHAKUMARAN2025100584,
title = {A systematic review of sentiment analytics in banking headlines},
journal = {Decision Analytics Journal},
volume = {15},
pages = {100584},
year = {2025},
issn = {2772-6622},
doi = {https://doi.org/10.1016/j.dajour.2025.100584},
url = {https://www.sciencedirect.com/science/article/pii/S2772662225000402},
author = {Muhunthan Jayanthakumaran and Nagesh Shukla and Biswajeet Pradhan and Ghassan Beydoun},
keywords = {Sentiment analysis, Predictive modelling, Text mining, Banking headlines, Market trends},
abstract = {This systematic review investigates sentiment analysis of news headlines in the banking sector, a field susceptible to public sentiment, as demonstrated by phenomena like bank runs leading to rapid deposit withdrawals. We trace the evolution of analytic methods from traditional machine learning to advanced deep learning models, notably Bidirectional Encoder Representations from Transformer (BERT) and Generative Pre-trained Transformer (GPT). Our study highlights their applications including headline generation, sentiment measurement, fake news detection, and analysis of political bias. Despite significant advancements, we uncover research gaps, such as the ineffective use of these methodologies in banking analysis, the underuse of GPT, and a focus on performance rather than practical application. Looking ahead, we note the increasing significance of Large Language Model (LLM), the untapped potential of headline analysis in banking, and the growing interest in this area spurred by rapid technological advancements. Our findings emphasise the pivotal role of sentiment analysis in deciphering market trends and improving decision making in finance, underscoring its strategic importance in the banking industry.}
}
@article{LENTSCHAT2022118332,
title = {A new method to extract n-Ary relation instances from scientific documents},
journal = {Expert Systems with Applications},
volume = {209},
pages = {118332},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118332},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422014567},
author = {Martin Lentschat and Patrice Buche and Juliette Dibie-Barthelemy and Mathieu Roche},
keywords = {N-ary relations, Natural language processing, Knowledge extraction, Information extraction, Ontological and terminological resource, Smart data},
abstract = {A new method to extract knowledge structured as n-Ary relations from scientific articles is presented. We designed and assessed different approaches to reconstruct instances of n-Ary relations extracted from scientific articles in experimental domains, driven by an Ontological and Terminological Resource (OTR) and based on multi-feature representation of relations and their arguments. The proposed method starts with the identification of partial n-Ary relations in tables of scientific articles and then seeks to reconstruct them with argument instances in the article texts. Based on the so-called Scientific Publication Representation (SciPuRe) of textual arguments and Scientific Table Representation (STaRe) of n-Ary relations representation of an n-Ary relation called STaRe (Scientific Table Representation, originating from partial n-Ary relations extracted from document tables), here we propose and evaluate different approaches for the selection of textual argument instances that could complement partial n-Ary relations: structural, frequentist and word embedding models. The application domain concerns food packaging, especially composition and permeability data. Experiments were conducted on a corpus of 332 relation instances composed of 1547 arguments. Corpora of full and partial relations recognized in document tables and argument instances extracted from texts are available online. Different methods and strategies were measured with an f-score ranging from .34 to .74. These results show that n-Ary relations reconstruction approach depends on the number of selected candidate argument instances.}
}
@article{TRAJANOV2023714,
title = {Review of Natural Language Processing in Pharmacology},
journal = {Pharmacological Reviews},
volume = {75},
number = {4},
pages = {714-738},
year = {2023},
issn = {0031-6997},
doi = {https://doi.org/10.1124/pharmrev.122.000715},
url = {https://www.sciencedirect.com/science/article/pii/S0031699724007762},
author = {Dimitar Trajanov and Vangel Trajkovski and Makedonka Dimitrieva and Jovana Dobreva and Milos Jovanovik and Matej Klemen and Aleš Žagar and Marko Robnik-Šikonja},
abstract = {Natural language processing (NLP) is an area of artificial intelligence that applies information technologies to process the human language, understand it to a certain degree, and use it in various applications. This area has rapidly developed in the past few years and now employs modern variants of deep neural networks to extract relevant patterns from large text corpora. The main objective of this work is to survey the recent use of NLP in the field of pharmacology. As our work shows, NLP is a highly relevant information extraction and processing approach for pharmacology. It has been used extensively, from intelligent searches through thousands of medical documents to finding traces of adversarial drug interactions in social media. We split our coverage into five categories to survey modern NLP: methodology, commonly addressed tasks, relevant textual data, knowledge bases, and useful programming libraries. We split each of the five categories into appropriate subcategories, describe their main properties and ideas, and summarize them in a tabular form. The resulting survey presents a comprehensive overview of the area, useful to practitioners and interested observers.
Significance Statement
The main objective of this work is to survey the recent use of NLP in the field of pharmacology in order to provide a comprehensive overview of the current state in the area after the rapid developments that occurred in the past few years. The resulting survey will be useful to practitioners and interested observers in the domain.}
}
@article{CAO2022102281,
title = {KSPMI: A Knowledge-based System for Predictive Maintenance in Industry 4.0},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {74},
pages = {102281},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2021.102281},
url = {https://www.sciencedirect.com/science/article/pii/S0736584521001617},
author = {Qiushi Cao and Cecilia Zanni-Merk and Ahmed Samet and Christoph Reich and François de Bertrand de Beuvron and Arnold Beckmann and Cinzia Giannetti},
keywords = {Industry 4.0, Predictive maintenance, Knowledge-based system, Chronicle mining, Ontology reasoning},
abstract = {In the context of Industry 4.0, smart factories use advanced sensing and data analytic technologies to understand and monitor the manufacturing processes. To enhance production efficiency and reliability, statistical Artificial Intelligence (AI) technologies such as machine learning and data mining are used to detect and predict potential anomalies within manufacturing processes. However, due to the heterogeneous nature of industrial data, sometimes the knowledge extracted from industrial data is presented in a complex structure. This brings the semantic gap issue which stands for the lack of interoperability among different manufacturing systems. Furthermore, as the Cyber-Physical Systems (CPS) are becoming more knowledge-intensive, uniform knowledge representation of physical resources and real-time reasoning capabilities for analytic tasks are needed to automate the decision-making processes for these systems. These requirements highlight the potential of using symbolic AI for predictive maintenance. To automate and facilitate predictive analytics in Industry 4.0, in this paper, we present a novel Knowledge-based System for Predictive Maintenance in Industry 4.0 (KSPMI). KSPMI is developed based on a novel hybrid approach that leverages both statistical and symbolic AI technologies. The hybrid approach involves using statistical AI technologies such as machine learning and chronicle mining (a special type of sequential pattern mining approach) to extract machine degradation models from industrial data. On the other hand, symbolic AI technologies, especially domain ontologies and logic rules, will use the extracted chronicle patterns to query and reason on system input data with rich domain and contextual knowledge. This hybrid approach uses Semantic Web Rule Language (SWRL) rules generated from chronicle patterns together with domain ontologies to perform ontology reasoning, which enables the automatic detection of machinery anomalies and the prediction of future events’ occurrence. KSPMI is evaluated and tested on both real-world and synthetic data sets.}
}
@article{RODLER2022103681,
title = {Memory-limited model-based diagnosis},
journal = {Artificial Intelligence},
volume = {305},
pages = {103681},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103681},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222000212},
author = {Patrick Rodler},
keywords = {Hitting set computation, Diagnosis, Search, Sound complete best-first diagnosis computation, Linear best-first search, Linear best-first hitting set search, Model-based diagnosis, Fault localization, Fault isolation, Recursive best first search, RBFS, Heuristic search, Memory-limited diagnosis search, Reiter's hitting set tree, HS-tree, Sequential diagnosis, Combinatorial search, Ontology debugging, Ontology quality assurance, Knowledge base debugging, Interactive debugging, OntoDebug},
abstract = {Various model-based diagnosis scenarios require the computation of the most preferred fault explanations. Existing algorithms that are sound (i.e., output only actual fault explanations) and complete (i.e., can return all explanations), however, require exponential space to achieve this task. As a remedy, and to enable successful diagnosis both on memory-restricted devices and for memory-intensive problem cases, we propose two novel diagnostic search algorithms which build upon tried and tested techniques from the heuristic search domain. The first method, dubbed Recursive Best-First Hitting Set Search (RBF-HS), is based on Korf's well-known Recursive Best-First Search (RBFS) algorithm. We show that RBF-HS can enumerate an arbitrary predefined finite number of fault explanations in best-first order within linear space bounds, without sacrificing the desirable soundness or completeness properties. The second algorithm, called Hybrid Best-First Hitting Set Search (HBF-HS), is a hybrid between RBF-HS and Reiter's seminal HS-Tree. The idea is to find a trade-off between runtime optimization and a restricted space consumption that does not exceed the available memory. Notably, both suggested algorithms are generally applicable to any model-based diagnosis problem, regardless of the used (monotonic) logical language to describe the diagnosed system and of the used reasoning mechanism. We conducted extensive experiments on real-world benchmarks from the knowledge-based systems field, a domain where the features soundness, completeness, the best-first property as well as a general applicability are pivotal and where Reiter's HS-Tree is the predominantly used diagnostic search. The evaluation reveals that, when computing fault explanations minimal-cardinality-first, RBF-HS compared to HS-Tree reduces memory requirements substantially in most cases by up to several orders of magnitude, while also saving runtime in more than a third of the cases. When computing fault explanations most-probable-first, RBF-HS compared to HS-Tree tends to trade memory savings more or less one-to-one for runtime overheads. Whenever runtime overheads were significant, using HBF-HS instead of RBF-HS reduced the runtime to values comparable with HS-Tree while keeping the used memory reasonably bounded.}
}
@article{ELMORR2021,
title = {A Virtual Community for Disability Advocacy: Development of a Searchable Artificial Intelligence–Supported Platform},
journal = {JMIR Formative Research},
volume = {5},
number = {11},
year = {2021},
issn = {2561-326X},
doi = {https://doi.org/10.2196/33335},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X21000810},
author = {Christo {El Morr} and Pierre Maret and Fabrice Muhlenbach and Dhayananth Dharmalingam and Rediet Tadesse and Alexandra Creighton and Bushra Kundi and Alexis Buettgen and Thumeka Mgwigwi and Serban Dinca-Panaitescu and Enakshi Dua and Rachel Gorman},
keywords = {virtual community, machine learning, Semantic Web, natural language processing, web intelligence, health informatics, Wikibase, disability rights, human rights, CRPD, equity, community, disability, ethics, rights, pilot, platform},
abstract = {Background
The lack of availability of disability data has been identified as a major challenge hindering continuous disability equity monitoring. It is important to develop a platform that enables searching for disability data to expose systemic discrimination and social exclusion, which increase vulnerability to inequitable social conditions.
Objective
Our project aims to create an accessible and multilingual pilot disability website that structures and integrates data about people with disabilities and provides data for national and international disability advocacy communities. The platform will be endowed with a document upload function with hybrid (automated and manual) paragraph tagging, while the querying function will involve an intelligent natural language search in the supported languages.
Methods
We have designed and implemented a virtual community platform using Wikibase, Semantic Web, machine learning, and web programming tools to enable disability communities to upload and search for disability documents. The platform data model is based on an ontology we have designed following the United Nations Convention on the Rights of Persons with Disabilities (CRPD). The virtual community facilitates the uploading and sharing of validated information, and supports disability rights advocacy by enabling dissemination of knowledge.
Results
Using health informatics and artificial intelligence techniques (namely Semantic Web, machine learning, and natural language processing techniques), we were able to develop a pilot virtual community that supports disability rights advocacy by facilitating uploading, sharing, and accessing disability data. The system consists of a website on top of a Wikibase (a Semantic Web–based datastore). The virtual community accepts 4 types of users: information producers, information consumers, validators, and administrators. The virtual community enables the uploading of documents, semiautomatic tagging of their paragraphs with meaningful keywords, and validation of the process before uploading the data to the disability Wikibase. Once uploaded, public users (information consumers) can perform a semantic search using an intelligent and multilingual search engine (QAnswer). Further enhancements of the platform are planned.
Conclusions
The platform ontology is flexible and can accommodate advocacy reports and disability policy and legislation from specific jurisdictions, which can be accessed in relation to the CRPD articles. The platform ontology can be expanded to fit international contexts. The virtual community supports information upload and search. Semiautomatic tagging and intelligent multilingual semantic search using natural language are enabled using artificial intelligence techniques, namely Semantic Web, machine learning, and natural language processing.}
}
@article{BRILHAULT20222348,
title = {Qualitative Analyses of Semantic Interoperability Approaches: Toward Learning based model transformations},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {2348-2353},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.059},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322020687},
author = {Q. Brilhault and E. Yahia and L. Roucoules},
keywords = {Digital continuity, Plug, play interoperability, Semantic interoperability, Ontology, Model-Driven Interoperability},
abstract = {The emergence of cyber-physical systems and connected objects is helping to increase the technical complexity that enables the integration and the collaboration of information systems. This highly heterogeneous digital environment, subject to constant change, requires both the improvement of companies’ capacity for change and the rapid and effective expansion of their digital organization in a flexible and agile manner. In a rapidly changing industrial context, ensuring, and maintaining the digital continuity of information at all levels of the company becomes a major challenge from an economic, technical, and organizational perspective. To meet Industry 4.0 requirements for agility, flexibility, and responsiveness, a "plug- and-play" approach that breaks away from fixed norms and standards seems to be the solution to promote sustainable and on demand interoperability of systems. In this paper, the specifications of the “plug-and-play” are presented. Thus, a qualitative comparison is made between the ontology approach and the Model-Driven-Engineering approach. The goal is to compare their ability to ensure semantic interoperability of systems, and thus digital continuity, based on the criteria of genericity, dynamism, connectivity, integration, and federation capability.}
}
@article{LIU2023104022,
title = {Cognitive digital twins for freight parking management in last mile delivery under smart cities paradigm},
journal = {Computers in Industry},
volume = {153},
pages = {104022},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.104022},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523001720},
author = {Yu Liu and Shenle Pan and Pauline Folz and Fano Ramparany and Sébastien Bolle and Eric Ballot and Thierry Coupaye},
keywords = {Last mile delivery, Freight parking management, Sustainability, Smart cities, Cognitive digital twins, Ontology and semantics},
abstract = {This paper examines the Freight Parking Management Problem (FPMP) of last-mile delivery within the context of Smart Cities where objects are managed by Digital Twins. Specifically, we investigate how Cognitive Digital Twins - Digital Twins with augmented semantic capabilities - can enhance instantly updated knowledge of parking connectivity to optimize logistics operations planning and urban resource allocation. We present a four-layer architectural framework to integrate individual logistics objects and systems into Smart Cities at a semantic level, with underlying enabling technologies and standards including Property Graph, Web Ontology Language (OWL), and Web of Things. Next, we conduct a case study of parcel delivery in Paris using a real-life Digital Twins platform called Thing in the future (Thing’in) by Orange France, coupled with an agent-based simulation model on AnyLogic, to demonstrate a real-world application of our approach. The results suggest that semantics-enabled Digital Twins connectivity can increase the comprehensive understanding of the delivery environment and enhance cooperation between heterogeneous systems, ultimately resulting in improved logistics efficiency, reduced negative externalities, and better utilization of resources. Furthermore, this work showcases potential new business services for logistics service providers and provides managerial insights for city planners and municipal policymakers. An actual mobile application prototype is presented to showcase the applicability of the work.}
}
@article{FONSECA2021101894,
title = {Multi-level conceptual modeling: Theory, language and application},
journal = {Data & Knowledge Engineering},
volume = {134},
pages = {101894},
year = {2021},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101894},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000215},
author = {Claudenir M. Fonseca and João Paulo A. Almeida and Giancarlo Guizzardi and Victorio A. Carvalho},
keywords = {Multi-level modeling, Modeling language, Conceptual modeling, Methodologies and tools},
abstract = {In many important subject domains, there are central real-world phenomena that span across multiple classification levels. In these subject domains, besides having the traditional type-level domain regularities (classes) that classify multiple concrete instances, we also have higher-order type-level regularities (metaclasses) that classify multiple instances that are themselves types. Multi-Level Modeling aims to address this technical challenge. Despite the advances in this area in the last decade, a number of requirements arising from representation needs in subject domains have not yet been addressed in current modeling approaches. In this paper, we address this issue by proposing an expressive multi-level conceptual modeling language (dubbed ML2). We follow a principled language engineering approach in the design of ML2, constructing its abstract syntax as to reflect a fully axiomatized theory for multi-level modeling (termed MLT*). We show that ML2 enables the expression of a number of multi-level modeling scenarios that cannot be currently expressed in the existing multi-level modeling languages. A textual syntax for ML2 is provided with an implementation in Xtext. We discuss how the formal theory influences the language in two aspects: (i) by providing rigorous justification for the language’s syntactic rules, which follow MLT* theorems and (ii) by forming the basis for model simulation and verification. We show that the language can reveal problems in multi-level taxonomic structures, using Wikidata fragments to demonstrate the language’s practical relevance.}
}
@article{FIEDLER202454,
title = {Generative Pre-trained Transformer for Pediatric Stroke Research: A Pilot Study},
journal = {Pediatric Neurology},
volume = {160},
pages = {54-59},
year = {2024},
issn = {0887-8994},
doi = {https://doi.org/10.1016/j.pediatrneurol.2024.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0887899424002522},
author = {Anna K. Fiedler and Kai Zhang and Tia S. Lal and Xiaoqian Jiang and Stuart M. Fraser},
keywords = {Pediatric stroke, IPSS, GPT, PS-GPT, LLM},
abstract = {Background
Pediatric stroke is an important cause of morbidity in children. Although research can be challenging, large amounts of data have been captured through collaborative efforts in the International Pediatric Stroke Study (IPSS). This study explores the use of an advanced artificial intelligence program, the Generative Pre-trained Transformer (GPT), to enter pediatric stroke data into the IPSS.
Methods
The most recent 50 clinical notes of patients with ischemic stroke or cerebral venous sinus thrombosis at the UTHealth Pediatric Stroke Clinic were deidentified. Domain-specific prompts were engineered for an offline artificial intelligence program (GPT) to answer IPSS questions. Responses from GPT were compared with the human rater. Percent agreement was assessed across 50 patients for each of the 114 queries developed from the IPSS database outcome questionnaire.
Results
GPT demonstrated strong performance on several questions but showed variability overall. In its early iterations it was able to match human judgment occasionally with an accuracy score of 1.00 (n = 20, 17.5%), but it scored as low as 0.26 in some patients. Prompts were adjusted in four subsequent iterations to increase accuracy. In its fourth iteration, agreement was 93.6%, with a maximum agreement of 100% and minimum of 62%. Of 2400 individual items assessed, our model entered 2247 (93.6%) correctly and 153 (6.4%) incorrectly.
Conclusions
Although our tailored generative model with domain-specific prompt engineering and ontological guidance shows promise for research applications, further refinement is needed to enhance its accuracy. It cannot enter data entirely independently, but it can be employed in tandem with human oversight contributing to a collaborative approach that reduces overall effort.}
}
@article{DEOLIVEIRA2024123046,
title = {SOAP classifier for free-text clinical notes with domain-specific pre-trained language models},
journal = {Expert Systems with Applications},
volume = {245},
pages = {123046},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.123046},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423035480},
author = {Jezer Machado {de Oliveira} and Rodolfo Stoffel Antunes and Cristiano Andrẽ {da Costa}},
keywords = {Natural language processing, Health informatics, Sentence classification, Electronic health record, SOAP},
abstract = {The increasing use of electronic health records (EHRs) in healthcare has led to a significant amount of unstructured clinical text data. This paper proposes a model for classifying free-text clinical notes in the Portuguese language in sentences following the SOAP (Subjective, Objective, Assessment, and Plan) note standard using domain-specific pre-trained language models. Among the five pre-trained BERT models tested, BioBERTptRT achieved the best results with a precision of 0.9461, accuracy of 0.9434, recall of 0.9437, and F1-score of 0.9435. BioBERTptRT, specialized in the Portuguese language, clinical terminology, and the medical group’s domain, outperformed the other models, showing a 0.28% increase in the F1-score compared to the second most specialized model. The proposed model focuses on high-level sentence classification rather than entity-level classification and aims to structure clinical notes at a broader level. The study utilizes a private database of 10,000 anonymized health records containing 234,673 clinical notes. These notes were divided into 1,183,345 unique sentences used to retrain BioBERTptRT. Additionally, 100,021 sentences were manually labeled for use in fine-tuning the models This work contributes to the structuring of clinical notes by showcasing the performance improvements obtained through domain specialization in BERT networks. Additionally, it presents an analysis of the performance gains achieved by BioBERTpt compared to mBERT, both in our study and other related works. Furthermore, this study provides a valuable comparison between the distribution of sentences and the results obtained in this research and similar studies conducted in English.}
}
@article{BOTH2023113635,
title = {Automated monitoring applications for existing buildings through natural language processing based semantic mapping of operational data and creation of digital twins},
journal = {Energy and Buildings},
volume = {300},
pages = {113635},
year = {2023},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2023.113635},
url = {https://www.sciencedirect.com/science/article/pii/S0378778823008654},
author = {Maximilian Both and Björn Kämper and Alina Cartus and Jo Beermann and Thomas Fessler and Dr. Jochen Müller and Dr. Christian Diedrich},
keywords = {Buildings' operation, Natural language processing for semantic mapping, Classification operational building data, Semantic digital twin, Industry 4.0 Asset Administration Shell, Automatic generation of monitoring applications, Energy saving, Interoperability of technical systems},
abstract = {Buildings' operation constitutes 36% of the German energy consumption. Currently, operators lack the knowledge on energy-saving techniques. There is a shortage of cost-effective and easily-implementable solutions to evaluate building performance. The cause of this problem lies with the semantically heterogeneous operational data used in technical applications. Integrating the data into monitoring applications demands substantial and costly manual efforts. This paper presents a method that enables automated generation of technical monitoring applications for existing buildings. The method outlined represents existing automation stations as digital twins and employs artificial intelligence to map the heterogeneous data to a standard and create semantic digital twins of buildings. The paper introduces a method using natural language processing for the semantic processing of data. The developed method involves a four-stage process for classification of data points, which are subsequently mapped to a uniform vocabulary. To classify the data points, language models were trained on a created dataset of 54,125 data points. Following successful training, the models can process semantically heterogeneous data points. The results, demonstrating an average F1-Score of over 95%, indicate that the developed method is suitable for automating data point mapping. The models were implemented as an Industry 4.0 service and integrated into an application.}
}
@article{WANG2024100111,
title = {Postdigital ethnography in applied linguistics: Beyond the online and offline in language learning},
journal = {Research Methods in Applied Linguistics},
volume = {3},
number = {2},
pages = {100111},
year = {2024},
issn = {2772-7661},
doi = {https://doi.org/10.1016/j.rmal.2024.100111},
url = {https://www.sciencedirect.com/science/article/pii/S277276612400017X},
author = {Chaoran Wang and Suresh Canagarajah},
keywords = {Postdigital, Ethnography, Hybrid spaces, Online, Language learning},
abstract = {We live in a postdigital world where traversing digital and in-person domains for teaching and learning is normalized, raising important methodological and ethical considerations for ethnographic approaches in language education. We define postdigital as a condition where the virtual and physical mediate each other to form hybrid spaces that transcend the online/offline distinction. We begin the article with theoretical discussions on the epistemological orientations that inform postdigital inquiry. Then we situate our language education research, discussing the value of ethnography for understanding hybrid spaces and identifying the challenges and limitations of existing ethnographic methodologies in addressing postdigital conditions. Following this, we illustrate ways to follow the chosen unit of analysis in postidigital ethnography through three case studies, specifically focusing on pedagogical activities. We also reflect on pedagogical insights from postdigital ethnography, sensitive ethical concerns involved in postdigital research, as well as considerations for addressing those ethical issues. We conclude the paper with practical suggestions for researchers on conducting postdigital ethnographies.}
}
@article{FU20247,
title = {A Knowledge-graph based Method for Datum Reference Frame Reasoning},
journal = {Procedia CIRP},
volume = {129},
pages = {7-12},
year = {2024},
note = {18th CIRP Conference on Computer Aided Tolerancing (CAT2024)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124011399},
author = {Hongsheng Fu and Chao Kong and Yanlong Cao and Yuanping Xu and Jin Jin and Tukun Li and Junding Luo and Zongzheng Zhang},
keywords = {DRF, knowledge reasoning, ontology, non-standard part, evaluation},
abstract = {In mechanical engineering, the datum is essential in the tolerance specification, which impacts assembly and functional requirements. The standard mechanical parts, such as shaft and gear, already have a formalized datum reference frame (DRF) design process. However, non-standard parts still mainly rely on manual design. To this end, a knowledge-graph-based DRF reasoning method for non-standard parts is proposed. In this paper, an OWL 2-based knowledge model is constructed according to industrial practice. It combines empirical knowledge, knowledge graphs, and ontology reasoning methods to perform DRF generation automatically and precisely. The finite element method is introduced into this work to evaluate reasoned DRF. The effectiveness and feasibility of the method are demonstrated via a case study.}
}
@article{MOURATIDIS2023103139,
title = {Modelling language for cyber security incident handling for critical infrastructures},
journal = {Computers & Security},
volume = {128},
pages = {103139},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103139},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823000494},
author = {Haralambos Mouratidis and Shareeful Islam and Antonio Santos-Olmo and Luis E. Sanchez and Umar Mukhtar Ismail},
keywords = {Critical infrastructure, Meta-model, Incident response, Cyber incident, Cyber course of action, Cyber threat intelligence, Security requirements},
abstract = {Cyber security incident handling is a consistent methodology with which to ensure overall business continuity. However, specifically handling incidents for critical information infrastructures is challenging owing to the inherent complexity and evolving nature of the threat. Despite the number of contributions made to cyber incident handling, there is little evidence of literature that focuses on modelling activities that will enhance developers’ abilities to model incident handling processes and activities according to different views. Modelling languages of this nature should integrate essential concepts and a descriptive implementation process in order to enable developers to analyse, represent and reason about the crucial incident handling efforts required to support critical information infrastructures. The aim of this paper is, as part of the CyberSANE EU project, to develop a Cyber Incident Handling Modelling Language (CIHML) that focuses explicitly on modelling incident handling in the context of a critical information infrastructure. The work is innovative in its approach because it consolidates concepts from various domains such as security requirements, forensics, threat intelligence, critical infrastructures and cyber incident handling. The approach will allow the phases of the incident handling lifecycle to be modelled from three different views (critical information infrastructures, threat and risk analysis, and incident response). An implementation process is also proposed, which will serve as a comprehensive guide for developers in order to create these modelling views. Finally, CIHML is evaluated using a real-life scenario from the CyberSANE project to demonstrate its applicability. The incident observed had a severe impact on the overall business continuity of the context studied. The results obtained from the study show that CIHML can help critical information infrastructure operators to identify, evaluate, represent and model cyber incidents in critical information systems, in addition to providing the support required to determine the response strategies needed in order to mitigate these cyber-attacks.}
}
@article{ROBLEDANOARILLO201967,
title = {Application of Linked Open Data to the coding and dissemination of Spanish Civil War photographic archives},
journal = {Journal of Documentation},
volume = {76},
number = {1},
pages = {67-95},
year = {2019},
issn = {0022-0418},
doi = {https://doi.org/10.1108/JD-06-2019-0112},
url = {https://www.sciencedirect.com/science/article/pii/S0022041819000572},
author = {Jesús Robledano-Arillo and Diego Navarro-Bonilla and Julio Cerdá-Díaz},
keywords = {Linked Open Data, Semantic Web, Ontologies, Digital humanities, OWL, Image retrieval, Photographic archives, Press photography, Spanish Civil War},
abstract = {Purpose
The purpose of this paper is to present a conceptual model for coding and dissemination of data associated with historical photographic archives. The model is based on Linked Open Data technology and seeks to exhaustively represent the most relevant characteristics for the tasks of contextualization of the documentary groupings and units, management, document retrieval, dissemination and sharing of data about the historical photographs.
Design/methodology/approach
An OWL ontology, called Ontophoto, was constructed following an adaptation of the methodology proposed by Uschold and Gruninger and Gruninger and Fox. The ontology was implemented using Protégé 5.5 software. Next a Graph DB® graph database application (Ontotext) was created to generate a query system based on the SPARQL language. To validate the consistency and effectiveness of the model and ontology, a competency questions methodology has been applied using a sample from the Skogler photographic archive.
Findings
The model facilitates the generation of systems for dissemination and retrieval of iconographic data for historical research, overcoming some of the limitations with respect to the design of methods of content and contextual information representation for heritage photographic archives.
Research limitations/implications
This study is based on a sample. Future work should consider the implementation of the model on the totality of a photographic collection.
Originality/value
This paper presents a comprehensive ontological model that allows the creation of distributed systems of knowledge representation, which can be queried through SPARQL language.}
}
@article{KOH20243454,
title = {Confronting the data deluge: How artificial intelligence can be used in the study of plant stress},
journal = {Computational and Structural Biotechnology Journal},
volume = {23},
pages = {3454-3466},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024002988},
author = {Eugene Koh and Rohan Shawn Sunil and Hilbert Yuen In Lam and Marek Mutwil},
keywords = {Plant stress resilience, Large-scale data, Artificial intelligence, Large language models},
abstract = {The advent of the genomics era enabled the generation of high-throughput data and computational methods that serve as powerful hypothesis-generating tools to understand the genomic and gene functional basis of plant stress resilience. The proliferation of experimental and analytical methods used in biology has resulted in a situation where plentiful data exists, but the volume and heterogeneity of this data has made analysis a significant challenge. Current advanced deep-learning models have displayed an unprecedented level of comprehension and problem-solving ability, and have been used to predict gene structure, function and expression based on DNA or protein sequence, and prominently also their use in high-throughput phenomics in agriculture. However, the application of deep-learning models to understand gene regulatory and signalling behaviour is still in its infancy. We discuss in this review the availability of data resources and bioinformatic tools, and several applications of these advanced ML/AI models in the context of plant stress response, and demonstrate the use of a publicly available LLM (ChatGPT) to derive a knowledge graph of various experimental and computational methods used in the study of plant stress. We hope this will stimulate further interest in collaboration between computer scientists, computational biologists and plant scientists to distil the deluge of genomic, transcriptomic, proteomic, metabolomic and phenomic data into meaningful knowledge that can be used for the benefit of humanity.}
}
@article{DO2022109934,
title = {Semantic-enhanced neural collaborative filtering models in recommender systems},
journal = {Knowledge-Based Systems},
volume = {257},
pages = {109934},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109934},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122010279},
author = {Pham Minh Thu Do and Thi Thanh Sang Nguyen},
keywords = {Recommender Systems, Ontology, Deep learning, Semantic, User behavior prediction, Collaborative Filtering},
abstract = {Recommendation systems or recommender systems (RSs) are very popular in entertainment websites. With the combination of neural networks and collaborative filtering, Neural Collaborative Filtering (NCF) recommendation methods have shown their outperformance in making item suggestions. However, the lack of semantic relationships between objects makes the NCF unable to capture the complex user–item interactions. Moreover, traditional NCF is unable to capture the dynamic user preference over time. To address these issues, in this paper, we propose novel semantic-enhanced NCF models which are applied to movie rating prediction and movie recommendation. Therefore, MovieLens and IMDB datasets are taken into account as case studies. The proposed models are the integration of ontology-like modeling and deep learning for recommendation tasks into two parts:(1) building the semantic knowledge base for movies and (2) building the user behavior analytic model that has semantic knowledge inference on the knowledge base combined with the sequential preference learned from user sessions, input into the NCF module for making predictions or recommendations. Several experiments have been conducted to show their better recommendation performance than the traditional NCF model.}
}
@article{BOUFRIDA20221150,
title = {Rule extraction from scientific texts: Evaluation in the specialty of gynecology},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {4},
pages = {1150-1160},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1319157820303736},
author = {Amina Boufrida and Zizette Boufaida},
keywords = {Text mining, Natural language processing, Knowledge extraction, Rule acquisition, Ontology Web Language (OWL) ontology, Semantic Web Rule Language (SWRL) rules},
abstract = {Due to the considerable increase in freely available data (especially on the Web), extracting relevant information from textual content is a critical challenge. Most of the available data is embedded in unstructured texts and is not linked to formalized knowledge structures such as ontologies or rules. A potential solution to this problem is to acquire such knowledge through natural language processing (NLP) tools and text mining techniques. Prior work has focused on the automatic extraction of ontologies from texts, but the acquired knowledge is generally limited to simple hierarchies of terms. This paper presents a polyvalent framework for acquiring complex relationships from texts and coding these in the form of rules. Our approach begins with existing domain knowledge represented as an OWL ontology, and applies NLP tools and text matching techniques to deduce different atoms, such as classes, properties and literals, to capture deductive knowledge in the form of new rules. For the reason, to enrich the existing domain ontology by these rules, in order to obtain higher relational expressiveness, make reasoning and produce new facts. The approach was tested using medical reports, specifically, in the specialty of gynecology. It reports an F-measure of 95.83% on test our corpus.}
}
@article{SAMANIPOUR2025103185,
title = {MDAPW3: MDA-based development of blockchain-enabled decentralized applications},
journal = {Science of Computer Programming},
volume = {239},
pages = {103185},
year = {2025},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2024.103185},
url = {https://www.sciencedirect.com/science/article/pii/S0167642324001084},
author = {Ali Samanipour and Omid Bushehrian and Gregorio Robles},
keywords = {Software quality, Model driven architecture, Web3.0 decentralized application, Blockchain technology},
abstract = {Web3.0 Decentralized Application (DApp) is a class of decentralized software in which at least the business logic of the software is implemented using blockchain-based smart contracts. Features such as transparency, decentralized execution environment, no need for a central authority, immutability of data from manipulation, as well as a native transaction-based payment system based on cryptographic tokens are the main advantages of Web3.0 DApps over conventional Web2.0 software in which the business logic and user data are centrally controlled by companies with no transparency. However, the development lifecycle of Web3.0 DApps involves many challenges due to the complexity of blockchain technology and smart contracts as well as the difficulties concerning with the integration of DApp on-chain and off-chain components. To alleviate these challenges, a Model Driven Architecture (MDA) approach for the development of Web3.0 DApps is proposed in this paper that streamlines the development of complex multi-lateral DApps and results in a product that is verifiable, traceable, low-cost, maintainable, less error-prone and in conformance with blockchain platform concepts. Opposed to previous studies in this area that applied MDA only for the development of smart contracts, our proposed MDA-based approach covers the full architecture of Web3.0 DApps: on-chain, off-chain and on-chain/off-chain communication patterns. The method application was demonstrated by implementing a land leasing Dapp where the requirement model (a BPMN choreography model) was transformed into CIM, PIM, and PSM instances successively, and finally, the code-base was generated based on the Ethereum platform technology stack. Epsilon Validation Language (EVL), Epsilon Object Language (EOL), and Epsilon Comparison Language (ECL) were used for the verification/validation of the model instances at each step. Furthermore, by evaluating the quality metrics of the proposed meta-models, we show that they have a better ontology coverage and are more reusable and understandable compared to previous meta-models.}
}
@article{OGOREK2025,
title = {AI-Powered Drug Classification and Indication Mapping for Pharmacoepidemiologic Studies: Prompt Development and Validation},
journal = {JMIR AI},
volume = {4},
year = {2025},
issn = {2817-1705},
doi = {https://doi.org/10.2196/65481},
url = {https://www.sciencedirect.com/science/article/pii/S2817170525000432},
author = {Benjamin Ogorek and Thomas Rhoads and Eric Finkelman and Isaac R Rodriguez-Chavez},
keywords = {generative language model, artificial intelligence, AI, large language models, LLMs, natural language processing, NLP, drug classification, Anatomical Therapeutic Chemical, ATC, spencer device, smart hub},
abstract = {Background
Pharmacoepidemiologic studies, which promote rational drug use and improve health outcomes, often require Anatomical Therapeutic Chemical Classification System (ATC) drug classification within real-world data (RWD) sources. Existing classification tools are expensive, brittle, or have restrictive terms of service, and lack context that may inform classification itself.
Objective
This study sought to establish large language models (LLMs) as an assisting technology in the drug classification task. This included developing artificial intelligence prompts that reason about drugs using RWD and showing that the resulting accuracy, efficiency, and effectiveness are favorable to alternative methods.
Methods
A prompt was constructed to classify aspirin as either an analgesic or antithrombotic and evaluated within 12,294 anonymized daily dose strings from a polychronic population residing in the United States and Canada. The patients used a smart medication dispenser called “spencer” and consented to the use of their data for research. The LLM prompt requested that the best and next-best second-level ATC code be returned, and grading was performed on a 3-point scale. After success in a pilot sample of 20, an inference sample of 200 was taken without replacement. Finite population inference was carried out on the proportion of outputs receiving 1 of the top 2 grades. As a benchmark, Google’s Programmable Search Engine was used to query the drug name plus “ATC code” followed by regex-based extraction of ATC codes. All imperfect results were reviewed.
Results
The population consisted of 12,294 daily dose strings from 86.26% (2908/3371) patients residing in Canada and 13.73% (463/3371) residing in the United States. A prompt using the chain-of-thought reasoning was able to distinguish between aspirin’s analgesic versus antithrombotic therapeutic uses and performed well in the pilot sample. In the inferential sample, 87.5% (175/200) were graded as perfect, 5% (10/200) had a minor issue, and 7.5% (15/200) had a major issue. The estimate of the proportion of at least mostly correct classification was 92.5% (185/200, 80% CI 90.1%-94.9%). For the search-based algorithm, 82.5% (165/200) were deemed acceptable. The chain-of-thought reasoning was most helpful with supplements (eg, folic acid) when high doses indicated antianemic preparations. The problem formulation of daily dose inputs and multiple ATC outputs was sometimes incompatible with the drug (eg, pregabalin, calcitriol, and methotrexate).
Conclusions
GPT-4o offers cost-effective drug classification from RWD without violating any terms of service. Using a chain-of-thought prompting technique, GPT-4o can reason about drug dosages that affect the class. The wide accessibility of LLMs gives every research team the ability to classify drugs at scale, a key prerequisite of pharmacoepidemiologic research.}
}
@article{SILVA2024122902,
title = {Integrating social media data: Venues, groups and activities},
journal = {Expert Systems with Applications},
volume = {243},
pages = {122902},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122902},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423034048},
author = {Thiago H. Silva and Mark S. Fox},
keywords = {Data integration, Social media, Urban data, Ontology},
abstract = {Social media has been fuelling necessary research in different areas, including the large-scale study of urban societies. Most research is done with a single source of information. Integrating data from multiple sources provides several benefits; for instance, we can have more information about the venues or groups in the city. However, the integration of different sources of social media is a complex task. A critical task in the interoperability between different social media platforms is to provide an integration link. We focus on location-based social network platforms and present solutions to integration based on physical venues, groups of users interacting with them, and activities performed in those venues. Besides, we also propose an ontology (Social Media Integration Ontology — SMIO) that provides a target data model into which data from multiple sources can be mapped with more precise, shared semantics. Our proposed approaches and ontology can help to enhance the variety of data that describes a venue or group and foster research into urban societies.}
}
@article{LI2023118598,
title = {A graph-based method for interactive mapping revision in DL-Lite},
journal = {Expert Systems with Applications},
volume = {211},
pages = {118598},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118598},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422016529},
author = {Weizhuo Li and Qiu Ji and Songmao Zhang and Xuefeng Fu and Guilin Qi},
keywords = {Semantic Web, Ontology mappings, Interactive revision, Graph representation},
abstract = {Discovering the semantic relationships among heterogeneous ontologies has been one of the core research topics in Semantic Web. As ontology matching systems inevitably adopt heuristic strategies, wrong mappings are often contained in final alignments. Most methods for mapping revision depend on dealing with logical incoherence. However, erroneous mappings that do not cause incoherence may be left out. Hence, manual validations with domain expertise are needed. Nevertheless, existing interactive methods for mapping revision still suffer from two limitations. Firstly, revision methods designed to present friendly graphical interfaces and contextual information related to mappings pay little attention to reducing manual decisions, leading to the efficiency problem. Secondly, revision methods focusing on automated reasoning to reduce manual decisions employ models of high complexity, leading to the practicality problem when dealing with alignments across large-scale ontologies. To address these problems, we propose a novel graph-based method for interactive mapping revision, aiming to reduce the manual efforts as much as possible. DL-Lite ontologies and their mappings are encoded into an integrated graph, where the mapping arcs will be judged by the experts. We define the decision space tailored for mapping revision, which can be used to improve the efficiency of manual making decisions. After a manual decision is made in each interaction, the mapping arcs will be automatically updated in the integrated graph. The whole update process modeled in our defined graph-based decision space could be accomplished in polynomial time. We further design an impact function based on the integrated graph and weights of mappings, which can display the most influential mappings to experts. In this way, the number of manual decisions can be reduced further. To cope with the practicality of our method for alignments of large-scale ontologies, we introduce the notion of “reliable” mappings as an attempt to alleviate the burden of experts for making decisions, and propose two soft principles to ensure the reliability of selected mappings. Moreover, we define influence relation and design a corresponding algorithm to enhance the method for detecting incoherence of the integrated graph, which is transformed from the ontologies beyond DL-Lite and their mappings. We implement our method and evaluate its efficiency by 16 alignments generated across real-world ontologies. Experimental results show that our proposed method can improve the efficiency by 19% on average and save more manual decisions than other interactive revision methods in most cases.}
}
@article{EVANGELISTI2023105416,
title = {A non-clinical and clinical IUCLID database for 530 pharmaceuticals (part I): Methodological aspects of its development},
journal = {Regulatory Toxicology and Pharmacology},
volume = {142},
pages = {105416},
year = {2023},
issn = {0273-2300},
doi = {https://doi.org/10.1016/j.yrtph.2023.105416},
url = {https://www.sciencedirect.com/science/article/pii/S0273230023000843},
author = {Martina Evangelisti and Marco Daniele Parenti and Greta Varchi and Jorge Franco and Jochen {vom Brocke} and Panagiotis G. Karamertzanis and Alberto {Del Rio} and Ingo Bichlmaier},
keywords = {IUCLID database, Animal testing, Human information, Standard product label, Ontology, Effect levels, Endocrine disruption, Repeat-dose toxicity, Carcinogenicity, Reproductive toxicity, Developmental toxicity, Animal-human correlation, New approach methodologies, NAMs},
abstract = {A new IUCLID database is provided containing results from non-clinical animal studies and human information for 530 approved drugs. The database was developed by extracting data from pharmacological reviews of repeat-dose, carcinogenicity, developmental, and reproductive toxicity studies. In the database, observed and no-observed effects are linked to the respective effect levels, including information on severity/incidence and transiency/reversibility. It also includes some information on effects in humans, that were extracted from relevant sections of standard product labels of the approved drugs. The database is complemented with a specific ontology for reporting effects that was developed as an improved version of the Ontology Lookup Service's mammalian and human phenotype ontologies and includes different hierarchical levels. The developed ontology contains novel and unique standardized terms, including ontological terms for reproductive and endocrine effects. The database aims to facilitate correlation and concordance analyses based on the link between observed and no-observed effects and their respective effect levels. In addition, it offers a robust dataset on drug information for the pharmaceutical industry and research. The reported ontology supports the analyses of toxicological information, especially for reproductive and endocrine endpoints and can be used to encode legacy data or develop additional ontologies. The new database and ontology can be used to support the development of alternative non-animal approaches, to elucidate mechanisms of toxicity, and to analyse human relevance. The new IUCLID database is provided free of charge at https://iuclid6.echa.europa.eu/us-fda-toxicity-data.}
}
@article{MARKAJ20232989,
title = {Towards retrieving functionalities from intentions and services in modular automation},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {2989-2994},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.1424},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323018323},
author = {Artan Markaj and Alexander Fay and Katharina Stark and Mario Hoernicke and Nicolai Schoch},
keywords = {intentions, intention-based engineering, model-driven engineering, modular automation, process industry, semantic technologies, service-oriented architectures},
abstract = {The automation of plants in the process industry, especially of modular process plants, is shifting towards a more decentralized and service-based paradigm. Required and offered services need to be described in such a way that they can be either easily described by plant operators and module vendors. This two-sided problem requires a semantic description of needed functionalities from intentions and abstraction of existing services onto offered functionalities. In this contribution, the authors present two algorithms for the modeling and derivation of functionalities from intentions and services in modular automation. The algorithms are based on the idea of functional reasoning which support the explanation and derivation of functions in a system. By using various ontologies, the approach builds upon semantic descriptions and exploits query- and rule-based languages to support the retrieval of functionalities.}
}
@article{BELETE2024100169,
title = {Contextual word disambiguates of Ge'ez language with homophonic using machine learning},
journal = {Ampersand},
volume = {12},
pages = {100169},
year = {2024},
issn = {2215-0390},
doi = {https://doi.org/10.1016/j.amper.2024.100169},
url = {https://www.sciencedirect.com/science/article/pii/S2215039024000079},
author = {Mequanent Degu Belete and Ayodeji Olalekan Salau and Girma Kassa Alitasb and Tigist Bezabh},
keywords = {Ge'ez language, WSD, Text vectorization, Machine learning},
abstract = {According to natural language processing experts, there are numerous ambiguous words in languages. Without automated word meaning disambiguation for any language, the development of natural language processing technologies such as information extraction, information retrieval, machine translation, and others are still challenging task. Therfore, this paper presents the development of a word sense disambiguation model for duplicate alphabet words for the Ge'ez language using corpus-based methods. Because there is no wordNet or public dataset for the Ge'ez language, 1010 samples of ambiguous words were gathered. Afterwards, the words were preprocessed and the text was vectorized using bag of words, Term Frequency-Inverse Document Frequency, and word embeddings such as word2vec and fastText. The vectorized texts are then analysed using the supervised machine learning algorithms such Naive Bayes, decision trees, random forests, K-nearest neighbor, linear support vector machine, and logistic regression. Bag of words paired with random forests outperformed all other combinations, with an accuracy of 99.52%. However, when Deep learning algorithms such as Deep neural network and Long Short-Term memory were used for the same dataset, a 100% accuracy was achieved.}
}
@article{DWI2025100136,
title = {Ethical and Psychological Implications of Generative AI in Digital Afterlife Technologies: A Systematic Literature Review on Responsible Inclusive Innovation},
journal = {Journal of Responsible Technology},
pages = {100136},
year = {2025},
issn = {2666-6596},
doi = {https://doi.org/10.1016/j.jrt.2025.100136},
url = {https://www.sciencedirect.com/science/article/pii/S2666659625000320},
author = {Mariyono Dwi},
keywords = {Generative AI, digital afterlife, DeathTech, cultural schemas, inclusive design, ethical governance, grief management, ritual adaptation},
abstract = {Rapid advances in generative artificial intelligence (GenAI) have given birth to digital afterlife technologies (DeathTech), which enable the preservation of the voices, memories, and personalities of deceased individuals. This study is a systematic review of 45 scientific articles (2020–2025) using a thematic-SWOT analysis approach and the Responsible Inclusive Innovation (RII) framework, to explore how cultural schemas, inclusive design, and governance models influence the acceptance of DeathTech across cultures. Key findings suggest that ritual adaptation and spiritual meanings are critical to the acceptance of this technology. Jewish and Japanese communities show high acceptance through cultural integration, while Hindu and Luhya communities experience ontological dissonance. Design failures such as linguistic exclusion and ritual incongruence impact marginalized groups. In addition, regulatory gaps exist, especially in post-death privacy protection and algorithmic bias. This study proposes a triadic framework for the development of ethical and equitable DeathTech: cultural mediation, inclusive design, and pluralistic governance. This contribution enriches the study of digital thanatology and provides recommendations for culturally and socially sustainable innovation.}
}
@article{WONG2022115,
title = {Cognitive engine for augmented human decision-making in manufacturing process control},
journal = {Journal of Manufacturing Systems},
volume = {65},
pages = {115-129},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522001480},
author = {Pooi-Mun Wong and Chee-Kong Chui},
keywords = {Cognitive engine, Dynamic ontology, Augmented decision-making, Process control, Smart manufacturing, Cyber-physical system},
abstract = {A pre-planned production workflow may need to adapt to instantaneous states of resources, materials, and workpieces. This adaptation poses an additional challenge when multiple custom workpieces are enmeshed in complex processes. This paper presents a novel framework, Cognitive Engine Process Controller (CEPC), which realizes a human-centric manufacturing system by augmenting the human in workflow selection and providing the flexibility in using alternative methods in the workflow. The CEPC streamlines overwhelming information for the human to select the next manufacturing steps via dynamic web ontology language (OWL) ontologies, a conflict checker, and a learner. The CEPC can monitor the completion of each step, determine the effect of a failed step, and suggest a set of suitable steps. Simulation results showed the potential of the CEPC via demonstrations. The CEPC has also been compared with an existing cognitive engine framework for workflow planning.}
}
@article{WANG2025122330,
title = {Semantic enrichment of decision rules: A framework for improving formal decision contexts},
journal = {Information Sciences},
volume = {717},
pages = {122330},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2025.122330},
url = {https://www.sciencedirect.com/science/article/pii/S0020025525004621},
author = {Yanling Wang and Liwei Sha and Li Zou and Hengfei Li and Luis Martínez and Christopher Nugent and Jun Liu},
keywords = {Formal concept analysis, Decision implication, Description logics, Ontology, Knowledge representation and reasoning},
abstract = {A Formal Decision Context (FDC) is a knowledge representation for decision-making, in which decision implications capture the dependencies between conditions and conclusions, primarily based on non-semantic information. Lack of semantic information can lead to rule redundancy and a lack of corresponding background knowledge in FDC. This paper addresses the limitations of FDC with regard to the lack of semantic information by introducing a semantic formal decision context (SFDC) that integrates domain ontologies into description logic. Hence, we construct an SFDC with semantic knowledge that enriches the decision-making process. We propose a semantic decision implication framework based on SFDC that leverages the reasoning capabilities of description logics to enhance the comprehensiveness and reduce redundancy in decision implication. To more effectively demonstrate that our method significantly reduces information redundancy while preserving the completeness of semantic decision implications, we have validated the algorithm for generating the simplest semantic decision implication set across six publicly available datasets. The experimental results demonstrate that our algorithm achieves remarkable performance in improving the reduction efficiency of decision implications.}
}
@article{BAUM2024105646,
title = {HERALD: A domain-specific query language for longitudinal health data analytics},
journal = {International Journal of Medical Informatics},
volume = {192},
pages = {105646},
year = {2024},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2024.105646},
url = {https://www.sciencedirect.com/science/article/pii/S1386505624003095},
author = {Lena Baum and Marco Johns and Armin Müller and Hammam {Abu Attieh} and Fabian Prasser},
keywords = {Secondary data analysis, Data aggregation, Data interpretation, Data analysis, Data visualization},
abstract = {Background
Large-scale health data has significant potential for research and innovation, especially with longitudinal data offering insights into prevention, disease progression, and treatment effects. Yet, analyzing this data type is complex, as data points are repeatedly documented along the timeline. As a consequence, extracting cross-sectional tabular data suitable for statistical analysis and machine learning can be challenging for medical researchers and data scientists alike, with existing tools lacking balance between ease-of-use and comprehensiveness.
Objective
This paper introduces HERALD, a novel domain-specific query language designed to support the transformation of longitudinal health data into cross-sectional tables. We describe the basic concepts, the query syntax, a graphical user interface for constructing and executing HERALD queries, as well as an integration into Informatics for Integrating Biology and the Bedside (i2b2).
Methods
The syntax of HERALD mimics natural language and supports different query types for selection, aggregation, analysis of relationships, and searching for data points based on filter expressions and temporal constraints. Using a hierarchical concept model, queries are executed individually for the data of each patient, while constructing tabular output. HERALD is closed, meaning that queries process data points and generate data points. Queries can refer to data points that have been produced by previous queries, providing a simple, but powerful nesting mechanism.
Results
The open-source implementation consists of a HERALD query parser, an execution engine, as well as a web-based user interface for query construction and statistical analysis. The implementation can be deployed as a standalone component and integrated into self-service data analytics environments like i2b2 as a plugin. HERALD can be valuable tool for data scientists and machine learning experts, as it simplifies the process of transforming longitudinal health data into tables and data matrices.
Conclusion
The construction of cross-sectional tables from longitudinal data can be supported through dedicated query languages that strike a reasonable balance between language complexity and transformation capabilities.}
}
@article{YANG2024107924,
title = {RDmaster: A novel phenotype-oriented dialogue system supporting differential diagnosis of rare disease},
journal = {Computers in Biology and Medicine},
volume = {169},
pages = {107924},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.107924},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524000088},
author = {Jian Yang and Liqi Shu and Mingyu Han and Jiarong Pan and Lihua Chen and Tianming Yuan and Linhua Tan and Qiang Shu and Huilong Duan and Haomin Li},
keywords = {Rare disease, Human phenotype ontology, Differential diagnosis, Phenomic and genomic diagnostics, Electronic differential diagnostic support system},
abstract = {Background
Clinicians often lack the necessary expertise to differentially diagnose multiple underlying rare diseases (RDs) due to their complex and overlapping clinical features, leading to misdiagnoses and delayed treatments. The aim of this study is to develop a novel electronic differential diagnostic support system for RDs.
Method
Through integrating two Bayesian diagnostic methods, a candidate list was generated with enhance clinical interpretability for the further Q&A based differential diagnosis (DDX). To achieve an efficient Q&A dialogue strategy, we introduce a novel metric named the adaptive information gain and Gini index (AIGGI) to evaluate the expected gain of interrogated phenotypes within real-time diagnostic states.
Results
This DDX tool called RDmaster has been implemented as a web-based platform (http://rdmaster.nbscn.org/). A diagnostic trial involving 238 published RD patients revealed that RDmaster outperformed existing RD diagnostic tools, as well as ChatGPT, and was shown to enhance the diagnostic accuracy through its Q&A system.
Conclusions
The RDmaster offers an effective multi-omics differential diagnostic technique and outperforms existing tools and popular large language models, particularly enhancing differential diagnosis in collecting diagnostically beneficial phenotypes.}
}
@article{PEREZPEREZ2024,
title = {Tracking the Spread of Pollen on Social Media Using Pollen-Related Messages From Twitter: Retrospective Analysis},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/58309},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124006848},
author = {Martín Pérez-Pérez and María {Fernandez Gonzalez} and Francisco Javier Rodriguez-Rajo and Florentino Fdez-Riverola},
keywords = {pollen, respiratory allergies, Twitter, large language model, LLM, knowledge reconstruction, text mining},
abstract = {Background
Allergy disorders caused by biological particles, such as the proteins in some airborne pollen grains, are currently considered one of the most common chronic diseases, and European Academy of Allergy and Clinical Immunology forecasts indicate that within 15 years 50% of Europeans will have some kind of allergy as a consequence of urbanization, industrialization, pollution, and climate change.
Objective
The aim of this study was to monitor and analyze the dissemination of information about pollen symptoms from December 2006 to January 2022. By conducting a comprehensive evaluation of public comments and trends on Twitter, the research sought to provide valuable insights into the impact of pollen on sensitive individuals, ultimately enhancing our understanding of how pollen-related information spreads and its implications for public health awareness.
Methods
Using a blend of large language models, dimensionality reduction, unsupervised clustering, and term frequency–inverse document frequency, alongside visual representations such as word clouds and semantic interaction graphs, our study analyzed Twitter data to uncover insights on respiratory allergies. This concise methodology enabled the extraction of significant themes and patterns, offering a deep dive into public knowledge and discussions surrounding respiratory allergies on Twitter.
Results
The months between March and August had the highest volume of messages. The percentage of patient tweets appeared to increase notably during the later years, and there was also a potential increase in the prevalence of symptoms, mainly in the morning hours, indicating a potential rise in pollen allergies and related discussions on social media. While pollen allergy is a global issue, specific sociocultural, political, and economic contexts mean that patients experience symptomatology at a localized level, needing appropriate localized responses.
Conclusions
The interpretation of tweet information represents a valuable tool to take preventive measures to mitigate the impact of pollen allergy on sensitive patients to achieve equity in living conditions and enhance access to health information and services.}
}
@article{DAO2021103852,
title = {Semantic framework for interdependent infrastructure resilience decision support},
journal = {Automation in Construction},
volume = {130},
pages = {103852},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103852},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521003034},
author = {Jicao Dao and S. Thomas Ng and Yifan Yang and Shenghua Zhou and Frank J. Xu and Martin Skitmore},
keywords = {Semantic Web, Ontology, Infrastructure systems, Interdependency, Data integration, Resilient decisions},
abstract = {The increasing need for interdependent infrastructure systems to withstand natural disasters has called for the co-creation of resilience decisions to minimize the impact on society. However, issues related to information integration across different infrastructure systems hamper decision making from a system-to-systems perspective. To resolve this problem, the Semantic Web technologies are presented in this paper to serve four functions: (i) linking cross domains through ontology development to represent different domain knowledge; (ii) integrating multiple-source heterogeneous data by a common data format; (iii) retrieving useful information using semantic query language; and (iv) deriving machine automatic logical reasoning by rule languages and logic engines to provide informed resilience decision making support. The proposed framework is tested by a case scenario involving intertwined drainage-transport-building systems under the influence of urban flooding. The result indicates that the framework effectively facilitates information integration between diverse infrastructure systems and helps decision-makers by providing resilience decision-making support.}
}
@article{WU2024122401,
title = {NLP-based approach for automated safety requirements information retrieval from project documents},
journal = {Expert Systems with Applications},
volume = {239},
pages = {122401},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122401},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423029032},
author = {Zhijiang Wu and Guofeng Ma},
keywords = {Project documents, Requirement retrieval, Document association, Natural language processing},
abstract = {Numerous project documents generated during the construction phase pertain to the actual safety requirements (SRs) specified by project managers (PMs), and these concealed requirements can aid in making safety decisions if accurately identified and utilized. However, PMs’ requirements are frequently recorded informally in project documents, necessitating extensive manual analysis to guide safety management practices. To address this limitation, this study developed a natural language processing (NLP)-based framework of requirement retrieval and document association (RRDA) to mine requirements and retrieve requirement-related documents. In particular, requirement-document (RD) association rules are designed to retrieve the requirement-related documents. The results demonstrate that our framework can retrieve the PMs’ requirements with a maximum ontology relevance of 91.37% and emotional preference of requirement with a maximum semantic tendency intensity of 0.91. The presented algorithm shows satisfactory performance in the number of iterations and threshold, and there are outstanding advantages in model training comparison. Additionally, there is a significant degree of matching between the retrieved documents and the requirements, which has significant managerial implications for requirement-oriented construction safety management.}
}
@article{WAN2024102706,
title = {Knowledge graph-based representation and recommendation for surrogate modeling method},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102706},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102706},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624003549},
author = {Silai Wan and Guoxin Wang and Zhenjun Ming and Yan yan and Anand Balu Nellippallil and Janet K. Allen and Farrokh Mistree},
keywords = {Surrogate model, Surrogate modeling method, Knowledge graph, Complex system design},
abstract = {Surrogate models have been widely used in engineering design for approximating a simulation system with high computational cost. Complex system design typically is a multi-stage and multi-discipline design problem, which requires a large number of surrogate models. The choice of surrogate modeling method (SMM) is critical as it directly impacts the performance of both the surrogate models and the designed systems. With the growing variety of SMMs, designers face challenges in selecting the appropriate methods for their specific applications. To address this, we propose a representation and recommendation framework for surrogate modeling methods based on knowledge graph. Firstly, we develop an ontology to formally represent core concepts involved in the recommendation for surrogate modeling methods, including surrogate modeling method, surrogate model, and data sets,etc. Secondly, we extract 460 samples from 46 benchmark functions using Latin hypercube sampling to construct a knowledge graph with 8,343 nodes and 16,100 relationships, which involves 7,820 surrogate models generated from 17 surrogate modeling methods. Finally, we propose a knowledge graph-based recommendation method for surrogate modeling method named KGRSMM to facilitate the selection of an appropriate surrogate modeling method. We test the efficacy of KGRSMM using examples of theoretical problems and engineering problems of hot rod rolling respectively. It is shown in the results that KGRSMM is capable of recommending surrogates with appropriate accuracy, robustness, and time to satisfy designers’ preferences.}
}
@article{HOSSEINIGOURABPASI2024109022,
title = {BIM-based automated fault detection and diagnostics of HVAC systems in commercial buildings},
journal = {Journal of Building Engineering},
volume = {87},
pages = {109022},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.109022},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224005904},
author = {Arash {Hosseini Gourabpasi} and Mazdak Nik-Bakht},
keywords = {Digital twin, BIM, HVAC, AFDD, Ontology},
abstract = {In order to meet the growing demand for effective Automated Fault Detection and Diagnostics (AFDD) for HVAC systems, innovative approaches are needed to address limitations in data diversity and access to contextual information. This study introduces a methodology that leverages Building Information Modeling (BIM) to enhance the development of the AFDD model. Feature engineering techniques are utilized to generate dynamic BIM features, compensating for the lack of sensory and contextual data in Building Management Systems (BMS). By integrating AFDD analytics with BIM, a comprehensive digital twin of the facility is created, which enables facility managers to compare, reuse, and develop AFDD models for HVAC systems. The proposed methodology demonstrates the potential of leveraging BIM-based knowledge models to overcome the challenges associated with the limited sensor and contextual information availability by utilizing BIM for feature generation and, conversely, updating the BIM model with AFDD analytics.}
}
@article{FASCIANO2025109767,
title = {A propagation-based ranking semantics in Explainable Bipolar Weighted Argumentation},
journal = {Engineering Applications of Artificial Intelligence},
volume = {142},
pages = {109767},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109767},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624019262},
author = {Corrado Fasciano and Giuseppe Loseto and Agnese Pinto and Michele Ruta and Floriano Scioscia},
keywords = {Bipolar weighted argumentation, Semantic matchmaking, Ranking semantics, Web ontology language, Multi-agent systems},
abstract = {The Semantic Web of Things enables exchanging knowledge-based fragments in pervasive computing through machine-to-machine interactions, sustaining collaborative decision-making in networks of smart objects. Abstract argumentation is widely acknowledged, instead, as a general framework for decision-making in multi-agent systems, able to solve negotiation conflicts and evaluate acceptable options for a given scenario. Nevertheless, abstract argumentation frameworks disregard the problem of characterizing the argument structure and their mutual relations. This paper proposes a novel approach integrating Dung-style abstract argumentation with semantic matchmaking in pervasive computing. Object interactions are seen as an argumentative dialogue, where annotations shared by each agent play the role of arguments. A matchmaking scheme, exploiting non-standard non-monotonic inferences, allows the appraisal of argument relations in a bipolar weighted argumentation approach. A propagation-based gradual semantics provides acceptability ranking of arguments with a formal explanation. A fading mechanism is also exploited to take into account limited computational resources. As the proposed approach enables applications in cyber-physical multi-agent systems engineering, early validation and performance experiments are provided by means of a case study on a real-time strategy game.}
}
@article{GUO2024108709,
title = {A survey on advancements in image–text multimodal models: From general techniques to biomedical implementations},
journal = {Computers in Biology and Medicine},
volume = {178},
pages = {108709},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108709},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524007947},
author = {Ruifeng Guo and Jingxuan Wei and Linzhuang Sun and Bihui Yu and Guiyong Chang and Dawei Liu and Sibo Zhang and Zhengbing Yao and Mingjun Xu and Liping Bu},
keywords = {Image–text multimodal models, Artificial intelligence, Technological evolution, Biomedical applications, Challenges and strategies},
abstract = {With the significant advancements of Large Language Models (LLMs) in the field of Natural Language Processing (NLP), the development of image–text multimodal models has garnered widespread attention. Current surveys on image–text multimodal models mainly focus on representative models or application domains, but lack a review on how general technical models influence the development of domain-specific models, which is crucial for domain researchers. Based on this, this paper first reviews the technological evolution of image–text multimodal models, from early explorations of feature space to visual language encoding structures, and then to the latest large model architectures. Next, from the perspective of technological evolution, we explain how the development of general image–text multimodal technologies promotes the progress of multimodal technologies in the biomedical field, as well as the importance and complexity of specific datasets in the biomedical domain. Then, centered on the tasks of image–text multimodal models, we analyze their common components and challenges. After that, we summarize the architecture, components, and data of general image–text multimodal models, and introduce the applications and improvements of image–text multimodal models in the biomedical field. Finally, we categorize the challenges faced in the development and application of general models into external factors and intrinsic factors, further refining them into 2 external factors and 5 intrinsic factors, and propose targeted solutions, providing guidance for future research directions. For more details and data, please visit our GitHub page: https://github.com/i2vec/A-survey-on-image-text-multimodal-models.}
}
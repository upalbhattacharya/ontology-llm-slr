@article{BILIDAS2021100639,
title = {Handling redundant processing in OBDA query execution over relational sources},
journal = {Journal of Web Semantics},
volume = {68},
pages = {100639},
year = {2021},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100639},
url = {https://www.sciencedirect.com/science/article/pii/S1570826821000147},
author = {Dimitris Bilidas and Manolis Koubarakis},
keywords = {Query translation, Data integration, Ontology-based data access, Ontop},
abstract = {Redundant processing is a key problem in the translation of initial queries posed over an ontology into SQL queries, through mappings, as it is performed by ontology-based data access systems. Examples of such processing are duplicate answers obtained during query evaluation, which must finally be discarded, or common expressions evaluated multiple times from different parts of the same complex query. Many optimizations that aim to minimize this problem have been proposed and implemented, mostly based on semantic query optimization techniques, by exploiting ontological axioms and constraints defined in the database schema. However, data operations that introduce redundant processing are still generated in many practical settings, and this is a factor that impacts query execution. In this work we propose a cost-based method for query translation, which starts from an initial result and uses information about redundant processing in order to come up with an equivalent, more efficient translation. The method operates in a number of steps, by relying on certain heuristics indicating that we obtain a more efficient query in each step. Through experimental evaluation using the Ontop system for ontology-based data access, we exhibit the benefits of our method.}
}
@article{MASMOUDI2021720,
title = {Knowledge hypergraph-based approach for data integration and querying: Application to Earth Observation},
journal = {Future Generation Computer Systems},
volume = {115},
pages = {720-740},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.09.029},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20311961},
author = {Maroua Masmoudi and Sana Ben Abdallah Ben Lamine and Hajer Baazaoui Zghal and Bernard Archimede and Mohamed Hedi Karray},
keywords = {Semantic data integration, Knowledge graph, Ontology, Query processing, Knowledge hypergraph, Earth observation},
abstract = {According to the world economic forum report, around 70% of generated data are not used. This limitation of usage is mainly due to the lack of interoperability and linking of data that resides in isolated silos. Indeed, the overwhelming amount of data has worsened heterogeneity problems, as have the types of sources generating data in heterogeneous formats and different semantics. Those data related problematics are frequent in the domain of Earth Observation (EO). Earth observed data use different terminologies, which are difficult to reconcile because they reflect overlapped disciplines. These issues lead to misunderstandings and inefficient exchange and management of data in terms of access, pricing, and data rights, which can hamper environmental phenomena understanding. Virtual Knowledge Graph (VKG), allows semantic integration of existing data sources into a wide Knowledge Graph. In this work we propose a knowledge hypergraph-based approach for data integration and querying, with an application to Earth Observation data. Our proposal takes place in two phases (1) a knowledge hypergraph-based virtual data integration and (2) a hypergraph-based query processing. The first phase allows to generate a virtual knowledge hypergraph consisting of RML mappings between an ontology and the data. The second phase consists of enhancing the user’s query by extracting and consolidating a global view of data from different sources based on the generated knowledge hypergraph. The proposed approach is implemented in the Onto-KIT tool (Ontology-based Knowledge hypergraph data Integration and querying Tool) and evaluated through real use case studies. The obtained results show that our proposal enhances query processing in terms of accuracy, completeness, and semantic richness of response.}
}
@article{GAO2024101848,
title = {Interpretable multi-modal artificial intelligence model for predicting gastric cancer response to neoadjuvant chemotherapy},
journal = {Cell Reports Medicine},
volume = {5},
number = {12},
pages = {101848},
year = {2024},
issn = {2666-3791},
doi = {https://doi.org/10.1016/j.xcrm.2024.101848},
url = {https://www.sciencedirect.com/science/article/pii/S2666379124006190},
author = {Peng Gao and Qiong Xiao and Hui Tan and Jiangdian Song and Yu Fu and Jingao Xu and Junhua Zhao and Yuan Miao and Xiaoyan Li and Yi Jing and Yingying Feng and Zitong Wang and Yingjie Zhang and Enbo Yao and Tongjia Xu and Jipeng Mei and Hanyu Chen and Xue Jiang and Yuchong Yang and Zhengyang Wang and Xianchun Gao and Minwen Zheng and Liying Zhang and Min Jiang and Yuying Long and Lijie He and Jinghua Sun and Yanhong Deng and Bin Wang and Yan Zhao and Yi Ba and Guan Wang and Yong Zhang and Ting Deng and Dinggang Shen and Zhenning Wang},
keywords = {gastric cancer, artificial intelligence, neoadjuvant chemotherapy, computed tomography, whole-slide image},
abstract = {Summary
Neoadjuvant chemotherapy assessment is imperative for prognostication and clinical management of locally advanced gastric cancer. We propose an incremental supervised contrastive learning model (iSCLM), an interpretable artificial intelligence framework integrating pretreatment CT scans and H&E-stained biopsy images, for improved decision-making regarding neoadjuvant chemotherapy. We have constructed and tested iSCLM using retrospective data from 2,387 patients across 10 medical centers and evaluated its discriminative ability in a prospective cohort (132 patients; ChiCTR2300068917). iSCLM achieves areas under receiver operating characteristic curves of 0.846–0.876 across different test cohorts. Computed tomography (CT) and pathological attention heatmaps from Shapley additive explanations and global sort pooling illustrate additional benefits for capturing morphological features through supervised contrastive learning. Specifically, pathological top-ranked tiles exhibit decreased distances to tumor-invasive borders and increased inflammatory cell infiltration in responders compared with non-responders. Moreover, CD11c expression is elevated in responders. The developed interpretable model at the molecular pathology level accurately predicts chemotherapy efficacy.}
}
@article{ZHANG2025113117,
title = {A review of building digital twins: Framework and enabling technologies},
journal = {Journal of Building Engineering},
volume = {111},
pages = {113117},
year = {2025},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2025.113117},
url = {https://www.sciencedirect.com/science/article/pii/S2352710225013543},
author = {Xiyuan Zhang and Jiehao Zheng and Peixian Li and Yue Yang and Yu Ye and Francesco Causone and Xing Shi},
keywords = {Digital twin, Building digital twins, Digital twin modeling, Digital twin framework, Model construction technologies},
abstract = {Digital twins, having gained prominence in industrial sectors, are emerging in construction for enhancing intelligent management through real-time monitoring, performance simulation, and data integration. Despite their potential, systematic analysis of frameworks and enabling technologies remains lacking. This review systematically investigates over 150 studies to: (1) analyze the general framework and its extensions for building digital twins, and (2) evaluate enabling technologies and tools based on the modeling procedure. We propose a general structure for building digital twins and analyze frameworks centered on four data types derived from different sources. Our analysis indicates that different types of frameworks exhibit distinct characteristics and inherent limitations, yet a standardized framework for integrating heterogeneous data is still lacking. Through systematic analysis of enabling technologies across four key aspects of the modeling procedure, we investigate building digital twin cases regarding their modeling procedures, technologies employed, and common issues. We identify four key challenges: (1) limited prediction data integration and data analysis, restricting frameworks to monitoring rather than decision-supporting; (2) multi-source data heterogeneity and poor tool interoperability; (3) complex and non-standardized data integration procedures; and (4) low automation in model development. Future research should focus on: (1) standardizing data formats and interoperability tools, (2) developing unified platforms for multi-source data integration, and (3) integrating predictive analytics to enhance decision-making. This study establishes connections between frameworks and enabling technologies, identifies existing problems, and provides actionable recommendations to accelerate the adoption of building digital twins.}
}
@article{VANWEERDENBURG2019143,
title = {Where to go and what to do: Extracting leisure activity potentials from Web data on urban space},
journal = {Computers, Environment and Urban Systems},
volume = {73},
pages = {143-156},
year = {2019},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2018.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0198971518302138},
author = {Demi {van Weerdenburg} and Simon Scheider and Benjamin Adams and Bas Spierings and Egbert {van der Zee}},
keywords = {Place affordance, Urban space, Knowledge extraction, City planning, Latent semantics, Multi-label classification},
abstract = {Web data is the most prominent source of information for deciding where to go and what to do. Exploiting this source for geographic analysis, however, does not come without difficulties. First, in recent years, the amount and diversity of available Web information about urban space have exploded, and it is therefore increasingly difficult to overview and exploit. Second, the bulk of information is in an unstructured form which is difficult to process and interpret by computers. Third, semi-structured sources, such as Web rankings, geolocated tags, check-ins, or mobile sensor data, do not fully reflect the more subtle qualities of a place, including the particular functions that make it attractive. In this article, we explore a method to capture leisure activity potentials from Web data on urban space using semantic topic models. We test three supervised multi-label machine learning strategies exploiting geolocated webtexts and place tags to estimate whether a given type of leisure activity is afforded or not. We train and validate these models on a manually curated dataset labeled with leisure ontology classes for the city of Zwolle, and discuss their potential for urban leisure and tourism research and related city policies and planning. We found that multi-label affordance estimation is not straightforward but can be made to work using both official webtexts and user-generated content on a medium semantic level. This opens up new opportunities for data-driven approaches to urban leisure and tourism studies.}
}
@article{WEN2024780,
title = {Fine-grained decomposition of complex digital twin systems driven by semantic-topological-dynamic associations},
journal = {Journal of Manufacturing Systems},
volume = {77},
pages = {780-797},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.10.023},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524002462},
author = {Xiaojian Wen and Yicheng Sun and Shimin Liu and Jinsong Bao and Dan Zhang},
keywords = {Digital Twin, Fine-grained Decomposition, Semantic Association, Topological Association, Dynamic Association},
abstract = {Complex digital twin (DT) systems offer a robust solution for design, optimization, and operational management in industrial domains. However, in an effort to faithfully replicate the dynamic changes of the physical world with high fidelity, the excessively intricate and highly coupled system components present modeling challenges, making it difficult to accurately capture the system's dynamic characteristics and internal correlations. Particularly in scenarios involving multi-scale and multi-physics coupling, complex systems lack adequate fine-grained decomposition (FGD) methods. This results in cumbersome information exchange and consistency maintenance between models of different granularities. To address these limitations, this paper proposes a method for multi-level decomposition of complex twin models. This method constructs a FGD model for DTs by integrating three key correlation mechanisms between components: semantic association, dynamic association, and topological association. The decomposed model achieves reasonable simplification and abstraction while maintaining the accuracy of the complex system, thereby balancing computational efficiency and simulation precision. The case study validation employed a marine diesel engine piston production line to test the proposed decomposition method, verifying the effectiveness of the approach.}
}
@article{ERKOYUNCU2020145,
title = {A design framework for adaptive digital twins},
journal = {CIRP Annals},
volume = {69},
number = {1},
pages = {145-148},
year = {2020},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2020.04.086},
url = {https://www.sciencedirect.com/science/article/pii/S0007850620301086},
author = {John Ahmet Erkoyuncu and Iñigo Fernández {del Amo} and Dedy Ariansyah and Dominik Bulka and Rok Vrabič and Rajkumar Roy},
keywords = {Digital twins, Design method, Ontology},
abstract = {Digital Twin (DT) is a ‘living’ entity that offers potential with monitoring and improving functionality of interconnected complex engineering systems (CESs). However, lack of approaches for adaptively connecting the existing brownfield systems and their data limits the use of DTs. This paper develops a new DT design framework that uses ontologies to enable co-evolution with the CES by capturing data in terms of variety, velocity, and volume across the asset life-cycle. The framework has been tested successfully on a helicopter gearbox demonstrator and a mobile robotic system across their life cycles, illustrating DT adaptiveness without the data architecture needing to be modified.}
}
@article{TAKAHASHI20232222,
title = {Catalysts informatics: paradigm shift towards data-driven catalyst design},
journal = {Chemical Communications},
volume = {59},
number = {16},
pages = {2222-2238},
year = {2023},
issn = {1359-7345},
doi = {https://doi.org/10.1039/d2cc05938j},
url = {https://www.sciencedirect.com/science/article/pii/S1359734523010819},
author = {Keisuke Takahashi and Junya Ohyama and Shun Nishimura and Jun Fujima and Lauren Takahashi and Takeaki Uno and Toshiaki Taniike},
abstract = {ABSTRACT
Designing catalysts is a challenging matter as catalysts are involved with various factors that impact synthesis, catalysts, reactor and reaction. In order to overcome these difficulties, catalysts informatics is proposed as an alternative way to design and understand catalysts. The underlying concept of catalysts informatics is to design the catalysts from trends and patterns found in catalysts data. Here, three key concepts are introduced: experimental catalysts database, knowledge extraction from catalyst data via data science, and a catalysts informatics platform. Methane oxidation is chosen as a prototype reaction for demonstrating various aspects of catalysts informatics. This work summarizes how catalysts informatics plays a role in catalyst design. The work covers big data generation via high throughput experiments, machine learning, catalysts network method, catalyst design from small data, catalysts informatics platform, and the future of catalysts informatics via ontology. Thus, the proposed catalysts informatics would help innovate how catalysts can be designed and understood.}
}
@article{CAIN2025103535,
title = {Caring for technologies, caring for Country},
journal = {Futures},
volume = {166},
pages = {103535},
year = {2025},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2024.103535},
url = {https://www.sciencedirect.com/science/article/pii/S0016328724002180},
author = {Anna Cain},
keywords = {Feminist care ethics, Caring for Country, Indigenous Australians, Energy social science, Sustainability transitions, Maintenance, Energy futures},
abstract = {Care is an emerging theoretical tool supporting analysis of socioecological equity impacts as energy systems are transformed to support more sustainable futures. Derived from feminist critiques of rationalist, market-led approaches, energy scholars use care to draw attention to the matters of care that are counted into energy system transitions and the care labour required to realise these transitions. Less attention has been applied to non-Western concepts of care and how they might provide alternative futures through energy. This paper draws on Tronto’s (2013) phases of care framework to investigate how care shapes, flows through and is enabled by renewable energy programs in remote Indigenous communities in Australia. Using data collected through multi-sited project ethnography, this analysis considers how care is defined and built into energy program design and implementation. Interrogating these care logics illustrates the importance of prioritising sociocultural alongside technical forms of care. Understanding energy in this way offers insights into the role of energy in underpinning Indigenous futures, by supporting Indigenous ontological imperatives to exist in and care for Country, as well as insights into what it means to care at scale with energy through sustainability transitions.}
}
@article{KURABAYASHI2023108379,
title = {Neocortical neuronal production and maturation defects in the TcMAC21 mouse model of Down syndrome},
journal = {iScience},
volume = {26},
number = {12},
pages = {108379},
year = {2023},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2023.108379},
url = {https://www.sciencedirect.com/science/article/pii/S2589004223024562},
author = {Nobuhiro Kurabayashi and Kazuki Fujii and Yuta Otobe and Shingo Hiroki and Masaharu Hiratsuka and Hikari Yoshitane and Yasuhiro Kazuki and Keizo Takao},
keywords = {Neuroscience, Cell biology, Proteomics},
abstract = {Summary
Down syndrome (DS) results from trisomy of human chromosome 21 (HSA21), and DS research has been conducted by the use of mouse models. We previously generated a humanized mouse model of DS, TcMAC21, which carries the long arm of HSA21. These mice exhibit learning and memory deficits, and may reproduce neurodevelopmental alterations observed in humans with DS. Here, we performed histologic studies of the TcMAC21 forebrain from embryonic to adult stages. The TcMAC21 neocortex showed reduced proliferation of neural progenitors and delayed neurogenesis. These abnormalities were associated with a smaller number of projection neurons and interneurons. Further, (phospho-)proteomic analysis of adult TcMAC21 cortex revealed alterations in the phosphorylation levels of a series of synaptic proteins. The TcMAC21 mouse model shows similar brain development abnormalities as DS, and will be a valuable model to investigate prenatal and postnatal causes of intellectual disability in humans with DS.}
}
@article{GOYAL201821,
title = {Recent Named Entity Recognition and Classification techniques: A systematic review},
journal = {Computer Science Review},
volume = {29},
pages = {21-43},
year = {2018},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2018.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1574013717302782},
author = {Archana Goyal and Vishal Gupta and Manish Kumar},
abstract = {Textual information is becoming available in abundance on the web, arising the requirement of techniques and tools to extract the meaningful information. One of such an important information extraction task is Named Entity Recognition and Classification. It is the problem of finding the members of various predetermined classes, such as person, organization, location, date/time, quantities, numbers etc. The concept of named entity extraction was first proposed in Sixth Message Understanding Conference in 1996. Since then, a number of techniques have been developed by many researchers for extracting diversity of entities from different languages and genres of text. Still, there is a growing interest among research community to develop more new approaches to extract diverse named entities which are helpful in various natural language applications. Here we present a survey of developments and progresses made in Named Entity Recognition and Classification research.}
}
@article{XU2023102124,
title = {Intelligent assembly modeling of complex product based on cognition of interaction structures},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102124},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102124},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623002525},
author = {Zhi-Jia Xu and Shan-Cong Mo and Wen-Bin Tang},
keywords = {Cognition behavior, Interaction structures, Interaction intent, Intelligent assembly modeling, Complex product},
abstract = {Interactions of a part with other parts in an assembly constitute the behavior of the part. However, currently parts interact with each other lifelessly in assembly environment due to their incapability of cognizing interaction structures between them, making the assembly modeling of complex products suffer from considerable manual interventions. An intelligent assembly modeling method is proposed for complex products in this paper, by endowing parts with the ability of cognizing interaction structures between them. In this method, the concept of interaction feature pair (IFP) and the corresponding automatic construction algorithms are developed to explicitly describe interaction intent (i.e., how a part will mate with other parts) in designer’s mind, providing carriers for part cognition behaviors and dynamical interaction intent in the assembly modeling process. On this basis, an object-oriented product information model considering IFP is established to facilitate the complete capture of interaction intent at part modeling stage and the consistent transmission of it to assembly modeling stage, endowing parts with the cognition ability of foreseeing how to mate with other parts before and after loading into assembly modeling environment. Then, IFP matching algorithms are designed to support the part cognition behaviors of actively cognizing interaction structures, perceiving potential mating parts and mating with other parts with high efficiency. Assembly modeling case studies of a reducer and a Mecanum wheel with few manual interventions indicate the feasibility of the proposed approach.}
}
@article{ACHICH20201141,
title = {Approach to Reasoning about Uncertain Temporal Data in OWL 2},
journal = {Procedia Computer Science},
volume = {176},
pages = {1141-1150},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.110},
url = {https://www.sciencedirect.com/science/article/pii/S187705092032010X},
author = {Nassira Achich and Fatma Ghorbel and Fayçal Hamdi and Elisabeth Métais and Faiez Gargouri},
keywords = {Certain Temporal Data, Uncertain Temporal Data, Temporal Representation, Temporal Reasoning, 4D-Fluents Approach, Allen’s Interval Algebra, Certain Ontology},
abstract = {In this paper, we propose an ontology-based approach for representing and reasoning about certain and uncertain temporal data. It handles temporal data in terms of quantitative time intervals and points and the qualitative relations between them (e.g., “before”). It includes three parts. (1) We extend the 4D-fluents approach with certain ontological components to represent the handled temporal data in OWL 2. (2) We extend the Allen’s interval algebra to reason about certain and uncertain time intervals. We adapt these relations to allow relating a time interval and a time point, and two time points. All relations can be used for temporal reasoning by means of transitivity tables. (3) The extended Allen’s algebra instantiates the 4D-fluents-based representation. Inferences are based on SWRL rules. Based on this ontology, a prototype is implemented and integrated into an ontology-based memory prosthesis for Alzheimer’s patients to handle certain and uncertain temporal data inputs.}
}
@article{LU2023110752,
title = {SympGAN: A systematic knowledge integration system for symptom–gene associations network},
journal = {Knowledge-Based Systems},
volume = {276},
pages = {110752},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110752},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123005026},
author = {Kezhi Lu and Kuo Yang and Hailong Sun and Qian Zhang and Qiguang Zheng and Kuan Xu and Jianxin Chen and Xuezhong Zhou},
keywords = {Deep learning, Knowledge graph, Relationship inference, Health care},
abstract = {Phenotypes (i.e., symptoms and clinical signs) are essential for clinical diagnosis and research related to symptom science and precision health. As clinical observational manifestations of a disease, symptoms are clinically significant because they act as direct causes for patients to seek medical care and the primary indicators for clinicians to provide diagnosis/treatments. However, a comprehensive phenotypic knowledge base and high-quality symptom–gene associations are lacking. Therefore, a thorough understanding of the relationships between symptoms and other entities is urgently needed to support scientific research and clinical health care. In this paper, we constructed a systematic, large-scale, and high-quality symptom-gene associations network system named SympGAN (accessible at http://www.sympgan.org/). We provide access to the database with millions of associations between symptoms, genes, diseases, and drugs, as well as the system for users to search, analyze, knowledge inference, and present data visualization. We utilize state-of-the-art machine learning and deep learning algorithms as the backbone to form the final dataset. In addition, we utilize the RoBERTa-PubMed neural network for name entity recognition to assist in data screening. The knowledge graph is adopted to organize the relationships between different entities. We adopt ConvE, TuckER, and HypER methods for knowledge completion experiments to validate the quality of final knowledge graph triples. Based on the results, we provide online automatic knowledge inference interfaces. The system, SympGAN, has promising value for disease diagnosis, decision support in health care, precision health, and scientific research, as researchers and practitioners can easily access information about symptoms, diseases, targets, gene ontology, and drugs.}
}
@article{JELISIC2022100385,
title = {A novel business context-based approach for improved standards-based systems integration—a feasibility study},
journal = {Journal of Industrial Information Integration},
volume = {30},
pages = {100385},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2022.100385},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X2200053X},
author = {Elena Jelisic and Nenad Ivezic and Boonserm Kulvatunyou and Pavle Milosevic and Sladjan Babarogic and Zoran Marjanovic},
keywords = {Enterprise systems integration, Business context model, Digitalization, Data exchange standard development, Data exchange standard usage},
abstract = {Systems integration processes need to become more efficient and effective in order to allow enterprises to be nimbler and more responsive in today's dynamic markets. Systems integration typically depends on data exchange standards (DESes) and the associated DES usage specification that provides precise standard implementation requirements. However, there are significant inefficiencies in DES usage specification management today. Therefore, to achieve the objective of more responsive enterprises, DES usage specification management, particularly reuse, needs to advance. The Core Component Technical Specification carries the promise to advance the reuse by introducing the notions of Core Components (CCs), as DES building blocks, and Business Information Entities (BIEs), as DES usage specification. While the CCs idea has been successfully implemented in industry DES, the BIEs idea has been implemented only in a basic form, falling short of enabling the BIE reuse to its full potential. To realize the full potential of the BIE reuse, BIE development in industry standard usage needs to utilize the notion of business context better. In this paper, we reviewed existing business context models including UN/CEFACT Context Model (UCM), Enhanced UCM (E-UCM), and Business Context Ontology (BCOnt) and found that they were promising tools to improve the effectiveness of the BIE development and reuse. In addition to that contribution, this research took a closer look at E-UCM in particular. Two novel assessment criteria called expressiveness and effectiveness were defined. Using an industry use case and the two assessments, we showed short-comings of E-UCM such as semantic ambiguity and business rule disconnection. From there, improvements were outlined for future work to device them into E-UCM to enable a more efficient and effective BIE development and reuse process.}
}
@article{FENWICK2023105892,
title = {Originality and the future of copyright in an age of generative AI},
journal = {Computer Law & Security Review},
volume = {51},
pages = {105892},
year = {2023},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2023.105892},
url = {https://www.sciencedirect.com/science/article/pii/S0267364923001024},
author = {Mark Fenwick and Paulius Jurcys},
keywords = {AI, Generative AI, Copyright, Creativity, Originality, Artificial intelligence, Data, Feist, David Guetta, ChatGPT, Input, Output, Machine Learning, Authorship, Intellectual Property},
abstract = {This paper takes the occasion of French DJ David Guetta's use of generative AI tools to create lyrics and a voice in the style of Eminem, which he then used in one of his concerts, as the basis for an exploration of the shifting meaning of creativity and originality in the age of generative AI. Our main contention is that the Guetta form of creativity with generative AI tools differs in certain important respects from what has come before. The paper describes an iterative, dynamic process of conception, prompting, generation, refining, and deployment to characterise creativity in this context. Nevertheless, we contend that copyright – specifically the concept of originality as articulated in US federal law – is a sufficiently durable legal mechanism that can manage these new cultural forms, and that the two basic requirements of modern copyright law (a tangible medium of expression and a modest degree of creativity) remain relevant in identifying the scope of legal protection. The paper argues that the David Guetta story reveals something more general about creativity in a digital age, namely that while hybrid-networked (i.e., human – corporate – machine) creators have always created hybrid-networked cultural forms (i.e., creations that blend human and technology-constituted elements), such hybridity becomes increasingly visible and complex in the context of a new world of generative AI. At the very least, earlier – and influential – models of creativity as human-driven involving creation ex nihilo become harder to sustain in a new age of generative AI. But this does not mean copyright or notions of originality are redundant or that copyright law cannot accommodate Guetta and other cases. Such an account seems important as it challenges the hegemonic and reductive view that AI “generates” artistic works autonomously and avoids reducing the copyright issues raised by such creative works to the related but distinct question of whether learning models rely on copyrighted data. As such, copyright law should remain an important mechanism to facilitate genuine creators who are using AI systems in innovative and unique ways to push the boundaries of their creativity.}
}
@article{MALDONADO2020105616,
title = {CLIN-IK-LINKS: A platform for the design and execution of clinical data transformation and reasoning workflows},
journal = {Computer Methods and Programs in Biomedicine},
volume = {197},
pages = {105616},
year = {2020},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2020.105616},
url = {https://www.sciencedirect.com/science/article/pii/S0169260720314498},
author = {José Alberto Maldonado and Mar Marcos and Jesualdo Tomás Fernández-Breis and Vicente Miguel Giménez-Solano and María del Carmen Legaz-García and Begoña Martínez-Salvador},
keywords = {Data workflow, Electronic health records, Health information interoperability, Semantic Web},
abstract = {Background and Objective
Effective sharing and reuse of Electronic Health Records (EHR) requires technological solutions which deal with different representations and different models of data. This includes information models, domain models and, ideally, inference models, which enable clinical decision support based on a knowledge base and facts. Our goal is to develop a framework to support EHR interoperability based on transformation and reasoning services intended for clinical data and knowledge.
Methods
Our framework is based on workflows whose primary components are reusable mappings. Key features are an integrated representation, storage, and exploitation of different types of mappings for clinical data transformation purposes, as well as the support for the discovery of new workflows. The current framework supports mappings which take advantage of the best features of EHR standards and ontologies. Our proposal is based on our previous results and experience working with both technological infrastructures.
Results
We have implemented CLIN-IK-LINKS, a web-based platform that enables users to create, modify and delete mappings as well as to define and execute workflows. The platform has been applied in two use cases: semantic publishing of clinical laboratory test results; and implementation of two colorectal cancer screening protocols. Real data have been used in both use cases.
Conclusions
The CLIN-IK-LINKS platform allows the composition and execution of clinical data transformation workflows to convert EHR data into EHR and/or semantic web standards. Having proved its usefulness to implement clinical data transformation applications of interest, CLIN-IK-LINKS can be regarded as a valuable contribution to improve the semantic interoperability of EHR systems.}
}
@article{FU20202893,
title = {Fog computing in health management processing systems},
journal = {Kybernetes},
volume = {49},
number = {12},
pages = {2893-2917},
year = {2020},
issn = {0368-492X},
doi = {https://doi.org/10.1108/K-09-2019-0621},
url = {https://www.sciencedirect.com/science/article/pii/S0368492X20000110},
author = {Chao Fu and Qing Lv and Reza G. Badrnejad},
keywords = {Cloud computing, Fog computing, Internet of things, Health management systems},
abstract = {Purpose
Fog computing (FC) is a new field of research and has emerged as a complement to the cloud, which can mitigate the problems inherent to the cloud computing (CC) and internet of things (IoT) model such as unreliable latency, bandwidth constraints, security and mobility. Because there is no comprehensive study on the FC in health management processing systems techniques, this paper aims at surveying and analyzing the existing techniques systematically as well as offering some suggestions for upcoming works.
Design/methodology/approach
The paper complies with the methodological requirements of systematic literature reviews (SLR). The present paper investigates the newest systems and studies their practical techniques in detail. The applications of FC in health management systems have been categorized into three major groups, including review articles, data analysis, frameworks and models mechanisms.
Findings
The results have indicated that despite the popularity of FC as having real-time processing, low latency, dynamic configuration, scalability, low reaction time (less than a second), high bandwidth, battery life and network traffic, a few issues remain unanswered, such as security. The most recent research has focused on improvements in remote monitoring of the patients, such as less latency and rapid response. Also, the results have shown the application of qualitative methodology and case study in the use of FC in health management systems. While FC studies are growing in the clinical field, CC studies are decreasing.
Research limitations/implications
This study aims to be comprehensive, but there are some limitations. This research has only surveyed the articles that are mined, according to a keyword exploration of FC health, FC health care, FC health big data and FC health management system. Fog-based applications in the health management system may not be published with determined keywords. Moreover, the publications written in non-English languages have been ignored. Some important research studies may be printed in a language other than English.
Practical implications
The results of this survey will be valuable for academicians, and these can provide visions into future research areas in this domain. This survey helps the hospitals and related industries to identify FC needs. Moreover, the disadvantages and advantages of the above systems have been studied, and their key issues have been emphasized to develop a more effective FC in health management processing mechanisms over IoT in the future.
Originality/value
Previous literature review studies in the field of SLR have used a simple literature review to find the tasks and challenges in the field. In this study, for the first time, the FC in health management processing systems is applied in a systematic review focused on the mediating role of the IoT and thereby provides a novel contribution. An SLR is conducted to find more specific answers to the proposed research questions. SLR helps to reduce implicit researcher bias. Through the adoption of broad search strategies, predefined search strings and uniform inclusion and exclusion criteria, SLR effectively forces researchers to search for studies beyond their subject areas and networks.}
}
@article{DHINDSA2025693,
title = {Genome-wide prediction of dominant and recessive neurodevelopmental disorder-associated genes},
journal = {The American Journal of Human Genetics},
volume = {112},
number = {3},
pages = {693-708},
year = {2025},
issn = {0002-9297},
doi = {https://doi.org/10.1016/j.ajhg.2025.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0002929725000485},
author = {Ryan S. Dhindsa and Blake A. Weido and Justin S. Dhindsa and Arya J. Shetty and Chloe F. Sands and Slavé Petrovski and Dimitrios Vitsios and Anthony W. Zoghbi},
keywords = {genetics, machine learning, computational biology, neurodevelopmental disease, epilepsy, autism spectrum disorder, intellectual disability},
abstract = {Summary
Despite great progress, thousands of neurodevelopmental disorder (NDD) risk genes remain to be discovered. We present a computational approach that accelerates NDD risk gene identification using machine learning. First, we demonstrate that models trained solely on single-cell RNA sequencing data can robustly predict genes implicated in autism spectrum disorder (ASD), developmental and epileptic encephalopathy (DEE), and developmental delay (DD). Notably, we find differences in gene expression patterns of genes with monoallelic and bi-allelic inheritance patterns in the developing human cortex. We then integrate expression data with 300 orthogonal features, including intolerance metrics, protein-protein interaction data, and others, in a semi-supervised machine learning framework (mantis-ml) to train inheritance-specific models for these disorders. The models have high predictive power (area under the receiver operator curves [AUCs]: 0.84–0.95), and the top-ranked genes were up to 2-fold (monoallelic models) and 6-fold (bi-allelic models) more enriched for high-confidence NDD risk genes compared to genic intolerance metrics alone. Additionally, genes ranking in the top decile were 45 to 180 times more likely to have literature support than those in the bottom decile. Collectively, this work provides robust NDD risk gene predictions that can complement large-scale gene discovery efforts and underscores the importance of considering inheritance in gene risk prediction.}
}
@article{DIBENEDETTO2020e00146,
title = {Knowledge-based model for geomaterials in the Ancient Centre of Naples (Italy): towards an integrated approach to cultural heritage},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {18},
pages = {e00146},
year = {2020},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2020.e00146},
url = {https://www.sciencedirect.com/science/article/pii/S2212054819300840},
author = {Claudia {Di Benedetto} and Antonio Gautiero and Vincenza Guarino and Vincenzo Allocca and Pantaleone {De Vita} and Vincenzo Morra and Piergiulio Cappelletti and Domenico Calcaterra},
keywords = {Geomaterials, Decay, Database, Cultural heritage, 3D model, Mobile application},
abstract = {An integrate approach, aimed at allowing the access to a large dataset, concerning Cultural Heritage, is proposed. The research, developed within Databenc-SNECS (High-Tech District for Cultural Heritage – Social Network of Historic Center Entities) project, aimed to represent and promote the knowledge of Campania historical centres through paradigms of technological intelligence. An innovative ICT (Information and Communication Technologies) architecture was designed to contain data concerning geomaterials (and relative decay) used in the historical site of Saints Marcellino and Festo (Naples, Italy), along with other ones from selected monuments of the Ancient Centre of Naples. Data, collected into a no-relational database, were processed to realize a 3D model whose prototype will be useful for preserving Cultural Heritage. A further interface, a geo-touristic application for mobile devices, NAppSTONES (Naples – Application – STONES), was developed. This latter, promoting the cultural heritage of the Ancient Centre of Naples, highlighted the close relationship between its geological and urban setting.}
}
@article{LIU2019523,
title = {A New Approach to Process the Unknown Words in Financial Public Opinion},
journal = {Procedia Computer Science},
volume = {162},
pages = {523-531},
year = {2019},
note = {7th International Conference on Information Technology and Quantitative Management (ITQM 2019): Information technology and quantitative management based on Artificial Intelligence},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.019},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919320290},
author = {Kuiyi Liu and Daji Ergu and Ying Cai and Bo Gong and Jiazhen Sheng},
keywords = {Unknown words, Financial public opinion, text classification, TFIDF},
abstract = {The processing of unknown words is an important issue that impedes the performance of many natural language systems. In terms of the processing of financial public opinion, due to a wide variety of terminology in the financial sector and the suddenness of financial public opinion events as well as the proliferation of network vocabulary, a variety of unknown new words emerge while the traditional text classification method is basically incapable of processing these unrecognized characters. In this paper, an approach equipped with a new directional substitution model for processing the unknown words is proposed. Based on the semantic similarity of the context in word2ve algorithm, the model trains synonym substitution list and substitutes the unknown words in the original texts with synonyms. Also, the TFIDF (term frequency-inverse document frequency) -weighted Naive Bayes classifier is used to carry out the text classification experiments on the traditional datasets and the synthetic datasets including the unknown words. The experimental results show that the model has better classification effect than the traditional methods and can accurately identify the categories of the financial public opinion texts involving the unknown words by transforming the meaningless unknown words into the words containing meaning.}
}
@article{RODLER2023251,
title = {DynamicHS: Streamlining Reiter’s Hitting-Set Tree for Sequential Diagnosis},
journal = {Information Sciences},
volume = {627},
pages = {251-279},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.08.029},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522009124},
author = {Patrick Rodler},
keywords = {Reiter’s hitting set tree, Model-based diagnosis, Diagnosis computation, Sequential diagnosis, Diagnostic search, Ontology debugging},
abstract = {Given a system that does not work as expected, sequential diagnosis aims at suggesting a series of system measurements to isolate the true explanation for the system’s misbehavior from a potentially large set of possible explanations. To reason about the best next measurement, sequential diagnosis methods usually require a sample of possible fault explanations at each step of the iterative diagnostic process. The computation of this sample can be accomplished by various diagnostic search algorithms. Among those, Reiter’s HS-Tree is one of the most popular due to its desirable properties and general applicability. Usually, HS-Tree is used in a stateless fashion throughout the diagnosis process to (re)compute a sample of possible fault explanations per iteration, each time given the latest (updated) system knowledge including all so-far collected measurements. At this, the built search tree is discarded between two iterations, albeit often large parts of the tree have to be rebuilt in the next iteration, involving redundant operations and calls to costly reasoning services. As a remedy to this, we propose DynamicHS, a variant of HS-Tree that maintains state throughout the diagnostic session and embraces special strategies to minimize the number of expensive reasoner invocations. DynamicHS provides an answer to a longstanding question posed by Raymond Reiter in his seminal paper from 1987, where he wondered if there is a reasonable strategy to reuse an existing search tree to compute fault explanations after new system information is obtained. We conducted extensive evaluations on real-world diagnosis problems from the domain of knowledge-based systems—a field where the usage of HS-Tree is state-of-the-art—under various diagnosis scenarios in terms of the number of fault explanations computed and the heuristic for measurement selection used. The results prove the reasonability of the novel approach and testify its clear superiority to HS-Tree wrt.computation time. More specifically: (1) DynamicHS required less time than HS-Tree in 96% of the executed sequential diagnosis sessions. (2) DynamicHS exhibited substantial and statistically significant time savings over HS-Tree in most scenarios, with median and maximal savings of 52% and 75%, respectively. (3) The relative amount of saved time appears to neither depend on the number of computed fault explanations nor on the used measurement selection heuristic. (4) In the hardest (most time-intensive) cases per diagnosis scenario, DynamicHS achieved even higher savings than on average, and could avoid median and maximal time overheads of over 175% and 800%, respectively, as opposed to a usage of HS-Tree. Remarkably, DynamicHS achieves these performance improvements while preserving all desirable properties as well as the general applicability of HS-Tree.}
}
@article{STADLER2020100614,
title = {Schema-agnostic SPARQL-driven faceted search benchmark generation},
journal = {Journal of Web Semantics},
volume = {65},
pages = {100614},
year = {2020},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2020.100614},
url = {https://www.sciencedirect.com/science/article/pii/S1570826820300482},
author = {Claus Stadler and Simon Bin and Lisa Wenige and Lorenz Bühmann and Jens Lehmann},
keywords = {Faceted search, Benchmark, SPARQL, RDF, Benchmark generator, Triple store},
abstract = {In this work, we present a schema-agnostic faceted browsing benchmark generation framework for RDF data and SPARQL engines. Faceted search is a technique that allows narrowing down sets of information items by applying constraints over their properties, whereas facets correspond to properties of these items. While our work can be used to realise real-world faceted search user interfaces, our focus lies on the construction and benchmarking of faceted search queries over knowledge graphs. The RDF model exhibits several traits that seemingly make it a natural foundation for faceted search: all information items are represented as RDF resources, property values typically already correspond to meaningful semantic classifications, and with SPARQL there is a standard language for uniformly querying instance and schema information. However, although faceted search is ubiquitous today, it is typically not performed on the RDF model directly. Two major sources of concern are the complexity of query generation and the query performance. To overcome the former, our framework comes with an intermediate domain-specific language. Thereby our approach is SPARQL-driven which means that every faceted search information need is intensionally expressed as a single SPARQL query. In regard to the latter, we investigate the possibilities and limits of real-time SPARQL-driven faceted search on contemporary triple stores. We report on our findings by evaluating systems performance and correctness characteristics when executing a benchmark generated using our generation framework. All components, namely the benchmark generator, the benchmark runners and the underlying faceted search framework, are published freely available as open source.}
}
@article{GASPARETTI2018595,
title = {Prerequisites between learning objects: Automatic extraction based on a machine learning approach},
journal = {Telematics and Informatics},
volume = {35},
number = {3},
pages = {595-610},
year = {2018},
note = {SI: EduWebofData},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2017.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0736585316304890},
author = {Fabio Gasparetti and Carlo {De Medio} and Carla Limongelli and Filippo Sciarrone and Marco Temperini},
keywords = {Curriculum sequencing, E-learning, Learning object, Machine learning, Prerequisite},
abstract = {One standing problem in the area of web-based e-learning is how to support instructional designers to effectively and efficiently retrieve learning materials, appropriate for their educational purposes. Learning materials can be retrieved from structured repositories, such as repositories of Learning Objects and Massive Open Online Courses; they could also come from unstructured sources, such as web hypertext pages. Platforms for distance education often implement algorithms for recommending specific educational resources and personalized learning paths to students. But choosing and sequencing the adequate learning materials to build adaptive courses may reveal to be quite a challenging task. In particular, establishing the prerequisite relationships among learning objects, in terms of prior requirements needed to understand and complete before making use of the subsequent contents, is a crucial step for faculty, instructional designers or automated systems whose goal is to adapt existing learning objects to delivery in new distance courses. Nevertheless, this information is often missing. In this paper, an innovative machine learning-based approach for the identification of prerequisites between text-based resources is proposed. A feature selection methodology allows us to consider the attributes that are most relevant to the predictive modeling problem. These features are extracted from both the input material and weak-taxonomies available on the web. Input data undergoes a Natural language process that makes finding patterns of interest more easy for the applied automated analysis. Finally, the prerequisite identification is cast to a binary statistical classification task. The accuracy of the approach is validated by means of experimental evaluations on real online coursers covering different subjects.}
}
@article{NAHAR2023,
title = {A Rule-Based Expert Advisory System for Restaurants Using Machine Learning and Knowledge-Based Systems Techniques},
journal = {International Journal on Semantic Web and Information Systems},
volume = {19},
number = {1},
year = {2023},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.333064},
url = {https://www.sciencedirect.com/science/article/pii/S1552628323000212},
author = {Khalid M. O. Nahar and Mustafa Banikhalaf and Firas Ibrahim and Mohammed Abual-Rub and Ammar Almomani and Brij B. Gupta},
keywords = {Advisory System, Decision Tree, Knowledge Base Systems, Machine Learning, Restaurant System, Rule-Based Expert System},
abstract = {ABSTRACT
A healthy diet and daily physical activity are a cornerstone in preventing serious diseases and conditions such as heart disease, diabetes, high blood pressure, and hypertension. They also play an important role in the healthy growth and cognitive development for young and old people. Thus, this paper presents a new restaurant advisory system (RAS) using artificial intelligence (AI) techniques such as machine learning, decision tree, and rule-based methods. The proposed system makes a smart decision based on the user’s input information to generate a list of appropriate meals that fit his/her health condition. For accuracy and efficiency measurement procedure in the decision-making process, a dataset from 1100 participants suffering from several diseases such as allergy, age, and body has been created and validated. The performance of the RAS was tested using Visual Basic.net Framework and prolog language. The RAS achieves an accuracy of 100% by testing 30 different live cases.}
}
@article{WICKETT20181175,
title = {A logic-based framework for collection/item metadata relationships},
journal = {Journal of Documentation},
volume = {74},
number = {6},
pages = {1175-1189},
year = {2018},
issn = {0022-0418},
doi = {https://doi.org/10.1108/JD-01-2018-0017},
url = {https://www.sciencedirect.com/science/article/pii/S0022041818000103},
author = {Karen Wickett},
keywords = {Semantics, Cataloguing, Metadata, Digital libraries, Linked data, Collections, Information modelling},
abstract = {Purpose
The purpose of this paper is to present a framework for the articulation of relationships between collection-level and item-level metadata as logical inference rules. The framework is intended to allow the systematic generation of relevant propagation rules and to enable the assessment of those rules for particular contexts and the translation of rules into algorithmic processes.
Design/methodology/approach
The framework was developed using first order predicate logic. Relationships between collection-level and item-level description are expressed as propagation rules – inference rules where the properties of one entity entail conclusions about another entity in virtue of a particular relationship those individuals bear to each other. Propagation rules for reasoning between the collection and item level are grouped together in the framework according to their logical form as determined by the nature of the propagation action and the attributes involved in the rule.
Findings
The primary findings are the analysis of relationships between collection-level and item-level metadata, and the framework of categories of propagation rules. In order to fully develop the framework, the paper includes an analysis of colloquial metadata records and the collection membership relation that provides a general method for the translation of metadata records into formal knowledge representation languages.
Originality/value
The method for formalizing metadata records described in the paper represents significant progress in the application of knowledge representation techniques to problems of metadata creation and management, providing a flexible technique for encoding colloquial metadata as a set of statements in first-order logic. The framework of rules for collection/item metadata relationships has a range of potential applications for the enhancement or metadata systems and vocabularies.}
}
@article{MORENTEMOLINERA202365,
title = {Reviving stagnated debates in Group Decision Making environments with high number of alternatives},
journal = {Procedia Computer Science},
volume = {221},
pages = {65-72},
year = {2023},
note = {Tenth International Conference on Information Technology and Quantitative Management (ITQM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923007068},
author = {J.A. Morente-Molinera and M. Barragán-Guzmán and J.R. Trillo and M.A. Martínez-Sánchez and F.J. Cabrerizo and E. Herrera-Viedma},
keywords = {Group Decision Making, Fuzzy Ontologies, Consensus measures},
abstract = {In group decision-making, experts try to obtain a consensus to determine how to order a series of alternatives. A consensus is a decision that reflects the opinions of every group member. Consensus requires discussion and deliberation between the group members. During the process, it is normal that the discussion process to stagnate. In this article, we present a new method of group decision-making that solves the stagnation of the process by including new information. The timing of this is determined by a process stagnation analysis. Fuzzy Ontologies allow experts to work in environments with large numbers of alternatives. The method takes into account the experts’ rankings of alternatives to determine which criteria the experts seem to like the most. When introducing new information into the debate, experts are given the possibility to explore alternatives that are very different from those already seen or to look for alternatives that meet the criteria that are preferred the most.}
}
@article{MIAO2023,
title = {Digital Health Implementation Strategies Coproduced With Adults With Acquired Brain Injury, Their Close Others, and Clinicians: Mixed Methods Study With Collaborative Autoethnography and Network Analysis},
journal = {Journal of Medical Internet Research},
volume = {25},
year = {2023},
issn = {1438-8871},
doi = {https://doi.org/10.2196/46396},
url = {https://www.sciencedirect.com/science/article/pii/S1438887123007288},
author = {Melissa Miao and Rosemary Morrow and Alexander Salomon and Ben Mcculloch and Jean-Christophe Evain and Meg Rebecca Wright and Marie Therese Murphy and Monica Welsh and Liz Williams and Emma Power and Rachael Rietdijk and Deborah Debono and Melissa Brunner and Leanne Togher},
keywords = {complexity, implementation science, internet interventions, brain injury, stroke, traumatic brain injury, delivery of health care, caregivers, digital health, psychosocial interventions, psychosocial, mobile phone},
abstract = {Background
Acquired brain injuries (ABIs), such as stroke and traumatic brain injury, commonly cause cognitive-communication disorders, in which underlying cognitive difficulties also impair communication. As communication is an exchange with others, close others such as family and friends also experience the impact of cognitive-communication impairment. It is therefore an internationally recommended best practice for speech-language pathologists to provide communication support to both people with ABI and the people who communicate with them. Current research also identifies a need for neurorehabilitation professionals to support digital communication, such as social media use, after ABI. However, with >135 million people worldwide affected by ABI, alternate and supplementary service delivery models are needed to meet these communication needs. The “Social Brain Toolkit” is a novel suite of 3 interventions to deliver communication rehabilitation via the internet. However, digital health implementation is complex, and minimal guidance exists for ABI.
Objective
This study aimed to support the implementation of the Social Brain Toolkit by coproducing implementation knowledge with people with ABI, people who communicate with people with ABI, clinicians, and leaders in digital health implementation.
Methods
A maximum variation sample (N=35) of individuals with living experience of ABI, close others, clinicians, and digital health implementation leaders participated in an explanatory sequential mixed methods design. Stakeholders quantitatively prioritized 4 of the 7 theoretical domains of the Nonadoption, Abandonment, Scale-up, Spread, and Sustainability (NASSS) framework as being the most important for Social Brain Toolkit implementation. Qualitative interview and focus group data collection focused on these 4 domains. Data were deductively analyzed against the NASSS framework with stakeholder coauthors to determine implementation considerations and strategies. A collaborative autoethnography of the research was conducted. Interrelationships between considerations and strategies were identified through a post hoc network analysis.
Results
Across the 4 prioritized domains of “condition,” “technology,” “value proposition,” and “adopters,” 48 digital health implementation considerations and 52 tailored developer and clinician implementation strategies were generated. Benefits and challenges of coproduction were identified. The post hoc network analysis revealed 172 unique relationships between the identified implementation considerations and strategies, with user and persona testing and responsive design identified as the potentially most impactful strategies.
Conclusions
People with ABI, close others, clinicians, and digital health leaders coproduced new knowledge of digital health implementation considerations for adults with ABI and the people who communicate with them, as well as tailored implementation strategies. Complexity-informed network analyses offered a data-driven method to identify the 2 most potentially impactful strategies. Although the study was limited by a focus on 4 NASSS domains and the underrepresentation of certain demographics, the wealth of actionable implementation knowledge produced supports future coproduction of implementation research with mutually beneficial outcomes for stakeholders and researchers.
International Registered Report Identifier (IRRID)
RR2-10.2196/35080}
}
@article{SUN2024107224,
title = {A user review data-driven supplier ranking model using aspect-based sentiment analysis and fuzzy theory},
journal = {Engineering Applications of Artificial Intelligence},
volume = {127},
pages = {107224},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.107224},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623014082},
author = {Bingli Sun and Xiao Song and Wenxin Li and Lu Liu and Guanghong Gong and Yan Zhao},
keywords = {Supplier selection, User review, Fuzzy theory, Aspect-based sentiment analysis, Gated mechanism},
abstract = {Background:
The supplier selection problem is a sophisticated decision-making process that involves evaluating multiple factors. While previous research has primarily focused on objective attributes, such as supplier qualifications, product quality, and price, the subjective opinions of users have often been overlooked. However, with the growing importance of user reviews and sentiment analysis in e-commerce, incorporating users’ opinions on supplier products can provide valuable insights.
Purpose:
This study aims to address the limitations of existing supplier selection approaches by proposing a comprehensive framework that integrates aspect-level sentiment analysis and a fuzzy multi-attribute decision model. The goal is to enhance the decision-making process by considering both objective attributes and subjective opinions.
Methods:
To achieve this, we develop a novel convolutional neural network (CNN) model with a gating mechanism to perform aspect-level sentiment analysis. Furthermore, we propose a fuzzy multi-attribute decision model that combines the predefined sentiment aspects with traditional evaluation criteria. The model is applied to a dataset specifically designed for automotive component supplier selection.
Results:
Experimental results demonstrate the superior performance of our approach compared to existing methods and datasets. A case study demonstrates the combination of aspect-level sentiment analysis and the fuzzy decision model allows for a more comprehensive evaluation of suppliers.
Conclusion:
By integrating aspect-level sentiment analysis and the fuzzy multi-attribute decision model, our proposed framework offers a novel perspective on supplier selection problems. The results highlight the feasibility and superiority of our approach, providing valuable insights for management in making informed decisions. This research contributes to the fields of supplier selection, sentiment analysis, and decision-making, with potential applications in various industries beyond the automotive sector.}
}
@article{YANG2024,
title = {An Online Multimodal Food Data Exploration Platform for Specific Population Health: Development Study},
journal = {JMIR Formative Research},
volume = {8},
year = {2024},
issn = {2561-326X},
doi = {https://doi.org/10.2196/55088},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X24006401},
author = {Lin Yang and Zhen Guo and Xiaowei Xu and Hongyu Kang and Jianqiang Lai and Jiao Li},
keywords = {Chinese food data, multimodal knowledge graph, online platform, population health promotion, health promotion, nutrients, diet, pregnant women},
abstract = {Background
Nutrient needs vary over the lifespan. Improving knowledge of both population groups and care providers can help with healthier food choices, thereby promoting population health and preventing diseases. Providing evidence-based food knowledge online is credible, low cost, and easily accessible.
Objective
This study aimed to develop an online multimodal food data exploration platform for easy access to evidence-based diet- and nutrition-related data.
Methods
We developed an online platform named Food Atlas in collaboration with a multidisciplinary expert group from the National Institute for Nutrition and Health and Peking Union Medical College Hospital in China. To demonstrate its feasibility for Chinese food for pregnant women, a user-friendly and high-quality multimodal food knowledge graph was constructed, and various interactions with graph-structured data were developed for easy access, including graph-based interactive visualizations, natural language retrieval, and image-text retrieval. Subsequently, we evaluated Food Atlas from both the system perspective and the user perspective.
Results
The constructed multimodal food knowledge graph contained a total of 2011 entities, 10,410 triplets, and 23,497 images. Its schema consisted of 11 entity types and 26 types of semantic relations. Compared with 5 other online dietary platforms (Foodwake, Boohee, Xiachufang, Allrecipes, and Yummly), Food Atlas offers a distinct and comprehensive set of data content and system functions desired by target populations. Meanwhile, a total of 28 participants representing 4 different user groups were recruited to evaluate its usability: preparing for pregnancy (n=8), pregnant (n=12), clinicians (n=5), and dietitians (n=3). The mean System Usability Scale index of our platform was 82.5 (SD 9.94; range 40.0-82.5). This above-average usability score and the use cases indicated that Food Atlas is tailored to the needs of the target users. Furthermore, 96% (27/28) of the participants stated that the platform had high consistency, illustrating the necessity and effectiveness of health professionals participating in online, evidence-based resource development.
Conclusions
This study demonstrates the development of an online multimodal food data exploration platform and its ability to meet the rising demand for accessible, credible, and appropriate evidence-based online dietary resources. Further research and broader implementation of such platforms have the potential to popularize knowledge, thereby helping populations at different life stages make healthier food choices.}
}
@incollection{BHAMIDIPATY2025133,
title = {Chapter 7 - Intelligent health care: applications of artificial intelligence and machine learning in computational medicine},
editor = {Tuan Anh Nguyen},
booktitle = {Blockchain and Digital Twin for Smart Hospitals},
publisher = {Elsevier},
pages = {133-169},
year = {2025},
isbn = {978-0-443-34226-4},
doi = {https://doi.org/10.1016/B978-0-443-34226-4.00008-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443342264000083},
author = {Veenadhari Bhamidipaty and Durgananda Lahari Bhamidipaty and Fayaz S.M and K.D.P. Bhamidipaty and Rajesh Botchu},
keywords = {Artificial intelligence, deep neural networks, electronic medical records, machine learning, radiomics},
abstract = {The application of artificial intelligence (AI) and machine learning (ML) to healthcare has revolutionized the fields of data analysis, disease detection, treatment planning, and comprehension of complex biological interactions. In the field of literature mining in particular, computational techniques have become indispensable instruments that enable the swift processing of large amounts of biomedical text to extract meaningful insights. AI algorithms may detect biological links that are essential to the pathophysiology and host responses of viral infections, highlighting ML's capacity to unearth intricate networks that go beyond manual curation. Furthermore, the neural concept recognizer (NCR) is a neural dictionary model using convolutional neural networks for concept detection in medical literature. The NCR surpasses rule-based baselines and displays ML's ability to generalize knowledge across multiple terminologies by utilizing ontological structures such as Systematized Nomenclature of Medicine - Clinical Terms (SNOMED-CT) and Human Phenotype Ontology (HPO). These advancements highlight the potential of AI-powered instruments in clinical decision systems and their function in providing researchers with rapid insights. However, there are issues that need to be addressed, like upholding existing ontologies, integrating different data sources seamlessly, guaranteeing algorithm openness, resolving biases in training datasets, and protecting patient privacy.}
}
@article{THIEU2021104351,
title = {A comprehensive study of mobility functioning information in clinical notes: Entity hierarchy, corpus annotation, and sequence labeling},
journal = {International Journal of Medical Informatics},
volume = {147},
pages = {104351},
year = {2021},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2020.104351},
url = {https://www.sciencedirect.com/science/article/pii/S1386505620318876},
author = {Thanh Thieu and Jonathan Camacho Maldonado and Pei-Shu Ho and Min Ding and Alex Marr and Diane Brandt and Denis Newman-Griffis and Ayah Zirikly and Leighton Chan and Elizabeth Rasch},
keywords = {Functioning information, Mobility, Clinical notes, Natural language processing, Text mining, Named entity recognition},
abstract = {Background
Secondary use of Electronic Health Records (EHRs) has mostly focused on health conditions (diseases and drugs). Function is an important health indicator in addition to morbidity and mortality. Nevertheless, function has been overlooked in accessing patients’ health status. The World Health Organization (WHO)’s International Classification of Functioning, Disability and Health (ICF) is considered the international standard for describing and coding function and health states. We pioneer the first comprehensive analysis and identification of functioning concepts in the Mobility domain of the ICF.
Results
Using physical therapy notes at the National Institutes of Health’s Clinical Center, we induced a hierarchical order of mobility-related entities including 5 entities types, 3 relations, 8 attributes, and 33 attribute values. Two domain experts manually curated a gold standard corpus of 14,281 nested entity mentions from 400 clinical notes. Inter-annotator agreement (IAA) of exact matching averaged 92.3 % F1-score on mention text spans, and 96.6 % Cohen’s kappa on attributes assignments. A high-performance Ensemble machine learning model for named entity recognition (NER) was trained and evaluated using the gold standard corpus. Average F1-score on exact entity matching of our Ensemble method (84.90 %) outperformed popular NER methods: Conditional Random Field (80.4 %), Recurrent Neural Network (81.82 %), and Bidirectional Encoder Representations from Transformers (82.33 %).
Conclusions
The results of this study show that mobility functioning information can be reliably captured from clinical notes once adequate resources are provided for sequence labeling methods. We expect that functioning concepts in other domains of the ICF can be identified in similar fashion.}
}
@article{MCMAHON2021100005,
title = {Sequencing-based genome-wide association studies reporting standards},
journal = {Cell Genomics},
volume = {1},
number = {1},
pages = {100005},
year = {2021},
issn = {2666-979X},
doi = {https://doi.org/10.1016/j.xgen.2021.100005},
url = {https://www.sciencedirect.com/science/article/pii/S2666979X21000057},
author = {Aoife McMahon and Elizabeth Lewis and Annalisa Buniello and Maria Cerezo and Peggy Hall and Elliot Sollis and Helen Parkinson and Lucia A. Hindorff and Laura W. Harris and Jacqueline A.L. MacArthur},
keywords = {GWAS, genome-wide association study, seqGWAS, sequencing-based GWAS, rare variant association, WGS, WES, FAIR data, common variant association, standards},
abstract = {Summary
Genome sequencing has recently become a viable genotyping technology for use in genome-wide association studies (GWASs), offering the potential to analyze a broader range of genome-wide variation, including rare variants. To survey current standards, we assessed the content and quality of reporting of statistical methods, analyses, results, and datasets in 167 exome- or genome-wide-sequencing-based GWAS publications published from 2014 to 2020; 81% of publications included tests of aggregate association across multiple variants, with multiple test models frequently used. We observed a lack of standardized terms and incomplete reporting of datasets, particularly for variants analyzed in aggregate tests. We also find a lower frequency of sharing of summary statistics compared with array-based GWASs. Reporting standards and increased data sharing are required to ensure sequencing-based association study data are findable, interoperable, accessible, and reusable (FAIR). To support that, we recommend adopting the standard terminology of sequencing-based GWAS (seqGWAS). Further, we recommend that single-variant analyses be reported following the same standards and conventions as standard array-based GWASs and be shared in the GWAS Catalog. We also provide initial recommended standards for aggregate analyses metadata and summary statistics.}
}
@article{CARTUYVELS2021143,
title = {Discrete and continuous representations and processing in deep learning: Looking forward},
journal = {AI Open},
volume = {2},
pages = {143-159},
year = {2021},
issn = {2666-6510},
doi = {https://doi.org/10.1016/j.aiopen.2021.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666651021000206},
author = {Ruben Cartuyvels and Graham Spinks and Marie-Francine Moens},
keywords = {Artificial intelligence, Deep learning, Machine learning, Representation learning, Natural language processing},
abstract = {Discrete and continuous representations of content (e.g., of language or images) have interesting properties to be explored for the understanding of or reasoning with this content by machines. This position paper puts forward our opinion on the role of discrete and continuous representations and their processing in the deep learning field. Current neural network models compute continuous-valued data. Information is compressed into dense, distributed embeddings. By stark contrast, humans use discrete symbols in their communication with language. Such symbols represent a compressed version of the world that derives its meaning from shared contextual information. Additionally, human reasoning involves symbol manipulation at a cognitive level, which facilitates abstract reasoning, the composition of knowledge and understanding, generalization and efficient learning. Motivated by these insights, in this paper we argue that combining discrete and continuous representations and their processing will be essential to build systems that exhibit a general form of intelligence. We suggest and discuss several avenues that could improve current neural networks with the inclusion of discrete elements to combine the advantages of both types of representations.}
}
@article{MADSEN2021491,
title = {Gaining customer centric understanding of retail displays for future innovations},
journal = {International Journal of Retail & Distribution Management},
volume = {49},
number = {4},
pages = {491-513},
year = {2021},
issn = {0959-0552},
doi = {https://doi.org/10.1108/IJRDM-08-2019-0280},
url = {https://www.sciencedirect.com/science/article/pii/S095905522100070X},
author = {Signe Mørk Madsen},
keywords = {Display, Sensemaking, Process, Flat ontology, Systems thinking, Omnichannel, Retail design, Customer centric},
abstract = {Purpose
The aim of this research is to provide insights for future display design through understanding the processes of sensemaking of retail displays in digitised retail.
Design/methodology/approach
The research applies media elicited interviews and engages thematic analysis to understand agency and advance mental models of retail display. Actor Network Theory (ANT) is engaged to flatten the ontology to traverse digital and physical realms as well as more semiotic sources.
Findings
The article presents a system comprising sensemaking processes of displays in digitised retail and traces the blending traits of physical and digital displays labelling an emerging display terminology applicable across realms.
Research limitations/implications
The participating retail concepts' limited resources for technological innovations plus the customers all being local and recruited through the physical store represent this study's limitations.
Practical implications
The developed system reveals a process for abandoning the familiar but obsolete understanding of retail displays to replace it with new insights to support the judgement and decision process for designing innovative future displays with a customer centric logic.
Originality/value
The article is novel in flattening the ontology of retail displays to fit an organisational interface perception of the link between customer and retailer.}
}
@article{TAIPALUS2020110576,
title = {The effects of database complexity on SQL query formulation},
journal = {Journal of Systems and Software},
volume = {165},
pages = {110576},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110576},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220300571},
author = {Toni Taipalus},
keywords = {Structured query language (SQL), Database, Database complexity, Education, Student learning},
abstract = {In Structured Query Language (SQL) education, students often execute queries against a simple exercise database. Recently, databases that are more realistic have been utilized to the effect that students find exercises more interesting and useful, as these databases more accurately mimic databases students are likely to encounter in their future work environments. However, using even the most engaging database can be counterproductive to learning, if a student is not able to formulate correct queries due to the complexity of the database schema. Scientific evidence on the effects of database complexity on student’s query formulation is limited, and with queries from 744 students against three databases of varying logical complexity, we set out to study how database complexity affects the success rates in query formulation. The success rates against a simple database were significantly higher than against a semi-complex and a complex database, which indicates that it is easier for students to write SQL queries against simpler databases. This suggests, at least in the scale of our exercise databases, that educators should also consider the negative effects of more realistic databases, even though they have been shown to increase student engagement.}
}
@article{ZENG2023100035,
title = {Multi-aspect attentive text representations for simple question answering over knowledge base},
journal = {Natural Language Processing Journal},
volume = {5},
pages = {100035},
year = {2023},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2023.100035},
url = {https://www.sciencedirect.com/science/article/pii/S2949719123000328},
author = {Zhixiang Zeng and Yuefeng Li and Jianming Yong and Xiaohui Tao and Vicky Liu},
keywords = {Question answering, Knowledge base, Deep learning},
abstract = {With the deepening of knowledge base research and application, question answering over knowledge base, also called KBQA, has recently received more and more attention from researchers. Most previous KBQA models focus on mapping the input query and the fact in KBs into an embedding format. Then the similarity between the query vector and the fact vector is computed eventually. Based on the similarity, each query can obtain an answer representing a tuple (subject, predicate, object) from the KBs. However, the information about each word in the input question will lose inevitably during the process. To retain as much original information as possible, we introduce an attention-based recurrent neural network model with interactive similarity matrixes. It can extract more comprehensive information from the hierarchical structure of words among queries and tuples stored in the knowledge base. This work makes three main contributions: (1) A neural network-based question-answering model for the knowledge base is proposed to handle single relation questions. (2) An attentive module is designed to obtain information from multiple aspects to represent queries and data, which contributes to avoiding losing potentially valuable information. (3) Similarity matrixes are introduced to obtain the interaction information between queries and data from the knowledge base. Experimental results show that our proposed model performs better on simple questions than state-of-the-art in several effectiveness measures.}
}
@article{CANALES2024105980,
title = {Agent-based models of groundwater systems: A review of an emerging approach to simulate the interactions between groundwater and society},
journal = {Environmental Modelling & Software},
volume = {175},
pages = {105980},
year = {2024},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2024.105980},
url = {https://www.sciencedirect.com/science/article/pii/S1364815224000410},
author = {Marcos Canales and Juan Castilla-Rho and Rodrigo Rojas and Sebastian Vicuña and James Ball},
keywords = {Sustainability, Water management, Complex adaptive system, Socio-ecological system, Socio-hydrology, Human behaviour},
abstract = {Understanding how society can address and mitigate threats to groundwater sustainability remains a pressing challenge in the Anthropocene era. This article presents the first comprehensive and critical review of coupling Groundwater Models and Agent-Based Models (GW-ABMs) to address four key challenges: (1) adequately representing human behaviour, (2) capturing spatial and temporal variations, (3) integrating two-way feedback loops between social and physical systems, and (4) incorporating water governance structures. Our findings indicate a growing effort to model bounded rationality in human behaviour (Challenge 1 or C1) and a dominant focus on policy applications (C4). Future research should address data scarcity issues through Epstein’s Backward approach (C2), capture feedbacks via tele-coupled GW-ABMs, and explore other modelling techniques like Analytic Elements Groundwater Models (C3). We conclude with recommendations to thrust future GW-ABMs to the highest standards, aiming to enhance their acceptance and impact in decision-making and policy formulation for sustainable groundwater management.}
}
@incollection{JOHNSON2020377,
title = {Discourse Analysis},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {377-383},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10814-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955108145},
author = {Melissa N.P. Johnson and Ethan McLean},
keywords = {Critical discourse analysis (CDA), Discourse, Discourse analysis, Epistemology, Foucault, Foucauldian discourse analysis (FDA), Hegemony, Methodology, Poststructuralism, Power, Power/knowledge, Representation, Social constructionism, Social theory, Textual analysis},
abstract = {Discourse analysis is a field of research composed of multiple heterogeneous, largely qualitative, approaches to the study of relationships between language-in-use and the social world. Researchers in the field typically view language as a form of social practice that influences the social world, and vice versa. Many contemporary varieties of discourse analysis have, explicitly or implicitly, been influenced by Michel Foucault's theories related to power, knowledge, and discourse. In part, Foucault's work has helped stimulate greater interest in researching the role language plays in the production and maintenance of specific knowledges, and inequitable power relations. Human geographers conducting discourse analyses typically draw on one of three prominent streams of discourse analysis: Foucauldian discourse analysis (FDA), critical discourse analysis (CDA), and Gramscian approaches. These approaches have various theoretical and methodological differences. Yet, each approach offers researchers a useful means to investigate and expose semiotic dimensions of power relations in particular sociospatial contexts. While these approaches have no fixed methods, scholars have identified several key investigative strategies which may usefully inform the performance of any type of discourse analysis project.}
}
@article{WANG2024107828,
title = {Generating bulk RNA-Seq gene expression data based on generative deep learning models and utilizing it for data augmentation},
journal = {Computers in Biology and Medicine},
volume = {169},
pages = {107828},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.107828},
url = {https://www.sciencedirect.com/science/article/pii/S0010482523012933},
author = {Yinglun Wang and Qiurui Chen and Hongwei Shao and Rongxin Zhang and Han Shen},
keywords = {Machine learning, Deep learning, Generative learning, Transcriptome},
abstract = {Large-scale high-throughput transcriptome sequencing data holds significant value in biomedical research. However, practical challenges such as difficulty in sample acquisition often limit the availability of large sample sizes, leading to decreased reliability of the analysis results. In practice, generative deep learning models, such as Generative Adversarial Networks (GANs) and Diffusion Models (DMs), have been proven to generate realistic data and may be used to solve this promblem. In this study, we utilized bulk RNA-Seq gene expression data to construct different generative models with two data preprocessing methods: Min-Max-GAN, Z-Score-GAN, Min-Max-DM, and Z-Score-DM. We demonstrated that the generated data from the Min-Max-GAN model exhibited high similarity to real data, surpassing the performance of the other models significantly. Furthermore, we trained the models on the largest dataset available to date, achieving MMD (Maximum Mean Discrepancy) of 0.030 and 0.033 on the training and independent datasets, respectively. Through SHAP (SHapley Additive exPlanations) explanations of our generative model, we also enhanced our model's credibility. Finally, we applied the generated data to data augmentation and observed a significant improvement in the performance of classification models. In summary, this study establishes a GAN-based approach for generating bulk RNA-Seq gene expression data, which contributes to enhancing the performance and reliability of downstream tasks in high-throughput transcriptome analysis.}
}
@article{ARTIGAPURCELL2024103076,
title = {Relational resources: Moving from plural to entangled extractivisms},
journal = {Political Geography},
volume = {110},
pages = {103076},
year = {2024},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2024.103076},
url = {https://www.sciencedirect.com/science/article/pii/S0962629824000258},
author = {James Alejandro Artiga-Purcell},
keywords = {Extractivism, Relational resources, Resource politics, Social movements, Decolonial},
abstract = {Increasing studies of extractivisms, in the plural, examine the diverse politics that permeate extractive activities. However useful, this appeal to plurality is problematic. Plurality without relationality risks justifying some extractive activities over others, without addressing how they relate. Drawing on anti-essentialist, decolonial, and political ecological scholarship, this article proposes an ontological reframing of extractivisms that troubles the division and comparison that pervade notions of taxonomic plurality, and emphasizes the mutual constitution articulated within the concept of relationality. Analyses of relational resources are better equipped to illuminate the power-laden associations through which supposedly distinct commodity values, extractive geographies, sectors, and governance regimes shape one another. Beyond complicating rigid definitions of extractive categories, this ontological shift requires distinct methodologies for understanding and contesting entangled extractive interests and logics. It implicates what forms of anti-extractive resistance, solidarities, and alternative futures become viable, thinkable, and necessary.}
}
@article{PURNOMO2022955,
title = {Business Model on M-Business: A Systematic Review},
journal = {Procedia Computer Science},
volume = {215},
pages = {955-962},
year = {2022},
note = {4th International Conference on Innovative Data Communication Technology and Application},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.098},
url = {https://www.sciencedirect.com/science/article/pii/S187705092202169X},
author = {Agung Purnomo and Nur Afia and Yogi Tri Prasetyo and Elsa Rosyidah and Satria Fadil Persada and Fairuz Iqbal Maulana and  Meiryani},
keywords = {Business model, Digital business, Entrepreneurship, M-Business, Systematic literature review},
abstract = {Business model research on m-business with an entrepreneurial mindset continues to grow. The purpose of this study was to review the body of knowledge and research on the relationship between business models and m-business from the standpoint of a systematic literature review. The PRISMA protocol for conducting and reporting the systematic review was followed when conducting the systematic literature review. Based on a systematic search of the Scopus database, a total of 21 peer-reviewed articles were included. The research agenda for further work in this field was provided in themes such as yearly research, multilevel, perspective, as well as the geographic context of the business model on m-business. The categorization of the business model on m-business was carried out to understand the direction of study in this field and the emphasis on certain aspects.}
}
@incollection{2025251,
title = {Index},
editor = {Chang S. Nam and Donggil Song and Heejin Jeong},
booktitle = {Human-Centered Metaverse},
publisher = {Morgan Kaufmann},
pages = {251-254},
year = {2025},
isbn = {978-0-443-21996-2},
doi = {https://doi.org/10.1016/B978-0-443-21996-2.09991-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780443219962099915}
}
@incollection{2025425,
title = {Index},
editor = {William Lawless and Ranjeev Mittu and Donald Sofge and Hesham Fouad},
booktitle = {Interdependent Human-Machine Teams},
publisher = {Academic Press},
pages = {425-430},
year = {2025},
isbn = {978-0-443-29246-0},
doi = {https://doi.org/10.1016/B978-0-443-29246-0.20001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443292460200011}
}
@article{OSBORNE2020100780,
title = {Codeswitching practices from “other tongues” to the “mother tongue” in the provincial Philippine classroom},
journal = {Linguistics and Education},
volume = {55},
pages = {100780},
year = {2020},
issn = {0898-5898},
doi = {https://doi.org/10.1016/j.linged.2019.100780},
url = {https://www.sciencedirect.com/science/article/pii/S0898589818304443},
author = {Dana Osborne},
keywords = {Codeswitching, Philippines, Ilocano, Minority languages, Phatic language, Dialogicity},
abstract = {Everyday speakers of the minority language of Ilocano in the Philippines often hold many ideological projects in their minds simultaneously; these become particularly productive and apparent in the linguistically regimented space of the classroom. Here, students and teachers work together to mutually construct a distinct sense of ethnolinguistic identity through critical codeswitching practices from “other tongues” to the “mother tongue.” The central frame of doing school is primarily constructed linguistically through the use of the licensed codes of English and/or Filipino. The frame of the classroom space may undergo temporary transformation through the invocation of the keys of discipline and play, dialogically marked through the switch into the mother tongue of Ilocano. In the highly regimented space of the classroom, salient possibilities for the bracketing of distinct ethnolinguistic identities are engendered by the discursive and structural effects brought about by codeswitching.}
}
@article{JIA2025104194,
title = {Hyper attack graph: Constructing a hypergraph for cyber threat intelligence analysis},
journal = {Computers & Security},
volume = {149},
pages = {104194},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104194},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824004991},
author = {Junbo Jia and Li Yang and Yuchen Wang and Anyuan Sang},
keywords = {Cyber threat intelligence, Hypergraph, Relation extraction, Knowledge graph},
abstract = {Cybersecurity experts are actively exploring and implementing automated technologies to extract and present attack information from Cyber Threat Intelligence. However, there are multiple relations among security entities within Cyber Threat Intelligence, a feature that existing technologies often overlook. Additionally, integrating external security knowledge into cyber threat intelligence intuitively during analysis and presentation poses challenges. We propose the Hyper Attack Graph (HAG) framework, the first work to apply hypergraph data structures in the analysis of cyber threat intelligence. Our approach uses a joint extraction model that incorporates a multi-head selection mechanism, effectively addressing the extraction of multiple relations among security entities. We use hypergraph to display tactics and techniques in cyber threat intelligence. Our evaluation of the HAG framework on 685 real-world cyber threat intelligence reports shows an increase in the F1 score for security entity extraction by 11.12% and for relation extraction by 6.71% over existing efforts. Furthermore, HAG’s ability to visually represent external security knowledge on hypergraphs demonstrates its potential as a valuable tool in cybersecurity analysis.}
}
@article{PATIL2024101535,
title = {Modeling type 1 diabetes progression using machine learning and single-cell transcriptomic measurements in human islets},
journal = {Cell Reports Medicine},
volume = {5},
number = {5},
pages = {101535},
year = {2024},
issn = {2666-3791},
doi = {https://doi.org/10.1016/j.xcrm.2024.101535},
url = {https://www.sciencedirect.com/science/article/pii/S2666379124002040},
author = {Abhijeet R. Patil and Jonathan Schug and Chengyang Liu and Deeksha Lahori and Hélène C. Descamps and Ali Naji and Klaus H. Kaestner and Robert B. Faryabi and Golnaz Vahedi},
keywords = {single-cell RNA-seq, type 1 diabetes, machine learning, human islets, autoantibody-positive},
abstract = {Summary
Type 1 diabetes (T1D) is a chronic condition in which beta cells are destroyed by immune cells. Despite progress in immunotherapies that could delay T1D onset, early detection of autoimmunity remains challenging. Here, we evaluate the utility of machine learning for early prediction of T1D using single-cell analysis of islets. Using gradient-boosting algorithms, we model changes in gene expression of single cells from pancreatic tissues in T1D and non-diabetic organ donors. We assess if mathematical modeling could predict the likelihood of T1D development in non-diabetic autoantibody-positive donors. While most autoantibody-positive donors are predicted to be non-diabetic, select donors with unique gene signatures are classified as T1D. Our strategy also reveals a shared gene signature in distinct T1D-associated models across cell types, suggesting a common effect of the disease on transcriptional outputs of these cells. Our study establishes a precedent for using machine learning in early detection of T1D.}
}
@article{GANGWAL2025104360,
title = {Artificial intelligence in preclinical research: enhancing digital twins and organ-on-chip to reduce animal testing},
journal = {Drug Discovery Today},
volume = {30},
number = {5},
pages = {104360},
year = {2025},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2025.104360},
url = {https://www.sciencedirect.com/science/article/pii/S135964462500073X},
author = {Amit Gangwal and Antonio Lavecchia},
keywords = {animal studies, artificial intelligence, 3Rs principle, machine learning, deep learning, organ-on-chip, digital twins, generative adversarial networks},
abstract = {Artificial intelligence (AI) is reshaping preclinical drug research offering innovative alternatives to traditional animal testing. Advanced techniques, including machine learning (ML), deep learning (DL), AI-powered digital twins (DTs), and AI-enhanced organ-on-a-chip (OoC) platforms, enable precise simulations of complex biological systems. AI plays a critical role in overcoming the limitations of DTs and OoC, improving their predictive power and scalability. These technologies facilitate early-stage, reliable evaluations of drug safety and efficacy, addressing ethical concerns, reducing costs, and accelerating drug development while adhering to the 3Rs principle (Replace, Reduce, Refine). By integrating AI with these advanced models, preclinical research can achieve greater accuracy and efficiency in drug discovery. This review examines the transformative impact of AI in preclinical research, highlighting its advancements, challenges, and the critical steps needed to establish AI as a cornerstone of ethical and efficient drug discovery.}
}
@article{DUNLAP202241,
title = {Is the Information-Theoretic Interpretation of Quantum Mechanics an ontic structural realist view?},
journal = {Studies in History and Philosophy of Science},
volume = {91},
pages = {41-48},
year = {2022},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2021.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0039368121001837},
author = {Lucas Dunlap},
abstract = {The Information-Theoretic Interpretation of Quantum Mechanics from (Bub & Pitowsky, 2010) has been criticized in two ways related to the ontological picture it supplies. This paper explores whether Ontic Structural Realism can supplement the metaphysics of ITIQM in a way that would satisfy its critics. The many similarities between the two views are detailed. And it is argued that the ITIQM view ca. 2010 does seem to be compatible with OSR, but as the view evolved in Bub's Bananaworld (2016), its fundamental metaphysical commitments shifted, making it a less clean fit with OSR.}
}
@article{DEMORAESACHCAR2022105679,
title = {South-South cooperation and the re-politicization of development in health},
journal = {World Development},
volume = {149},
pages = {105679},
year = {2022},
issn = {0305-750X},
doi = {https://doi.org/10.1016/j.worlddev.2021.105679},
url = {https://www.sciencedirect.com/science/article/pii/S0305750X21002941},
author = {Helena {de Moraes Achcar}},
keywords = {South-South cooperation, Cooperation in health, Re-politicization of development, Post-structuralism, Logics of critical explanation, Mozambican Pharmaceutical Limited},
abstract = {Brazil’s South-South cooperation (SSC) has been accused of using a depoliticizing language of similarity and horizontality that hid structural asymmetries between very divergent realities. Focusing on a SSC project in health between Brazil and Mozambique, the Mozambican Pharmaceutical Ltd. (SMM), this article seeks to understand whether SSC can in fact re-politicize development. Drawing on a poststructuralist approach to discourse, I see re-politicization as challenging views of development in line with foreign aid (privatization in this context) and the enactment of initiatives in line with SSC principles (state-ownership). I explore the political negotiations and conflict around the implementation of the SMM and argue that while initially the language of horizontality masked structural differences between Brazil and Mozambique, it was later mobilized to challenge Mozambique’s desire to privatize the SMM. A compromise between stakeholders allowed the SMM to be majority state-owned, in what I say represented some degree of structural transformation. My analysis shows that development principles are neither universal (a criticism long addressed at foreign aid) nor do they have a single effect. The implementation of SSC projects that aim to effect structural transformation on highly divergent contexts will be subject to contestation, negotiation and accommodation by stakeholders, and the strategic employment of principles. The article suggests that SSC would require a more frequent engagement between partners so that SSC norms become naturalized. More broadly, it echoes part of the SSC literature that calls for a focus on development encounters, political dynamics and local constructions of reality rather than generic policy statements or principles.}
}
@article{MICHALOWSKI2024104681,
title = {Provision and evaluation of explanations within an automated planning-based approach to solving the multimorbidity problem},
journal = {Journal of Biomedical Informatics},
volume = {156},
pages = {104681},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104681},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000996},
author = {Martin Michalowski and Szymon Wilk and Wojtek Michalowski and Malvika Rao and Marc Carrier},
keywords = {Explainability, Automated planning, Multimorbidity, Clinical decision support},
abstract = {The multimorbidity problem involves the identification and mitigation of adverse interactions that occur when multiple computer interpretable guidelines are applied concurrently to develop a treatment plan for a patient diagnosed with multiple diseases. Solving this problem requires decision support approaches which are difficult to comprehend for physicians. As such, the rationale for treatment plans generated by these approaches needs to be provided.
Objective:
To develop an explainability component for an automated planning-based approach to the multimorbidity problem, and to assess the fidelity and interpretability of generated explanations using a clinical case study.
Methods:
The explainability component leverages the task-network model for representing computer interpretable guidelines. It generates post-hoc explanations composed of three aspects that answer why specific clinical actions are in a treatment plan, why specific revisions were applied, and how factors like medication cost, patient’s adherence, etc. influence the selection of specific actions. The explainability component is implemented as part of MitPlan, where we revised our planning-based approach to support explainability. We developed an evaluation instrument based on the system causability scale and other vetted surveys to evaluate the fidelity and interpretability of its explanations using a two dimensional comparison study design.
Results:
The explainability component was implemented for MitPlan and tested in the context of a clinical case study. The fidelity and interpretability of the generated explanations were assessed using a physician-focused evaluation study involving 21 participants from two different specialties and two levels of experience. Results show that explanations provided by the explainability component in MitPlan are of acceptable fidelity and interpretability, and that the clinical justification of the actions in a treatment plan is important to physicians.
Conclusion:
We created an explainability component that enriches an automated planning-based approach to solving the multimorbidity problem with meaningful explanations for actions in a treatment plan. This component relies on the task-network model to represent computer interpretable guidelines and as such can be ported to other approaches that also use the task-network model representation. Our evaluation study demonstrated that explanations that support a physician’s understanding of the clinical reasons for the actions in a treatment plan are useful and important.}
}
@article{SCHOUTEN201968,
title = {Heracles: A framework for developing and evaluating text mining algorithms},
journal = {Expert Systems with Applications},
volume = {127},
pages = {68-84},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419301587},
author = {Kim Schouten and Flavius Frasincar and Rommert Dekker and Mark Riezebos},
keywords = {Text mining, Algorithm evaluation, Research and development, Developers framework},
abstract = {Many of today’s businesses are driven by data, and while traditionally only quantitative data is considered, the role of textual data in our digital world is rapidly increasing. Text mining allows to extract and aggregate numerical data from textual documents, which in turn can be used to improve key decision processes. In this paper, we propose Heracles, a framework for developing and evaluating text mining algorithms, with a broad range of applications in industry. In contrast to other frameworks, Heracles supports both the development and evaluation stages of text mining algorithms. A practical use case shows the versatility and ease-of-use of the proposed framework in the domain of aspect-based sentiment analysis.}
}
@article{LOTURCO2020102639,
title = {Towards a phygital heritage approach for museum collection},
journal = {Journal of Archaeological Science: Reports},
volume = {34},
pages = {102639},
year = {2020},
issn = {2352-409X},
doi = {https://doi.org/10.1016/j.jasrep.2020.102639},
url = {https://www.sciencedirect.com/science/article/pii/S2352409X20304302},
author = {Massimiliano {Lo Turco} and Elisabetta Caterina Giovannini},
keywords = {Museum, Building Information Modeling, Digital Heritage, Visual Programming Language, Cultural Heritage},
abstract = {The paper presents different experiences developed at the Department of Architecture and Design of Politecnico di Torino in collaboration with Fondazione Museo delle Antichità Egizie in Turin, Italy. These research offered interesting suggestions for working on the crucial relationship between content (collection) and container (museum) through shared and interoperable digital workflows. In the B.A.C.K. TO T.H.E. F.U.T.U.RE. project the research emphatise the role of artworks, that are characterized by intangible and historical values. Artworks are also connected with eterogeneous documentary heritage that enlighted the need of creating new narratives avoiding the descriptive and analytical ones. The project offers a workflow that using different vocabularies (database and open data) is able to structure data and use visual media (3d web publishing) to create different degree of accessibility to cultural heritage content. The recently launched SMART MUSEUM research, works on automated procedures to show, through graphics, the complex phenomena triggered by the attractive weight of the collections. In this case, the artwork assumes artistic, social and media values that contribute to create novel attributes able to identify an attractive weight of the artwork. These value affects the attractiveness of an artwork within the exhibition project and can be an interesting subject for a correct foreshadowing of visitor flows. The involved elements are the exhibition area (the graphic field), the collection (the attractive elements) and the users. The conceived procedure, once automated, becomes a prototype to support the curators to control and improve the efficiency of the exhibition layout.}
}
@article{AHANI2021519,
title = {Evaluating medical travelers’ satisfaction through online review analysis},
journal = {Journal of Hospitality and Tourism Management},
volume = {48},
pages = {519-537},
year = {2021},
issn = {1447-6770},
doi = {https://doi.org/10.1016/j.jhtm.2021.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1447677021001261},
author = {Ali Ahani and Mehrbakhsh Nilashi and Waleed Abdu Zogaan and Sarminah Samad and Nojood O. Aljehane and Ashwaq Alhargan and Saidatulakmal Mohd and Hossein Ahmadi and Louis Sanzogni},
keywords = {Medical tourism, Online reviews, Big social data, Semantic filtering, Ontology, Traveler satisfaction},
abstract = {Medical tourism is increasing quickly since it contributes to both the health and tourism sectors. The use of big social data has been effective in the development of medical tourism as a huge amount of data is produced and shared by travelers about the services through different social media platforms. Indeed, communicative information and knowledge can be mined from a large amount of information provided by travelers about medical tourism services. It is important to analyze such data to understand the customers' satisfaction level and their demands. Although several studies have been conducted to find the factors influencing customer satisfaction in medical tourism, there is a lack of studies about big social data and online behavioral analysis of medical travelers. In addition, the analysis of customers' online reviews is fairly unexplored by machine learning techniques in the context of medical tourism. Hence, this research aims to fill this gap and develop a new method to reveal travelers' choice preferences and satisfaction with medical tourism services through the analysis of the online review. Text mining and ontology approaches are used in the proposed method. The method can mine data from medical tourism websites, discover the satisfaction dimensions, and reveal the satisfaction level of medical tourists through textual reviews. We rely on the demographic information of medical tourists and ontological semantic filtering approaches to better detect the travelers’ preferences in medical tourism websites. The proposed method is evaluated through the numerical and textual reviews obtained from medical tourism websites. The results of data analysis showed that the proposed method is effective for big data analysis in the medical tourism context and may help medical tourism organizers to improve their medical tourism services to obtain a high level of medical travelers' satisfaction.}
}
@article{NATHANI2025125573,
title = {Targeting EGFR-TKI resistance in lung cancer: Role of miR-5193/miR-149-5p loaded NK-EVs and Carboplatin combination},
journal = {International Journal of Pharmaceutics},
volume = {675},
pages = {125573},
year = {2025},
issn = {0378-5173},
doi = {https://doi.org/10.1016/j.ijpharm.2025.125573},
url = {https://www.sciencedirect.com/science/article/pii/S0378517325004107},
author = {Aakash Nathani and Li Sun and Yan Li and Jassy Lazarte and Mounika Aare and Mandip Singh},
keywords = {NK-EVs, miR-5193, miR-149-5p, PD-L1/PD-1 axis, FOXM1, Osimertinib resistance, EGFR mutation, NSCLC},
abstract = {Lung cancer remains the leading cause of cancer-related deaths, and there is an urgent need for innovative therapies. MicroRNA (miRNA)-based gene therapy has shown promise, but efficient delivery systems are required for its success. This study investigates the use of extracellular vehicles (EVs) secreted by natural killer (NK) cells as delivery systems for miRNAs targeting PD-L1/PD-1 immune checkpoint and FOXM1, in combination with Carboplatin, to enhance anticancer efficacy in lung cancer models. NK-EVs were isolated from NK92-MI cells and characterized using nanoparticle tracking analysis (NTA), proteomics and Western blotting, confirming their exosomal characteristics. Gene ontology profiling and RNA-seq identified highly expressed miRNAs such as miR-5193 and miR-149-5p, which were loaded into NK-EVs via electroporation. Agarose gel electrophoresis confirmed their entrapment and Quickdrop spectrophotometer was used to estimate the quantity. In vitro, miRNA-loaded NK-EVs demonstrated significant cytotoxicity against Osimertinib-resistant PDX (TM0019, Jackson Labs) and H1975R (with L858R mutations) lung cancer cells, with approximately 1.2 to 1.6-fold (p < 0.01) decrease in cell viability compared to NK-EVs alone. In vivo, the combination of miRNA-loaded NK-EVs and Carboplatin significantly reduced tumor volumes (3.5 to 4-fold, p < 0.001) in PDX and H1975R xenograft models, with the most pronounced effect observed in combination therapies. Western blot analysis showed downregulation of tumor-associated markers: PD-1/PD-L1, FOXM1, Survivin, NF-κB and others vs untreated group, p < 0.001) suggesting immune checkpoint inhibition, apoptosis and anti-inflammatory activity. These findings highlight the potential of NK-EVs as effective carriers for miRNAs in combination with chemotherapy, offering a promising therapeutic strategy for NSCLC with EGFR mutations.}
}
@article{JIANG2024134568,
title = {The role of AbaI quorum sensing molecule synthase in host cell inflammation induced by Acinetobacter baumannii and its effect on zebrafish infection model},
journal = {International Journal of Biological Macromolecules},
volume = {278},
pages = {134568},
year = {2024},
issn = {0141-8130},
doi = {https://doi.org/10.1016/j.ijbiomac.2024.134568},
url = {https://www.sciencedirect.com/science/article/pii/S014181302405373X},
author = {Xingyu Jiang and Xuchun Shan and Junzhen Jia and Xiaomeng Yang and Ming Yang and Shiqi Hou and Yan Chen and Zhaohui Ni},
keywords = {AHLs, , Inflammatory response},
abstract = {Acinetobacter baumannii is currently one of the most important opportunistic pathogens causing severe nosocomial infections worldwide. Quorum Sensing (QS) system is a widespread mechanism in bacteria to coordinate group behavior by sensing the density of bacterial populations and affect eukaryotic host cell. In Acinetobacter baumannii, AbaI protein is used as QS molecule synthetase to synthesize N- acyl homoserine lactones (AHLs). Currently, QS has made great progress in the study of drug resistance, but there is still a lack of complete understanding of its damage to host cells after adhesion and invasion. Thus, in this study, we examined the effects of abaI mutant (ΔabaI) on the functions of adhesion and invasion, cell viability, inflammation, apoptosis in A. baumannii infected A549 cells, to evaluate the effects of ΔabaI in a zebrafish model. We found the group infected with ΔabaI increased cell viability, reduced adhesion and invasion, cell injury, inflammatory cytokine production and apoptosis. By RNA-Seq, we explored the possibility that abaI stimulated A549 cells inflammation by A. baumannii infection via TLR4/MAPK signaling pathway. In addition, the ΔabaI significantly reduced pathogenicity and recruitment to neutrophils in zebrafish. These observations suggest that abaI plays a major role in A. baumannii infection.}
}
@article{SU2024e38076,
title = {Supergroup algorithm and knowledge graph construction in museum digital display platform},
journal = {Heliyon},
volume = {10},
number = {19},
pages = {e38076},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e38076},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024141072},
author = {Liping Su and Hongli Liu and Wenru Zhao},
keywords = {Knowledge graph, Museum digital display platform, Supergroup algorithm, K-means algorithm, Cultural relic knowledge extraction},
abstract = {In response to the problems of low entity recognition accuracy, low user satisfaction, and weak interactivity in the construction of knowledge graph for digital display of museum cultural relics, this article studied the application of supergroup algorithms and knowledge graph construction in museum digital display platforms to solve the existing problems. By utilizing the K-means algorithm in the supergroup algorithm to conduct a survey of visitors to Museum A and analyze the behavior of 180 selected visitors, the display effect and audience satisfaction can be improved. Various knowledge graph technologies were utilized to construct a knowledge graph of museum cultural relics. Various knowledge resources in museums were associated and integrated, and through the collection and processing of museum cultural relic data, cultural relic ontology construction and relationship extraction were achieved, providing viewers with richer and more in-depth display content. Through experiments, it was found that the visitor satisfaction rate based on the K-means algorithm was above 92.68 %, and the average visitor satisfaction rate after 10 experiments was 94.25 %. The accuracy, recall, and F1 values of the museum cultural relics knowledge graph studied in this article were 90.12 %, 84.69 %, and 82.23 %, respectively, which were much higher than other types of knowledge graphs. By applying these advanced technologies to the digital display platform of museums, not only can the visitor experience be improved, but also the digitalization process of museums can be promoted, contributing to cultural dissemination and development.}
}
@article{GONNERMANN20201376,
title = {Skill Modeling in Cyber-Physical Production Systems for Process Monitoring},
journal = {Procedia CIRP},
volume = {93},
pages = {1376-1381},
year = {2020},
note = {53rd CIRP Conference on Manufacturing Systems 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.03.095},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120307721},
author = {Clemens Gonnermann and Johannes Weth and Gunther Reinhart},
keywords = {Process Monitoring, Skill-Based Modeling, Cyber-Physical Systems, Quality Management},
abstract = {The assembly of individualized products requires an increasing number of process changes that reconfigurable production systems have to cope with today. Multiple changes of the production system and assembly processes lead to process uncertainties and thus to defective products. To avoid additional, non-value-adding quality assurance processes, the inline monitoring of primary processes is a promising approach. This paper focuses on sensorial skills with the aim of an automatized setup of process monitoring alongside assembly planning. By identifying skills of subcomponents of production resources, skills for monitoring are identified and a skill model of the assembly line is generated.}
}
@article{ALSAYEDKASSEM2025107550,
title = {The EPI framework: A data privacy by design framework to support healthcare use cases},
journal = {Future Generation Computer Systems},
volume = {165},
pages = {107550},
year = {2025},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.107550},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24005144},
author = {Jamila {Alsayed Kassem} and Tim Müller and Christopher A. Esterhuyse and Milen G. Kebede and Anwar Osseyran and Paola Grosso},
keywords = {Data sharing policies, Privacy as a service, Healthcare, Workflows, Privacy risk assessment model, Data protection, Usage control},
abstract = {Data sharing is key to enabling data analysis and research advancement, and that is especially true in healthcare. Due to the inherited sensitivity of health data, institutions are still wary of sharing their data, especially with the increasing number of breaches in recent years and the strict privacy legislation involved (GDPR, HIPAA, etc.). Privacy and security concerns exist when making data available for use or processing. To tackle these concerns, we initially incorporate Privacy by Design (PbD) principles. This informs our approach to constructing a data-sharing framework that aligns with said principles. Subsequently, we introduce examples of data-centric use cases requiring support, followed by the delineation of the computation events model and data properties intrinsic to a use case. Furthermore, to gain insight into the potential privacy risks associated with executing a workflow request, we expand upon the privacy threat assessment model to quantitatively evaluate the risks of data likability, identifiability, non-repudiation, detectability, unintended disclosure, indulgence, and policy & consent noncompliance. Subsequently, we construct a framework; the EPI framework; aimed at mitigating these identified risks, via adhering to PbD properties and provisioning extra services.}
}
@article{LIU2023e21302,
title = {Modeling and efficiency analysis of blockchain agriculture products E-commerce cold chain traceability system based on Petri net},
journal = {Heliyon},
volume = {9},
number = {11},
pages = {e21302},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e21302},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023085109},
author = {Shouchen Liu and Zhaoyu Yu},
keywords = {Blockchain, E-commerce cold chain, Traceability, Petri net, Incidence matrix},
abstract = {The cold chain supply chain of e-commerce has the characteristics of long chain, dispersed production, heterogeneous information sources, etc., which is easy to cause the upstream and downstream information of the supply chain to be broken and opaque. Traditional traceability data is stored in each node enterprise, and there are problems such as low cooperation trust and poor authenticity in the upstream and downstream data transmission process of the supply chain, resulting in consumers' trust crisis on the authenticity of traceability information. Blockchain traceability system through the establishment of multi-party participation, joint maintenance of distributed database, and the use of cryptography and consensus mechanism to establish trust relationships, with data can not be tampered with, sharing of high credibility. On the basis of stochastic Petri net theory, this study constructed a blockchain e-commerce cold chain traceability model centering on the actual application of blockchain technology in logistics traceability to improve the reliability and validity of e-commerce cold chain traceability. Correlation matrix and invariants were used to analyze the validity of the model, and an isomorphic Markov chain was constructed to analyze the effectiveness evolution of the model. This study aims to deepen the understanding of the cold chain traceability system of blockchain e-commerce from the three aspects of the best optimization link, the best synergism scale, and the adaptive ability of the system to provide reference for the improvement of cold chain traceability ability by blockchain technology.}
}
@article{MCCARTAN2022100282,
title = {Methodological tensions for non-Indigenous people in Indigenous research: A critique of critical discourse analysis in the Australian context},
journal = {Social Sciences & Humanities Open},
volume = {6},
number = {1},
pages = {100282},
year = {2022},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2022.100282},
url = {https://www.sciencedirect.com/science/article/pii/S2590291122000365},
author = {Julia McCartan and Julie Brimblecombe and Karen Adams},
keywords = {Settler colonialism, Critical discourse analysis, Indigenous peoples, Methodology, Reflexivity, Racism, Power},
abstract = {There are complex ethical issues about appropriate roles, responsibilities and methodologies for non-Indigenous people researching Indigenous Peoples and contexts. As a research methodology, critical discourse analysis (CDA) may expose colonial power and inequities through an examination of language and discourse. This study examined, via a scoping review, non-Indigenous researcher CDA application when analysing discourse relating to Aboriginal and Torres Strait Islander Peoples in Australia. Eighteen articles were included in the analysis. Within most articles, settler colonialism, imperialism, white supremacy and/or structural racism were named when examining oppressive power structures. Whilst some studies incorporated the scholarship of critical theorists and critical Indigenous theorists, this review raised further questions about the methodological underpinning of studies. For example, there were a high number of authors with unexamined power, sociocultural position and standpoint, particularly when analysing Aboriginal and Torres Strait Islander Peoples’ talk and text. In proposing a way forward for CDA research in Indigenous contexts, this review identifies three areas for non-Indigenous researchers to consider: critical reflexivity, colonial power analysis and demonstrable anti-racist action.}
}
@article{WAITES20182812,
title = {A Genetic Circuit Compiler: Generating Combinatorial Genetic Circuits with Web Semantics and Inference},
journal = {ACS Synthetic Biology},
volume = {7},
number = {12},
pages = {2812-2823},
year = {2018},
issn = {2161-5063},
doi = {https://doi.org/10.1021/acssynbio.8b00201},
url = {https://www.sciencedirect.com/science/article/pii/S2161506318000347},
author = {William Waites and Göksel Mısırlı and Matteo Cavaliere and Vincent Danos and Anil Wipat},
keywords = {semantic web, inference, program generation, synthetic biology, genetic circuits},
abstract = {A central strategy of synthetic biology is to understand the basic processes of living creatures through engineering organisms using the same building blocks. Biological machines described in terms of parts can be studied by computer simulation in any of several languages or robotically assembled in vitro. In this paper we present a language, the Genetic Circuit Description Language (GCDL) and a compiler, the Genetic Circuit Compiler (GCC). This language describes genetic circuits at a level of granularity appropriate both for automated assembly in the laboratory and deriving simulation code. The GCDL follows Semantic Web practice, and the compiler makes novel use of the logical inference facilities that are therefore available. We present the GCDL and compiler structure as a study of a tool for generating κ-language simulations from semantic descriptions of genetic circuits.
}
}
@article{ALAYYOUB2019320,
title = {A comprehensive survey of arabic sentiment analysis},
journal = {Information Processing & Management},
volume = {56},
number = {2},
pages = {320-342},
year = {2019},
note = {Advance Arabic Natural Language Processing (ANLP) and its Applications},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2018.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0306457316306689},
author = {Mahmoud Al-Ayyoub and Abed Allah Khamaiseh and Yaser Jararweh and Mohammed N. Al-Kabi},
keywords = {Arabic text processing, Sentiment analysis (SA), Binary/ternary SA, Multi-Way SA, Aspect-based SA},
abstract = {Sentiment analysis (SA) is a continuing field of research that lies at the intersection of many fields such as data mining, natural language processing and machine learning. It is concerned with the automatic extraction of opinions conveyed in a certain text. Due to its vast applications, many studies have been conducted in the area of SA especially on English texts, while other languages such as Arabic received less attention. This survey presents a comprehensive overview of the works done so far on Arabic SA (ASA). The survey groups published papers based on the SA-related problems they address and tries to identify the gaps in the current literature laying foundation for future studies in this field.}
}
@article{SERNA2018291,
title = {TRANSPORT ANALYSIS APPROACH BASED ON BIG DATA AND TEXT MINING ANALYSIS FROM SOCIAL MEDIA},
journal = {Transportation Research Procedia},
volume = {33},
pages = {291-298},
year = {2018},
note = {XIII Conference on Transport Engineering, CIT2018},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2018.10.105},
url = {https://www.sciencedirect.com/science/article/pii/S235214651830262X},
author = {Ainhoa Serna and Slaven Gasparovic},
keywords = {transport, social media, text mining, natural language processing, user generated content},
abstract = {The goal of the study of the paper is to propose a dashboard with dynamic graphics using a qualitatively and quantitatively approach to investigate the tourists’ satisfaction according by transport mode used. The methodology implemented in the research includes data collection from TripAdvisor.com with geographic locations and their integration with statistical territorial data. Text mining techniques are applied in order to assess tourists’ perceptions on success factors, which may be used as planning support tools. The case study concerns Croatia country and shows the value and complementarity of Social Media-related data with official statistics for transport and tourism planning.}
}
@article{VINGERHOETS20191,
title = {Phenotypes in hemispheric functional segregation? Perspectives and challenges},
journal = {Physics of Life Reviews},
volume = {30},
pages = {1-18},
year = {2019},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2019.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1571064519300910},
author = {Guy Vingerhoets},
keywords = {Functional segregation, Brain organization, Brain asymmetry, Atypical language lateralization, Functional lateralization, Language dominance},
abstract = {Directional hemispheric dominance has been established for numerous cognitive functions in the human brain. Strong population biases with some functions favoring the left and others the right hemisphere generated the popular idea of an advantageous prototypical division of labor between both halves of the brain, molded by evolution and genetically blueprinted. As most empirical studies on functional lateralization focused on a single function at a time, little is known about the relation between different asymmetric functions and the consequences of atypical functional segregation in healthy individuals. Recent investigations suggest the existence of at least three different phenotypes in human functional segregation relevant for future neuroscientific and genetic research. Using atypical language dominance as a starting point, I summarize the existing literature about its behavioral and neural consequences and explore the evidence for intermediate phenotypes in brain functional segregation that could bridge behavioral and genetic data.}
}
@article{SHU2023e19406,
title = {Emergency treatment mechanism of laboratory safety accidents in university based on IoT and context aware computing},
journal = {Heliyon},
volume = {9},
number = {9},
pages = {e19406},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e19406},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023066148},
author = {Qiang Shu and Yan Li and Wei Gao},
keywords = {IoT, Context aware, Laboratory safety},
abstract = {In recent years, safety accidents in university laboratories have occurred frequently. Not only do the accidents result in property damage, but also in injuries. Real-time environmental monitoring of the laboratory through IoT enables early detection of potential safety risks such as high temperatures, high humidity and gas leaks, and timely action to reduce the likelihood of accidents. To ensure laboratory safety, in the paper, an emergency treatment mechanism for laboratory safety accidents was proposed based on IoT and context perception. The mechanism uses sensors to collect environmental information and fill a feature characterization architecture for unified safety management. Subsequently, the meta-rule algorithm is used to discover services in the prior knowledge model to form a workflow engine, so as to drive the security business management. Additionally, based on the standard measurement model, we normalize the fuzzy uncertainty measurement model with different granularities and define the fuzzy uncertainty of different emergency decision-making knowledge. Based on this, a knowledge fusion method for emergency decision-making under different fuzzy uncertainties is proposed, which improves laboratory safety emergency response performance based on situational awareness. The implementation of the proposed mechanism in a chemical laboratory demonstrates its efficacy in optimizing operational processes and discovering operational flow through multi-dimensional information analysis. This capability significantly aids safety administrators in their daily laboratory safety management.}
}
@article{SHIGAROV2019100270,
title = {TabbyXL: Software platform for rule-based spreadsheet data extraction and transformation},
journal = {SoftwareX},
volume = {10},
pages = {100270},
year = {2019},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2019.100270},
url = {https://www.sciencedirect.com/science/article/pii/S2352711018302966},
author = {A. Shigarov and V. Khristyuk and A. Mikhailov},
keywords = {Table understanding, Information extraction, Unstructured data management, Rule-based programming, Spreadsheet data, Software development},
abstract = {Spreadsheets are widely used in science, engineering, business, and other activities. Overall, they conceal a large volume of data in a form intended to be interpreted by humans. We present a novel software platform facilitated for liberating such data. It provides rule-based spreadsheet data extraction and transformation to a structured form. Its core consists of a flexible table object model and a domain-specific rule language for table analysis. They serve to represent knowledge of table layout and content features, as well as their interpretation depending on transformation goals. This enables processing arbitrary tables originating from various domains. Our empirical results demonstrate that one ruleset can be applied to process arbitrary tables having the same features of layout, style, or content. The paper also describes two applications using the software platform to develop programs for rule-based converting data from arbitrary spreadsheet tables.}
}
@article{BAI2024112,
title = {A derived information framework for a dynamic knowledge graph and its application to smart cities},
journal = {Future Generation Computer Systems},
volume = {152},
pages = {112-126},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23003825},
author = {Jiaru Bai and Kok Foong Lee and Markus Hofmeister and Sebastian Mosbach and Jethro Akroyd and Markus Kraft},
keywords = {Dynamic knowledge graph, Derived information, Data provenance, Directed acyclic graph, Smart cities},
abstract = {In this work, we develop a derived information framework to semantically annotate how a piece of information can be obtained from others in a dynamic knowledge graph. We encode this using the notion of a “derivation” and capture its metadata with a lightweight ontology. We provide an agent template designed to monitor derivations and to standardise agents performing this and related operations. We implement both synchronous and asynchronous communication modes for agents interacting with the knowledge graph. When occurring in conjunction, directed acyclic graphs of derivations can arise, with changing data propagating through the knowledge graph by means of agents’ actions. While the framework itself is domain-agnostic, we apply it in the context of smart cities as part of the World Avatar project and demonstrate that it is capable of handling sequential events across different timescales. Starting from source information, the framework automatically populates derived data and ensures they remain up to date upon access for a potential flood impact assessment use case.}
}
@incollection{WOODWARD2020375,
title = {Poststructuralism/Poststructuralist Geographies},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {375-385},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10686-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955106869},
author = {Keith Woodward and Deborah P. Dixon and John {Paul Jones}},
keywords = {Actor–network theory, Decentered, Deconstruction, Discourse, Epistemology, Feminism, Marxism, Ontology, Poststructuralism, Social construction, Structuralism},
abstract = {Poststructuralism brought to the field of geography in the late 1980s and 1990s a critique that unsettled both the epistemological (i.e., theories on how we know the world) and ontological (theories on what that world consists of and how it works) moorings of the then dominant theoretical frameworks: spatial science, critical realism and Marxism, and humanism. Its originality lies in its rigorous interrogation of core concepts—such as objectivity and subjectivity, center and margin, materialism and idealism, truth and fiction—that underpin much of modern-day academia. By claiming that any ontology is always already an outcome of epistemology, of our socially constructed ways of knowing, poststructuralists asked that we reflect not only on how we know but also on how elements of ontology—such as space, place, nature, culture, individual, and society—become framed in thought in the first instance. Criticisms that poststructuralists have been concerned only with discourse and representation, as opposed to the “real” material conditions within which these meanings were considered to be embedded, had a profound impact on geographic debate during the 1990s. In responding, poststructuralists have interrogated more closely the ontological ramifications of the work of Derrida and Foucault and have also explored the work of Deleuze and Latour.}
}
@incollection{DUBEY202123,
title = {Chapter 3 - Conversion between semantic data models: the story so far, and the road ahead},
editor = {Sarika Jain and Vishal Jain and Valentina Emilia Balas},
booktitle = {Web Semantics},
publisher = {Academic Press},
pages = {23-30},
year = {2021},
isbn = {978-0-12-822468-7},
doi = {https://doi.org/10.1016/B978-0-12-822468-7.00006-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128224687000067},
author = {Shripriya Dubey and Archana Patel and Sarika Jain},
keywords = {Ontology, RDF, XML, OWL, Semantic Web, converter},
abstract = {At this time and age where use of internet and its popularity is at peak and the data present on the World Wide Web is increasing day by day, the interrelatedness among these data also increases. The need of the hour is to make this data machine understandable and not merely machine readable. Many publishers publish their data online on a similar domain and as likely to happen, the data from these different publishers (sources) add on a little something to each other in a complementing manner. Another problem arises when these datasets are described in different formats (e.g., XML, CSV, DB, and Excel) that is, different way of representation and different Universal Resource Identifiers for the same concept. To handle this diversity among the data with respect to their formats and process the large quantity of data into a consistent, commutable, compatible, and suitable format, we need a technology which is capable of being extended and reused. The Resource Description Framework (RDF) is such a semantic common data format brought as a unifying model by the semantic computing community. There are many existing tools which have been proposed over the time which made conversion of data formats into RDF possible. Presenting data in an RDF format on the World Wide Web provides reusability and interoperability, which helps make it machine understandable. Thus such a solution is needed which converts the data into target format (such as RDF) and augments the reusability, reproducibility, and accessibility of that data. The aim of this chapter is to review and put forward many of such techniques which have been presented by learned authors, the limitations they overcome, what future scope they have, and what other points they could have covered.}
}
@article{LIU2022103622,
title = {scenario modeling for government big data governance decision-making: Chinese experience with public safety services},
journal = {Information & Management},
volume = {59},
number = {3},
pages = {103622},
year = {2022},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2022.103622},
url = {https://www.sciencedirect.com/science/article/pii/S0378720622000349},
author = {Zhao-ge LIU and Xiang-yang LI and Xiao-han ZHU},
keywords = {Government big data governance, Scenario-based decision-making, Scenario modeling, Model-driven, Data link network, Public safety services},
abstract = {In the public safety service context, government big data governance (GBDG) is a challenging decision-making problem that encompasses uncertainties in the arenas of big data and its complex links. Modeling and collaborating the key scenario information required for GBDG decision-making can minimize system uncertainties. However, existing scenario-building methods are limited by their rigidity as they are employed in various application contexts and the associated high costs of modeling. In this paper, using a design science paradigm, a model-driven scenario modeling approach is proposed to achieve flexible scenario modeling for various applications through the transfer of generic domain knowledge. The key component of the proposed approach is a scenario meta-model that is built from existing literatures and practices by integrating qualitative, quantitative, and meta-modeling analysis. An instantiation mechanism of the scenario meta-model is also proposed to generate customized scenarios under Antecedent-Behavior-Consequence (ABC) theory. Two real-world safety service cases in Wuhan, China were evaluated to find that the proposed approach reduces GBDG decision-making uncertainties significantly by providing key information for GBDG problem identification, solution design, and solution value perception. This scenario-building approach can be further used to develop other GBDG systems for public safety services with reduced uncertainties and complete decision-making functions.}
}
@article{ZHONG20181707,
title = {Equipment selection knowledge base system for industrial styrene process},
journal = {Chinese Journal of Chemical Engineering},
volume = {26},
number = {8},
pages = {1707-1712},
year = {2018},
note = {Special Issue for the 2017 Process Systems Engineering Annual Meeting AND Special Issue for the 28th Chinese Process Control Conference.},
issn = {1004-9541},
doi = {https://doi.org/10.1016/j.cjche.2017.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S1004954117313897},
author = {Weimin Zhong and Shuming Liu and Feng Wan and Zhi Li},
keywords = {Equipment selection, Ontology technology, Knowledge base system, Styrene process},
abstract = {Equipment selection for industrial process usually requires the extensive participation of industrial experts and technologists, which causes a serious waste of resources. This work presents an equipment selection knowledge base system for industrial styrene process (S-ESKBS) based on the ontology technology. This structure includes a low-level knowledge base and a top-level interactive application. As the core part of the S-ESKBS, the low-level knowledge base consists of the equipment selection ontology library, equipment selection rule set and Pellet inference engine. The top-level interactive application is implemented using S-ESKBS, including the parsing storage layer, inference query layer and client application layer. Case studies for the industrial styrene process equipment selection of an analytical column and an alkylation reactor are demonstrated to show the characteristics and implementability of the S-ESKBS.}
}
@article{SARWAR2025110559,
title = {Two-sided matching optimization model for green housing technology selection based on hesitant 2-tuple linguistic rough numbers},
journal = {Engineering Applications of Artificial Intelligence},
volume = {151},
pages = {110559},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110559},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625005597},
author = {Musavarah Sarwar and Ghous Ali and Wajeeha Gulzar and Muhammad Akram and Dragan Pamucar},
keywords = {Two-sided matching optimization, HR (hesitant rough) numbers, H2tLR (hesitant 2-tuple linguistic rough) numbers, Incomplete criteria weights, Green housing technology},
abstract = {In two-sided matching decision problems, the matching objects with different knowledge, experiences, and cultures provide linguistic assessments using diverse or multi-granular sets with a factor that the information provided is hesitant in nature due to different opinions given by experts. In the proposed approach, the hesitant 2-tuple linguistic information is integrated with rough approximations to develop the two novel approaches called hesitant rough numbers and hesitant 2-tuple linguistic rough numbers. The proposed novel approximations are implemented on a two-sided matching optimization model to study hesitant multi-granular uncertainty. Firstly, the matching objects provide their evaluations in the form of hesitant multi-granular terms converted into hesitant 2-tuple linguistic rough numbers. Secondly, certain optimization models based on hesitant 2-tuple linguistic rough approximations are constructed to compute the criteria weights using incomplete information. In hesitant 2-tuple linguistic rough optimization models, the maximizing deviation technique is used to find the distance between two proposed novel coefficients. To maximize the level of satisfaction with matching objects, a hesitant 2-tuple linguistic rough optimization model is developed to evaluate the overall satisfaction degree and stability of matching objects. The significance of the proposed two-sided matching optimization model is illustrated with a case study of matching between the green building technology supply and demand. The out-performance of the proposed model is highlighted by a comparison analysis with existing approaches to analyze that it can provide hesitant multi-granular rough flexibility and deals with incomplete information regarding criterion weights.}
}
@article{MILLARWILSON2022105421,
title = {Multiscale modeling in the framework of biological systems and its potential for spaceflight biology studies},
journal = {iScience},
volume = {25},
number = {11},
pages = {105421},
year = {2022},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2022.105421},
url = {https://www.sciencedirect.com/science/article/pii/S2589004222016935},
author = {Andrew Millar-Wilson and Órla Ward and Eolann Duffy and Gary Hardiman},
keywords = {systems biology, in silico biology, space sciences},
abstract = {Summary
A central tenet of systems biology is that biological systems are greater than the sum of their component parts. Spaceflight is associated with hazards including radiation exposure and microgravity which impact different echelons of biological organizations spanning molecular, cellular, organ, and organismal levels. These insults lead to physical damage, including muscle and bone loss, neurological damage, and impaired immunity. Mitochondrial dysfunction and biological alterations occurring during spaceflight have been reported. The health challenges presented by long-term space travel must be addressed and appropriate countermeasures developed to protect astronauts. Increasing quantity of multiomics data are being generated from cells and model organisms flown in space, with physiological data from astronauts. Systems biology approaches leveraging mathematical reasoning and computational modeling are required to characterize these components in a holistic fashion. In this review, we provide an historic perspective on multiscale biological systems modeling, followed by a discussion on its utility for spaceflight biology research.}
}
@article{LU2023101291,
title = {GWmodelS: A software for geographically weighted models},
journal = {SoftwareX},
volume = {21},
pages = {101291},
year = {2023},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2022.101291},
url = {https://www.sciencedirect.com/science/article/pii/S2352711022002096},
author = {Binbin Lu and Yigong Hu and Dongyang Yang and Yong Liu and Liuqi Liao and Zuoyao Yin and Tianyang Xia and Zheyi Dong and Paul Harris and Chris Brunsdon and Lex Comber and Guanpeng Dong},
keywords = {Spatial heterogeneity, Spatio-temporal models, Visualization, High-performance, Local techniques},
abstract = {Spatial heterogeneity or non-stationarity has become a popular and necessary concern in exploring relationships between variables. In this regard, geographically weighted (GW) models provide a powerful collection of techniques in its quantitative description. We developed a user-friendly, high-performance and systematic software, named GWmodelS, to promote better and broader usages of such models. Apart from a variety of GW models, including GW descriptive statistics, GW regression models, and GW principal components analysis, data management and mapping tools have also been incorporated with well-designed interfaces.}
}
@article{AYETIRAN2021106902,
title = {EDS-MEMBED: Multi-sense embeddings based on enhanced distributional semantic structures via a graph walk over word senses},
journal = {Knowledge-Based Systems},
volume = {219},
pages = {106902},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106902},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121001659},
author = {Eniafe Festus Ayetiran and Petr Sojka and Vít Novotný},
keywords = {Multi-sense embeddings, Graph walk, Language generation, Distributional semantics, Distributional structures, Word sense disambiguation, Knowledge-based systems, Word similarity, Semantic applications},
abstract = {Several language applications often require word semantics as a core part of their processing pipeline either as precise meaning inference or semantic similarity. Multi-sense embeddings (m-se) can be exploited for this important requirement. m-se seeks to represent each word by their distinct senses in order to resolve the conflation of meanings of words as used in different contexts. Previous works usually approach this task by training a model on a large corpus and often ignore the effect and usefulness of the semantic relations offered by lexical resources. However, even with large training data, coverage of all possible word senses is still an issue. In addition, a considerable percentage of contextual semantic knowledge are never learned because a huge amount of possible distributional semantic structures are never explored. In this paper, we leverage the rich semantic structures in WordNet using a graph-theoretic walk technique over word senses to enhance the quality of multi-sense embeddings. This algorithm composes enriched texts from the original texts. Furthermore, we derive new distributional semantic similarity measures for m-se from prior ones. We adapt these measures to word sense disambiguation (wsd) aspect of our experiment. We report evaluation results on 11 benchmark datasets involving wsd and Word Similarity tasks and show that our method for enhancing distributional semantic structures improves embeddings quality on the baselines. Despite the small training data, it achieves state-of-the-art performance on some of the datasets.}
}
@article{OSMAN2024102869,
title = {Semantic-based assembly framework for the generation of travel demand},
journal = {Simulation Modelling Practice and Theory},
volume = {131},
pages = {102869},
year = {2024},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2023.102869},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X23001466},
author = {Taha Osman and Gregory L. Albiston and Evtim Peytchev and Eiman Kanjo},
keywords = {Activity-based models, Travel demand, Traffic simulation, Knowledge-based systems, Semantic web},
abstract = {This work applies a knowledge modelling approach in the design of a framework for the generation of travel demand for traffic simulation applications. The proposed framework is based on interchangeable modules that integrate the main stages of travel demand modelling supported by an engineered knowledge base. This approach is intended to promote greater behavioural diversity, incorporate more diverse contextual data, facilitate access to online datasets, and support users to undertake and validate investigations across a range of models and implementations. The framework provides the user with direct control to modify the schema, control the selection of data, select alternative modules to execute, and the potential to remotely retrieve data and execute modules. The framework is investigated through a prototype, which generated travel demand across a full day and performed simulation utilising two third-party traffic simulators based on the configuration by the user schema. The problem of travel demand generation was separated into discrete task modules with identification of features for alternative design or further modularisation. The prototype evaluation generated multimode travel demand that was successfully tested on third-party traffic simulators and evidenced the fitness of semantic technologies in building simulator-agnostic interchangeable framework modules that satisfy the need for configurable travel modelling. The findings of the paper also contribute to the understanding of the challenges in utilising Semantic Web technologies for implementing travel demand generation. It is proposed that the framework provides a basis to develop new and existing approaches to travel demand generation to improve modelling outcomes and adoption.}
}
@article{YIN20235932,
title = {Development of a dynamic model for associating the identification criteria and symptoms of the infectious disease: Take the influenza in Chinese population as an example},
journal = {Asian Journal of Surgery},
volume = {46},
number = {12},
pages = {5932-5934},
year = {2023},
issn = {1015-9584},
doi = {https://doi.org/10.1016/j.asjsur.2023.08.233},
url = {https://www.sciencedirect.com/science/article/pii/S101595842301391X},
author = {Tianlu Yin and Xiaohuan Gao and Dayan Wang and Hongpu Hu},
keywords = {Association dataset, Dynamic series, Infectious diseases, Influenza, Meta-analysis, Public health, Surveillance system, Symptom identification}
}
@article{BELLIGH2021101310,
title = {What's in a code? The code-inference distinction in Neo-Gricean Pragmatics, Relevance Theory, and Integral Linguistics},
journal = {Language Sciences},
volume = {83},
pages = {101310},
year = {2021},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2020.101310},
url = {https://www.sciencedirect.com/science/article/pii/S0388000120300425},
author = {Thomas Belligh and Klaas Willems},
keywords = {Neo-Gricean Pragmatics, Relevance Theory, Integral Linguistics, Code-inference, Semantics-pragmatics, Underspecification},
abstract = {In this article we examine how Neo-Gricean Pragmatics, Relevance Theory, and Integral Linguistics account for the code-inference distinction in natural language. Although the distinction between encoded and inferred meaning figures prominently in each of these theoretical frameworks, the way the distinction is articulated varies considerably. We compare how the three frameworks establish the code-inference distinction with respect to seven issues: 1) the extension of the code (its degree of underspecification) vis-à-vis its intension (the content contained in the code), 2) the criteria for distinguishing code and inference, 3) the differentiation of the pragmatic domain into nonce realizations and default senses, 4) the linguistic phenomena to which the code-inference distinction is applied, 5) the role of truth-conditions, 6) the levels and heuristics involved in the process of getting from encoded meaning to speaker meaning, and 7) the scope of the distinction in the overall study of language. On the basis of the comparison the article explores the main strengths and limits of the three theoretical frameworks.}
}
@article{MAO2020107094,
title = {Development of process safety knowledge graph: A Case study on delayed coking process},
journal = {Computers & Chemical Engineering},
volume = {143},
pages = {107094},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.107094},
url = {https://www.sciencedirect.com/science/article/pii/S0098135420304786},
author = {Shuai Mao and Yunmeng Zhao and Jinhe Chen and Bing Wang and Yang Tang},
keywords = {Process safety, Knowledge graph, Delayed coking process},
abstract = {Process safety is one of the essential preconditions for the achievement of green manufacturing. The improvement of process safety management requires a comprehensive risk analysis based on the collection of almost all safety related information, which are usually unstructured knowledge and experience. To handle the information and support the risk analysis, a process safety knowledge graph is prompted and the development of domain ontology on delayed coking process is elaborated. The combined top-down and bottom-up approaches are used in defining the process safety schema on ontology level. Several multi-structured data sources are introduced in establishing the process safety knowledge, in which the hazard and operability analysis (HAZOP) reports and process diagrams are most important. The ontology design and data extraction are demonstrated in the manuscript while various related applications are discussed. This process safety knowledge graph might empower the knowledge-based analysis abilities in discovering the hidden relationships between possible risk causes and consequences in an emergency situation, and could provide a foundation for more application related to process safety.}
}
@article{KIM2023302,
title = {Proteomic analysis for the effects of non-saponin fraction with rich polysaccharide from Korean Red Ginseng on Alzheimer's disease in a mouse model},
journal = {Journal of Ginseng Research},
volume = {47},
number = {2},
pages = {302-310},
year = {2023},
issn = {1226-8453},
doi = {https://doi.org/10.1016/j.jgr.2022.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S1226845322001245},
author = {Sujin Kim and Yunkwon Nam and Min-jeong Kim and Seung-hyun Kwon and Junhyeok Jeon and Soo Jung Shin and Soyoon Park and Sungjae Chang and Hyun Uk Kim and Yong Yook Lee and Hak Su Kim and Minho Moon},
keywords = {Alzheimer's disease, Non-saponin fraction, Korean Red Ginseng, Proteomics, Genome-scale metabolic model},
abstract = {Background
The most common type of dementia, Alzheimer's disease (AD), is marked by the formation of extracellular amyloid beta (Aβ) plaques. The impairments of axons and synapses appear in the process of Aβ plaques formation, and this damage could cause neurodegeneration. We previously reported that non-saponin fraction with rich polysaccharide (NFP) from Korean Red Ginseng (KRG) showed neuroprotective effects in AD. However, precise molecular mechanism of the therapeutic effects of NFP from KRG in AD still remains elusive.
Methods
To investigate the therapeutic mechanisms of NFP from KRG on AD, we conducted proteomic analysis for frontal cortex from vehicle-treated wild-type, vehicle-treated 5XFAD mice, and NFP-treated 5XFAD mice by using nano-LC-ESI-MS/MS. Metabolic network analysis was additionally performed as the effects of NFP appeared to be associated with metabolism according to the proteome analysis.
Results
Starting from 5,470 proteins, 2,636 proteins were selected for hierarchical clustering analysis, and finally 111 proteins were further selected for protein-protein interaction network analysis. A series of these analyses revealed that proteins associated with synapse and mitochondria might be linked to the therapeutic mechanism of NFP. Subsequent metabolic network analysis via genome-scale metabolic models that represent the three mouse groups showed that there were significant changes in metabolic fluxes of mitochondrial carnitine shuttle pathway and mitochondrial beta-oxidation of polyunsaturated fatty acids.
Conclusion
Our results suggested that the therapeutic effects of NFP on AD were associated with synaptic- and mitochondrial-related pathways, and they provided targets for further rigorous studies on precise understanding of the molecular mechanism of NFP.}
}
@article{YOU2024112308,
title = {TCLNet: Turn-level contrastive learning network with reranking for dialogue state tracking},
journal = {Knowledge-Based Systems},
volume = {302},
pages = {112308},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112308},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124009420},
author = {Chaobin You and Deyi Xiong},
keywords = {Dialogue state tracking, Turn-level contrastive learning, Token reranking, Task-oriented dialogue},
abstract = {Dialogue state tracking (DST) plays a crucial role in task-oriented dialogue systems, as it interprets and tracks user intentions throughout the dialogue. The accuracy of DST directly impacts system efficiency and user experience. While existing generation-based DST models have shown promising results, effectively modelling long dialogue sequences remains a challenge. Furthermore, the greedy search decoding strategy employed by most previous works for speed is suboptimal because the probability of ground truth tokens is not always the highest. In this study, we propose a novel learning framework that addresses these issues by incorporating turn-level contrastive learning and a reranking module. Specifically, we adopt turn-level contrastive learning to progressively rectify the intermediate states of lengthy dialogues by contrasting data points at a finer granularity. Additionally, the integration of the reranking module empowers the model to make more reliable decisions at each time step by further considering alternative tokens with high probabilities. Experimental results demonstrate that our approach consistently enhances the model’s capability to handle long dialogue sequences and achieves superior performance over strong baselines on both the MultiWOZ 2.1 and MultiWOZ 2.2 datasets.}
}
@article{NIAZMAND2022100082,
title = {Efficient semantic summary graphs for querying large knowledge graphs},
journal = {International Journal of Information Management Data Insights},
volume = {2},
number = {1},
pages = {100082},
year = {2022},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2022.100082},
url = {https://www.sciencedirect.com/science/article/pii/S2667096822000258},
author = {Emetis Niazmand and Gezim Sejdiu and Damien Graux and Maria-Esther Vidal},
keywords = {Knowledge graph, Summarization graph, SPARQL evaluation, Embedding model, Distributed context},
abstract = {Knowledge Graphs (KGs) integrate heterogeneous data, but one challenge is the development of efficient tools for allowing end users to extract useful insights from these sources of knowledge. In such a context, reducing the size of a Resource Description Framework (RDF) graph while preserving all information can speed up query engines by limiting data shuffle, especially in a distributed setting. This paper presents two algorithms for RDF graph summarization: Grouping Based Summarization (GBS) and Query Based Summarization (QBS). The latter is an optimized and lossless approach for the former method. We empirically study the effectiveness of the proposed lossless RDF graph summarization to retrieve complete data, by rewriting an RDF Query Language called SPARQL query with fewer triple patterns using a semantic similarity. We conduct our experimental study in instances of four datasets with different sizes. Compared with the state-of-the-art query engine Sparklify executed over the original RDF graphs as a baseline, QBS query execution time is reduced by up to 80% and the summarized RDF graph is decreased by up to 99%.}
}
@article{SINGH2025102510,
title = {Identifying pan-cancer and cancer subtype miRNAs using interpretable convolutional neural network},
journal = {Journal of Computational Science},
volume = {85},
pages = {102510},
year = {2025},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2024.102510},
url = {https://www.sciencedirect.com/science/article/pii/S187775032400303X},
author = {Joginder Singh and Shubhra Sankar Ray and Sukriti Roy},
keywords = {CNN, Explainable AI, Optimization, Machine learning, Data mining, miRNA expression, Computational oncology, Pan-cancer, Cancer subtypes},
abstract = {Background:
MiRNAs are short-length (∼22nt) non-coding RNAs and are considered to be important biomarkers in pan-cancer analysis. Pan-cancer analysis is the study of finding the commonalities and differences in genetic and cellular alterations in various types of cancers. A common computational challenge in handling miRNA expression data is that it is high dimensional and complex (HDC) in nature. In this regard, convolutional neural networks are proven to be good performers due to their nature of finding patterns in complex data.
Methodology:
An interpretable convolutional neural network model (ICNNM) is developed for classifying miRNA expression based pan-cancer data. The ICNNM is a one dimensional model. The layers and other hyperparameters are optimized using Bayesian optimization with multivariate tree parzen estimator (BoMTPE). An interpretable approach is developed using SHapley Additive exPlanations (SHAP) values for explaining the behavior of ICNNM. This approach helps in introducing an attribution score for identifying relevant miRNAs using SHAP values. The attribution scores are assigned higher values for those miRNAs which help in the accurate prediction of tumor class of patients by utilizing the game theory concept in computing the SHAP values. The model is evaluated on 9 datasets among which 6 datasets (4 general pan cancer and two subtypes) are derived from a single TCGA pan-cancer dataset, one dataset is downloaded as Breast sub-type from TCGA, and two datasets, nasopharyngeal carcinoma and bone and soft tissue sarcoma, are downloaded from GEO as rare cancer ones.
Results:
The ICNNM is seen to perform better as compared to related techniques such as three variations of the CNN model, random forest RF, SVM, Gboost, XGboost, and Catboost. The performance is evaluated in terms of F1-score, discriminability power of expressions between normal and tumor classes, and biological significance of the selected miRNAs. The biological significance is established through existing literatures and online databases such as gene ontology and KEGG pathways after obtaining the target genes using miRDB database. While the performance of ICNNM in terms of F1-score varies from 0.95 to 0.99 for 4 general pan-cancer datasets, it varies from 0.91 to 0.99 for 3 subtype datasets and from 0.76 to 0.90 for rare cancer datasets. Many of the selected miRNAs are found to be the key biomarkers in various tumor classes according to existing investigations. Three miRNAs miR-503, miR-202, and miR-135a can be considered as novel predictions for cancer classes prostate and rectum, mesothelioma, and testicular germ cells, respectively, as their target genes are involved in related cancer pathways, obtained using miRDB database.}
}
@article{LIU2024113192,
title = {Deciphering the mechanisms of the IL-6/CXCL1 and NOD-like receptor signaling pathways in otitis media with effusion in rodents},
journal = {International Immunopharmacology},
volume = {142},
pages = {113192},
year = {2024},
issn = {1567-5769},
doi = {https://doi.org/10.1016/j.intimp.2024.113192},
url = {https://www.sciencedirect.com/science/article/pii/S1567576924017144},
author = {Yixuan Liu and Tingting Qian and Nanfeng Zhang and Jiazhen Cao and Xiaoling Lu and Qiling Tong and Xinyuan Wang and Huawei Li and Shan Sun and Huiqian Yu},
keywords = {Otitis media with effusion, NLRP3 activation, IL-6, NF-κB},
abstract = {Background
Otitis media with effusion (OME) often leads to pediatric hearing loss and is influenced by innate and adaptive immune responses. Innate immunity serves as the non-specific first line of defense against OME.
Methods
We induced OME in rats using ovalbumin. We administered IL-6 monoclonal antibodies intranasally to inhibit IL-6, and we injected an NF-κB inhibitor intraperitoneally to explore the role of IL-6 in innate immunity and its interaction with the NOD-like receptor signaling pathway. We analyzed RNA-sequencing data with Gene Ontology and Kyoto Encyclopedia of Genes and Genomes pathways to assess signaling pathways involved in OME. We also utilized Western blot, quantitative real-time PCR, and immunohistochemistry on middle ear samples and used microscopy to identify immune cells in ear wash fluids.
Results
Our study suggests a pivotal role for IL-6 in the immune pathways of rats with OME via the regulation of CXCL1-mediated pathways. Increased levels of IL-6 and CXCL1 were observed in the middle ear tissues, and activation of the NLRP3 inflammasome in OME rats led to an immune response via NF-κB, thus promoting IL-6 and CXCL1 production, which was reduced by IL-6 antibody treatment.
Conclusions
Our findings confirm that IL-6 and CXCL1 play significant roles in the innate immune response in OME in rodents, predominantly via the NOD-like receptor signaling pathway and NLRP3 inflammasome activation. This research sheds light on OME pathogenesis and its immune-related mechanisms.}
}
@article{ALKHATIB20211255,
title = {A New Enhanced Arabic Light Stemmer for IR in Medical Documents},
journal = {Computers, Materials and Continua},
volume = {68},
number = {1},
pages = {1255-1269},
year = {2021},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2021.016155},
url = {https://www.sciencedirect.com/science/article/pii/S1546221821009541},
author = {Ra’ed M. Al-Khatib and Taha Zerrouki and Mohammed M. Abu Shquier and Amar Balla and Asef Al-Khateeb},
keywords = {Machine learning, information retrieval systems, medical documents, stemming algorithms, arabic light stemmer, natural language processing},
abstract = {This paper introduces a new enhanced Arabic stemming algorithm for solving the information retrieval problem, especially in medical documents. Our proposed algorithm is a light stemming algorithm for extracting stems and roots from the input data. One of the main challenges facing the light stemming algorithm is cutting off the input word, to extract the initial segments. When initiating the light stemmer with strong initial segments, the final extracting stems and roots will be more accurate. Therefore, a new enhanced segmentation based on deploying the Direct Acyclic Graph (DAG) model is utilized. In addition to extracting the powerful initial segments, the main two procedures (i.e., stems and roots extraction), should be also reinforced with more efficient operators to improve the final outputs. To validate the proposed enhanced stemmer, four data sets are used. The achieved stems and roots resulted from our proposed light stemmer are compared with the results obtained from five other well-known Arabic light stemmers using the same data sets. This evaluation process proved that the proposed enhanced stemmer outperformed other comparative stemmers.}
}
@article{KONDO2023110620,
title = {Equation-based modeling and optimization-based parameter estimation in multimodal virtual sensing platforms for smart buildings},
journal = {Building and Environment},
volume = {243},
pages = {110620},
year = {2023},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2023.110620},
url = {https://www.sciencedirect.com/science/article/pii/S0360132323006479},
author = {Koichi Kondo and Arika Fukushima and Takufumi Yoshida and Kiyotaka Matsue},
keywords = {Constraint satisfaction, Declarative equation-based modeling, Occupancy estimation, Optimization, Smart building, Virtual sensing},
abstract = {This paper considers a unified approach for virtual sensing in smart buildings that utilizes equation-based modeling and optimization-based parameter estimation. A modeling method is introduced to describe relationships among existing sensory information and application-oriented parameters in a reusable manner. Measurement errors in existing sensors are also described in this equation-based model. When parameters to be estimated and existing sensors used for virtual sensing are identified, relevant equations are selected and combined as simultaneous equations. The parameter estimation method takes an optimization-based approach to cope with uncertainty in sensory information. Simultaneous equations corresponding to target sensing are used as a constraint in the optimization problem. In cases where many different sensors are utilized to increase accuracy, we may need to consider compromising contradictions among sensory information due to sensing errors. If the target building does not have enough sensors for the intended parameter estimation, we may need certain assumptions to determine the value. Such over- or under-defined situations are automatically detected and considered in our parameter estimation mechanism. For this purpose, we introduce a method, based on an idea inspired by randomized algorithms, for repeatedly solving an optimization problem with different weight factors in the objective function and for analyzing any fluctuations in estimated values. This is computationally inexpensive and can be applied to big real-world problems. We applied this method to building occupancy estimations for efficient air-conditioning control and to customer attribute analysis for an office building cafeteria during lunch time.}
}
@article{SAMARAS2023120577,
title = {Sentiment analysis of COVID-19 cases in Greece using Twitter data},
journal = {Expert Systems with Applications},
volume = {230},
pages = {120577},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120577},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423010795},
author = {Loukas Samaras and Elena García-Barriocanal and Miguel-Angel Sicilia},
keywords = {Sentiment analysis, Twitter, Pandemic, Web data, Public health},
abstract = {Background
Syndromic surveillance with the use of Internet data has been used to track and forecast epidemics for the last two decades, using different sources from social media to search engine records. More recently, studies have addressed how the World Wide Web could be used as a valuable source for analysing the reactions of the public to outbreaks and revealing emotions and sentiment impact from certain events, notably that of pandemics.
Objective
The objective of this research is to evaluate the capability of Twitter messages (tweets) in estimating the sentiment impact of COVID-19 cases in Greece in real time as related to cases.
Methods
153,528 tweets were gathered from 18,730 Twitter users totalling 2,840,024 words for exactly one year and were examined towards two sentimental lexicons: one in English language translated into Greek (using the Vader library) and one in Greek. We then used the specific sentimental ranking included in these lexicons to track i) the positive and negative impact of COVID-19 and ii) six types of sentiments: Surprise, Disgust, Anger, Happiness, Fear and Sadness and iii) the correlations between real cases of COVID-19 and sentiments and correlations between sentiments and the volume of data.
Results
Surprise (25.32%) mainly and secondly Disgust (19.88%) were found to be the prevailing sentiments of COVID-19. The correlation coefficient (R2) for the Vader lexicon is −0.07454 related to cases and −0.,70668 to the tweets, while the other lexicon had 0.167387 and −0.93095 respectively, all measured at significance level of p < 0.01. Evidence shows that the sentiment does not correlate with the spread of COVID-19, possibly since the interest in COVID-19 declined after a certain time.}
}
@article{AGRAWAL2025252,
title = {Characterizing the pan-cancer role of exosomal miRNAs in metastasis across cancers},
journal = {Computational and Structural Biotechnology Journal},
volume = {27},
pages = {252-264},
year = {2025},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.12.025},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024004483},
author = {Piyush Agrawal and Gulden Olgun and Arashdeep Singh and Vishaka Gopalan and Sridhar Hannenhalli},
keywords = {Exosomal miRNA, Cancer metastasis, Pre-metastatic Niche, Survival Analysis, Machine Learning},
abstract = {Exosomal microRNAs (exomiRs) play a critical role in intercellular communication, especially in cancer, where they regulate key cellular processes like proliferation, angiogenesis, and metastasis, highlighting their significance as potential diagnostic and therapeutic targets. Here, we aimed to characterize the role of exomiRs, derived from seven cancer types (four cell lines and three tumors), in influencing the pre-metastatic niche (PMN). In each cancer type we extracted high confidence exomiRs (LogFC >= 2 in exosomes relative to control), their experimentally validated targets, and the enriched pathways among those targets. We then selected the top100 high-confidence targets based on their frequency of appearance in the enriched pathways. We observed significantly higher GC content in exomiRs relative to genomic background. Gene Ontology analysis revealed both general cancer processes, such as wound healing and epithelial cell proliferation, as well as cancer-specific processes, such as “angiogenesis” in the kidney and “ossification” in the lung. ExomiR targets were enriched for cancer-specific tumor suppressor genes and downregulated in PMN formed in lungs compared to normal. Motif analysis showed high inter-cancer similarity among motifs enriched in exomiRs. Our analysis recapitulated exomiRs associated with M2 macrophage differentiation and chemoresistance, such as miR-21 and miR-222–3p, regulating signaling pathways like PTEN/PI3/Akt, NF-kB, etc. Additionally, Cox regression analysis in TCGA indicated that exomiR targets are significantly associated with better overall survival of patients. Lastly, support vector machine model using exomiR targets gene expression classified responders and non-responders to therapy with an AUROC ranging from 0.72 to 0.96, higher than previously reported gene signatures.}
}
@article{PANT2025100877,
title = {A global scoping review of alternative food movements calls for food justice and justice beyond individual humans},
journal = {Global Food Security},
volume = {46},
pages = {100877},
year = {2025},
issn = {2211-9124},
doi = {https://doi.org/10.1016/j.gfs.2025.100877},
url = {https://www.sciencedirect.com/science/article/pii/S2211912425000525},
author = {Laxmi Prasad Pant and Sharada Prasad Wasti and Charoula Konstantia Nikolaou and Prajal Pradhan and Georgie Hurst and Kiran Kumari Bhattarai},
keywords = {Food justice, Topic modeling, Nutrition, Food security, Non-dualism, Food system transformation, Alternative food movements},
abstract = {This study aims to synthesise evidence on an under-researched area of food systems, the justice implications of alternative food movements (AFMs) globally across all possible contexts (e.g., geographic, socio-political, and historical). The search strategy involves two sets of keywords, representing food justice and alternative food movements, and three databases (Scopus, Web of Science, and Medline). A total of 140 peer-reviewed studies met the inclusion criteria and were subjected to topic modeling. The modeling exercise resulted in nine topics: (1) genesis of food banks; (2) second-generation food banks; (3) food aid for nutrition security; (4) food aid for health equity; (5) food policy coalitions; (6) food advocacy coalitions; (7) bringing back nature into agriculture; (8) the new garden city movement; and (9) food sovereignty. Cluster analysis grouped these topics into two themes: technical aspects of food provisioning and institutional dimensions of food system governance. Together, these themes describe how the literature addresses multiple dimensions of food justice: anthropocentric, multispecies, and planetary justice. The findings reveal that literature on AFMs focuses more on reformist, protectionist approaches within urban-centric public and private welfare systems than on emancipatory, transformative food justice movements. Our findings suggest an important gap in the literature in understanding structural barriers to food justice and how expanding the subject of food justice beyond individual humans advances emancipatory food movements toward more-than-human non-dualism.}
}
@article{LE201953,
title = {iEnhancer-5Step: Identifying enhancers using hidden information of DNA sequences via Chou's 5-step rule and word embedding},
journal = {Analytical Biochemistry},
volume = {571},
pages = {53-61},
year = {2019},
issn = {0003-2697},
doi = {https://doi.org/10.1016/j.ab.2019.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S0003269719300788},
author = {Nguyen Quoc Khanh Le and Edward Kien Yee Yapp and Quang-Thai Ho and N. Nagasundaram and Yu-Yen Ou and Hui-Yuan Yeh},
keywords = {Skip gram, Continuous bag of words, Regulatory transcription factor, Support vector machine, Two-layer classification, Sequence analysis},
abstract = {An enhancer is a short (50–1500bp) region of DNA that plays an important role in gene expression and the production of RNA and proteins. Genetic variation in enhancers has been linked to many human diseases, such as cancer, disorder or inflammatory bowel disease. Due to the importance of enhancers in genomics, the classification of enhancers has become a popular area of research in computational biology. Despite the few computational tools employed to address this problem, their resulting performance still requires improvements. In this study, we treat enhancers by the word embeddings, including sub-word information of its biological words, which then serve as features to be fed into a support vector machine algorithm to classify them. We present iEnhancer-5Step, a web server containing two-layer classifiers to identify enhancers and their strength. We are able to attain an independent test accuracy of 79% and 63.5% in the two layers, respectively. Compared to current predictors on the same dataset, our proposed method is able to yield superior performance as compared to the other methods. Moreover, this study provides a basis for further research that can enrich the field of applying natural language processing techniques in biological sequences. iEnhancer-5Step is freely accessible via http://biologydeep.com/fastenc/.}
}
@article{FECHTER201839,
title = {Integrated Process Planning and Resource Allocation for Collaborative Robot Workplace Design},
journal = {Procedia CIRP},
volume = {72},
pages = {39-44},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.179},
url = {https://www.sciencedirect.com/science/article/pii/S221282711830338X},
author = {Manuel Fechter and Carsten Seeber and Shengjian Chen},
keywords = {Human Robot Collaboration, Integrated Process, Job Shop Scheduling, Assembly Planning, Ontology, AutomationML},
abstract = {The design of human-robot-collaborative workplaces is a challenging task, whose outcome is highly dependent on the assembly planners knowledge and experience, as usually only a small fraction of the design space is considered. This often results in unappropriated workplace designs with process-related and economical drawbacks. This paper outlines an approach to a collaborative workplace design tool-chain considering different strengths of robot and human, starting from assembly group CAD model data input to an ontology based resource allocation and permutations on workplace design proposals. All steps are connected by the open exchange data format of AutomationML.}
}
@article{IDAKWO2025e41067,
title = {Geo-parsing and analysis of road traffic crash incidents for data-driven emergency response planning},
journal = {Heliyon},
volume = {11},
number = {4},
pages = {e41067},
year = {2025},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e41067},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024170983},
author = {Patricia Ojonoka Idakwo and Olubayo Adekanmbi and Anthony Soronnadi and Amos David},
abstract = {Road traffic crashes (RTCs) are a major public health concern worldwide, particularly in Nigeria, where road transport is the most common mode of transportation. This study presents the geo-parsing approach for geographic information extraction (IE) of RTC incidents from news articles. We developed two custom, spaCy-based, RTC domain-specific named entity recognition (NER) models: RTC NER Baseline and RTC NER. These models were trained on a dataset of Nigerian RTC news articles. Evaluation of the models’ performances shows that the RTC NER model outperforms the RTC NER Baseline model on both Nigerian and international test data across all three standard metrics of precision, recall and F1-score. The RTC NER model exhibits precision, recall and F1-score values of 93.63, 93.61 and 93.62, respectively, on the Nigerian test data, and 91.9, 87.88 and 89.84, respectively, on the international test data, thus showing its versatility in IE from RTC reports irrespective of country. We further applied the RTC NER model for feature extraction using geo-parsing techniques to extract RTC location details and retrieve corresponding geographical coordinates, creating a structured Nigeria RTC dataset for exploratory data analysis. Our study showcases the use of the RTC NER model in IE from RTC-related reports for analysis aimed at identifying RTC risk areas for data-driven emergency response planning.}
}
@article{CHHIBBAR20252269,
title = {Machine learning approaches enable the discovery of therapeutics across domains},
journal = {Molecular Therapy},
volume = {33},
number = {5},
pages = {2269-2278},
year = {2025},
issn = {1525-0016},
doi = {https://doi.org/10.1016/j.ymthe.2025.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1525001625002758},
author = {Prabal Chhibbar and Jishnu Das},
keywords = {interpretable machine learning, deep learning, foundation models, therapeutics},
abstract = {Multi-modal datasets have grown exponentially in the last decade. This has created an enormous demand for machine learning models that can predict complex outcomes by leveraging cellular, molecular, and humoral profiles. Corresponding inference of mechanisms can help to uncover new therapeutic targets. Here, we discuss how biological principles guide the design of predictive models and how interpretable machine learning can lead to novel mechanistic insights. We provide descriptions of multiple learning techniques and how suited they are to domain adaptations. Finally, we talk about broad learning capabilities of foundation models on large datasets and whether they can be used to provide meaningful inference about biological datasets.}
}
@article{WANG2021104464,
title = {Development and evaluation of novel ophthalmology domain-specific neural word embeddings to predict visual prognosis},
journal = {International Journal of Medical Informatics},
volume = {150},
pages = {104464},
year = {2021},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2021.104464},
url = {https://www.sciencedirect.com/science/article/pii/S1386505621000903},
author = {Sophia Wang and Benjamin Tseng and Tina Hernandez-Boussard},
keywords = {Ophthalmology, Natural language processing, Deep learning, Vision, Low, Informatics},
abstract = {Objective
To develop and evaluate novel word embeddings (WEs) specific to ophthalmology, using text corpora from published literature and electronic health records (EHR).
Materials and Methods
We trained ophthalmology-specific WEs using 121,740 PubMed abstracts and 89,282 EHR notes using word2vec continuous bag-of-words architecture. PubMed and EHR WEs were compared to general domain GloVe WEs and general biomedical domain BioWordVec embeddings using a novel ophthalmology-domain-specific 200-question analogy test and prediction of prognosis in 5547 low vision patients using EHR notes as inputs to a deep learning model.
Results
We found that many words representing important ophthalmic concepts in the EHR were missing from the general domain GloVe vocabulary, but covered in the ophthalmology abstract corpus. On ophthalmology analogy testing, PubMed WEs scored 95.0 %, outperforming EHR (86.0 %) and GloVe (91.0 %) but less than BioWordVec (99.5 %). On predicting low vision prognosis, PubMed and EHR WEs resulted in similar AUROC (0.830; 0.826), outperforming GloVe (0.778) and BioWordVec (0.784).
Conclusion
We found that using ophthalmology domain-specific WEs improved performance in ophthalmology-related clinical prediction compared to general WEs. Deep learning models using clinical notes as inputs can predict the prognosis of visually impaired patients. This work provides a framework to improve predictive models using domain-specific WEs.}
}
@article{ABEYSEKERA2023100115,
title = {Sensemaking of family enterprise business model},
journal = {Journal of Open Innovation: Technology, Market, and Complexity},
volume = {9},
number = {3},
pages = {100115},
year = {2023},
issn = {2199-8531},
doi = {https://doi.org/10.1016/j.joitmc.2023.100115},
url = {https://www.sciencedirect.com/science/article/pii/S2199853123002172},
author = {Indra Abeysekera},
keywords = {Attachment, Attachment theory, Business, Business model, Enterprise, Family business, Family enterprise, Kinship},
abstract = {This paper aims to provide an alternative perspective on the constructs of family enterprise model. The conceptual model shows the nexus of family and enterprises in four quadrants: Kin, Business; Kin, Not-business; Not-kin, Business; and, Not-kin, Not-business. This construct synthesis aims to cover family enterprises from ancient to contemporary times to contribute to a theoretically founded conceptual framework. The two constructs to form a definition of family enterprise are enterprise, and attachment. The enterprise construct consists of two dimensions, Business and Not-Business. The attachment construct comprises two dimensions, Kin and Not-Kin, which are then offered with conceptual definitions. The paper proposes several ways to appropriately measure the constructs to address research questions. To the best of my knowledge, it is the first academic paper that includes not businesses such as not-for-profit as part of family enterprise business model. It examines the concept of family as member attachment to the enterprise rather than member ancestry. Practitioners and policymakers seeking solutions to family enterprises can use the four quadrants to investigate the issues, as each quadrant contains unique characteristics.}
}
@article{CARRERASGUZMAN2021105458,
title = {An integrated safety and security analysis for cyber-physical harm scenarios},
journal = {Safety Science},
volume = {144},
pages = {105458},
year = {2021},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2021.105458},
url = {https://www.sciencedirect.com/science/article/pii/S0925753521003015},
author = {Nelson H. {Carreras Guzman} and Igor Kozine and Mary Ann Lundteigen},
keywords = {Cyber-Physical Systems (CPSs), Autonomous Systems, Cyber-Physical Harm Analysis for Safety and Security (CyPHASS), Bowtie Method},
abstract = {Increasing digitalization and autonomous solutions in physical systems promise to enhance their performance, cost-efficiency and reliability. However, the integration of novel information technologies with safety-related systems also brings new vulnerabilities and risks that challenge the traditional field of safety analysis. Particularly, cyber security threats are becoming key factors in complex accident scenarios in cyber-physical systems (CPSs), where unintentional errors and design flaws overlap with cyber security vulnerabilities that could lead to harm to humans and assets. This overlap between safety and security analysis is still a loosely defined domain without established theories and methods, leading to complications during the risk analysis of CPSs. In this paper, we first describe how the domain of safety science increasingly overlaps with security analysis. Subsequently, based on this overlapping, we illustrate and complement an integrated method for the identification of harm scenarios in CPSs. This method, coined Uncontrolled Flows of Information and Energy (UFoI-E), offers a distinct theoretical foundation rooted in accident causation models and a framework to design diagrammatic representations of CPSs during the analysis. After summarizing these features of the UFoI-E method, we present our original contribution to the method, which is a new practical toolkit for risk identification composed of an ontology of harm scenarios and a database of checklists built from lessons learned analysis and expert knowledge. Finally, we demonstrate an application of the method in an illustrative case and show representative fields for future work.}
}
@article{DELAVARA2024103803,
title = {Assessment of the quality of the text of safety standards with industrial semantic technologies},
journal = {Computer Standards & Interfaces},
volume = {88},
pages = {103803},
year = {2024},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2023.103803},
url = {https://www.sciencedirect.com/science/article/pii/S0920548923000843},
author = {Jose Luis {de la Vara} and Hector Bahamonde and Clara Ayora},
keywords = {Safety-critical system, Safety standard, Quality, RQA - Quality studio, DO-178C},
abstract = {Most safety-critical systems are subject to rigorous assurance processes to justify that the systems are dependable. These processes are typically conducted in compliance with safety standards, e.g., DO-178C for software in aerospace. This can be a prerequisite so that a system is allowed to operate. However, following these standards can be challenging in practice because of issues in their text such as imprecision, ambiguity, and inconsistency. These issues can hinder compliance, delaying it and making it more expensive, or even preventing it. As a solution, we aim to define means that aid in the identification of the issues and thus facilitate their resolution. We have developed an approach for assessment of the quality of the text of safety standards with RQA - Quality Studio, an industrial tool for requirements quality analysis with semantic technologies. The approach is based on the extraction of analysis units from a standard, on the specification and exploitation of ontologies, and on the reuse of metrics provided by RQA - Quality Studio to evaluate text quality. The approach has been applied on the DO-178C standard, assessing its text as a whole and its different main individual parts. The quality of most of the text of the standard can be regarded as high. The most frequent issues in DO-178C are the use of passive voice, of synonyms, and of imprecise modal verbs. To the best of our knowledge, this is the first study that provides a means for a broad and detailed assessment of the quality of the text of safety standards, leading to the identification of specific aspects that could be improved in the text and indicating the extent to which quality issues affect it.}
}
@article{BELLE2020181,
title = {Semiring programming: A semantic framework for generalized sum product problems},
journal = {International Journal of Approximate Reasoning},
volume = {126},
pages = {181-201},
year = {2020},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2020.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X20302073},
author = {Vaishak Belle and Luc {De Raedt}},
keywords = {Weighted model counting, Declarative languages, Semantic abstractions, Semiring frameworks},
abstract = {To solve hard problems, AI relies on a variety of disciplines such as logic, probabilistic reasoning, machine learning and mathematical programming. Although it is widely accepted that solving real-world problems requires an integration amongst these, contemporary representation methodologies offer little support for this. In an attempt to alleviate this situation, we position and motivate a new declarative programming framework in this paper. We focus on the semantical foundations in service of providing abstractions of well-known problems such as SAT, Bayesian inference, generative models, learning and convex optimization. Programs are understood in terms of first-order logic structures with semiring labels, which allows us to freely combine and integrate problems from different AI disciplines and represent non-standard problems over unbounded domains. Thus, the main thrust of this paper is to view such well-known problems through a unified lens in the hope that appropriate solver strategies (exact, approximate, portfolio or hybrid) may emerge that tackle real-world problems in a principled way.}
}
@article{HUHNT2022101790,
title = {Modeling bounded and unbounded space with polyhedra: Topology and operators for manifold cell complexes},
journal = {Advanced Engineering Informatics},
volume = {54},
pages = {101790},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101790},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622002488},
author = {Wolfgang Huhnt and Maximilian Sternal and Peter Jan Pahl},
abstract = {This paper proposes polyhedral space partitioning as an alternative to component assembly of digital models of objects with complex linear shapes. A partition is specified with a path-connected user model, where each object is bounded by n-manifolds. Faces and cells can be non-convex, multiply-connected and unbounded. The user interacts with the user model and specifies work steps. Each work step splits one edge, face or cell of the partition, or merges two neighboring objects of equal dimension. As a consequence, only a small subset of the model objects, consisting of the user specified objects and their neighbors, are affected by a work step. The user model is automatically mapped to a core model containing methods for topological relations and navigation. The topological structure is described by bundles of twin arrows of opposite direction arranged in polygons, twin facets with normal vectors of opposite direction and dihedral facet cycles at the edges. Imaginary topological objects are introduced to define unbounded cells, faces and edges. The approach guarantees that there is no overlap or gap between any pair of neighboring objects. It supports modelling of non-convex and multiply connected bounded and unbounded objects. For verification, several example models are presented and visualized. The paper ends with conclusions and an outlook to ongoing and planned further research in this field.}
}
@article{CHAPARRO2024100823,
title = {Hybrid languaging and literacy practices as cross-cultural competence in a Spanish-English Two-Way Immersion Program},
journal = {Learning, Culture and Social Interaction},
volume = {47},
pages = {100823},
year = {2024},
issn = {2210-6561},
doi = {https://doi.org/10.1016/j.lcsi.2024.100823},
url = {https://www.sciencedirect.com/science/article/pii/S221065612400031X},
author = {Sofía E. Chaparro},
keywords = {Bilingual education, Two-way immersion programs, Hybrid languaging, Hybrid literacy practices, Intercultural understanding, African American students, cross-cultural competence},
abstract = {How do children from different cultural and linguistic backgrounds interact with one another within a bilingual Spanish-medium classroom? That is the question this analysis sought to answer. As part of an ethnographic discourse-analytic study of a new two-way immersion bilingual program within an urban school, I analyze one particular literacy event amongst three young readers from different linguistic and ethno-racial positionings and varying knowledge of Spanish. This literacy event illustrates how students expand their communicative repertoires to include a variety of Englishes and Spanishes as they experiment with different ways of communicating and expressing themselves. In this event, English, Spanish, Spanglish, and African American English were spoken and used by all three participants as they reacted to the text. This analysis illustrates how students show alignment towards each other through their hybrid languaging and literacy practices, and in this way, learn more than simply language from one another, children learn subtle ways of becoming inter-culturally competent through their languaging.}
}
@article{LUNDBERG2025103648,
title = {Interdisciplinary futures? A conceptual approach},
journal = {Futures},
volume = {172},
pages = {103648},
year = {2025},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2025.103648},
url = {https://www.sciencedirect.com/science/article/pii/S0016328725001107},
author = {Robert Lundberg and Sarah Pink and Zane Pinyon},
keywords = {Interdisciplinarity, Futures concepts, Futures theory, Futures conceptual framework},
abstract = {In this article we argue that to adequately investigate futures across disciplines a genuinely interdisciplinary agenda is required, through which futures might be collectively conceived (if not agreed on) and that the most urgent first step is to establish interdisciplinary common ground. We examine how the use of concepts in scholarship can be mobilised for this purpose, and we propose engaging interdisciplinary concepts in futures research. In doing so we draw on our ethnographic review of existing futures-focused academic literatures. Our research revealed an abundance of diverse futures theory, however it suggested greater possibility to create spaces for interdisciplinary convergence, debate and potential collaboration at the conceptual level. We propose a conceptual framework composed of three layers or categories: ontological framing concepts, epistemological encountering concepts and phenomenological experiential concepts. We explore the relations within and between these layers of concepts to propose that our framework might be used in two ways: to generate collaboration focused on the concepts; and to track the possible consequences of alignments between concepts from different categories.}
}
@article{KOVARI2025e42077,
title = {Explainable AI chatbots towards XAI ChatGPT: A review},
journal = {Heliyon},
volume = {11},
number = {2},
pages = {e42077},
year = {2025},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2025.e42077},
url = {https://www.sciencedirect.com/science/article/pii/S2405844025004578},
author = {Attila Kovari},
keywords = {Explainable AI (XAI), ChatGPT, AI chatbots, Natural language processing (NLP), Transparency, Controllable AI},
abstract = {Advances in artificial intelligence (AI) have had a major impact on natural language processing (NLP), even more so with the emergence of large-scale language models like ChatGPT. This paper aims to provide a critical review of explainable AI (XAI) methodologies for AI chatbots, with a particular focus on ChatGPT. Its main objectives are to investigate the applied methods that improve the explainability of AI chatbots, identify the challenges and limitations within them, and explore future research directions. Such goals emphasize the need for transparency and interpretability of AI systems to build trust with users and allow for accountability. While integrating such interdisciplinary methods, such as hybrid methods combining knowledge graphs with ChatGPT, enhancing explainability, they also highlight industry needs for explainability and user-centred design. This will be followed by a discussion of the balance between explainability and performance, then the role of human judgement, and finally the future of verifiable AI. These are the avenues through which insights can be used to guide the development of transparent, reliable and efficient AI chatbots.}
}
@incollection{LOVERING2024365,
title = {Chapter 22 - Gene annotation: Resources for the heart},
editor = {Dhavendra Kumar and Arthur A.M. Wilde and Perry Mark Elliott},
booktitle = {Genomic and Molecular Cardiovascular Medicine},
publisher = {Academic Press},
pages = {365-375},
year = {2024},
series = {Genomic and Precision Medicine in Clinical Practice},
isbn = {978-0-12-822951-4},
doi = {https://doi.org/10.1016/B978-0-12-822951-4.00022-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128229514000229},
author = {Ruth C. Lovering},
keywords = {Biocuration, Cardiovascular, Disease ontology, Gene annotation, Gene ontology, Heart, High-throughput analysis, Human phenotype ontology},
abstract = {Gene annotations are essential to facilitate analyses and interpretation of high-throughput biological datasets. Many different gene annotation resources have been developed over the past 2decades. These are being curated by a wide range of different groups, based at scientific institutions around the world. The UCL Functional Gene Annotation team focusses on Gene Ontology (GO) annotation of human gene products. Our group has led initiatives to systematically annotate proteins and microRNAs across specific biomedical fields. Funding by the British Heart Foundation has enabled our group to contribute to the GO vocabulary to describe a wide range of cardiovascular processes, from plasma lipoprotein particle assembly to heart development. This chapter provides an overview of GO, the Human Phenotype and Disease Ontologies and explain how biocurators and ontology developers are improving cardiovascular gene annotations. These improvements will benefit those seeking to interpret a wide range of genomic, transcriptomic, proteomic and metabolomic datasets.}
}
@article{GRUENDNER2022,
title = {The Architecture of a Feasibility Query Portal for Distributed COVID-19 Fast Healthcare Interoperability Resources (FHIR) Patient Data Repositories: Design and Implementation Study},
journal = {JMIR Medical Informatics},
volume = {10},
number = {5},
year = {2022},
issn = {2291-9694},
doi = {https://doi.org/10.2196/36709},
url = {https://www.sciencedirect.com/science/article/pii/S2291969422001545},
author = {Julian Gruendner and Noemi Deppenwiese and Michael Folz and Thomas Köhler and Björn Kroll and Hans-Ulrich Prokosch and Lorenz Rosenau and Mathias Rühle and Marc-Anton Scheidl and Christina Schüttler and Brita Sedlmayr and Alexander Twrdik and Alexander Kiel and Raphael W Majeed},
keywords = {federated feasibility queries, FHIR, distributed analysis, feasibility study, HL7 FHIR, FHIR Search, CQL, COVID-19, pandemic, health data, query, patient data, consensus data set, medical informatics, Fast Healthcare Interoperability Resources},
abstract = {Background
An essential step in any medical research project after identifying the research question is to determine if there are sufficient patients available for a study and where to find them. Pursuing digital feasibility queries on available patient data registries has proven to be an excellent way of reusing existing real-world data sources. To support multicentric research, these feasibility queries should be designed and implemented to run across multiple sites and securely access local data. Working across hospitals usually involves working with different data formats and vocabularies. Recently, the Fast Healthcare Interoperability Resources (FHIR) standard was developed by Health Level Seven to address this concern and describe patient data in a standardized format. The Medical Informatics Initiative in Germany has committed to this standard and created data integration centers, which convert existing data into the FHIR format at each hospital. This partially solves the interoperability problem; however, a distributed feasibility query platform for the FHIR standard is still missing.
Objective
This study described the design and implementation of the components involved in creating a cross-hospital feasibility query platform for researchers based on FHIR resources. This effort was part of a large COVID-19 data exchange platform and was designed to be scalable for a broad range of patient data.
Methods
We analyzed and designed the abstract components necessary for a distributed feasibility query. This included a user interface for creating the query, backend with an ontology and terminology service, middleware for query distribution, and FHIR feasibility query execution service.
Results
We implemented the components described in the Methods section. The resulting solution was distributed to 33 German university hospitals. The functionality of the comprehensive network infrastructure was demonstrated using a test data set based on the German Corona Consensus Data Set. A performance test using specifically created synthetic data revealed the applicability of our solution to data sets containing millions of FHIR resources. The solution can be easily deployed across hospitals and supports feasibility queries, combining multiple inclusion and exclusion criteria using standard Health Level Seven query languages such as Clinical Quality Language and FHIR Search. Developing a platform based on multiple microservices allowed us to create an extendable platform and support multiple Health Level Seven query languages and middleware components to allow integration with future directions of the Medical Informatics Initiative.
Conclusions
We designed and implemented a feasibility platform for distributed feasibility queries, which works directly on FHIR-formatted data and distributed it across 33 university hospitals in Germany. We showed that developing a feasibility platform directly on the FHIR standard is feasible.}
}
@article{GAROZZO2021102736,
title = {Knowledge-based generative adversarial networks for scene understanding in Cultural Heritage},
journal = {Journal of Archaeological Science: Reports},
volume = {35},
pages = {102736},
year = {2021},
issn = {2352-409X},
doi = {https://doi.org/10.1016/j.jasrep.2020.102736},
url = {https://www.sciencedirect.com/science/article/pii/S2352409X20305277},
author = {Raissa Garozzo and Cettina Santagati and Concetto Spampinato and Giuseppe Vecchio},
keywords = {Digital Cultural Heritage, Semantic Data Modeling, Information retrieval, Image classification, Digital documentation, Generative Adversarial Networks, Ontology-driven deep learning},
abstract = {This study shows the results of an interdisciplinary research aimed at devising artificial intelligence methods to support data understanding in Cultural Heritage. The objective is to create automated methods for automatically classifying and successively retrieving photographic data leveraging the current breed of AI methods based on Deep Learning paradigm. In this work, the lack of images to be used for the training of the AI system is addressed by testing an approach based on Generative Adversarial Networks (GANs) for automatically synthesizing unrealistic photos, which, in turn, can be used for training image classification and retrieval systems. Specifically, we propose a method to drive the generation of realistic classical order images using GAN approaches anchored to semantic ontology domain representation. More specifically, the proposed AI leverages the advantages of knowledge-based and data-driven methods and foresees a level image generation: the first one that synthesizes isolated objects corresponding to each ontology concept and the second one that, instead, combines the generated objects according to the spatial information provided by the ontology in order to generate realistic scenes. The resulting images can then be employed to train automated classifiers and more reliable retrieval methods by minimizing the efforts usually required by human operators to manually annotate data. In addition, the proposed generative strategy allows for a deeper understanding of cultural heritage visual data enabling the possibility to enhance design capabilities in simulation contexts.}
}
@article{WANG2023105028,
title = {A method for assisting the accident consequence prediction and cause investigation in petrochemical industries based on natural language processing technology},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {83},
pages = {105028},
year = {2023},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2023.105028},
url = {https://www.sciencedirect.com/science/article/pii/S095042302300058X},
author = {Feng Wang and Wunan Gu and Yan Bai and Jing Bian},
keywords = {HAZOP, LDA, Data mining, Association analysis},
abstract = {Risk analysis for production processes in the petrochemical industry is an important procedure for consequence prediction and investigation of accidents. The analyzer must grasp the correlations between the possible causes and consequences. From the potential cause and effect found in risk analysis reports, complete clarification should be obtained. Therefore, this study presents a method for assisting accident consequence prediction and investigation in the petrochemical industry based on risk analysis reports using natural language processing technology. First, a hazard and operability (HAZOP) historical data table is established by filling over 7200 HAZOP analysis data points. Both the causes and consequences in the table are classified into 20 categories each using the Latent Dirichlet Allocation (LDA) models. The LDA clustering results are assigned classification for the cause and consequence topics to the cause and consequences of the HAZOP analysis data. Based on part-of-speech (POS) tagging, all the words in each cause and consequence record are divided into subject and action words. Next, the word combinations of subject and action words with a higher occurrence are considered the key phrases for describing and representing the corresponding cause and consequence topic classifications. The Apriori algorithm is used to determine the frequent item sets, acquire the association rules, and calculate the association degree to obtain the sort order; it can highlight general trends in relational cause and consequence topics. According to the results, the most likely cause of the consequence and the most likely consequence that the cause may lead to are identified. Finally, a visual interface is developed to present the data for the consequence prediction and cause investigation of accidents. The results reveal that the quantity and quality of historic data are important factors that may influence the results. This method can contribute to predicting the accident evolution trend of an abnormal situation, taking preventive measures in advance, improving the accuracy of early warning, and supporting emergency response measures.}
}
@article{ILYAS2024101029,
title = {Rethinking entrepreneurship and management education for engineering students: The appropriateness of design thinking},
journal = {The International Journal of Management Education},
volume = {22},
number = {3},
pages = {101029},
year = {2024},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2024.101029},
url = {https://www.sciencedirect.com/science/article/pii/S1472811724001009},
author = {Imran M. Ilyas and Juha Kansikas and Alain Fayolle},
keywords = {Educational needs, Science and technology entrepreneurship education, Design thinking, Teaching model, Lean methodology},
abstract = {The study argues that the educational needs of engineering students for entrepreneurship and managerial education are specific and evolving over time toward a set of skills and knowledge needed in digital and dynamic world. Existing research largely ignored the distinct and evolving nature of these educational needs and their implications for entrepreneurship and managerial education of engineering students. Using design thinking and teaching model literature, we proposed teaching model framework and derived propositions from conceptual arguments to address these educational needs effectively. The proposed conceptual teaching model framework elaborates on the incorporation of cognitive acts of design in various aspects at ontological, didactical, and contextual levels. The framework views education as a process of co-construction, centered on students, where the role of the teacher is similar to that of a coach. Students work in teams and practice the cognitive acts of design that lead to the development of interpersonal, entrepreneurial, and managerial skills. For this purpose, open-ended questioning, real-life customer problems, design thinking methodology, and lean methodology are proposed as effective content and pedagogies to promote the entrepreneurial behaviors required in the current industrial scenario.}
}
@article{ABBAS2021100007,
title = {Modelling and exploiting taxonomic knowledge for developing mobile learning systems to enhance children’s structural and functional categorization},
journal = {Computers and Education: Artificial Intelligence},
volume = {2},
pages = {100007},
year = {2021},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2021.100007},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X21000011},
author = {MuhammadAzeem Abbas and Gwo-Jen Hwang and Saheed Ajayi and Ghulam Mustafa and Muhammad Bilal},
keywords = {Taxonomic knowledge, Ontology, Mobile learning, Pre-school education, Structural and functional categorization},
abstract = {The recent decade has seen increased attention focused on understanding category formation–a cognition ability of preschool aged children. Children organize their knowledge about real-world objects by categorizing them under some common properties or functions. The advancement and popularity of mobile devices with touch screens provide a good opportunity for young children to learn and practice. In this study, an approach that models structural and functional categorization knowledge for developing mobile learning systems with dynamic categorization exemplars is proposed. A mobile application was implemented based on the proposed model for pre-schoolers (aged 3–6 years). Moreover, the quasi-experimental pre-test and post-test method was used to evaluate the effectiveness of the proposed knowledge-based application in terms of categorization ability learning. The results show that the children who experienced dynamically created categorization exemplars from the modelled knowledge achieved increased scores compared to those who followed the traditional teaching using books and worksheets.}
}
@article{KEENA2025106050,
title = {Housing Passport knowledge graph: Promoting a circular economy in urban residential buildings},
journal = {Sustainable Cities and Society},
volume = {119},
pages = {106050},
year = {2025},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2024.106050},
url = {https://www.sciencedirect.com/science/article/pii/S2210670724008722},
author = {Naomi Keena and Avi Friedman and Mojtaba Parsaee and Madeline Mussio and Ava Klein and Martha Pomasonco-Alvis and Paulo Pinheiro},
keywords = {Data Intelligence, Semantic Technology, Circular Economy, Built Environment, Housing},
abstract = {This paper introduces the Housing Passport knowledge graph (HPKG) as a novel digital standardization framework with a robust semantic data infrastructure to promote a circular economy in the home-building industry. Unstandardized and dispersed housing data impedes a comprehensive assessment of housing stock characteristics and life cycle impacts, hindering the implementation of circular economy principles. The HPKG addresses this challenge by providing (1) a standardized framework for integrated analysis of residential buildings’ affordability and circularity across various spatiotemporal scales and socioeconomic contexts, and (2) a scalable semantic infrastructure using web ontologies that enhances the sharability, searchability, readability, and interoperability of housing-related data. A case study involving five Canadian cities demonstrates the HPKG's effectiveness in semantically linking and standardizing approximately 62 million data points representing over 1.2 million residential buildings. The results show how the HPKG enables a multi-scale integrated assessment of Canadian housing stock, focusing on affordability, energy efficiency, and environmental footprints. As a key conclusion, the HPKG supports informed decisions regarding housing stock by enabling the exploration of circular economy scenarios that prioritize the reuse and recycling of residential building materials. The HPKG empowers stakeholders to develop residential typologies that promote affordability, circularity, and sustainability across diverse socioeconomic contexts.}
}
@article{ZHANG2018102,
title = {Adapted TextRank for Term Extraction: A Generic Method of Improving Automatic Term Extraction Algorithms},
journal = {Procedia Computer Science},
volume = {137},
pages = {102-108},
year = {2018},
note = {Proceedings of the 14th International Conference on Semantic Systems 10th – 13th of September 2018 Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316144},
author = {Ziqi Zhang and Johann Petrak and Diana Maynard},
keywords = {automatic term extraction, NLP, terminology, ontology engineering},
abstract = {Automatic Term Extraction is a fundamental Natural Language Processing task often used in many knowledge acquisition processes. It is a challenging NLP task due to its high domain dependence: no existing methods can consistently outperform others in all domains, and good ATE is very much an unsolved problem. We propose a generic method for improving the ranking of terms extracted by a potentially wide range of existing ATE methods. We re-design the well-known TextRank algorithm to work at corpus level, using easily obtainable domain resources in the form of seed words or phrases, to compute a score for a word from the target dataset. This is used to refine a candidate term’s score computed by an existing ATE method, potentially improving the ranking of real terms to be selected for tasks such as ontology engineering. Evaluation shows consistent improvement on 10 state of the art ATE methods by up to 25 percentage points in average precision measured at top-ranked K candidates.}
}
@article{LI2023102229,
title = {Design-Oriented product fault knowledge graph with frequency weight based on maintenance text},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102229},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102229},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623003579},
author = {Siqi Li and Junfeng Wang and Jin Rong},
keywords = {Design-Oriented knowledge graph, Design support, Relationship mining, Maintenance text, Knowledge recommendation},
abstract = {During the product operation and maintenance stage, the maintenance process of fault parts will be recorded as textual data. These texts can provide designers with valuable insights into fault knowledge, which can guide the improvement and optimization of next-generation products design. The design-oriented product fault knowledge graph with frequency weight (DPFKG) is proposed for mining textual fault knowledge to support design. First, the DPFKG is constructed using maintenance text, including ontology modeling, knowledge unit extraction and graph visualization. Second, the relationships mining method between graph nodes is proposed based on the BiLSTM + CRF model and Levenshtein distance. Third, a potential risk calculation method for to-be-designed parts under different design conditions is proposed. It can rank the knowledge to be recommended, which can help to provide necessary knowledge to designers. Finally, the validation of the method is carried out using the Tunnel Boring Machine as an example. DPFKG supports the product closed-loop iterative design with feedback of maintenance text to design stage.}
}
@article{LIU2024114197,
title = {Developing a goal-driven data integration framework for effective data analytics},
journal = {Decision Support Systems},
volume = {180},
pages = {114197},
year = {2024},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2024.114197},
url = {https://www.sciencedirect.com/science/article/pii/S0167923624000307},
author = {Dapeng Liu and Victoria Y. Yoon},
keywords = {Data interoperability, Data integration, Data management, Ontology, Open data},
abstract = {Data integration plays a crucial role in business intelligence, aiding decision-makers by consolidating data from heterogeneous sources to provide deep insights into business operations and performance. In the big data era, automated data integration solutions need to process high volumes of disparate data robustly and seamlessly for various analytical needs or operational actions. Existing data integration solutions exhibit limited capabilities for capturing and modeling users' needs to execute on-demand data integration. This study, underpinned by affordance theory and the goal definition principles from the Goal-Question-Metric approach, designs and instantiates a goal-driven data integration framework for data analytics. The proposed innovative design automates data integration for non-technical data users. Specifically, it demonstrates how to elicit and ontologize users' data-analytic goals and addresses semantic heterogeneity, thereby recognizing goal-relevant datasets. In a structured evaluation using the context of counter-terrorism analytics, our design artifact shows promising performance in capturing diverse and dynamic user goals for data analytics and in generating integrated data tailored to these goals. Our research establishes a theoretical framework to guide future scholars and practitioners in building smart, goal-driven data integration.}
}
@article{TSALAPATI2021113550,
title = {Enhancing polymer electrolyte membrane fuel cell system diagnostics through semantic modelling},
journal = {Expert Systems with Applications},
volume = {163},
pages = {113550},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113550},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420303742},
author = {E. Tsalapati and C.W.D. Johnson and T.W. Jackson and L. Jackson and D. Low and B. Davies and L. Mao and A. West},
keywords = {PEMFC, Semantic technologies, System monitoring, Ontology Based Data Access, Diagnostic system},
abstract = {Polymer electrolyte membrane fuel cells (PEMFC) are a promising technology for economic and environmentally friendly energy production. However, they haven’t reached their full potential in the market yet as only few reliable PEMFC systems have successfully passed the prototyping face. A drawback of the current diagnostic tools is that only a select few are of high genericity, reliability and can perform efficiently on-line at the same time. Furthermore, there is only limited research identifying both PEMFC stack faults and ancillary system faults simultaneously. While none of the existing tools can be interrogated by the end-user. In this research, we develop novel artificial intelligence-based technologies to overcome these existing barriers, i.e., (i) a semantically enriched integrating schema (ontology) of the overall operation and structure of the PEMFC that allows automatic inference engines to automatically deduce fault detection; (ii) a knowledge-based, light-weight, on-line fuel cell system diagnosis (FuCSyDi) platform. FuCSyDi detects and provides the location of failures by considering only the data from the reliable sensors. Additionally, it provides the reasons underpinning any forthcoming failures and enables the end-user to interrogate the platform for further information regarding its operation and structure. Our platform is validated by performing tests against common automotive stress conditions. This innovative approach enhances the reliability of the fuel cell system diagnosis and, hence, its lifetime performance.}
}
@incollection{AVASTHI2025273,
title = {Chapter 13 - Transformer models for Topic Extraction from narratives and biomedical text analysis},
editor = {Sujata Dash and Subhendu Kumar Pani and Wellington Pinheiro Dos Santos and Jake Y. Chen},
booktitle = {Mining Biomedical Text, Images and Visual Features for Information Retrieval},
publisher = {Academic Press},
pages = {273-286},
year = {2025},
isbn = {978-0-443-15452-2},
doi = {https://doi.org/10.1016/B978-0-443-15452-2.00013-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443154522000133},
author = {Sandhya Avasthi and Tanushree Sanwal and Suman Lata Tripathi and Meenakshi Tyagi},
keywords = {Electronic health records, Machine learning, Natural language processing, Patient narratives, Topic extraction, Transformers model},
abstract = {Language can tell you a lot about a person and their culture in general. Also, the digitization of information has made more textual data more accessible. But, on their own, these numbers are just numbers. Transformer, an example of a deep learning (DL) model, has exhibited exceptional performance in analyzing natural language, including clinical content. Transformers can handle the complex syntactic and semantic patterns inherent in medical languages, such as medical terms, abbreviations, and jargon, making them ideally adapted for the analysis of clinical content. Transformer models summarize clinical narratives so healthcare practitioners can quickly find crucial information and insights without reading lengthy documentation. The model’s text patterns and correlations allow it to automatically construct these summaries, maintaining a consistent focus on providing high-quality care services to patients and other stakeholders. Over the previous few decades, the healthcare sector has undergone several drastic upheavals, propelling it to new heights with some major developments. This chapter provides a detailed overview of the biomedical domain, Healthcare 4.0, transformer models to provide clinical text and narrative analysis.}
}
@article{CHIPOFYA2020105005,
title = {Local Domain Models for Land Tenure Documentation and their Interpretation into the LADM},
journal = {Land Use Policy},
volume = {99},
pages = {105005},
year = {2020},
issn = {0264-8377},
doi = {https://doi.org/10.1016/j.landusepol.2020.105005},
url = {https://www.sciencedirect.com/science/article/pii/S0264837720309522},
author = {Malumbo Chipofya and Mina Karamesouti and Carl Schultz and Angela Schwering},
keywords = {Indigenous knowledge representation, Customary land tenures, Conditional land rights, Land administration, Sketch maps},
abstract = {Abstract
With an estimated 50% of global land held, used, or otherwise managed by communities, interfacing indigenous, customary, and informal land tenure systems with official land administration systems is critical to achieving universal land tenure security at a global scale. The complexity and organic nature of these tenure systems, however, makes their modelling and documentation within standard, generic land administration systems extremely difficult. This paper presents a model that loosely integrates a Local Domain Model (LDM) developed for a Maasai community in Kenya with the Land Administration Domain Model (LADM). The LDM is an ontological schema which captures local knowledge in a systematic, formal way that is directly or indirectly relevant to land administration. The integration with LADM is achieved through an ontological schema called the Adaptor Model. The concept of conditional RRR (Rights, Restrictions, Responsibilities) is introduced within the Adaptor Model to express the dynamics of social tenures. The three domain models LDM, LADM, Adaptor Model are used in the community-based land tenure recording tool SmartSkeMa. Four implementation examples demonstrate how the case-specific LDM extends the range of concepts representable in LADM in order to meet land administration needs from the local community’s perspective. A panel of land administration experts found the LDM model and the functionality of the Adaptor Model to be fit-for-purpose for the Kenyan case and to be addressing an important gap in the land administration tools landscape.}
}
@article{COTSAFTIS2023101199,
title = {Designing conditions for coexistence},
journal = {Design Studies},
volume = {87},
pages = {101199},
year = {2023},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2023.101199},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X23000406},
author = {Olivier Cotsaftis and Nina Williams and Gyungju Chyon and John Sadar and Daphne Mohajer {Va Pesaran} and Samuel Wines and Sarah Naarden}
}
@article{THEODOSIOU20243247,
title = {BioTextQuest v2.0: An evolved tool for biomedical literature mining and concept discovery},
journal = {Computational and Structural Biotechnology Journal},
volume = {23},
pages = {3247-3253},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024002757},
author = {Theodosios Theodosiou and Konstantinos Vrettos and Ismini Baltsavia and Fotis Baltoumas and Nikolas Papanikolaou and Andreas Ν. Antonakis and Dimitrios Mossialos and Christos A. Ouzounis and Vasilis J. Promponas and Makrina Karaglani and Ekaterini Chatzaki and Sven Brandau and Georgios A. Pavlopoulos and Evangelos Andreakos and Ioannis Iliopoulos},
keywords = {Biomedical literature mining, Concept discovery},
abstract = {The process of navigating through the landscape of biomedical literature and performing searches or combining them with bioinformatics analyses can be daunting, considering the exponential growth of scientific corpora and the plethora of tools designed to mine PubMed(®) and related repositories. Herein, we present BioTextQuest v2.0, a tool for biomedical literature mining. BioTextQuest v2.0 is an open-source online web portal for document clustering based on sets of selected biomedical terms, offering efficient management of information derived from PubMed abstracts. Employing established machine learning algorithms, the tool facilitates document clustering while allowing users to customize the analysis by selecting terms of interest. BioTextQuest v2.0 streamlines the process of uncovering valuable insights from biomedical research articles, serving as an agent that connects the identification of key terms like genes/proteins, diseases, chemicals, Gene Ontology (GO) terms, functions, and others through named entity recognition, and their application in biological research. Instead of manually sifting through articles, researchers can enter their PubMed-like query and receive extracted information in two user-friendly formats, tables and word clouds, simplifying the comprehension of key findings. The latest update of BioTextQuest leverages the EXTRACT named entity recognition tagger, enhancing its ability to pinpoint various biological entities within text. BioTextQuest v2.0 acts as a research assistant, significantly reducing the time and effort required for researchers to identify and present relevant information from the biomedical literature.}
}
@article{ZHOU2021102775,
title = {Integrating computer vision and traffic modeling for near-real-time signal timing optimization of multiple intersections},
journal = {Sustainable Cities and Society},
volume = {68},
pages = {102775},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.102775},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721000676},
author = {Shenghua Zhou and S. Thomas Ng and Yifan Yang and J. Frank Xu},
keywords = {Computer vision, Transfer learning, Ontology, Traffic modeling, Signal timing},
abstract = {Adaptive signal timing optimizations can improve the efficiency of road networks and reduce the emissions of pollutants, but most of the current studies still rely on simplified analytical methods to depict complex road transport systems and focus on optimizing traffic signals at an isolated intersection. A framework that integrates computer vision and traffic modeling is proposed to link the real-world transport systems and operable virtual traffic models for the signal timing optimization at multiple intersections. The integrative framework consists of six main steps, including configuring real-time video sources, conducting transfer-learning to develop the vehicle detector, comparing and selecting vehicle trackers, collecting traffic parameters by referring to the CV-TM ontology, establishing and running the traffic model, and operating simulation-based optimizations. The proposed integrative framework is demonstrated through a case study of the signal timing optimization at multi-intersections in a real-world road network. Three critical information items including the traffic volumes, vehicle compositions, and vehicles’ turning ratios are derived from real-time surveillance videos, and the extracted information is then automatically incorporated into TM to optimize the signal timings of interconnected intersections in a near-real-time manner. In comparison with the original signal scheme, the optimized one can reduce 14.2 % of average vehicle delays, 18.9 % of vehicle stops, 9.1 % of average travel time, and 2.3 % of pollutant emissions in this specific case. The results indicate that synchronously optimizing signal timings at multiple intersections increase not only the transportation efficiency but also the environmental friendliness of road transport systems. The proposed CV-TM integration framework is demonstrated to be a promising way for conducting near-real-time signal timing optimizations in intricate traffic scenes instead of at isolated intersections, helping decision-makers to promptly respond to the time-varying traffic conditions during various real-world events, and facilitating the transportation systems and cities to achieve sustainable development goals.}
}
@article{BISWAS2024108769,
title = {scHD4E: Novel ensemble learning-based differential expression analysis method for single-cell RNA-sequencing data},
journal = {Computers in Biology and Medicine},
volume = {178},
pages = {108769},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108769},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524008540},
author = {Biplab Biswas and Nishith Kumar and Masahiro Sugimoto and Md Aminul Hoque},
keywords = {scRNA-seq, scDEA, Differential expression, scHD4E},
abstract = {Differential expression (DE) analysis between cell types for scRNA-seq data by capturing its complicated features is crucial. Recently, different methods have been developed for targeting the scRNA-seq data analysis based on different modeling frameworks, assumptions, strategies and test statistic in considering various data features. The scDEA is an ensemble learning-based DE analysis method developed recently, yielding p-values using Lancaster's combination, generated by 12 individual DE analysis methods, and producing more accurate and stable results than individual methods. The objective of our study is to propose a new ensemble learning-based DE analysis method, scHD4E, using top performers in only 4 separate methods. The top performer 4 methods have been selected through an evaluation process using six real scRNA-seq data sets. We conducted comprehensive experiments for five experimental data sets to evaluate our proposed method based on the sample size effects, batch effects, type I error control, gene ontology enrichment analysis, runtime, identified matched DE genes, and semantic similarity measurement between methods. We also perform similar analyses (except the last 3 terms) and compute performance measures like accuracy, F1 score, Mathew's correlation coefficient etc. for a simulated data set. The results show that scHD4E is performs better than all the individual and scDEA methods in all the above perspectives. We expect that scHD4E will serve the modern data scientists for detecting the DEGs in scRNA-seq data analysis. To implement our proposed method, a Github R package scHD4E and its shiny application has been developed, and available in the following links: https://github.com/bbiswas1989/scHD4E and https://github.com/bbiswas1989/scHD4E-Shiny.}
}
@article{TRAN2021101120,
title = {Variational model for low-resource natural language generation in spoken dialogue systems},
journal = {Computer Speech & Language},
volume = {65},
pages = {101120},
year = {2021},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2020.101120},
url = {https://www.sciencedirect.com/science/article/pii/S088523082030053X},
author = {Van-Khanh Tran and Le-Minh Nguyen},
keywords = {Neural language generation, Domain adaptation, Low-resource data, Variational autoencoder, Deconvolutional neural network, CNN, RNN, LSTM},
abstract = {Natural Language Generation (NLG) plays a critical role in Spoken Dialogue Systems (SDSs), aims at converting a meaning representation into natural language utterances. Recent deep learning-based generators have shown improving results irrespective of providing sufficient annotated data. Nevertheless, how to build a generator that can effectively utilize as much of knowledge from a low-resource setting data is a crucial issue for NLG in SDSs. This paper presents a variational-based NLG framework to tackle the NLG problem of having limited annotated data in two scenarios, domain adaptation and low-resource in-domain training data. Based on this framework, we propose a novel adversarial domain adaptation NLG taclking the former issue, while the latter issue is also handled by a second proposed dual variational model. We extensively conducted the experiments on four different domains in a variety of training scenarios, in which the experimental results show that the proposed methods not only outperform previous methods when having sufficient training dataset but also show its ability to work acceptably well when there is a small amount of in-domain data or adapt quickly to a new domain with only a low-resource target domain data.}
}
@article{QIAN2021502,
title = {Assembly sequence planning method based on knowledge and ontostep},
journal = {Procedia CIRP},
volume = {97},
pages = {502-507},
year = {2021},
note = {8th CIRP Conference of Assembly Technology and Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.05.266},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120314980},
author = {Jiahui Qian and Zhijing Zhang and Chao Shao and Hanqing Gong and Dongmei Liu},
keywords = {Intelligent assembly, Assembly sequence planning, Ontology, Expert knowledge, OntoSTEP},
abstract = {In today's manufacturing environment, the technology of intelligent assembly has developed rapidly, and the technique level of intelligent assembly sequence planning has been improved significantly. However, the current assembly sequence planning always focuses on modern heuristic algorithms with single goals. Although these methods increase the automation level, they still cannot be applied to actual production because they ignore artificial experience and mature knowledge, which are of great value. This paper comprehensively considers the five key factors that affect the assembly sequence, proposes an assembly sequence planning method based on experience and knowledge, and realizes the sequence automatic reasoning by constructing the corresponding domain ontology. In addition, the OntoSTEP tool is used to complete the ontology mapping of geometric model's information, so as to verify the feasibility of the assembly sequence. The method has guiding significance for actual production, such as assisting process designer do assembly sequence planning effectively.}
}
@article{MEYERS202213,
title = {Knowledge Graphs in Digital Twins for Manufacturing - Lessons Learned from an Industrial Case at Atlas Copco Airpower},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {13-18},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.361},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322016263},
author = {Bart Meyers and Johan {Van Noten} and Pieter Lietaert and Bavo Tielemans and Hristo Hristov and Davy Maes and Klaas Gadeyne},
keywords = {Intelligent manufacturing systems, Modeling of manufacturing operations, Quality assurance, maintenance},
abstract = {In this paper we introduce an architecture for a cognitive digital twin that uses an ontology-based knowledge graph to improve the analysis of manufacturing systems. The architecture is evaluated in a use case at Atlas Copco Airpower, revolving around an adaptive measurement strategy for quality control. We report on the requirements, outcomes, and the resulting challenges of using a cognitive digital twin, and provide a roadmap for future research.}
}
@article{CHEN2025103024,
title = {A review of medical text analysis: Theory and practice},
journal = {Information Fusion},
volume = {119},
pages = {103024},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103024},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525000971},
author = {Yani Chen and Chunwu Zhang and Ruibin Bai and Tengfang Sun and Weiping Ding and Ruili Wang},
keywords = {Medical text analysis, Natural language processing, Systematic review},
abstract = {Medical data analysis has emerged as an important driving force for smart healthcare with applications ranging from disease analysis to triage, diagnosis, and treatment. Text data plays a crucial role in providing contexts and details that other data types cannot capture alone, making its analysis an indispensable resource in medical research. Natural language processing, a key technology for analyzing and interpreting text, is essential for extracting meaningful insights from medical text data. This systematic review explores the analysis of text data in medicine, focusing on the applications of natural language processing methods. We retrieved a total of 4,784 publications from four databases. After applying rigorous exclusion criteria, 192 relevant publications are selected for in-depth analysis. These studies are evaluated from five critical perspectives: emerging trends of medical text analysis, commonly employed methodologies, major data sources, research topics, and applications in real-world problem-solving. Our analysis provides a comprehensive overview of the current state of medical text analysis, highlighting its advantages, limitations, and future potential. Finally, we identify key challenges and outline future research directions for advancing medical text analysis.}
}
@article{MIDINGOYI2021105055,
title = {Crop2ML: An open-source multi-language modeling framework for the exchange and reuse of crop model components},
journal = {Environmental Modelling & Software},
volume = {142},
pages = {105055},
year = {2021},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2021.105055},
url = {https://www.sciencedirect.com/science/article/pii/S1364815221000980},
author = {Cyrille Ahmed Midingoyi and Christophe Pradal and Andreas Enders and Davide Fumagalli and Hélène Raynal and Marcello Donatelli and Ioannis N. Athanasiadis and Cheryl Porter and Gerrit Hoogenboom and Dean Holzworth and Frédérick Garcia and Peter Thorburn and Pierre Martre},
keywords = {Crop model, Crop2ML, Component-based software, Model exchange and reuse},
abstract = {Process-based crop models are popular tools to analyze and simulate the response of agricultural systems to weather, agronomic, or genetic factors. They are often developed in modeling platforms to ensure their future extension and to couple different crop models with a soil model and a crop management event scheduler. The intercomparison and improvement of crop simulation models is difficult due to the lack of efficient methods for exchanging biophysical processes between modeling platforms. We developed Crop2ML, a modeling framework that enables the description and the assembly of crop model components independently of the formalism of modeling platforms and the exchange of components between platforms. Crop2ML is based on a declarative architecture of modular model representation to describe the biophysical processes and their transformation to model components that conform to crop modeling platforms. Here, we present Crop2ML framework and describe the mechanisms of import and export between Crop2ML and modeling platforms.}
}
@article{MORENTEMOLINERA202365,
title = {Reviving stagnated debates in Group Decision Making environments with high number of alternatives},
journal = {Procedia Computer Science},
volume = {221},
pages = {65-72},
year = {2023},
note = {Tenth International Conference on Information Technology and Quantitative Management (ITQM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923007068},
author = {J.A. Morente-Molinera and M. Barragán-Guzmán and J.R. Trillo and M.A. Martínez-Sánchez and F.J. Cabrerizo and E. Herrera-Viedma},
keywords = {Group Decision Making, Fuzzy Ontologies, Consensus measures},
abstract = {In group decision-making, experts try to obtain a consensus to determine how to order a series of alternatives. A consensus is a decision that reflects the opinions of every group member. Consensus requires discussion and deliberation between the group members. During the process, it is normal that the discussion process to stagnate. In this article, we present a new method of group decision-making that solves the stagnation of the process by including new information. The timing of this is determined by a process stagnation analysis. Fuzzy Ontologies allow experts to work in environments with large numbers of alternatives. The method takes into account the experts’ rankings of alternatives to determine which criteria the experts seem to like the most. When introducing new information into the debate, experts are given the possibility to explore alternatives that are very different from those already seen or to look for alternatives that meet the criteria that are preferred the most.}
}
@article{KIM2018173,
title = {Integration of ifc objects and facility management work information using Semantic Web},
journal = {Automation in Construction},
volume = {87},
pages = {173-187},
year = {2018},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2017.12.019},
url = {https://www.sciencedirect.com/science/article/pii/S0926580516303934},
author = {Karam Kim and Hyunjoo Kim and Wooyoung Kim and Changduk Kim and Jaeyo Kim and Jungho Yu},
keywords = {Building information modeling, Facility management, Linked data, Ontology, Semantic Web},
abstract = {The management of information throughout a building's lifecycle is becoming increasingly important, and building information modeling (BIM) is often used to ensure the interoperability of data. However, BIM-based facility information from the construction phase is difficult to access and use during the operation and maintenance phase. This occurs because the BIM information is not utilized well in facility management (FM). In this research, we propose an approach to effectively manage BIM-based FM information by linking the BIM-based building elements and FM work information in an FM system database. We present a Semantic Web-based FM information system that semantically links BIM data to relevant historical work records. The proposed ontology was evaluated using a sample dataset of the architectural maintenance work records of an office building. Using the proposed approach, facility managers will be able to increase their efficiency in searching related work records that consider shared BIM objects by enhancing the interoperability and accessibility of FM data via the Semantic Web.}
}
@article{KALYAN2020103323,
title = {SECNLP: A survey of embeddings in clinical natural language processing},
journal = {Journal of Biomedical Informatics},
volume = {101},
pages = {103323},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103323},
url = {https://www.sciencedirect.com/science/article/pii/S1532046419302436},
author = {Katikapalli Subramanyam Kalyan and S. Sangeetha},
keywords = {Embeddings, Distributed representations, Medical, Natural language processing, Survey},
abstract = {Distributed vector representations or embeddings map variable length text to dense fixed length vectors as well as capture prior knowledge which can transferred to downstream tasks. Even though embeddings have become de facto standard for text representation in deep learning based NLP tasks in both general and clinical domains, there is no survey paper which presents a detailed review of embeddings in Clinical Natural Language Processing. In this survey paper, we discuss various medical corpora and their characteristics, medical codes and present a brief overview as well as comparison of popular embeddings models. We classify clinical embeddings and discuss each embedding type in detail. We discuss various evaluation methods followed by possible solutions to various challenges in clinical embeddings. Finally, we conclude with some of the future directions which will advance research in clinical embeddings.}
}
@article{RESHMA2023e00257,
title = {Cultural heritage preservation through dance digitization: A review},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {28},
pages = {e00257},
year = {2023},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2023.e00257},
url = {https://www.sciencedirect.com/science/article/pii/S2212054823000024},
author = {M.R. Reshma and B. Kannan and V.P. {Jagathy Raj} and S. Shailesh},
keywords = {Intangible cultural heritage preservation, Dance automation, Motion capture technology, Virtual museum, Volumetric capture, Annotation, Ontology, Pose estimation, Gesture recognition},
abstract = {‘Cultural heritage conservation’ encompasses all actions and measures taken towards the life of cultural heritage while strengthening the long-term preservation of its messages and values. It has acquired significant heedfulness in recent years due to its wide applications in the potential research fields of image analysis, machine intelligence, computer vision, and pattern recognition. Cultural heritage preservation comprises both tangible and intangible resources. A significant part of intangible resources constitutes performing art such as dance or music. The era of digitization made way for the digitized form of heritage artifacts, which helps preserve knowledge. Many researchers have developed various approaches to automate the dance, identify the gesture, poses, and stance (Pose Recognition), recognize the dance forms, dance movement classification, etc., with impressive achievements. We present a comprehensive view of approaches proposed in the various fields of computerized dance modeling that aid in cultural heritage preservation.}
}
@article{CHEN2023101838,
title = {Automated facility inspection using robotics and BIM: A knowledge-driven approach},
journal = {Advanced Engineering Informatics},
volume = {55},
pages = {101838},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101838},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622002968},
author = {Junjie Chen and Weisheng Lu and Yonglin Fu and Zhiming Dong},
keywords = {Facility management, Inspection, Robotics, Building information modeling (BIM), Knowledge formalization, Ontology},
abstract = {Facility inspection is crucial for ensuring the performance of built assets. A traditional inspection, characterized by humans’ physical presence, is laborious, time-consuming, and becomes difficult to implement because of travel restrictions amid the pandemic. This laborious practice can potentially be automated by emerging smart technologies such as robotics and building information model (BIM). However, little has been known on how such automation can be achieved, concerning the knowledge-intensive nature of facility inspection. To fill the gap, this research aims to develop a knowledge-driven approach that can synergize knowledge of diverse sources (e.g., explicit knowledge from BIM, and tacit experience in human minds) to allow autonomous implementation of facility inspection by robotic agents. At the core the approach is an integrated scene-task-agent (iSTA) model that formalizes engineering priori in facility management and integrates the rich contextual information from BIM. Experiments demonstrated the applicability of the approach, which can endow robots with autonomy and knowledge to navigate the challenging built environments and deliver facility inspection outcomes. The iSTA model is publicized online, in hope of further extension by the research community and practical deployment to enable automated facility inspection using robotics and BIM.}
}
@article{NAKAWALA201850,
title = {Development of an intelligent surgical training system for Thoracentesis},
journal = {Artificial Intelligence in Medicine},
volume = {84},
pages = {50-63},
year = {2018},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2017.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0933365717301525},
author = {Hirenkumar Nakawala and Giancarlo Ferrigno and Elena {De Momi}},
keywords = {Surgical training, Thoracentesis, Ontology, Production rules, Tracking, Object recognition, Phase recognition},
abstract = {Surgical training improves patient care, helps to reduce surgical risks, increases surgeon’s confidence, and thus enhances overall patient safety. Current surgical training systems are more focused on developing technical skills, e.g. dexterity, of the surgeons while lacking the aspects of context-awareness and intra-operative real-time guidance. Context-aware intelligent training systems interpret the current surgical situation and help surgeons to train on surgical tasks. As a prototypical scenario, we chose Thoracentesis procedure in this work. We designed the context-aware software framework using the surgical process model encompassing ontology and production rules, based on the procedure descriptions obtained through textbooks and interviews, and ontology-based and marker-based object recognition, where the system tracked and recognised surgical instruments and materials in surgeon’s hands and recognised surgical instruments on the surgical stand. The ontology was validated using annotated surgical videos, where the system identified “Anaesthesia” and “Aspiration” phase with 100% relative frequency and “Penetration” phase with 65% relative frequency. The system tracked surgical swab and 50mL syringe with approximately 88.23% and 100% accuracy in surgeon’s hands and recognised surgical instruments with approximately 90% accuracy on the surgical stand. Surgical workflow training with the proposed system showed equivalent results as the traditional mentor-based training regime, thus this work is a step forward a new tool for context awareness and decision-making during surgical training.}
}
@article{MARTIN2024102937,
title = {Sleepiness should be reinvestigated through the lens of clinical neurophysiology: A mixed expertal and big-data Natural Language Processing approach},
journal = {Neurophysiologie Clinique},
volume = {54},
number = {2},
pages = {102937},
year = {2024},
note = {Clinical Neurophysiology of Hypersomnolence},
issn = {0987-7053},
doi = {https://doi.org/10.1016/j.neucli.2023.102937},
url = {https://www.sciencedirect.com/science/article/pii/S0987705323000941},
author = {Vincent P. Martin and Christophe Gauld and Jacques Taillard and Laure Peter-Derex and Régis Lopez and Jean-Arthur Micoulaud-Franchi},
keywords = {Natural language processing, Neurophysiology, Sleep medicine, Sleepiness, Text mining},
abstract = {Historically, the field of sleep medicine has revolved around electrophysiological tools. However, the use of these tools as a neurophysiological method of investigation seems to be underrepresented today, from both international recommendations and sleep centers, in contrast to behavioral and psychometric tools. The aim of this article is to combine a data-driven approach and neurophysiological and sleep medicine expertise to confirm or refute the hypothesis that neurophysiology has declined in favor of behavioral or self-reported dimensions in sleep medicine for the investigation of sleepiness, despite the use of electrophysiological tools. Using Natural Language Processing methods, we analyzed the abstracts of the 18,370 articles indexed by PubMed containing the terms ‘sleepiness’ or ‘sleepy’ in the title, abstract, or keywords. For this purpose, we examined these abstracts using two methods: a lexical network, enabling the identification of concepts (neurophysiological or clinical) related to sleepiness in these articles and their interconnections; furthermore, we analyzed the temporal evolution of these concepts to extract historical trends. These results confirm the hypothesis that neurophysiology has declined in favor of behavioral or self-reported dimensions in sleep medicine for the investigation of sleepiness. In order to bring sleepiness measurements closer to brain functioning and to reintroduce neurophysiology into sleep medicine, we discuss two strategies: the first is reanalyzing electrophysiological signals collected during the standard sleep electrophysiological test; the second takes advantage of the current trend towards dimensional models of sleepiness to situate clinical neurophysiology at the heart of the redefinition of sleepiness.}
}
@article{GUILLOD2025103737,
title = {A Systematic Approach to Prioritise Diagnostically Useful Findings for Inclusion in Electronic Health Records as Discrete Data to Improve Clinical Artificial Intelligence Tools and Genomic Research},
journal = {Clinical Oncology},
volume = {39},
pages = {103737},
year = {2025},
issn = {0936-6555},
doi = {https://doi.org/10.1016/j.clon.2024.103737},
url = {https://www.sciencedirect.com/science/article/pii/S0936655524005417},
author = {P. Guillod and A. Savvas and P.N. Robinson and D. Nai and K.N. Naresh and G. Ott and A. Schuh and W.A. Sewell and M. Anderson and N. Matentzoglu and D. Durgavarjhula and M.L. Xu and M.J. Druzdzel and J.M. Astle},
keywords = {Artificial Intelligence, bayesian network, diagnostic decision support, electronic health records, genomic, phenomic},
abstract = {Aims
The recent widespread use of electronic health records (EHRs) has opened the possibility for innumerable artificial intelligence (AI) tools to aid in genomics, phenomics, and other research, as well as disease prevention, diagnosis, and therapy. Unfortunately, much of the data contained in EHRs are not optimally structured for even the most sophisticated AI approaches. There are very few published efforts investigating methods for recording discrete data in EHRs that would not slow current clinical workflows or ways to prioritise patient characteristics worth recording. Here, we propose an approach to identify and prioritise findings (phenotypes) useful for differentiating diseases, with an initial focus on relatively common small B-cell lymphomas.
Materials and methods
A website enabling crowd-sourced recording of diseases and phenotypes was developed. An expert committee in the field of B-cell lymphomas standardised phenotype terminology for use in digital resources, and select terms were included in the Human Phenotype Ontology (HPO). A total of 100 patient lymph node biopsy samples were evaluated, and phenotypes were recorded as discrete data. Bayesian networks (BNs) were developed based on these data, and their diagnostic accuracy and ability to prioritise these phenotypes for inclusion in EHRs were assessed.
Results
Out of 146 phenotypes identified from the website as potentially useful for differentiating four different lymphomas from each other and from benign lymph nodes, 70–75 were included in BNs. The diagnostic accuracy of different naïve BNs was 96.3% for non–marginal zone lymphoma cases and 50% for marginal zone lymphoma cases when all of the included phenotypes were used and 93.8% for non–marginal zone lymphoma cases and 27.5% for marginal zone lymphoma cases when only 15 phenotypes were included in the BNs.
Conclusion
This pilot provides a starting point for systematic improvement and a dataset for comparing related approaches.}
}
@article{QIN202396,
title = {A Knowledge Graph-based knowledge representation for adaptive manufacturing control under mass personalization},
journal = {Manufacturing Letters},
volume = {35},
pages = {96-104},
year = {2023},
note = {51st SME North American Manufacturing Research Conference (NAMRC 51)},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2023.08.086},
url = {https://www.sciencedirect.com/science/article/pii/S2213846323001438},
author = {Zhaojun Qin and Yuqian Lu},
keywords = {Mass personalization, Smart manufacturing, Self-organizing manufacturing network, Knowledge Graph, Adaptive production scheduling},
abstract = {Mass personalization is an achievable manufacturing paradigm, which requires flexible and responsible manufacturing operations in response to dynamic batch sizes of personalized products. A Self-Organizing Manufacturing Network (SOMN) has been proposed to achieve mass personalization. A crucial aspect of SOMN is adaptive manufacturing control, and the Knowledge Graph, a powerful tool, has been recognized as a promising solution to enhance manufacturing intelligence. However, the current Knowledge Graph research mainly focuses on the modeling and ontology definition of the manufacturing environment, but neglects the interaction between manufacturing resources, the dynamic features of the manufacturing environment, and the application of the Knowledge Graph towards adaptive manufacturing control. Therefore, this paper proposes a Knowledge Graph-based semantic representation for adaptive manufacturing control under dynamic manufacturing environments. The proposed approach develops the Knowledge Graph based on historical and real-time scheduling data. Based on the established Knowledge Graph, Multi-Agent Reinforcement Learning has been introduced as an illustrative example of achieving adaptive scheduling control.}
}
@article{ARABI2022118034,
title = {Improving plagiarism detection in text document using hybrid weighted similarity},
journal = {Expert Systems with Applications},
volume = {207},
pages = {118034},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118034},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422012489},
author = {Hamed Arabi and Mehdi Akbari},
keywords = {Extrinsic plagiarism, Word Embedding Technique, Bag of Word Technique, Structural Similarity, FastText},
abstract = {Plagiarism is a misconduct, which refers to the use of scientific and literary content contained in other sources without reference to them. Today, the rise of plagiarism has become a serious problem for publishers and researchers. Many researchers have discussed this problem and tried to identify types of plagiarism; however, most of these methods are not effective in detecting intelligent plagiarism. In other words, most of these methods focus on direct copying. Therefore, in this study, two methods are proposed to identify Extrinsic plagiarism. In both methods, to limit the search space, two stages of filtering based on the bag of word (BoW) technique are used at the document level and at the sentence level, and plagiarism is investigated only in the outputs of these two stages. In the first method to detect similarities in suspicious documents and sentences, the combination of pre-trained network technique of words embedding FastText and TF-IDF weighting technique to form two structural and semantic matrices and in the second method to form the two matrices, WordNet ontology and weighting TF-IDF is used. After forming the above matrices and calculating the similarity between the pairs of matrices of each sentence, using the Dice similarity and the structural similarity of the weighted composition, two similarity values are calculated. By comparing the similarity of suspicious sentences with the minimum threshold, the document containing the suspicious sentence receives the label of plagiarism or non-plagiarism. Experimental results on the PAN-PC-11 database show that the first method has achieved 95.1% precision and the second method 93.8% precision, which shows that the use of word embedding network compared to WordNet ontology can be more successful in detecting Extrinsic plagiarism.}
}
@article{ALMEIDA2024,
title = {The Use of Natural Language Processing Methods in Reddit to Investigate Opioid Use: Scoping Review},
journal = {JMIR Infodemiology},
volume = {4},
year = {2024},
issn = {2564-1891},
doi = {https://doi.org/10.2196/51156},
url = {https://www.sciencedirect.com/science/article/pii/S2564189124000227},
author = {Alexandra Almeida and Thomas Patton and Mike Conway and Amarnath Gupta and Steffanie A Strathdee and Annick Bórquez},
keywords = {opioid, Reddit, natural language processing, NLP, machine learning},
abstract = {Background
The growing availability of big data spontaneously generated by social media platforms allows us to leverage natural language processing (NLP) methods as valuable tools to understand the opioid crisis.
Objective
We aimed to understand how NLP has been applied to Reddit (Reddit Inc) data to study opioid use.
Methods
We systematically searched for peer-reviewed studies and conference abstracts in PubMed, Scopus, PsycINFO, ACL Anthology, IEEE Xplore, and Association for Computing Machinery data repositories up to July 19, 2022. Inclusion criteria were studies investigating opioid use, using NLP techniques to analyze the textual corpora, and using Reddit as the social media data source. We were specifically interested in mapping studies’ overarching goals and findings, methodologies and software used, and main limitations.
Results
In total, 30 studies were included, which were classified into 4 nonmutually exclusive overarching goal categories: methodological (n=6, 20% studies), infodemiology (n=22, 73% studies), infoveillance (n=7, 23% studies), and pharmacovigilance (n=3, 10% studies). NLP methods were used to identify content relevant to opioid use among vast quantities of textual data, to establish potential relationships between opioid use patterns or profiles and contextual factors or comorbidities, and to anticipate individuals’ transitions between different opioid-related subreddits, likely revealing progression through opioid use stages. Most studies used an embedding technique (12/30, 40%), prediction or classification approach (12/30, 40%), topic modeling (9/30, 30%), and sentiment analysis (6/30, 20%). The most frequently used programming languages were Python (20/30, 67%) and R (2/30, 7%). Among the studies that reported limitations (20/30, 67%), the most cited was the uncertainty regarding whether redditors participating in these forums were representative of people who use opioids (8/20, 40%). The papers were very recent (28/30, 93%), from 2019 to 2022, with authors from a range of disciplines.
Conclusions
This scoping review identified a wide variety of NLP techniques and applications used to support surveillance and social media interventions addressing the opioid crisis. Despite the clear potential of these methods to enable the identification of opioid-relevant content in Reddit and its analysis, there are limits to the degree of interpretive meaning that they can provide. Moreover, we identified the need for standardized ethical guidelines to govern the use of Reddit data to safeguard the anonymity and privacy of people using these forums.}
}
@article{ZHANG20241050,
title = {Accelerating drug discovery, development, and clinical trials by artificial intelligence},
journal = {Med},
volume = {5},
number = {9},
pages = {1050-1070},
year = {2024},
issn = {2666-6340},
doi = {https://doi.org/10.1016/j.medj.2024.07.026},
url = {https://www.sciencedirect.com/science/article/pii/S2666634024003088},
author = {Yilun Zhang and Mohamed Mastouri and Yang Zhang},
keywords = {artificial intelligence, deep learning, drug development, clinical trial, small molecule, antibody, RNA},
abstract = {Summary
Artificial intelligence (AI) has profoundly advanced the field of biomedical research, which also demonstrates transformative capacity for innovation in drug development. This paper aims to deliver a comprehensive analysis of the progress in AI-assisted drug development, particularly focusing on small molecules, RNA, and antibodies. Moreover, this paper elucidates the current integration of AI methodologies within the industrial drug development framework. This encompasses a detailed examination of the industry-standard drug development process, supplemented by a review of medications presently undergoing clinical trials. Conclusively, the paper tackles a predominant obstacle within the AI pharmaceutical sector: the absence of AI-conceived drugs receiving approval. This paper also advocates for the adoption of large language models and diffusion models as a viable strategy to surmount this challenge. This review not only underscores the significant potential of AI in drug discovery but also deliberates on the challenges and prospects within this dynamically progressing field.}
}
@article{CANTILLANA2024105256,
title = {Bringing water values into play in the Atacama desert water crisis},
journal = {Journal of Arid Environments},
volume = {225},
pages = {105256},
year = {2024},
issn = {0140-1963},
doi = {https://doi.org/10.1016/j.jaridenv.2024.105256},
url = {https://www.sciencedirect.com/science/article/pii/S0140196324001368},
author = {Raphael Cantillana and José Luis Molina and Irene Iniesta-Arandia},
keywords = {Customary values, Indigenous values, Pragmatic values, Anthropology, Chile},
abstract = {The current water crisis in the Atacama Desert is explained by extractivist models based on the overexploitation of this resource. Extensive mega-mining is devastating water reserves and causing inequities in access for local indigenous communities. In the present work, through ethnographic research in the community of Mamiña, we show how the water values of different local actors come into conflict. Through a positioned analysis model, we observe two types of water-related values, some determined by worldviews and others created by socio-environmental relationships throughout history. Along these two axes, we observe the emergence of new pragmatic values that make an adequate collective response to the water crisis challenging to implement. We argue that this paradox can be better understood through an approach focused on the sociocultural analysis of water values, deriving from the context of their emergence, as well as their complementarities and dynamics.}
}
@incollection{KATO20242839,
title = {Prototype of Automated Physical Model Builder: Challenges and Opportunities},
editor = {Flavio Manenti and Gintaras V. Reklaitis},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {53},
pages = {2839-2844},
year = {2024},
booktitle = {34th European Symposium on Computer Aided Process Engineering / 15th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-28824-1.50474-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443288241504749},
author = {Shota Kato and Manabu Kano},
keywords = {Artificial intelligence, Physical model, Digital twin, Natural language processing, Process modelling},
abstract = {In the process industry, physical models are indispensable, yet current models sometimes compromise accuracy or incur substantial computational costs. Such cases require new physical models, but the traditional approach to building physical models is reliant on expert knowledge and is time-consuming. This necessitates the development of a new efficient physical model building methodology. Our research aims to establish automated physical model builder, AutoPMoB, which builds physical models from manufacturing process literature. The realization of AutoPMoB requires developing several methods, including those for collecting documents related to the target process and accurately extracting information for physical model building from the documents. In this study, we develop an AutoPMoB prototype, employing a large language model alongside a model building approach previously proposed in our research. The prototype's application to a continuous stirred tank reactor showed its capability to extract necessary data accurately, although the initial attempts did not yield the anticipated models. Subsequent modifications in unifying expressions led to successful model building, underscoring the effectiveness of our system in leveraging literature for physical model building. Advancing AutoPMoB towards practical deployment necessitates specific enhancements, particularly in methods for equivalence judgment of definitions, retrieval of relevant documents, integration of non-documentary information, and domain-specific adaptation.}
}
@article{WEGENER202416,
title = {Bio-Intelligent Machine Tool: Vision and Steps Towards Realisation},
journal = {Procedia CIRP},
volume = {125},
pages = {16-23},
year = {2024},
note = {CIRP BioM 2024},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124003585},
author = {Konrad Wegener and Adriaan Spierings and Lukas Weiss and Daniel Knüttel},
keywords = {bio-intelligence, human-machine interface, process monitoring, ontology},
abstract = {Artificial intelligence enables machines to increase their autonomy. A concept of a bio-intelligent manufacturing machine has been developed. This is characterised by a self-learning expert system and a sensor strategy which are intensively integrated with each other. The machine tool control is at any time completely aware of the process performance, the environmental conditions and the machine status. It exploits further information streams from human operators and other machines in a federated learning approach and is connected to the MES system of the company. The expert system learns from human operators by mere commanding the operation. It is capable to generate new process parameters and deal with incomplete process data by adding the missing information.}
}
@article{LENTERS2021101206,
title = {Integration and harmonization of trait data from plant individuals across heterogeneous sources},
journal = {Ecological Informatics},
volume = {62},
pages = {101206},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2020.101206},
url = {https://www.sciencedirect.com/science/article/pii/S1574954120301564},
author = {Tim P. Lenters and Andrew Henderson and Caroline M. Dracxler and Guilherme A. Elias and Suzanne Mogue Kamga and Thomas L.P. Couvreur and W. Daniel Kissling},
keywords = {Data mobilization, Data science, Functional traits, Standards, Ontology, Semantics},
abstract = {Trait data represent the basis for ecological and evolutionary research and have relevance for biodiversity conservation, ecosystem management and earth system modelling. The collection and mobilization of trait data has strongly increased over the last decade, but many trait databases still provide only species-level, aggregated trait values (e.g. ranges, means) and lack the direct observations on which those data are based. Thus, the vast majority of trait data measured directly from individuals remains hidden and highly heterogeneous, impeding their discoverability, semantic interoperability, digital accessibility and (re-)use. Here, we integrate quantitative measurements of verbatim trait information from plant individuals (e.g. lengths, widths, counts and angles of stems, leaves, fruits and inflorescence parts) from multiple sources such as field observations and herbarium collections. We develop a workflow to harmonize heterogeneous trait measurements (e.g. trait names and their values and units) as well as additional information related to taxonomy, measurement or fact and occurrence. This data integration and harmonization builds on vocabularies and terminology from existing metadata standards and ontologies such as the Ecological Trait-data Standard (ETS), the Darwin Core (DwC), the Thesaurus Of Plant characteristics (TOP) and the Plant Trait Ontology (TO). A metadata form filled out by data providers enables the automated integration of trait information from heterogeneous datasets. We illustrate our tools with data from palms (family Arecaceae), a globally distributed (pantropical), diverse plant family that is considered a good model system for understanding the ecology and evolution of tropical rainforests. We mobilize nearly 140,000 individual palm trait measurements in an interoperable format, identify semantic gaps in existing plant trait terminology and provide suggestions for the future development of a thesaurus of plant characteristics. Our work thereby promotes the semantic integration of plant trait data in a machine-readable way and shows how large amounts of small trait data sets and their metadata can be integrated into standardized data products.}
}
@article{KASTRATI20191618,
title = {The impact of deep learning on document classification using semantically rich representations},
journal = {Information Processing & Management},
volume = {56},
number = {5},
pages = {1618-1632},
year = {2019},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2019.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0306457318307714},
author = {Zenun Kastrati and Ali Shariq Imran and Sule Yildirim Yayilgan},
keywords = {Document representation, Document classification, Deep learning, Ontology, Machine learning},
abstract = {This paper presents a semantically rich document representation model for automatically classifying financial documents into predefined categories utilizing deep learning. The model architecture consists of two main modules including document representation and document classification. In the first module, a document is enriched with semantics using background knowledge provided by an ontology and through the acquisition of its relevant terminology. Acquisition of terminology integrated to the ontology extends the capabilities of semantically rich document representations with an in depth-coverage of concepts, thereby capturing the whole conceptualization involved in documents. Semantically rich representations obtained from the first module will serve as input to the document classification module which aims at finding the most appropriate category for that document through deep learning. Three different deep learning networks each belonging to a different category of machine learning techniques for ontological document classification using a real-life ontology are used. Multiple simulations are carried out with various deep neural networks configurations, and our findings reveal that a three hidden layer feedforward network with 1024 neurons obtain the highest document classification performance on the INFUSE dataset. The performance in terms of F1 score is further increased by almost five percentage points to 78.10% for the same network configuration when the relevant terminology integrated to the ontology is applied to enrich document representation. Furthermore, we conducted a comparative performance evaluation using various state-of-the-art document representation approaches and classification techniques including shallow and conventional machine learning classifiers.}
}
@article{YANG202461,
title = {Representation Learning of Biological Concepts: A Systematic Review},
journal = {Current Bioinformatics},
volume = {19},
number = {1},
pages = {61-72},
year = {2024},
issn = {1574-8936},
doi = {https://doi.org/10.2174/1574893618666230612161210},
url = {https://www.sciencedirect.com/science/article/pii/S157489362400006X},
author = {Yuntao Yang and Xu Zuo and Avisha Das and Hua Xu and Wenjin Zheng},
keywords = {Machine learning, biological concepts, representation learning, embedding, natural language processing, graph neural networks},
abstract = {Objective
Representation learning in the context of biological concepts involves acquiring their numerical representations through various sources of biological information, such as sequences, interactions, and literature. This study has conducted a comprehensive systematic review by analyzing both quantitative and qualitative data to provide an overview of this field.
Methods
Our systematic review involved searching for articles on the representation learning of biological concepts in PubMed and EMBASE databases. Among the 507 articles published between 2015 and 2022, we carefully screened and selected 65 papers for inclusion. We then developed a structured workflow that involved identifying relevant biological concepts and data types, reviewing various representation learning techniques, and evaluating downstream applications for assessing the quality of the learned representations.
Results
The primary focus of this review was on the development of numerical representations for gene/DNA/RNA entities. We have found Word2Vec to be the most commonly used method for biological representation learning. Moreover, several studies are increasingly utilizing state-of-the-art large language models to learn numerical representations of biological concepts. We also observed that representations learned from specific sources were typically used for single downstream applications that were relevant to the source.
Conclusion
Existing methods for biological representation learning are primarily focused on learning representations from a single data type, with the output being fed into predictive models for downstream applications. Although there have been some studies that have explored the use of multiple data types to improve the performance of learned representations, such research is still relatively scarce. In this systematic review, we have provided a summary of the data types, models, and downstream applications used in this task.}
}
@article{GUZMAN20251415,
title = {Variants in BSN, encoding the presynaptic protein Bassoon, result in a distinct neurodevelopmental disorder with a broad phenotypic range},
journal = {The American Journal of Human Genetics},
volume = {112},
number = {6},
pages = {1415-1429},
year = {2025},
issn = {0002-9297},
doi = {https://doi.org/10.1016/j.ajhg.2025.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S0002929725001727},
author = {Stacy G. Guzman and Sarah M. Ruggiero and Shiva Ganesan and Colin A. Ellis and Alicia G. Harrison and Katie R. Sullivan and Zornitza Stark and Natasha J. Brown and Sajel L. Kana and Anabelle Tuttle and Jair Tenorio and Pablo Lapunzina and Julián Nevado and Marie T. McDonald and Courtney Jensen and Patricia G. Wheeler and Lila Stange and Jennifer Morrison and Boris Keren and Solveig Heide and Meg W. Keating and Kameryn M. Butler and Mike A. Lyons and Shailly Jain and Mehdi Yeganeh and Michelle L. Thompson and Molly Schroeder and Hoanh Nguyen and Jorge Granadillo and Kari M. Johnston and Chaya N. Murali and Katie Bosanko and T. Andrew Burrow and Syreeta Morgan and Deborah J. Watson and Hakon Hakonarson and Ingo Helbig},
keywords = {epilepsy, genetics, developmental and epileptic encephalopathy, , neurodevelopmental disorders, longitudinal EMR analysis, human phenotype ontology},
abstract = {Summary
Disease-causing variants in synaptic function genes are a common cause of neurodevelopmental disorders (NDDs) and epilepsy. Here, we describe 14 individuals with de novo disruptive variants in BSN, which encodes the presynaptic protein Bassoon. To expand the phenotypic spectrum, we identified 15 additional individuals with protein-truncating variants (PTVs) from large biobanks. Clinical features were standardized using the Human Phenotype Ontology (HPO) across all 29 individuals, which revealed common clinical characteristics including epilepsy (13/29, 45%), febrile seizures (7/29, 25%), generalized tonic-clonic seizures (5/29, 17%), and focal-onset seizures (3/29, 10%). Behavioral phenotypes were present in almost half of all individuals (14/29, 48%), which included ADHD (7/29, 25%) and autistic behavior (5/29, 17%). Additional common features included developmental delay (11/29, 38%), obesity (10/29, 34%), and delayed speech (8/29, 28%). In adults with BSN PTVs, milder features were common, suggesting phenotypic variability, including a range of individuals without obvious neurodevelopmental features (7/29, 24%). To detect gene-specific signatures, we performed association analysis in a cohort of 14,895 individuals with NDDs. A total of 66 clinical features were associated with BSN, including febrile seizures (p = 1.26e−06) and behavioral disinhibition (p = 3.39e−17). Furthermore, individuals carrying BSN variants were phenotypically more similar than expected by chance (p = 0.00014), exceeding phenotypic relatedness in 179/256 NDD-related conditions. In summary, integrating information derived from community-based gene matching and large data repositories through computational phenotyping approaches, we identify BSN variants as the cause of a synaptic disorder with a broad phenotypic range across the age spectrum.}
}
@article{AU2022100167,
title = {Long covid and medical gaslighting: Dismissal, delayed diagnosis, and deferred treatment},
journal = {SSM - Qualitative Research in Health},
volume = {2},
pages = {100167},
year = {2022},
issn = {2667-3215},
doi = {https://doi.org/10.1016/j.ssmqr.2022.100167},
url = {https://www.sciencedirect.com/science/article/pii/S2667321522001299},
author = {Larry Au and Cristian Capotescu and Gil Eyal and Gabrielle Finestone},
keywords = {Long covid, Chronic illness, Gaslighting, Diagnostic odyssey, Patient-physician relationship},
abstract = {While we know a lot more about Long Covid today, patients who were infected with Covid-19 early on in the pandemic and developed Long Covid had to contend with medical professionals who lacked awareness of the potential for extended complications from Covid-19. Long Covid patients have responded by labeling their contentious interactions with medical professionals, organizations, and the broader medical system as “gaslighting.” We argue that the charge of medical gaslighting can be understood as a form of ontological politics. Not only do patients demand that their version of reality be recognized, but they also blame the experts who hold gatekeeping power over their medical care for producing a distorted version of said reality. By analyzing results from an online survey of Long Covid patients active on social media in the United States (n ​= ​334), we find that experiences of contention and their reframing as “gaslighting” were common amongst our respondents. In short answer responses about their experience obtaining medical care for Long Covid, our respondents described encountering medical professionals who dismissed their experience, leading to lengthy diagnostic odysseys and lack of treatment options for Long Covid. Even though we are limited by characteristics of our sample, there is good reason to believe that these experiences and their contentious reframing as medical gaslighting are exacerbated by gender, class, and racial inequalities.}
}
@article{CHAU2024102666,
title = {Advancing plant single-cell genomics with foundation models},
journal = {Current Opinion in Plant Biology},
volume = {82},
pages = {102666},
year = {2024},
issn = {1369-5266},
doi = {https://doi.org/10.1016/j.pbi.2024.102666},
url = {https://www.sciencedirect.com/science/article/pii/S1369526624001572},
author = {Tran N. Chau and Xuan Wang and John M. McDowell and Song Li},
abstract = {Single-cell genomics, combined with advanced AI models, hold transformative potential for understanding complex biological processes in plants. This article reviews deep-learning approaches in single-cell genomics, focusing on foundation models, a type of large-scale, pretrained, multi-purpose generative AI models. We explore how these models, such as Generative Pre-trained Transformers (GPT), Bidirectional Encoder Representations from Transformers (BERT), and other Transformer-based architectures, are applied to extract meaningful biological insights from diverse single-cell datasets. These models address challenges in plant single-cell genomics, including improved cell-type annotation, gene network modeling, and multi-omics integration. Moreover, we assess the use of Generative Adversarial Networks (GANs) and diffusion models, focusing on their capacity to generate high-fidelity synthetic single-cell data, mitigate dropout events, and handle data sparsity and imbalance. Together, these AI-driven approaches hold immense potential to enhance research in plant genomics, facilitating discoveries in crop resilience, productivity, and stress adaptation.}
}
@incollection{MALLETTE2023895,
title = {Are we there yet? Onto-epistemological qualitative language and literacy research},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {895-901},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.07089-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305070895},
author = {Marla H. Mallette and Jillian Mason and Matthew McConn},
keywords = {Qualitative research, Language, Literacy, Onto-epistemology, Social justice, Equity, Intersectionalities, Counter narratives, Transliteracy, Translanguaging},
abstract = {Through a rich, albeit short, history of rigorous and rich qualitative research, our understanding of language and literacy has deepened and broadened. Building on this strong foundation, using a forward-looking approach, we focused on where the field is going by exploring the moments featured in contemporary qualitative research. A review of studies published in the past 5 years revealed a growing number of studies situated in an onto-epistemological perspective. The studies featured in this piece capture the ways in which literacy is more deeply understood when the researcher is not separated from the researched and how the nature of reality and of knowing cannot be considered separately nor truly understood unless the reality of the learner, from the learner's perspective, is at the heart of the research. These qualitative studies exemplify researchers seeking to understand language and literacy phenomena by demonstrating clear commitments to social justice, equity and empowerment, dissolving and crossing boundaries, probing intersectionalities, and interrogating the intra-activity of people, materials, time, and place.}
}
@article{KEFALIDIS2024104203,
title = {The question answering system GeoQA2 and a new benchmark for its evaluation},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {134},
pages = {104203},
year = {2024},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2024.104203},
url = {https://www.sciencedirect.com/science/article/pii/S1569843224005594},
author = {Sergios-Anestis Kefalidis and Dharmen Punjani and Eleni Tsalapati and Konstantinos Plas and Maria-Aggeliki Pollali and Pierre Maret and Manolis Koubarakis},
keywords = {Geospatial knowledge graphs, Geospatial question answering},
abstract = {We present the question answering engine GeoQA2 which is able to answer geospatial questions over the union of knowledge graphs YAGO2 and YAGO2geo. We also present the dataset GeoQuestions1089 which consists of 1089 natural language questions, their corresponding SPARQL or GeoSPARQL queries and their answers over the union of the same knowledge graphs. We use this dataset to compare the effectiveness of GeoQA2 and the system of Hamzei et al. 2022 and make it publicly available to be used by other researchers. Our evaluation shows that although the engine GeoQA2 performs better than the engine of Hamzei et al. 2022, both engines have ample room for improvement in their question answering performance.}
}
@article{AGARONNIK2022121,
title = {Natural language processing for automated surveillance of intraoperative neuromonitoring in spine surgery},
journal = {Journal of Clinical Neuroscience},
volume = {97},
pages = {121-126},
year = {2022},
issn = {0967-5868},
doi = {https://doi.org/10.1016/j.jocn.2022.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S0967586822000224},
author = {Nicole D. Agaronnik and Anne Kwok and Andrew J. Schoenfeld and Charlotta Lindvall},
keywords = {Natural language processing, Machine learning, Spinal fusion, Spine surgery, Quality improvement},
abstract = {We sought to develop natural language processing (NLP) methods for automated detection and characterization of neuromonitoring documentation from free-text operative reports in patients undergoing spine surgery. We included 13,718 patients who received spine surgery at two tertiary academic medical centers between December 2000 – December 2020. We first validated a rule-based NLP method for identifying operative reports containing neuromonitoring documentation, comparing performance to standard administrative codes. We then trained a deep learning model in a subset of 993 patients to characterize neuromonitoring documentation and identify events indicating change in status or difficulty establishing baseline signals. Performance of the deep learning model was compared to gold-standard manual chart review. In our patient population, 3,606 (26.3%) patients had neuromonitoring documentation identified using NLP. Our NLP method identified notes containing neuromonitoring documentation with an F1-score of 1.0, surpassing performance of standard administrative codes which had an F1-score of 0.64. In the subset of 993 patients used for training, validation, and testing a deep learning model, the prevalence of change in status was 6.5% and difficulty establishing neuromonitoring baseline signals was 6.6%. The deep learning model had an F1-score = 0.80 and AUC-ROC = 1.0 for identifying change in status, and an F1-score = 0.80 and AUC-ROC = 0.97 for identifying difficulty establishing baseline signals. Compared to gold standard manual chart review, our methodology has greater efficiency for identifying infrequent yet important types of neuromonitoring documentation. This method may facilitate large-scale quality improvement initiatives that require timely analysis of a large volume of EHRs.}
}
@article{JULIANIRANZO2023118858,
title = {Bousi∼Prolog: Design and implementation of a proximity-based fuzzy logic programming language},
journal = {Expert Systems with Applications},
volume = {213},
pages = {118858},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118858},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422018760},
author = {Pascual Julián-Iranzo and Fernando Sáenz-Pérez},
keywords = {Fuzzy logic programming, Fuzzy prolog, Bousi∼Prolog, Weak unification, Weak SLD resolution, Proximity relations},
abstract = {The fuzzy logic programming language Bousi∼Prolog extends Prolog with a weak unification algorithm based on proximity relations and truth degree annotations. The weak unification algorithm makes the search for answers more flexible, while rule annotations make possible knowledge-based applications where the rules may be uncertain. In this paper, after recalling the main concepts supporting this language, we detail its design and implementation. We describe the implementation of its operational semantics, which is based on compiling programs and queries into Prolog, and those important features that makes it more applicable: fuzzy sets, integration with WordNet and efficiency techniques. The result is a high-level open-source implementation of the Bousi∼Prolog system, written on top of SWI-Prolog, and publicly available. We also summarise some experiments measuring its performance compared to other systems.}
}
@article{BRUNOE2018592,
title = {Framework for Integrating Production System Models and Product Family Models},
journal = {Procedia CIRP},
volume = {72},
pages = {592-597},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.020},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118301197},
author = {Thomas Ditlev Brunoe and Daniel Grud {Hellerup Sørensen} and Ann-Louise Andersen and Kjeld Nielsen},
keywords = {mass customization, ontology, co-platforming, product model, process model},
abstract = {As the demand for product customization increases globally, companies must increasingly manufacture individually configured products, which stresses the traditional business processes and manufacturing systems. Although previous research indicates potentials in integrating product and process models, few practical implementations of this are found partly due to lacking systems integration. This paper proposes a framework for integrating existing information repositories for manufacturing system and product family model information. By integrating this information, manufacturing information can improve the product configuration process and product configuration information can support the manufacturing process.}
}
@article{SILVAMUNOZ2019101724,
title = {A time-indexed mereology for SUMO},
journal = {Data & Knowledge Engineering},
volume = {123},
pages = {101724},
year = {2019},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2019.101724},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17303749},
author = {Lydia {Silva Muñoz} and Michael Grüninger},
keywords = {Mereology, Time-indexed mereology, Temporary mereology, Ontology, Upper-level ontology, Foundational ontology, SUMO, DOLCE, Ontology mapping, Change, Mereological change},
abstract = {While the period of time during which a subprocess occurs is precisely the time during which the part-whole relation with its main process takes place, part-whole relations between objects do not obey such a rule. The parts of an object can exist before the object is conformed as such, and can survive its dismantlement. In fact, there are no means for knowing when, during the existence of the part and the whole, their parthood relation holds unless an explicit account of time is represented. A time-indexed mereology characterizes how objects gain and lose parts over time by associating a time index to their part-whole relations. Keeping an account of when objects lose or gain parts is necessary for the correct representation of their spatial location and their participation in processes. Upper-level ontologies characterize the properties of the most basic, domain-independent entities, such as time, space, objects and processes. Two upper-level ontologies broadly used are The Descriptive Ontology for Linguistic and Cognitive Engineering (DOLCE) and The Suggested Upper Merged Ontology (SUMO). However, while DOLCE provides a first-order time-indexed mereology for structuring its entities over time, SUMO provides a weaker axiomatization that does not represent the rules that determine how the mereological structure of objects evolve through time in the real world. This work proposes a first-order logic time-indexed mereology for SUMO based on its current representation of objects, time, and temporal location, thereby characterizing how objects gain and lose parts over time. The proposed theory sets the stage for the development of a time-indexed theory of spatial location, and for the representation of temporal restrictions on the participation of objects in processes. The time-indexed mereology of DOLCE and the proposed theory are formally compared, and their relative strength established by using ontology mapping. In order to achieve such a comparison, the representations of time, and temporal location of both upper-level ontologies are also formally compared.}
}
@article{AGRAWAL2024122470,
title = {Revolutionizing subjective assessments: A three-pronged comprehensive approach with NLP and deep learning},
journal = {Expert Systems with Applications},
volume = {239},
pages = {122470},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122470},
url = {https://www.sciencedirect.com/science/article/pii/S095741742302972X},
author = {Raghav Agrawal and Harshit Mishra and Ilanthenral Kandasamy and Shrishail Ravi Terni and Vasantha W.B.},
keywords = {Deep Neural Networks (DNN), Natural Language Processing (NLP), Question answering, Yet Another Keyword Extractor (YAKE), KeyBERT, Simple Contrastive Sentence Embedding Framework (simCSE), Camembert, Sentence Bidirectional Encoder Representations from Transformers (SBERT)},
abstract = {The enhanced answer evaluation system is a cutting-edge automated tool that evaluates subjective answers in various contexts, such as educational assessments, surveys, and feedback forms. The proposed system leverages Natural Language Processing (NLP) and deep learning techniques to analyse subjective answers and provide evaluation scores with precision. Students’ answers are evaluated based on various criteria, such as keywords, context, relevance, coherence, and similarity. This paper introduces an architecture for a subjective answer evaluator using three main aspects: detection of keywords, similarity matrix, and presence of named entities. It combines the three aspects and provides a final score. It provides a standardized mechanism to score a given user answer compared to the particular model answer without human prejudice. This research aims to transcend traditional methodologies that predominantly utilize keyword or keyphrase scoring (text-based similarity) to determine the final score of an answer without delving into its technical intricacies. The semantic similarity (vector-based) employs vector data representations for score calculation. This approach necessitates partitioning data into multiple vectors for a comprehensive analysis. While text similarity is effective for short answers, its efficacy diminishes as the length of the answer increases. Therefore, this study emphasizes the critical role of similarity scoring and Named Entity Recognition (NER) scoring in evaluating more extended responses based on the stsb-en-main dataset (short answers) and a custom dataset with 190 records. This research reveals its remarkable performance, which excels through a dynamic three-pronged approach: keyword scoring, semantic similarity, and NER scoring with models like Yet Another Keyword Extractor (YAKE), SimCSE and Camembert. These three independent components synergize to produce unmatched results, establishing a new standard in the field. This enhancement led to Root Mean Square Error (RMSE) scores of 0.031 (optimized error rate) and an impressive 71%+ accuracy for our comprehensive system. This achievement surpasses existing works, which typically reached accuracies ranging between 40%–60% for long answers.}
}
@article{CIOARA2018368,
title = {Expert system for nutrition care process of older adults},
journal = {Future Generation Computer Systems},
volume = {80},
pages = {368-383},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.05.037},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17311056},
author = {Tudor Cioara and Ionut Anghel and Ioan Salomie and Lina Barakat and Simon Miles and Dianne Reidlinger and Adel Taweel and Ciprian Dobre and Florin Pop},
keywords = {Expert system, Nutrition care, Inference engine, Malnutrition, Ontology},
abstract = {This paper presents an expert system for a nutrition care process tailored for the specific needs of elders. Dietary knowledge is defined by nutritionists and encoded as Nutrition Care Process Ontology, and then used as underlining base and standardized model for the nutrition care planning. An inference engine is developed on top of the ontology, providing semantic reasoning infrastructure and mechanisms for evaluating the rules defined for assessing short and long term elders’ self-feeding behaviours, to identify unhealthy dietary patterns and detect the early instauration of malnutrition. Our expert system provides personalized intervention plans covering nutrition education, diet prescription and food ordering adapted to the older adult’s specific nutritional needs, health conditions and food preferences. In-lab evaluation results are presented proving the usefulness and quality of the expert system as well as the computational efficiency, coupling and cohesion of the defined ontology.}
}
@article{CHONDAMRONGKUL2021102631,
title = {Formal security analysis for software architecture design: An expressive framework to emerging architectural styles},
journal = {Science of Computer Programming},
volume = {206},
pages = {102631},
year = {2021},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2021.102631},
url = {https://www.sciencedirect.com/science/article/pii/S0167642321000241},
author = {Nacha Chondamrongkul and Jing Sun and Ian Warren},
keywords = {Software architecture, Security analysis, Ontology Web language, Model checking, Security Scenarios},
abstract = {Analysing security in the architecture design of modern software systems is a challenging task. Emerging technologies utilised in building software systems may pose security threats, so software engineers need to consider both the structure and behaviour of architectural styles that employ these supporting technologies. This paper presents an automated approach to security analysis that helps to identify security characteristics at the architectural level. Key techniques used by our approach include the use of metrics, vulnerability identification and attack scenarios. Our modelling is expressive in defining architectural styles and security characteristics. Our analysis approach gives insightful results that allow software engineers to trace through the design to find parts of the system that may be impacted by attacks. We have developed an analysis tool that allows user to seamlessly model the software architecture design and analyse security. The evaluation has been conducted to assess the accuracy and performance of our approach. The results show that our analysis approach performs reasonably well to analyse the security in the architectural design.}
}
@article{TURCHET2023100758,
title = {Semantic Web of Musical Things: Achieving interoperability in the Internet of Musical Things},
journal = {Journal of Web Semantics},
volume = {75},
pages = {100758},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2022.100758},
url = {https://www.sciencedirect.com/science/article/pii/S1570826822000427},
author = {Luca Turchet and Francesco Antoniazzi},
keywords = {Internet of Musical Things, Web of Things, Smart musical instruments, Ontologies, Semantic audio},
abstract = {The Internet of Musical Things (IoMusT) refers to the extension of the Internet of Things paradigm to the musical domain. Interoperability represents a central issue within this domain, where heterogeneous Musical Things serving radically different purposes are envisioned to communicate between each other. Automatic discovery of resources is also a desirable feature in IoMusT ecosystems. However, the existing musical protocols are not adequate to support discoverability and interoperability across the wide heterogeneity of Musical Things, as they are typically not flexible, lack high resolution, are not equipped with inference mechanisms that could exploit on board the information on the whole application environment. Besides, they hardly ever support easy integration with the Web. In addition, IoMusT applications are often characterized by strict requirements in terms of latency of the exchanged messages. Semantic Web of Things technologies have the potential to overcome the limitations of existing musical protocols by enabling discoverability and interoperability across heterogeneous Musical Things. In this paper we propose the Musical Semantic Event Processing Architecture (MUSEPA), a semantically-based architecture designed to meet the IoMusT requirements of low-latency communication, discoverability, interoperability, and automatic inference. The architecture is based on the CoAP protocol, a semantic publish/subscribe broker, and the adoption of shared ontologies for describing Musical Things and their interactions. The code implementing MUSEPA can be accessed at: https://github.com/CIMIL/MUSEPA/.}
}
@article{LOPEZUBEDA20221486,
title = {Natural Language Processing in Pathology: Current Trends and Future Insights},
journal = {The American Journal of Pathology},
volume = {192},
number = {11},
pages = {1486-1495},
year = {2022},
issn = {0002-9440},
doi = {https://doi.org/10.1016/j.ajpath.2022.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0002944022002449},
author = {Pilar López-Úbeda and Teodoro Martín-Noguerol and José Aneiros-Fernández and Antonio Luna},
abstract = {Natural language processing (NLP) plays a key role in advancing health care, being key to extracting structured information from electronic health reports. In the last decade, several advances in the field of pathology have been derived from the application of NLP to pathology reports. Herein, a comprehensive review of the most used NLP methods for extracting, coding, and organizing information from pathology reports is presented, including how the development of tools is used to improve workflow. In addition, this article discusses, from a practical point of view, the steps necessary to extract data and encode natural language information for its analytical processing, ranging from preprocessing of text to its inclusion in complex algorithms. Finally, the potential of NLP-based automatic solutions for improving workflow in pathology and their further applications in the near future is highlighted.}
}
@article{EMARA2022310,
title = {Workflow for building interoperable food and nutrition security (FNS) data platforms},
journal = {Trends in Food Science & Technology},
volume = {123},
pages = {310-321},
year = {2022},
issn = {0924-2244},
doi = {https://doi.org/10.1016/j.tifs.2022.03.022},
url = {https://www.sciencedirect.com/science/article/pii/S0924224422001133},
author = {Yasmine Emara and Barbara {Koroušić Seljak} and Eileen R. Gibney and Gorjan Popovski and Igor Pravst and Peter Fantke},
keywords = {Data integration, Interoperability criteria, FNS-Cloud, Ontology, Machine learning, Natural language processing, Branded food data},
abstract = {Background
In response to growing needs for the integration of heterogeneous data on food and nutrition security (FNS), and the current fragmentation of interoperability resources, the ‘FNS-Cloud project’ aims to develop a cross-domain, interoperable data platform that integrates diverse FNS data. Currently, there is insufficient guidance on how to develop such an FNS data platform and integrate a variety of FNS data types that differ in both their syntax and semantics.
Scope and approach
In the present study, we propose a generalizable workflow to guide data managers in building interoperable, cross-domain FNS data platforms, which centres around the definition of interoperability criteria that capture standardized data structures, terminologies and reporting formats for key variables across FNS data types. Information technology tools for automating different workflow steps are discussed. Finally, we include an illustrative case study, where we harmonize and link branded food datasets based on pre-defined interoperability criteria to answer an example research question.
Key findings and conclusions
Our work highlights the unique harmonization requirements within the FNS field. We provide two examples of how generic and domain-specific interoperability criteria addressing these requirements can be defined. Incoming FNS data must comply with defined criteria in order to enable their (semi-)automated integration into any data platform. Our case study reinforces the importance of semantic annotation of FNS data, and the need for clear mapping rules to be included into platform-internal semantic data models. The proposed workflow can be applied to any setting in which data managers strive towards harmonized and linked FNS data, and, thus, promotes an open-data and open-science environment.}
}
@article{MARTINS2024105385,
title = {Unlocking human-like conversations: Scoping review of automation techniques for personalized healthcare interventions using conversational agents},
journal = {International Journal of Medical Informatics},
volume = {185},
pages = {105385},
year = {2024},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2024.105385},
url = {https://www.sciencedirect.com/science/article/pii/S1386505624000480},
author = {Ana Martins and Ana Londral and Isabel {L. Nunes} and Luís {V. Lapão}},
keywords = {Conversational Agents, Automation, Personalization, Natural Language Processing, Artificial Intelligence, Healthcare},
abstract = {Background
Conversational agents (CAs) offer a sustainable approach to deliver personalized interventions and improve health outcomes.
Objectives
To review how human-like communication and automation techniques of CAs in personalized healthcare interventions have been implemented. It is intended for designers and developers, computational scientists, behavior scientists, and biomedical engineers who aim at developing CAs for healthcare interventions.
Methodology
A scoping review was conducted in accordance with PRISMA Extension for Scoping Review. A search was performed in May 2023 in Web of Science, Pubmed, Scopus and IEEE databases. Search results were extracted, duplicates removed, and the remaining results were screened. Studies that contained personalized and automated CAs within the healthcare domain were included. Information regarding study characterization, and human-like communication and automation techniques was extracted from articles that met the eligibility criteria.
Results
Twenty-three studies were selected. These articles described the development of CAs designed for patients to either self-manage their diseases (such as diabetes, mental health issues, cancer, asthma, COVID-19, and other chronic conditions) or to enhance healthy habits. The human-like communication characteristics studied encompassed aspects like system flexibility, personalization, and affective characteristics. Seven studies used rule-based models, eleven applied retrieval-based techniques for content delivery, five used AI models, and six integrated affective computing.
Conclusions
The increasing interest in employing CAs for personalized healthcare interventions is noteworthy. The adaptability of dialogue structures and personalization features is still limited. Unlocking human-like conversations may encompass the use of affective computing and generative AI to help improve user engagement. Future research should focus on the integration of holistic methods to describe the end-user, and the safe use of generative models.}
}
@article{JELOKHANINIARAKI2018104,
title = {Semantic interoperability of GIS and MCDA tools for environmental assessment and decision making},
journal = {Environmental Modelling & Software},
volume = {100},
pages = {104-122},
year = {2018},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2017.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S1364815217300282},
author = {Mohammadreza Jelokhani-Niaraki and Abolghasem Sadeghi-Niaraki and Soo-Mi Choi},
keywords = {Environmental assessment and decision making, GIS, MCDA, Semantic interoperability, Ontology},
abstract = {A promising approach to environmental assessment and decision making analyses is based on integrating Multicriteria Decision Analysis (MCDA) and GIS tools. Integration of GIS and MCDA tools can be potentially achieved through interoperability, where these tools can exchange relevant information to tackle a particular environmental problem. However, the problem of semantic heterogeneity caused by different meanings of data, terminologies, and models used in GIS and MCDA has been recognized as an obstacle in the interoperability of these tools. Conventionally, exchange of data between GIS and MCDA systems for environmental applications relies on prior knowledge to mediate meanings between the two components. This paper proposes an ontology-enabled framework for semantic interoperability of GIS and MCDA web services. In particular, this study has made significant contribution to environmental decision making by providing an interoperable framework to exchange environmental data with intended and unambiguous meanings between GIS and MCDA services.}
}
@article{KONYS20191629,
title = {Methods Supporting Supplier Selection Processes – Knowledge-based Approach},
journal = {Procedia Computer Science},
volume = {159},
pages = {1629-1641},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.333},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919315340},
author = {Agnieszka Konys},
keywords = {sustainable supplier selection, supplier selection methods, ontology-based model},
abstract = {Nowadays, companies have to improve their practices in the management of sustainable supply chain with increased awareness of environmental, economic and social issues globally. Selecting the optimum sustainable supplier is crucial for sustainable supply chain management, which is a challenging multi-dimensional problem. From a systematic perspective, sustainable supplier selection problem can be separated into two parts, including criteria and methods. This paper concentrates on the method selection aspects, referring to their application examples. Thus, the objectives of this paper are twofold: one is to summarize the literature on supplier selection issues from related articles, then to present an attempt to ontology-based approach to handling knowledge about sustainable supplier selection methods. To meet this aim, ontology-based model is proposed to synthetize the knowledge of analyzed methods and related papers. Finally, the proposed ontology-based model is demonstrated by an empirical example of using competency questions to confirm its correctness and effectiveness.}
}
@article{ALSKAF2024101418,
title = {Machine learning outcome prediction using stress perfusion cardiac magnetic resonance reports and natural language processing of electronic health records},
journal = {Informatics in Medicine Unlocked},
volume = {44},
pages = {101418},
year = {2024},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2023.101418},
url = {https://www.sciencedirect.com/science/article/pii/S2352914823002642},
author = {Ebraham Alskaf and Simon M. Frey and Cian M. Scannell and Avan Suinesiaputra and Dijana Vilic and Vlad Dinu and Pier Giorgio Masci and Divaka Perera and Alistair Young and Amedeo Chiribiri},
keywords = {Machine learning, Coronary artery disease, Cardiac magnetic resonance, Electronic health records, Outcome prediction, Natural language processing}
}
@article{HERR2024100341,
title = {Estimating prevalence of rare genetic disease diagnoses using electronic health records in a children’s hospital},
journal = {Human Genetics and Genomics Advances},
volume = {5},
number = {4},
pages = {100341},
year = {2024},
issn = {2666-2477},
doi = {https://doi.org/10.1016/j.xhgg.2024.100341},
url = {https://www.sciencedirect.com/science/article/pii/S2666247724000812},
author = {Kate Herr and Peixin Lu and Kessi Diamreyan and Huan Xu and Eneida Mendonca and K. Nicole Weaver and Jing Chen},
keywords = {rare genetic diseases, natural language processing, bioinformatics, Orphanet, electronic health record, genetic testing},
abstract = {Summary
Rare genetic diseases (RGDs) affect a significant number of individuals, particularly in pediatric populations. This study investigates the efficacy of identifying RGD diagnoses through electronic health records (EHRs) and natural language processing (NLP) tools, and analyzes the prevalence of identified RGDs for potential underdiagnosis at Cincinnati Children’s Hospital Medical Center (CCHMC). EHR data from 659,139 pediatric patients at CCHMC were utilized. Diagnoses corresponding to RGDs in Orphanet were identified using rule-based and machine learning-based NLP methods. Manual evaluation assessed the precision of the NLP strategies, with 100 diagnosis descriptions reviewed for each method. The rule-based method achieved a precision of 97.5% (95% CI: 91.5%, 99.4%), while the machine-learning-based method had a precision of 73.5% (95% CI: 63.6%, 81.6%). A manual chart review of 70 randomly selected patients with RGD diagnoses confirmed the diagnoses in 90.3% (95% CI: 82.0%, 95.2%) of cases. A total of 37,326 pediatric patients were identified with 977 RGD diagnoses based on the rule-based method, resulting in a prevalence of 5.66% in this population. While a majority of the disorders showed a higher prevalence at CCHMC compared with Orphanet, some diseases, such as 1p36 deletion syndrome, indicated potential underdiagnosis. Analyses further uncovered disparities in RGD prevalence and age of diagnosis across gender and racial groups. This study demonstrates the utility of employing EHR data with NLP tools to systematically investigate RGD diagnoses in large cohorts. The identified disparities underscore the need for enhanced approaches to guarantee timely and accurate diagnosis and management of pediatric RGDs.}
}
@article{MAJUMDAR2018773,
title = {Relating language, logic, and imagery},
journal = {Procedia Computer Science},
volume = {145},
pages = {773-781},
year = {2018},
note = {Postproceedings of the 9th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2018 (Ninth Annual Meeting of the BICA Society), held August 22-24, 2018 in Prague, Czech Republic},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.11.031},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918323172},
author = {Arun K. Majumdar and John F. Sowa},
keywords = {Deep Learning, Computational Creativity, Cognitive Science},
abstract = {The world is a continuum, but words are discrete. Sensory organs map the continuous world to continuous mental models of sights, sounds, actions, and feelings. Those mental models, which represent a moving 3-D virtual reality (VR) are the semantic foundation for all versions of language and logic. A common model for cognition must be able to process and relate all modalities. Kyndi technology represents all information in graphs. They include conceptual graphs for symbolic information and arbitrary graphs for 2-D icons or 3-D VR. All graphs are stored in Cognitive Memory, which can find approximate mappings for arbitrary graphs in logarithmic time. Those mappings include formal unification for logics, and informal analogies for case-based reasoning. The analogies can even map conceptual graphs to the graphs derived from imagery or VR simulations. For reasoning, Peirce’s rules of inference for existential graphs can support operations on arbitrary icons, such as the diagrams in Euclid’s geometry. Those rules, when adapted to conceptual graphs, can map symbolic languages to and from Euclidean style geometrical reasoning. With two new rules of inference, called observation and imagination, the Standard Model of Cognition can support mental models without making any current software obsolete.}
}
@incollection{PREISIG20182197,
title = {Visual Modelling with Networks},
editor = {Mario R. Eden and Marianthi G. Ierapetritou and Gavin P. Towler},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {44},
pages = {2197-2202},
year = {2018},
booktitle = {13th International Symposium on Process Systems Engineering (PSE 2018)},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-64241-7.50361-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044464241750361X},
author = {Heinz A Preisig and Arne Tobias Elve and Sigve Karolius},
keywords = {Ontology-base modelling, process simulation, multi-scale},
abstract = {Visual modelling serves the purpose of designing process models, discussing them on the back of an envelope, a serviette, the meeting-room board and define & edit them in the graphical user interface ModelComposer, which is a component of our ontology-based simulation environment. Models that spread over large range of time-scales do not connect straightforwardly as the fast time-scale usually is computationally intensive to the extent where on the large time-scale one uses surrogate models derived from the detailed models. The network-of-network approach extends readily to multi-scale system – from quantum to mechanical properties.}
}
@article{GIMENEZMEDINA2023118968,
title = {A systematic review of capability and maturity innovation assessment models: Opportunities and challenges},
journal = {Expert Systems with Applications},
volume = {213},
pages = {118968},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118968},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422019868},
author = {M. Giménez-Medina and J.G. Enríquez and F.J. Domínguez-Mayo},
keywords = {Innovation assessment, Innovation management, Technological innovation, Capability maturity model, Innovation management systems, Systematic mapping study},
abstract = {Public funding, being the primary source for innovation, imposes restrictions caused by a lack of trust between the roles of public funders and organisations in the innovation process. Capability and maturity innovation assessment models can improve the process by combining both roles to create an agile and trusting environment. This paper aims to provide a current description of the state-of-the-art on capability and maturity innovation assessment models in the context of Information and Communication Technologies. To this end, a Systematic Mapping Study was carried out considering high-quality published research from four relevant digital libraries since 2000. The 78 primary studies analysed show several gaps and challenges. In particular, a common ontology has not been achieved, and Innovation Management Systems are scarcely considered. Concepts such as open innovation have not been correctly applied to incorporate all Quadruple Helix stakeholders, especially the government and its role as a public funder. This implies that no studies explore a standard agile public–private maturity model based on capabilities since the public funders’ restrictions have not been considered. Furthermore, although some concepts of innovation capabilities have evolved, none of the studies analysed offer a comprehensive coverage of capabilities. As potential future lines of research, this paper proposes 11 challenges based on the 5 shortcomings found in the literature.}
}
@article{ARSHAD2019147,
title = {A multilayered semantic framework for integrated forensic acquisition on social media},
journal = {Digital Investigation},
volume = {29},
pages = {147-158},
year = {2019},
issn = {1742-2876},
doi = {https://doi.org/10.1016/j.diin.2019.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1742287618303785},
author = {Humaira Arshad and Aman Jantan and Gan Keng Hoon and Anila Sahar Butt},
keywords = {Online social network forensics, Hybrid Ontology model, Social network analysis, Social network forensic automation},
abstract = {In recent years, examination of the social media networks has become an integral part of investigations. Law enforcement agencies and legal practitioners frequently utilize social networks to quickly access the information related to the participants of any illicit incident. However, the forensic process needs collection and analysis of the information which is immense, heterogeneous, and spread across multiple social networks. This process is technically intricate due to heterogeneous and unstructured online social networks (OSNs). Hence, creating cognitive challenges and massive workloads for the investigators. Therefore, it is imperative to develop automated and reliable solutions to assist investigators. Capturing the forensic information in the structured form is crucial for automation, sharing, and interoperability. This paper introduces the design of a multi-layer framework; from collection to evidence analysis. The central component of this framework is a hybrid ontology approach that involves multiple ontologies to manage the unstructured data and integrate various social media data collections. This approach aims to find the evidence by automated methods that are trustworthy and therefore admissible in a court of law.}
}
@article{ELHOSARY2025105751,
title = {Intelligent countermeasures analysis in oil and gas projects utilizing topic modeling},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {98},
pages = {105751},
year = {2025},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2025.105751},
url = {https://www.sciencedirect.com/science/article/pii/S0950423025002098},
author = {Ehab Elhosary and Osama Moselhi},
keywords = {HAZOP, BERTopic, LDA, Safety Systems (SS), Coherence Score, Countermeasures Breakdown Structure (CBS)},
abstract = {The oil and gas industry is inherently complex and high-risk, with potential fires, explosions, and releases of hazardous substances posing significant safety challenges. Despite robust safety management systems, accidents persist, highlighting the importance of learning from past incidents and hazard reports. Historical Hazard and Operability (HAZOP) reports generate valuable countermeasures—safeguards and recommendations—that inform the design of protection systems to enhance safety management. However, the sheer volume of countermeasures produced makes addressing each one prohibitively expensive and time-consuming. Additionally, current HAZOP literature and software tools lack automation of these countermeasures, impeding the efficient dissemination of information to the appropriate departments for detailed design. This paper introduces categorizing countermeasures utilizing the BERTopic algorithm in natural language processing (NLP). The methodology comprises data preprocessing, SBERT (a modification of the Bidirectional Encoder Representations from Transformers) for generating embeddings, Uniform manifold approximation and projection (UMAP) for dimensionality reduction, hierarchical density-based spatial clustering of applications with noise (HDBSCAN) for clustering, and KeyBERT for topic representation. Applied to 1574 records from a HAZOP report of an oil pump station, the BERTopic model achieved 84.6 % coherence score and 90.7 % topic diversity score, resulting in 15 final topics, outperforming Latent Dirichlet Allocation (LDA) (45.3 % and 84.7 %) and Latent Semantic Analysis (LSA) (53 % and 96 %). The study identified included and excluded topics for each node and the most frequent topics by risk rate. The generated safety systems (SS) were validated against API RP 750 and RP 752 standards and the Countermeasures Breakdown Structure (CBS) was introduced to organize safety systems hierarchically. The developed model was tested on another dataset of an oil and gas production facility, comprising 512 records and 21 nodes, achieving 85.29 % coherence and 98.33 % topic diversity, confirming its robustness and consistency. This research benefits HAZOP participants by improving hazard identification, emphasizing key preventative actions, and assigning them to relevant departments for design-stage deployment.}
}
@article{CARDILLO2022164,
title = {Fuzzy OWL-Boost: Learning fuzzy concept inclusions via real-valued boosting},
journal = {Fuzzy Sets and Systems},
volume = {438},
pages = {164-186},
year = {2022},
note = {Fuzzy and Neurofuzzy Models},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2021.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0165011421002426},
author = {Franco Alberto Cardillo and Umberto Straccia},
keywords = {OWL 2 ontologies, Machine learning, Real-valued AdaBoost, Fuzzy logic, Concept inclusion axioms},
abstract = {OWL ontologies are nowadays a quite popular way to describe structured knowledge in terms of classes, relations among classes and class instances. In this paper, given an OWL ontology and a target class T, we address the problem of learning fuzzy concept inclusion axioms that describe sufficient conditions for being an individual instance of T (and to which degree). To do so, we present Fuzzy OWL-Boost that relies on the Real AdaBoost boosting algorithm adapted to the (fuzzy) OWL case. We illustrate its effectiveness by means of an experimentation with several ontologies.}
}
@article{MARTINEZARELLANO20237402,
title = {Enabling Coordinated Elastic Responses of Manufacturing Systems through Semantic Modelling},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {7402-7407},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.617},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323009886},
author = {Giovanna Martínez-Arellano and Karol Niewiadomski and Fan Mo and Basem Elshafei and Jack C. Chaplin and Duncan McFarlane and Svetan Ratchev},
keywords = {Semantic modelling, Manufacturing ontology, Reconfigurable systems, Elastic reasoning, resilience},
abstract = {Resilience to supply chain disruptions and to changing product volumes and specifications are currently major challenges for the manufacturing sector. To maintain quality and productivity, manufacturers need to be able to respond to disruption using a coordinated set of strategies across different levels of the business, from changes on the shop floor to changes in business strategy. To achieve this coordinated response in the most effective way – what we refer to as an elastic response – a first step is to clearly understand what resources, capabilities and business strategies are available, and then identify viable solutions that may include adding or removing equipment, re-purposing assets, adapting shifts, changing suppliers, or outsourcing part of the process. As manufacturing systems move towards more dynamic, flexible environments, a digital representation of the capabilities at all levels of the business as well as real-time status of these will play a key role in achieving a true picture of the state of a system and support the decision-maker to deliver an effective elastic response. This paper presents a semantic approach to the underpinning models needed to enable such response. By semantically representing capabilities at all levels, a semi-automated process can be implemented to reason and match process demands to capabilities. This is the first step in understanding if the existing system can cope with the disruption or if there are any other existing means in the business that can be used to enable an effective response.}
}
@article{RBOUL2023101834,
title = {South-South acculturation: Majority-group students’ relation to Sub-Saharan students in Moroccan universities},
journal = {International Journal of Intercultural Relations},
volume = {96},
pages = {101834},
year = {2023},
issn = {0147-1767},
doi = {https://doi.org/10.1016/j.ijintrel.2023.101834},
url = {https://www.sciencedirect.com/science/article/pii/S0147176723000822},
author = {Hamza R’boul and Fred Dervin and Benachour Saidi},
keywords = {Majority group acculturation, South-South acculturation, Mobilization of subalternity, University students, Moroccan students, Sub-Saharan students},
abstract = {This paper explores the complexities of majority-group members’ acculturation in the changing culture-scape of Morocco within the larger framework of South-South acculturation and the mobilization of subalternity. Taking into account linguistic, cultural and epistemic hierarchies, this study explores the dynamics of majority groups’ acculturation and the manifestations that might confound, signal and/or deny acculturation among Moroccan university students towards Sub-Saharan students. In-depth interviews with Moroccan and Sub-Saharan university students and some auto-ethnographic accounts are used to account for the complexities in researching majority-group members’ acculturation and to understand participants’ subjectivities, ontologies and perspectives in greater nuance and depth. Findings revealed that Moroccan students’ discourses articulate strong support for the increasing presence of Sub-Saharan people on campuses, however, this positive rhetoric does not necessarily translate into ‘willingness’ to adopt their ontologies. While this could be mere hospitable recognition, the claims of majority group acculturation require a stronger foundation of the appeal of migrants from the Global South beyond the local society’s humanitarian hospitality. Implications of this study include developing the notion of ‘mobilization of subalternity’ which refers to the re-projection of the overlapping systems of privilege/subordination and the importance of sociopolitical framing that does not distance the geopolitical, racial and power-/image-laden from the very psychological factors that influence acculturation.}
}
@article{ESCOTT2019100754,
title = {‘Being in the Bin’: Affective understandings of prescriptivism and spelling in video narratives co-produced with children in a post-industrial area of the UK},
journal = {Linguistics and Education},
volume = {53},
pages = {100754},
year = {2019},
issn = {0898-5898},
doi = {https://doi.org/10.1016/j.linged.2019.100754},
url = {https://www.sciencedirect.com/science/article/pii/S0898589818303607},
author = {Hugh Francis Escott and Kate Heron Pahl}
}
@article{LYNCH2023102118,
title = {Epistemological entanglements: Decolonizing understandings of identity and knowledge in English language teaching},
journal = {International Journal of Educational Research},
volume = {118},
pages = {102118},
year = {2023},
issn = {0883-0355},
doi = {https://doi.org/10.1016/j.ijer.2022.102118},
url = {https://www.sciencedirect.com/science/article/pii/S0883035522001926},
author = {Renee Lynch and Suhanthie Motha},
keywords = {Identity, Teacher identity, Global South, Epistemologies, Ubuntu, Decolonizing, English, Knowledge},
abstract = {This article explores how educators might move towards decolonizing our understandings of identity and knowledge. How might we both acknowledge and move beyond the reality that knowledge about identity is simultaneously constructed by colonialism and inseparable from the teaching and learning of English? Drawing on ethnographic data, including observations and interviews, from two studies of teacher identity, one of English teachers collaborating in Tanzania and one of MA-TESOL students in the U.S., this article conceptualizes identities outside the confines of colonial ideologies and Global North-dominated theories. Drawing on Santos's epistemologies of the South and Ubuntu-inspired identity-in-community, the authors describe how localized notions of identity among English teachers can challenge and reformulate those dominant in Global North education.}
}
@article{HSU2022101232,
title = {At the critical moment: The rhizomatic organization and “Democracy to Come”},
journal = {Scandinavian Journal of Management},
volume = {38},
number = {4},
pages = {101232},
year = {2022},
issn = {0956-5221},
doi = {https://doi.org/10.1016/j.scaman.2022.101232},
url = {https://www.sciencedirect.com/science/article/pii/S0956522122000392},
author = {Shih-wei Hsu},
keywords = {Rhizome, Social Movement Organization, Deleuze, Guattari},
abstract = {In the field of Organization Studies (OS), there has been strong interest in the use of the metaphor of rhizomes as developed by Gilles Deleuze and Félix Guattari. The merit of the rhizome metaphor is that it offers strong explanatory power in capturing the chaotic, unpredictable, and uncontrollable nature of organizations, which problematizes a managerial reading of organizational process that is often preoccupied with the managerial goals of performance, efficiency, and effectiveness. In the past two decades, with the development of the internet and social media, the concept of rhizome has been concretized as an organizational ontology but, ironically, we have witnessed that the features of rhizome may selectively be employed and developed into a military tactic, to serve a totalitarian interest of control. For instance, the idea of rhizome manoeuvre has widely been adopted by (terrorist) organizations that seek to justify the act of violence. This paper seeks to defend the rhizome and its emancipatory potential but suggests that the use of rhizome must have its provisional boundary. To substantiate this argument, the author conducted an ethnographical study in a social movement organization. The idea of a provisional boundary is in line with Deleuze and Guattari's view of provisional dualism. In this regard, the author suggests that rhizomatic organizations and their actions should be grounded in Deleuze and Guattari's philosophical framework of "democracy-to-come", embedded in Bergson's open society. With the help of the empirical data, this paper identifies some tentative principles to inform the actions of rhizomatic organizations.}
}
@article{CHEN2020101484,
title = {The design and practice of a semantic-enabled urban analytics data infrastructure},
journal = {Computers, Environment and Urban Systems},
volume = {81},
pages = {101484},
year = {2020},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2020.101484},
url = {https://www.sciencedirect.com/science/article/pii/S0198971519303655},
author = {Yiqun Chen and Soheil Sabri and Abbas Rajabifard and Muyiwa Elijah Agunbiade and Mohsen Kalantari and Sam Amirebrahimi},
keywords = {Urban analytics, Ontology, Semantic enrichment, OGC standards},
abstract = {The complexity, variety and volume of urban datasets have soared in the past decade. By utilising these datasets, urban planners and researchers have built a wide range of evidence-based methods and analytical tools for planning and decision-making purposes. However, the data heterogeneity remains one of the key problems in this process. Building generic urban analytics tools adaptable to diverse data formats remains largely unresolved in practice. This work proposes an innovative system called Urban Data Analytics Infrastructure (UADI) to tackle these challenges by leveraging on the advantages of ontology technologies. The proposed technique implements a bi-level mapping approach to consolidate heterogeneous datasets into a uniformed structure. This is presented in ontology schemas and hence offers a new means for developing generic tools for urban analytics. By applying bi-level mapping between data and ontology, the datasets are semantically enriched. This strategy allows data harmonisation, thus, heterogeneity problems could be mitigated. When building an analytics tool, researchers can simply declare the input data type as a specific concept of ontology and then follow the ontology schema to implement the code. The developed tool can be registered into the UADI system and then can work with any data mapped to the concept. The core components of the system include data registration, data enrichment, ontology management, translation engine, tool development and tool management. These are elaborately designed and developed to meet the design goals. The system usability and performance are validated by building a series of ISO 37120 indicators (for city services and quality of life) within the UADI.}
}
@article{SCHIFFERKANE2024104659,
title = {Converting OMOP CDM to phenopackets: A model alignment and patient data representation evaluation},
journal = {Journal of Biomedical Informatics},
volume = {155},
pages = {104659},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104659},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000777},
author = {Kayla Schiffer-Kane and Cong Liu and Tiffany J. Callahan and Casey Ta and Jordan G. Nestor and Chunhua Weng},
keywords = {Phenopackets schema, OMOP-CDM, Health data standards, Interoperability, Phenotyping, Data model},
abstract = {Objective
This study aims to promote interoperability in precision medicine and translational research by aligning the Observational Medical Outcomes Partnership (OMOP) and Phenopackets data models. Phenopackets is an expert knowledge-driven schema designed to facilitate the storage and exchange of multimodal patient data, and support downstream analysis. The first goal of this paper is to explore model alignment by characterizing the common data models using a newly developed data transformation process and evaluation method. Second, using OMOP normalized clinical data, we evaluate the mapping of real-world patient data to Phenopackets. We evaluate the suitability of Phenopackets as a patient data representation for real-world clinical cases.
Methods
We identified mappings between OMOP and Phenopackets and applied them to a real patient dataset to assess the transformation’s success. We analyzed gaps between the models and identified key considerations for transforming data between them. Further, to improve ambiguous alignment, we incorporated Unified Medical Language System (UMLS) semantic type-based filtering to direct individual concepts to their most appropriate domain and conducted a domain-expert evaluation of the mapping’s clinical utility.
Results
The OMOP to Phenopacket transformation pipeline was executed for 1,000 Alzheimer’s disease patients and successfully mapped all required entities. However, due to missing values in OMOP for required Phenopacket attributes, 10.2 % of records were lost. The use of UMLS-semantic type filtering for ambiguous alignment of individual concepts resulted in 96 % agreement with clinical thinking, increased from 68 % when mapping exclusively by domain correspondence.
Conclusion
This study presents a pipeline to transform data from OMOP to Phenopackets. We identified considerations for the transformation to ensure data quality, handling restrictions for successful Phenopacket validation and discrepant data formats. We identified unmappable Phenopacket attributes that focus on specialty use cases, such as genomics or oncology, which OMOP does not currently support. We introduce UMLS semantic type filtering to resolve ambiguous alignment to Phenopacket entities to be most appropriate for real-world interpretation. We provide a systematic approach to align OMOP and Phenopackets schemas. Our work facilitates future use of Phenopackets in clinical applications by addressing key barriers to interoperability when deriving a Phenopacket from real-world patient data.}
}
@article{SHAH2025101397,
title = {De-centering the anthropocentric worldview in language textbooks: A posthumanist call for discursive reparations for sustainable ELT},
journal = {Linguistics and Education},
volume = {86},
pages = {101397},
year = {2025},
issn = {0898-5898},
doi = {https://doi.org/10.1016/j.linged.2025.101397},
url = {https://www.sciencedirect.com/science/article/pii/S0898589825000154},
author = {Waqar Ali Shah and Qammar-un-nisa Jatoi and Ume Rabab Shah},
keywords = {ELT textbooks, Anthropocentrism, Neoliberalisation, Posthumanism, Discursive reparations},
abstract = {Our ecosystem has suffered severe material and epistemic damages as a consequence of contemporary neoliberal forces and Eurocentric onto-epistemologies in the Anthropocene era. These epistemic damages are also visible in how ELT textbooks are designed in local contexts. Informed by posthumanism and Southern epistemology, the present study analyzes the discursive/semiotic representation of nature, environment and human-nature relations in English language textbooks in Sindh province of Pakistan – highly affected climate region in South Asia. The study used Eco-CLA and multimodality as theoretical frameworks. The findings suggest that the English textbooks fail to incorporate localized sustainable thinking as well as lack references to marginalized communities affected by deteriorating ecological conditions in Pakistan. Instead, the textbooks tend to normalize the unsustainable stories connecting learning to the physical world in an anthropocentric, aestheticized, and neoliberal consumerist manner, while disregarding nonhuman entities as sentient entities. In light of our findings, we call for posthumanist discursive reparations informed by Southern epistemologies in order to rethink the writing of textbooks in ecologically affected global regions, including Sindh – the authors’ geo-epistemic context. This requires shifting away from anthropocentric disembodied conception of world to an embodied world characterized by nonduality, co-existence, entanglement and harmony.}
}
@article{RIBONI2019709,
title = {Sensor-based activity recognition: One picture is worth a thousand words},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {709-722},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.07.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19303863},
author = {Daniele Riboni and Marta Murtas},
keywords = {Activity recognition, Intelligent systems, Pervasive computing, Activity models, Unsupervised reasoning},
abstract = {In several domains, including healthcare and home automation, it is important to unobtrusively monitor the activities of daily living (ADLs) carried out by people at home. A popular approach consists in the use of sensors attached to everyday objects to capture user interaction, and ADL models to recognize the current activity based on the temporal sequence of used objects. Often, ADL models are automatically extracted from labeled datasets of activities and sensor events, using supervised learning techniques. Unfortunately, acquiring such datasets in smart homes is expensive and violates users’ privacy. Hence, an alternative solution consists in manually defining ADL models based on common sense, exploiting logic languages such as description logics. However, manual specification of ADL ontologies is cumbersome, and rigid ontological definitions fail to capture the variability of activity execution. In this paper, we introduce a radically new approach enabled by the recent proliferation of tagged visual contents available on the Web. Indeed, thanks to the popularity of social network applications, people increasingly share pictures and videos taken during the execution of every kind of activity. Often, shared contents are tagged with metadata, manually specified by their owners, that concisely describe the depicted activity. Those metadata represent an implicit activity label of the picture or video. Moreover, today’s computer vision tools support accurate extraction of tags describing the situation and the objects that appear in the visual content. By reasoning with those tags and their corresponding activity labels, we can reconstruct accurate models of a comprehensive set of human activities executed in the most disparate situations. This approach overcomes the main shortcomings of existing techniques. Compared to supervised learning methods, it does not require the acquisition of training sets of sensor events and activities. Compared to knowledge-based methods, it does not involve any manual modeling effort, and it captures a comprehensive array of execution modalities. Through extensive experiments with large datasets of real-world ADLs, we show that this approach is practical and effective.}
}
@article{KAZIM2022100526,
title = {On the sui generis value capture of new digital technologies: The case of AI},
journal = {Patterns},
volume = {3},
number = {7},
pages = {100526},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100526},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922001234},
author = {Emre Kazim and Enzo Fenoglio and Airlie Hilliard and Adriano Koshiyama and Catherine Mulligan and Markus Trengove and Abigail Gilbert and Arthur Gwagwa and Denise Almeida and Phil Godsiff and Kaska Porayska-Pomsta},
keywords = {value theory, information theory, digital assets, ontology, artificial intelligence},
abstract = {Summary
Much of the academic interest surrounding the emergence of new digital technologies has focused on forwarding the engineering literature, concentrating on the potential opportunities (economic, innovation, etc.) and harms (ethics, climate, etc.), with less focus on the foundational and theoretical shifts brought about by these technologies (e.g., what are “digital things”? What is the ontological nature and state of phenomena produced by and expressed in terms of digital products? Are there distinctions between the traditional conceptions of digital and non-digital technologies?. We investigate the question of what value is being expressed by an algorithm, which we conceptualize in terms of a digital asset, defining a digital asset as a valued digital thing that is derived from a particular digital technology (in this case, an algorithmic system). Our main takeaway is to invite the reader to consider artificial intelligence as a representation of the capture of value sui generis and that this may be a step change in the capture of value vis à vis the emergence of digital technologies.}
}
@article{NEJKOVIC2022103696,
title = {Head pose healthiness prediction using a novel image quality based stacked autoencoder},
journal = {Digital Signal Processing},
volume = {130},
pages = {103696},
year = {2022},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2022.103696},
url = {https://www.sciencedirect.com/science/article/pii/S105120042200313X},
author = {Valentina Nejkovic and Muhammed Maruf Öztürk and Nenad Petrovic},
keywords = {Head pose healthiness prediction, Stacked autoencoder, Image quality, Pose estimation, Ontology},
abstract = {This paper introduces an approach aiming to determine head pose healthiness of computer users. The main contributions of this paper are: 1) Image Quality Assessment (IQA) based Stacked Autoencoder (referred to as IQASAE) which adjusts the value of learning rate based on the quality of images; 2) Head Pose Healthiness Prediction (HPHP) framework which leverages the proposed IQASAE algorithm in combination with image processing operations; 3) A set of features suitable for face analysis applications; 4) Ontology-driven semantic framework which enables further exploiting pose estimation results within applications in synergy with healthcare expert domain knowledge about pose healthiness. Our framework was evaluated on both offline (BIWI and AFLW) and online (our own, collected using Arduino) datasets. Furthermore, it was compared to several state-of-art methods, including Multi-Layer Perceptron (MLP), CART, Random Forest, Convolutional Neural Networks (CNN), Temporal Deep Learning Model (TDLM), hybrid CNN with Support Vector Machine (SVM), Quatnet and Trinet. According to the achieved experimental results, it reaches accuracy up to 79.63% outperforming all of them, except Quatnet and Trinet. However, the main advantages of IQASAE compared to state-of-art methods are: 1) it does not require selection of features, so the processing time is reduced, 2) utilizing angle between chin and mouth reduces training time for SAE, 3) leveraging vector-based feature set to create training data resulted in a significant improvement, especially in offline facial images.}
}
@article{MOUSSALLEM20181,
title = {Machine Translation using Semantic Web Technologies: A Survey},
journal = {Journal of Web Semantics},
volume = {51},
pages = {1-19},
year = {2018},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300301},
author = {Diego Moussallem and Matthias Wauer and Axel-Cyrille Ngonga Ngomo},
keywords = {Machine Translation, Semantic Web, Ontology, Linked Data, Multilinguality, Knowledge Graphs},
abstract = {A large number of machine translation approaches have recently been developed to facilitate the fluid migration of content across languages. However, the literature suggests that many obstacles must still be dealt with to achieve better automatic translations. One of these obstacles is lexical and syntactic ambiguity. A promising way of overcoming this problem is using Semantic Web technologies. This article presents the results of a systematic review of machine translation approaches that rely on Semantic Web technologies for translating texts. Overall, our survey suggests that while Semantic Web technologies can enhance the quality of machine translation outputs for various problems, the combination of both is still in its infancy.}
}
@article{WU2022104059,
title = {Natural language processing for smart construction: Current status and future directions},
journal = {Automation in Construction},
volume = {134},
pages = {104059},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.104059},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521005100},
author = {Chengke Wu and Xiao Li and Yuanjun Guo and Jun Wang and Zengle Ren and Meng Wang and Zhile Yang},
keywords = {Smart construction, Construction 4.0, Artificial intelligence, Text mining, Natural language processing, Data mining, Project management, Construction management, Review},
abstract = {Unstructured texts dominate data in construction projects. With the achievements of natural language processing (NLP) techniques, mining unstructured text data for smart construction has become increasingly significant. To understand state-of-the-art NLP for smart construction, uncover related issues, and propose potential improvements, this paper presents a comprehensive review of bottom-level techniques and mainstream applications of NLP in the industry. In total, 124 journal articles published in the last two decades are reviewed. NLP involves five core steps supported by various techniques, e.g., syntactic parsing, heuristic rules, machine learning, and deep learning. NLP has been applied for information extraction and exchanging and many downstream applications to facilitate management and decision-making. The role of NLP in smart construction and current challenges for fully reaping its benefits are discussed, and four research directions are identified, i.e., improving relation extraction, realising knowledge base auto-development, integrating multi-modal information, and achieving an accuracy-efficiency trade-off by developing an NLP application framework. It is envisioned that outcomes of this paper can assist both researchers and industrial practitioners with appreciating the research and practice frontier of NLP for smart construction and soliciting the latest NLP techniques.}
}
@article{REHM2022100391,
title = {Researching digitalized work arrangements: A Laws of Form perspective},
journal = {Information and Organization},
volume = {32},
number = {2},
pages = {100391},
year = {2022},
issn = {1471-7727},
doi = {https://doi.org/10.1016/j.infoandorg.2022.100391},
url = {https://www.sciencedirect.com/science/article/pii/S1471772722000045},
author = {Sven-V. Rehm and Lakshmi Goel and Iris Junglas},
keywords = {Qualitative research, Interpretive research, Qualitative method, Ontology, Laws of Form, Digital, Agency, Identity, Artifact, Entanglement, Sociomaterial},
abstract = {Advances in digitalization have changed our apprehension of technology from discrete devices and application software as bounded artifacts, to dynamically evolving social-material entanglements in Digitalized Work Arrangements (DWA). This development makes studying DWAs increasingly difficult and challenges us to advance our methods that define how we can study, observe, and conceptualize DWAs. In this essay, we draw on the mathematical-logical formalism of the Laws of Form (LoF) (Spencer-Brown, 1969) to analyze how six illustrative IS studies conceptualize the social and the material to arrive at distinct perspectives on DWAs. Our analysis reveals three archetypes that capture these studies' conceptualizations and that inform a discussion of how IS research can extend qualitative methods beyond those six works. We offer two contributions. First, we provide novel insights and explanations to key conceptualizations in the study of DWAs. Second, we present the LoF as a pre-ontological and pre-theoretical formalism that enables commensurability of methods and development of novel qualitative empirical methods. Specifically, we demonstrate how the formalism helps articulating the distinctions we draw to refine our object of study and to critically examine and reconstruct other researchers' reasoning.}
}
@article{ZHANG2025112416,
title = {Modeling and verifying resources and capabilities of ubiquitous scenarios for Unmanned Aerial Vehicle swarm},
journal = {Journal of Systems and Software},
volume = {226},
pages = {112416},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2025.112416},
url = {https://www.sciencedirect.com/science/article/pii/S0164121225000846},
author = {Manqing Zhang and Yunwei Dong and Tao Zhang and Kang Su and Zeshan Li},
keywords = {Unmanned Aerial Vehicle swarm, Application scenarios, Meta-level theory, Formal verification},
abstract = {Unmanned Aerial Vehicle (UAV) swarms are increasingly deployed in both military and civilian sectors due to their ability to manage numerous resources, execute complex functionalities, and operate under strict spatiotemporal constraints in task-driven environments. However, existing task description methods are often restricted to specific operations and lack the flexibility to represent dynamic and intricate scenarios. To overcome these limitations, we introduce a meta-level theory-based UAV swarm application scenario model. This approach abstracts three primary meta-models: mission meta-model, resource meta-model, and constraint meta-model. We developed the UAV Swarm Application Scenario Modeling Language (ASML), which enables formal scenario descriptions and analysis. Additionally, we establish a set of transformation rules to convert ASML representations into timed automata. To validate the effectiveness of the proposed approach, we apply it to a highway inspection scenario and utilize the UPPAAL model checking tool to verify the correctness of the model. The experimental results from the highway inspection scenario show that our approach significantly enhances the accuracy of UAV swarm scenario modeling while improving adaptability to dynamic environments. Moreover, the results also demonstrate the model’s correctness, reinforcing the reliability of our framework.}
}
@article{YADAV202185,
title = {A comprehensive review on resolving ambiguities in natural language processing},
journal = {AI Open},
volume = {2},
pages = {85-92},
year = {2021},
issn = {2666-6510},
doi = {https://doi.org/10.1016/j.aiopen.2021.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666651021000127},
author = {Apurwa Yadav and Aarshil Patel and Manan Shah},
keywords = {Natural language processing, Requirement engineering, Machine learning, Ambiguity, Disambiguation},
abstract = {Natural language processing is a known technology behind the development of some widely known AI assistants such as: SIRI, Natasha, and Watson. However, NLP is a diverse technology used for numerous purposes. NLP based tools are widely used for disambiguation in requirement engineering which will be the primary focus of this paper. A requirement document is a medium for the user to deliver one's expectations from the software. Hence, an ambiguous requirement document may eventually lead to misconceptions in a software. Various tools are available for disambiguation in RE based on different techniques. In this paper, we analyzed different disambiguation tools in order to compare and evaluate them. In our survey, we noticed that even though some disambiguation tools reflect promising results and can supposedly be relied upon, they fail to completely eliminate the ambiguities. In order to avoid ambiguities, the requirement document has to be written using formal language, which is not preferred by users due to its lack of lucidity and readability. Nevertheless, some of the tools we mentioned in this paper are still under development and in future might become capable of eliminating ambiguities. In this paper, we attempt to analyze some existing research work and present an elaborative review of various disambiguation tools.}
}
@article{DING2023,
title = {Constructing a Knowledge Graph for the Chinese Subject Based on Collective Intelligence},
journal = {International Journal on Semantic Web and Information Systems},
volume = {19},
number = {1},
year = {2023},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.327355},
url = {https://www.sciencedirect.com/science/article/pii/S1552628323000108},
author = {Guozhu Ding and Peiying Yi and Xinru Feng},
keywords = {Knowledge Graph, Knowledge Ontology, Ontology Construction, Ontology Evolution, Subject Matter Learning Cell},
abstract = {ABSTRACT
Knowledge graphs are a valuable tool for intelligent tutoring systems and are typically constructed with a focus on objectivity and accuracy. However, they may not effectively capture the subjectivity and complex relationships often present in the humanities. To address this issue, a dynamic visualization of subject matter knowledge graph was developed using a collective intelligence approach that integrates the individual intelligence of learners and considers cognitive diversity to construct and evolve the knowledge graph. The approach resulted in the construction of 722 knowledge associations and the evolution of 584 triples. A survey assessed the effectiveness and user-friendliness, revealing that this approach is effective, easy to use, and can improve subject matter knowledge ontology. In conclusion, combining individual and collective intelligence is a promising approach for building effective knowledge graphs in subject areas with subjectivity and complexity.}
}
@article{JOHANSEN2024105159,
title = {Automated performance assessment of prevention through design and planning (PtD/P) strategies in construction},
journal = {Automation in Construction},
volume = {157},
pages = {105159},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105159},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523004193},
author = {K.W. Johansen and C. Schultz and J. Teizer},
keywords = {Automated construction safety analysis, Performance assessment, Benchmark and ground truth, Construction safety ontology, Construction safety innovation and transformation, Digital safety rule-checking and reasoning, Human-assisted decision-making tools},
abstract = {Construction sites are among the most dangerous workplaces due to their complex, dynamic, continuously changing work environment. Many existing workplace safety planning techniques rely on two-dimensional drawings and manual expertise. Such efforts are cumbersome as safety plans quickly become outdated as construction work progresses. There has been significant research into automated safety planning, yet no community-wide standard exists for objectively measuring and comparing automated safety assessment efficacy. To address this, an automated performance assessment framework is proposed. It evaluates input solutions regarding newly formalized quantitative soundness, completeness, and spatial correctness indicators. The ground truth of the deadliest hazard falls-from-height is collected through a workshop with domain experts. We validate the proposed framework in a case study, where the performance of our previously developed automated safety planning algorithm is assessed by our new performance assessment framework. The results yield valuable insights into the importance of automated evaluation frameworks that can convince practitioners to invest in human-assisted Prevention through Design and Planning strategies.}
}
@article{RIHM2024100031,
title = {The Digital Lab Facility Manager: Automating operations of research laboratories through “The World Avatar”},
journal = {Nexus},
volume = {1},
number = {3},
pages = {100031},
year = {2024},
issn = {2950-1601},
doi = {https://doi.org/10.1016/j.ynexs.2024.100031},
url = {https://www.sciencedirect.com/science/article/pii/S2950160124000299},
author = {Simon D. Rihm and Yong Ren Tan and Wilson Ang and Hou Yee Quek and Xinhong Deng and Michael Teguh Laksana and Jiaru Bai and Sebastian Mosbach and Jethro Akroyd and Markus Kraft},
keywords = {laboratory automation, facility management, airflow control, Internet of Things, The World Avatar, dynamic knowledge graphs, immersive reality technology},
abstract = {This paper presents a novel framework for digitalizing and automating the management of specialized research laboratories using The World Avatar, a general all-encompassing dynamic knowledge graph. Semantic Web technology empowers users to effectively utilize data across various systems and formats without restricting them to a single software or protocol. Employing agents and ontologies, it enables seamless data sharing, computational reasoning, and gradual automation of tasks, addressing obstacles in interoperability and adaptability. Showcasing the capabilities of this approach, we tackle some common challenges in lab facility management, including cost-effective Internet of Things (IoT) sensor network integration with an existing building management system and efficient airflow management for fume hoods via “human-in-the-loop” interventions. We also develop unified platform-agnostic interfaces that complement the framework. These advancements represent a significant stride in the holistic digitalization and automation of research laboratories at the nexus of fundamental science and smart building applications, setting a foundation for future research facilities to achieve operational excellence and sustainability.}
}
@incollection{MILANO2019199,
title = {Computing Languages for Bioinformatics: R},
editor = {Shoba Ranganathan and Michael Gribskov and Kenta Nakai and Christian Schönbach},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology},
publisher = {Academic Press},
address = {Oxford},
pages = {199-205},
year = {2019},
isbn = {978-0-12-811432-2},
doi = {https://doi.org/10.1016/B978-0-12-809633-8.20367-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128096338203671},
author = {Marianna Milano},
keywords = {Bioinformatics, Computer science, Computing language, Programming, R, R software},
abstract = {Bioinformatic analyses involve different tasks and processes. In order to manage various bioinformatics applications, different programs have been written by using various available computing languages. The languages used to tackle bioinformatics problems and related analysis are, for example, R, a statistical programming language, scripting languages such as Perl and Python, and compiled languages such as C, C++, and Java. Among these, R is becoming one of the most widely used software tools for bioinformatics. This is mainly due to its flexibility, and data handling and modeling capabilities.}
}
@article{ALACA2021103513,
title = {AgentDSM-Eval: A framework for the evaluation of domain-specific modeling languages for multi-agent systems},
journal = {Computer Standards & Interfaces},
volume = {76},
pages = {103513},
year = {2021},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2021.103513},
url = {https://www.sciencedirect.com/science/article/pii/S0920548921000088},
author = {Omer Faruk Alaca and Baris Tekin Tezel and Moharram Challenger and Miguel Goulão and Vasco Amaral and Geylani Kardas},
keywords = {Multi-agent system, Domain-specific modeling language, Agent-oriented software engineering, Evaluation framework, Human factors in Modelling, Usability evaluation, AgentDSM-Eval},
abstract = {Software development required for constructing multi-agent systems (MAS) usually becomes challenging and time-consuming due to the properties of autonomy, distributedness, and openness of these systems in addition to the complicated nature of internal agent behaviors and agent interactions. To facilitate MAS development, the researchers propose various domain-specific modeling languages (DSMLs) by enriching MAS metamodels with a defined syntax and semantics. Although the descriptions of these languages are given in the related studies with the examples of their use, unfortunately, many are not evaluated in terms of either the usability (being hard to learn, understand and use) or the quality of the generated artifacts. Hence, in this paper, we introduce an evaluation framework, called AgentDSM-Eval, with its supporting tool which can be used to evaluate MAS DSMLs systematically according to various quantitative and qualitative aspects of agent software development. The empirical evaluation, presented by the AgentDSM-Eval framework, was successfully applied for one of the well-known MAS DSMLs. The assessment showed that both MAS domain coverage of DSMLs and the agent developers’ adoption of modeling elements can be determined with this framework. Moreover, the tool's quantitative results can assess MAS DSML's performance on the development time and throughput. AgentDSM-Eval also enables the qualitative assessment of MAS DSML features according to novel quality characteristics and measures, which it defines specifically for the MAS domain.}
}
@article{KROPP2025105306,
title = {Modernist social imaginaries in research funding for urban experimentation – a qualitative analysis of expectations and visions in German-language funding calls},
journal = {Research Policy},
volume = {54},
number = {9},
pages = {105306},
year = {2025},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2025.105306},
url = {https://www.sciencedirect.com/science/article/pii/S0048733325001350},
author = {Cordula Kropp},
keywords = {Transformative Research, Real-World Labs, Living Labs, Urban Experimentation, Visions, Sociotechnical Imaginaries, Research Funding Policy},
abstract = {Real-world labs, urban living labs and similar forms of participatory experimental research formats have spread not least in response to extensive research funding. The aim of this article is to examine the underlying funding expectations and visions, as expressed in public third-party funding calls, which may influence design and outcomes of transformative research in urban labs. Conceptually, the research builds on insights about the political role of visions and imaginaries in science and technology studies. The findings from the German-language funding context indicate that only vague notions of sustainability are articulated in the calls for proposals, whereas a clear call for transdisciplinary cooperation is made. Three transformation visions were identified that guide the calls for third-party funding for urban experimentation: the achievement of future sustainability through Changes in Behavior and Awareness; Technological Problem Solving; and the Reorganization of Infrastructures. The analysis suggests that the funding calls in all three visions conveyed a modernist imaginary of how the research should contribute to urban sustainability. Few incentives were provided to experiment with fundamental change beyond ecological modernization.}
}
@article{MOREIRA2018137,
title = {Semantic interoperability and pattern classification for a service-oriented architecture in pregnancy care},
journal = {Future Generation Computer Systems},
volume = {89},
pages = {137-147},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.04.031},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17325438},
author = {Mário W.L. Moreira and Joel J.P.C. Rodrigues and Arun K. Sangaiah and Jalal Al-Muhtadi and Valery Korotaev},
keywords = {Semantic interoperability, Service-oriented architectures, Clinical decision support systems, Ontology, Electronic health systems, Hypertensive disorders in pregnancy},
abstract = {Semantic interoperability represents one of the main challenges in health information systems. The development of novel interoperability models should promote the integration of heterogeneous information in the acquisition and semantic analysis of complex data patterns, which are typically used in clinical information. The purpose of this study is to develop a knowledge-based decision support system that uses ontologies for integrating data related to hypertensive disorders in pregnancy. This model allows, when dealing with new cases, inferring from a knowledge base and predicting high-risk situations that could lead to serious problems during gestation in both pregnant women and fetuses. Results demonstrate that the use of ontologies to address semantically acquired patterns from different electronic health records has the potential to significantly influence a service-oriented architecture implementation for clinical decision support systems.}
}
@article{BOGUSLAV2023104405,
title = {Creating an ignorance-base: Exploring known unknowns in the scientific literature},
journal = {Journal of Biomedical Informatics},
volume = {143},
pages = {104405},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104405},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423001260},
author = {Mayla R. Boguslav and Nourah M. Salem and Elizabeth K. White and Katherine J. Sullivan and Michael Bada and Teri L. Hernandez and Sonia M. Leach and Lawrence E. Hunter},
keywords = {Natural language processing, Knowledge representation, Knowledge-base, Information extraction, Epistemology},
abstract = {Background:
Scientific discovery progresses by exploring new and uncharted territory. More specifically, it advances by a process of transforming unknown unknowns first into known unknowns, and then into knowns. Over the last few decades, researchers have developed many knowledge bases to capture and connect the knowns, which has enabled topic exploration and contextualization of experimental results. But recognizing the unknowns is also critical for finding the most pertinent questions and their answers. Prior work on known unknowns has sought to understand them, annotate them, and automate their identification. However, no knowledge-bases yet exist to capture these unknowns, and little work has focused on how scientists might use them to trace a given topic or experimental result in search of open questions and new avenues for exploration. We show here that a knowledge base of unknowns can be connected to ontologically grounded biomedical knowledge to accelerate research in the field of prenatal nutrition.
Results:
We present the first ignorance-base, a knowledge-base created by combining classifiers to recognize ignorance statements (statements of missing or incomplete knowledge that imply a goal for knowledge) and biomedical concepts over the prenatal nutrition literature. This knowledge-base places biomedical concepts mentioned in the literature in context with the ignorance statements authors have made about them. Using our system, researchers interested in the topic of vitamin D and prenatal health were able to uncover three new avenues for exploration (immune system, respiratory system, and brain development) by searching for concepts enriched in ignorance statements. These were buried among the many standard enriched concepts. Additionally, we used the ignorance-base to enrich concepts connected to a gene list associated with vitamin D and spontaneous preterm birth and found an emerging topic of study (brain development) in an implied field (neuroscience). The researchers could look to the field of neuroscience for potential answers to the ignorance statements.
Conclusion:
Our goal is to help students, researchers, funders, and publishers better understand the state of our collective scientific ignorance (known unknowns) in order to help accelerate research through the continued illumination of and focus on the known unknowns and their respective goals for scientific knowledge.}
}
@article{AKANBI2021103835,
title = {Design information extraction from construction specifications to support cost estimation},
journal = {Automation in Construction},
volume = {131},
pages = {103835},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103835},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521002867},
author = {Temitope Akanbi and Jiansong Zhang},
keywords = {Cost estimation, Automation, BIM interoperability, Natural language processing, Knowledge modeling, Ontology, Automated design information extraction},
abstract = {Construction cost estimation is a labor-intensive task that involves several processes. Although some of these processes have been automated, construction cost estimation still relies heavily on manual inputs. To compute a cost estimate, an estimator needs to: (1) take off quantities and extract some required cost information from the architectural model or drawing; (2) extract other required cost information from the construction specifications; (3) assign building elements to work or cost items; and (4) retrieve the unit cost of the work or cost items to further compute the cost estimate. To achieve full automation of construction cost estimation, the manual inputs required to classify building elements, and to retrieve pricing information of work items need to be automated. To address that, the authors proposed a new method that uses semantic modeling and natural language processing techniques in developing algorithms that automate the manual processes involved in: (1) extracting design information from construction specifications; (2) using the extracted information to match specified material in the construction specifications with items from an established database; and (3) retrieving the pricing information of the materials specified in the construction specifications. To test the validity of the authors' proposed method, an experiment was conducted using eight wood construction projects in Detroit, MI. The proposed method was utilized to develop an algorithm that can process the construction specifications automatically and retrieve the unit cost of materials from a database. The results from the developed algorithm were compared with the gold standard (results manually generated by industry experts). The developed algorithms achieved 99.2% precision and 99.2% recall (i.e., 99.2% F1-measure) for extracted design information instances; 100% precision and 96.5% recall (i.e., 98.2% F1-measure) for extracted materials from the database. The authors demonstrated that as the training data increases, the performance levels increase. The developed algorithms utilized 5.56% of the time it took using the current traditional method of extracting design information from construction specifications manually. These results showed that the proposed method is promising in developing algorithms that automate the processing of construction specifications to extract the design related information in fulfilling essential information requirements of detailed wood construction cost estimation and in retrieving the unit costs.}
}
@article{WANG20241915,
title = {Vocabulary Matters: An Annotation Pipeline and Four Deep Learning Algorithms for Enzyme Named Entity Recognition},
journal = {Journal of Proteome Research},
volume = {23},
number = {6},
pages = {1915-1925},
year = {2024},
issn = {1535-3907},
doi = {https://doi.org/10.1021/acs.jproteome.3c00367},
url = {https://www.sciencedirect.com/science/article/pii/S1535390724002981},
author = {Meiqi Wang and Avish Vijayaraghavan and Tim Beck and Joram M. Posma},
keywords = {biomedical natural language processing, deep learning, named entity recognition},
abstract = {Enzymes are indispensable in many biological processes, and with biomedical literature growing exponentially, effective literature review becomes increasingly challenging. Natural language processing methods offer solutions to streamline this process. This study aims to develop an annotated enzyme corpus for training and evaluating enzyme named entity recognition (NER) models. A novel pipeline, combining dictionary matching and rule-based keyword searching, automatically annotated enzyme entities in >4800 full-text publications. Four deep learning NER models were created with different vocabularies (BioBERT/SciBERT) and architectures (BiLSTM/transformer) and evaluated on 526 manually annotated full-text publications. The annotation pipeline achieved an F1-score of 0.86 (precision = 1.00, recall = 0.76), surpassed by fine-tuned transformers for F1-score (BioBERT: 0.89, SciBERT: 0.88) and recall (0.86) with BiLSTM models having higher precision (0.94) than transformers (0.92). The annotation pipeline runs in seconds on standard laptops with almost perfect precision, but was outperformed by fine-tuned transformers in terms of F1-score and recall, demonstrating generalizability beyond the training data. In comparison, SciBERT-based models exhibited higher precision, and BioBERT-based models exhibited higher recall, highlighting the importance of vocabulary and architecture. These models, representing the first enzyme NER algorithms, enable more effective enzyme text mining and information extraction. Codes for automated annotation and model generation are available from https://github.com/omicsNLP/enzymeNER and https://zenodo.org/doi/10.5281/zenodo.10581586.
}
}
@article{REISI2020102095,
title = {Transport sustainability indicators for an enhanced urban analytics data infrastructure},
journal = {Sustainable Cities and Society},
volume = {59},
pages = {102095},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102095},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720300822},
author = {Marzieh Reisi and Soheil Sabri and Muyiwa Agunbiade and Abbas Rajabifard and Yiqun Chen and Mohsen Kalantari and Azadeh Keshtiarast and Yan Li},
keywords = {Transport sustainability indicators, Data integration, Ontology-based analytics, Urban analytics},
abstract = {This paper examines capabilities of a new Spatial Data Infrastructure called Urban Analytics Data Infrastructure (UADI11Spatial Data Infrastructure called Urban Analytics Data Infrastructure), through deriving and evaluating transport sustainability indicators. The UADI was developed in Australia to support multi-disciplinary and cross-jurisdictional analytics to overcome the challenges related to model generalisation, data accessibility, data integration, data heterogeneity and city performance benchmarking. In this paper, the UADI were evaluated through 5 technical lenses of: data accessibility; integration; harmonisation; data reliability and model reliability. The paper shows that using open geospatial standards in UADI enabled transport sustainability indicators to be derived through accessing and integrating different spatial data layers and the process of mapping data to ontology facilitated data harmonisation. In addition, the input data, tool processing, and output of ontology-based transport sustainability indicators could be traced in UADI, which addresses the challenge of model and data reliability. The paper highlights the role of spatial data infrastructures in decision support systems for uncertainty analysis and promoting smart cities and resilient environment.}
}
@article{SCHAFER2024639,
title = {BioKGrapher: Initial evaluation of automated knowledge graph construction from biomedical literature},
journal = {Computational and Structural Biotechnology Journal},
volume = {24},
pages = {639-660},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024003386},
author = {Henning Schäfer and Ahmad Idrissi-Yaghir and Kamyar Arzideh and Hendrik Damm and Tabea M.G. Pakull and Cynthia S. Schmidt and Mikel Bahn and Georg Lodde and Elisabeth Livingstone and Dirk Schadendorf and Felix Nensa and Peter A. Horn and Christoph M. Friedrich},
keywords = {Knowledge graph, Named entity recognition, Entity linking, Clinical guidelines, Software},
abstract = {Background The growth of biomedical literature presents challenges in extracting and structuring knowledge. Knowledge Graphs (KGs) offer a solution by representing relationships between biomedical entities. However, manual construction of KGs is labor-intensive and time-consuming, highlighting the need for automated methods. This work introduces BioKGrapher, a tool for automatic KG construction using large-scale publication data, with a focus on biomedical concepts related to specific medical conditions. BioKGrapher allows researchers to construct KGs from PubMed IDs. Methods The BioKGrapher pipeline begins with Named Entity Recognition and Linking (NER+NEL) to extract and normalize biomedical concepts from PubMed, mapping them to the Unified Medical Language System (UMLS). Extracted concepts are weighted and re-ranked using Kullback-Leibler divergence and local frequency balancing. These concepts are then integrated into hierarchical KGs, with relationships formed using terminologies like SNOMED CT and NCIt. Downstream applications include multi-label document classification using Adapter-infused Transformer models. Results BioKGrapher effectively aligns generated concepts with clinical practice guidelines from the German Guideline Program in Oncology (GGPO), achieving F1-Scores of up to 0.6. In multi-label classification, Adapter-infused models using a BioKGrapher cancer-specific KG improved micro F1-Scores by up to 0.89 percentage points over a non-specific KG and 2.16 points over base models across three BERT variants. The drug-disease extraction case study identified indications for Nivolumab and Rituximab. Conclusion BioKGrapher is a tool for automatic KG construction, aligning with the GGPO and enhancing downstream task performance. It offers a scalable solution for managing biomedical knowledge, with potential applications in literature recommendation, decision support, and drug repurposing.}
}
@article{MULLER20211890,
title = {Production specific language characteristics to improve NLP applications on the shop floor},
journal = {Procedia CIRP},
volume = {104},
pages = {1890-1895},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.319},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121012178},
author = {Marvin Müller and Joachim Metternich},
keywords = {digital shop floor management, shop floor language specifics, natural language processing},
abstract = {A variety of assistance functions have been developed based on the rising data availability on the shop floor and increasing capabilities of artificial intelligence applications. An often-mentioned risk is the low data quality, especially in manual text entries for e.g. deviations or defects. This paper aims to evaluate production specific language characteristics to adjust natural language processing applications. To achieve this goal three industry data sets are analyzed, and the findings are used to improve a recommendation engine for previously solved problems.}
}
@article{BROCHHAGEN2022105179,
title = {When do languages use the same word for different meanings? The Goldilocks principle in colexification},
journal = {Cognition},
volume = {226},
pages = {105179},
year = {2022},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2022.105179},
url = {https://www.sciencedirect.com/science/article/pii/S0010027722001676},
author = {Thomas Brochhagen and Gemma Boleda},
keywords = {Language universals, Colexification, Cognitive effort, Ambiguity, Efficient communication},
abstract = {Lexical ambiguity is pervasive in language, and often systematic. For instance, the Spanish word dedo can refer to a toe or a finger, that is, these two meanings colexify in Spanish; and they do so as well in over one hundred other languages. Previous work shows that related meanings are more likely to colexify. This is attributed to cognitive pressure towards simplicity in language, as it makes lexicons easier to learn and use. The present study examines the interplay between this pressure and the competing pressure for languages to support accurate information transfer. We hypothesize that colexification follows a Goldilocks principle that balances the two pressures: meanings are more likely to attach to the same word when they are related to an optimal degree—neither too much, nor too little. We find support for this principle in data from over 1200 languages and 1400 meanings. Our results thus suggest that universal forces shape the lexicons of natural languages. More broadly, they contribute to the growing body of evidence suggesting that languages evolve to strike a balance between competing functional and cognitive pressures.}
}
@article{HOLTZAPPLE20242621,
title = {The BioRECIPE Knowledge Representation Format},
journal = {ACS Synthetic Biology},
volume = {13},
number = {8},
pages = {2621-2624},
year = {2024},
issn = {2161-5063},
doi = {https://doi.org/10.1021/acssynbio.4c00096},
url = {https://www.sciencedirect.com/science/article/pii/S2161506324000445},
author = {Emilee Holtzapple and Gaoxiang Zhou and Haomiao Luo and Difei Tang and Niloofar Arazkhani and Casey Hansen and Cheryl A. Telmer and Natasa Miskov-Zivanov},
keywords = {modeling, representation format, FAIR principles, signaling pathways, networks, automation},
abstract = {The BioRECIPE (Biological system Representation for Evaluation, Curation, Interoperability, Preserving, and Execution) knowledge representation format was introduced to standardize and facilitate human–machine interaction while creating, verifying, evaluating, curating, and expanding executable models of intra- and intercellular signaling. This format allows a human user to easily preview and modify any model component, while it is at the same time readable by machines and can be processed by a suite of model development and analysis tools. The BioRECIPE format is compatible with multiple representation formats, natural language processing tools, modeling tools, and databases that are used by the systems and synthetic biology communities.
}
}
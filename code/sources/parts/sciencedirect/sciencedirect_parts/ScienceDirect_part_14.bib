@article{KHANBABAEI201878,
title = {Developing an integrated framework for using data mining techniques and ontology concepts for process improvement},
journal = {Journal of Systems and Software},
volume = {137},
pages = {78-95},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.11.019},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217302613},
author = {Mohammad Khanbabaei and Farzad Movahedi Sobhani and Mahmood Alborzi and Reza Radfar},
keywords = {Data mining, Process improvement, Ontology, Classification, Clustering},
abstract = {Process, as an important knowledge resource, must be effectively managed and improved. The main problems are the large number of processes, their specific features, and the complicated relationships between them, which all lead to the increase in complexity and create a high-dimensionality problem. Traditional process management systems are unable to manage and improve processes with a high volume of data. Data mining techniques, however, can be employed to identify valuable patterns. With the aid of these patterns, suggestions for process improvement can be presented. Further, process ontology can be applied to share the process patterns between people, facilitate the process understanding, and develop the reusability of the extracted patterns for process improvement. This study presents a combined three-part, five-stage framework of data mining, process improvement, and process ontology. To evaluate the applicability and effectiveness of the proposed framework, a real process dataset is applied. Two clustering and classification techniques are used to discover valuable patterns as the process ontology. The output of these two techniques can be considered as the recommendations for improving the processes. The proposed framework can be exploited to support process improvement methodologies in organizations.}
}
@article{ANNANE2020100563,
title = {GBKOM: A generic framework for BK-based ontology matching},
journal = {Journal of Web Semantics},
volume = {63},
pages = {100563},
year = {2020},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2020.100563},
url = {https://www.sciencedirect.com/science/article/pii/S1570826820300111},
author = {Amina Annane and Zohra Bellahsene},
keywords = {Ontology matching, Ontology alignment, Background knowledge, Indirect matching, External resource, Anchoring, Derivation, Background knowledge selection},
abstract = {BK-based matching exploits external background knowledge resources (BK) to fill the semantic gap between the ontologies to align. Existing BK-based matchers implement the indirect matching approach in their internal architecture, which makes any adaptation or reuse of the code difficult. Indeed, to improve a particular step in the BK-based matching process, it is necessary to code the whole process from scratch which requires a lot of time and effort. To overcome this issue, we propose a flexible framework called Generic BK-based Matcher (GBKOM). GBKOM is an extension that can be added to any existing matcher. It is a configurable framework that implements the BK-based matching process, with a rich set of parameters making it customizable and suitable for performing experimental evaluations. GBKOM has participated, with YAM++ as a direct matcher, in the OAEI 2017 and OAEI 2017.5 campaigns, where it has been successful on the biomedical benchmarks, and top-ranked in several tasks. Furthermore, we have performed experiments with two other direct matchers (i.e., LogMap and LogMapLite) to show that the effectiveness of GBKOM is independent of the direct matcher used.}
}
@article{VECCHI2020101245,
title = {DNA is not an ontologically distinctive developmental cause},
journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
volume = {81},
pages = {101245},
year = {2020},
issn = {1369-8486},
doi = {https://doi.org/10.1016/j.shpsc.2019.101245},
url = {https://www.sciencedirect.com/science/article/pii/S1369848618301031},
author = {Davide Vecchi},
keywords = {Development, Causation, Determination, Switch-point, Causal specificity, Causal parity, Robustness},
abstract = {In this article I critically evaluate the thesis that DNA is an ontologically distinctive developmental cause. I shall critically analyse different versions of the latter thesis by taking into consideration concrete developmental cases. I shall argue that DNA is neither a developmental determinant nor an ontologically distinctive developmental cause. Instead, I shall argue that mechanistic analysis shows that DNA's causal role in development depends on the higher robustness of the developmental processes in which it exerts its causal capacities. The focus on process and developmental system implies a metaphysical shift: rather than attributing to DNA molecules biochemically unique properties, I suggest that it might be better to think about DNA's causal role in development in terms of the causal capacities that DNA molecules manifest in a rich developmental milieu. I shall also suggest that my position is distinct both from the view advocating the instrumental primacy of DNA-centric biology and developmental constructionism. It is different from the former because it provides a substantial answer to the question of what makes DNA causally central in developmental processes. Finally, I argue that evolutionary considerations pose an important challenge to developmental constructionism.}
}
@incollection{TROKANAS2018471,
title = {Towards a Methodology for Reusable Ontology Engineering: Application to the Process Engineering Domain},
editor = {Anton Friedl and Jiří J. Klemeš and Stefan Radl and Petar S. Varbanov and Thomas Wallek},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {43},
pages = {471-476},
year = {2018},
booktitle = {28th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-64235-6.50084-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044464235650084X},
author = {Nikolaos Trokanas and Linsey Koo and Franjo Cecelja},
keywords = {Ontological Engineering, Methodology, Ontology Reuse},
abstract = {This paper proposes a methodology for ontological engineering with an aim to develop reusable ontologies. The proposed methodology combines experience of developing ontology engineering using a ‘good’ practice with established methodologies and concomitantly implementing reusability in the eSymbiosis ontology.}
}
@article{ELGHARBAWY2019234,
title = {Ontology-based adaptive testing for automated driving functions using data mining techniques},
journal = {Transportation Research Part F: Traffic Psychology and Behaviour},
volume = {66},
pages = {234-251},
year = {2019},
issn = {1369-8478},
doi = {https://doi.org/10.1016/j.trf.2019.07.021},
url = {https://www.sciencedirect.com/science/article/pii/S1369847818306351},
author = {M. Elgharbawy and A. Schwarzhaupt and M. Frey and F. Gauterin},
keywords = {Event-based time-series analysis, Data mining, Hierarchical agglomerative clustering, Ontology-based test scenario synthesis, Hardware-in-the-Loop co-simulation platform},
abstract = {This paper presents an adaptive verification framework for automated driving functions based on ontologies and data-mining techniques. Despite the recent rapid growth of driver assistance systems to consolidate road safety, they still have various challenges coping with dynamic traffic situations of daily life. Therefore, automotive systems engineering has established data- and knowledge-driven test methods to assure the required functional safety and reliability in a highly safety-critical context. However, the reliance on field testing is inadequate and, in particular, time- and cost-intensive when applied to the next generation of automated driving functions, e.g. collision-free emergency braking and vehicle platooning. The presented framework utilises an ontology-based test scenario synthesis to identify criticality margins using a Hardware-in-the-Loop co-simulation platform for automated driving functions. Additionally, we demonstrate a systematic process to complement virtual testing by extracting insights from field testing database using event-based time-series analysis. To this end, data mining techniques are used to obtain representative scenarios witnessed in real-world traffic. Agglomerative hierarchical clustering is performed to extract homogeneous groups (clusters) from recorded triggering events by proximity metrics using normalised cross-correlations. Extracted scenarios are subsequently used at earlier stages of development to effectively and efficiently ensure reliability and safety. In summary, the results show the benefits and some of the challenges of using the industry-proven framework, which enables a cost-effective extension of test domain vaidility throughout software product engineering.}
}
@incollection{MORRIS202599,
title = {Chapter 7 - Natural language processing and large language models use in healthcare},
editor = {Robert J.T. Morris},
booktitle = {Healthcare Transformation using Artificial Intelligence},
publisher = {Academic Press},
pages = {99-111},
year = {2025},
isbn = {978-0-443-28969-9},
doi = {https://doi.org/10.1016/B978-0-443-28969-9.00015-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443289699000157},
author = {Robert J.T. Morris},
keywords = {Alignment, Concept normalization, Continual pretraining, Medical LLMs, Named entity recognition, Question-answering, Relationship extraction, Summarization, Supervised fine tuning},
abstract = {The use of AI techniques of natural language understanding and generation is described and compared in the context of question-answering and summarization for healthcare. Leading medical LLMs are examined and compared, and one is tested with examples.}
}
@article{BOTOEVA20191,
title = {Query inseparability for ALC ontologies},
journal = {Artificial Intelligence},
volume = {272},
pages = {1-51},
year = {2019},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370219300189},
author = {Elena Botoeva and Carsten Lutz and Vladislav Ryzhikov and Frank Wolter and Michael Zakharyaschev},
keywords = {Description logic, Knowledge base, Conjunctive query, Query inseparability, Computational complexity, Tree automaton},
abstract = {We investigate the problem whether two ALC ontologies are indistinguishable (or inseparable) by means of queries in a given signature, which is fundamental for ontology engineering tasks such as ontology versioning, modularisation, update, and forgetting. We consider both knowledge base (KB) and TBox inseparability. For KBs, we give model-theoretic criteria in terms of (finite partial) homomorphisms and products and prove that this problem is undecidable for conjunctive queries (CQs), but 2ExpTime-complete for unions of CQs (UCQs). The same results hold if (U)CQs are replaced by rooted (U)CQs, where every variable is connected to an answer variable. We also show that inseparability by CQs is still undecidable if one KB is given in the lightweight DL EL and if no restrictions are imposed on the signature of the CQs. We also consider the problem whether two ALC TBoxes give the same answers to any query over any ABox in a given signature and show that, for CQs, this problem is undecidable, too. We then develop model-theoretic criteria for HornALC TBoxes and show using tree automata that, in contrast, inseparability becomes decidable and 2ExpTime-complete, even ExpTime-complete when restricted to (unions of) rooted CQs.}
}
@article{RAZIASULTHANA2019498,
title = {Ontology and context based recommendation system using Neuro-Fuzzy Classification},
journal = {Computers & Electrical Engineering},
volume = {74},
pages = {498-510},
year = {2019},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2018.01.034},
url = {https://www.sciencedirect.com/science/article/pii/S0045790617337382},
author = {A. {Razia Sulthana} and Subburaj Ramasamy},
keywords = {Recommendation systems, Context, Domain ontology, Neuro-fuzzy rules, Classification, Features set, Weightage matrix, Cold start},
abstract = {Recommendation Systems (RS) identify the products of likely interest to the user. In earlier studies, the recommendation systems classify the reviews of products as positive or negative, verbatim on the features contained in the reviews, without reference to their context. In this paper, we develop an Ontology and Context Based Recommendation System (OCBRS) to assess the context of and determine the opinion of the review. We propose a Neuro-Fuzzy Classification approach using fuzzy rules to extract the context of the review. This approach automatically classifies the reviews under the respective fuzzy rule. Ontology facilitates a hierarchical and systematic methodology to group the context and acts as repository of context. The proposed approach appears to improve the accuracy of the RS.}
}
@article{KONDINSKI20242070,
title = {Knowledge graph representation of zeolitic crystalline materials††Electronic supplementary information (ESI) available. See DOI: https://doi.org/10.1039/d4dd00166d},
journal = {Digital Discovery},
volume = {3},
number = {10},
pages = {2070-2084},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d4dd00166d},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24001669},
author = {Aleksandar Kondinski and Pavlo Rutkevych and Laura Pascazio and Dan N. Tran and Feroz Farazi and Srishti Ganguly and Markus Kraft},
abstract = {Zeolites are complex and porous crystalline inorganic materials that serve as hosts for a variety of molecular, ionic and cluster species. Formal, machine-actionable representation of this chemistry presents a challenge as a variety of concepts need to be semantically interlinked. This work demonstrates the potential of knowledge engineering in overcoming this challenge. We develop ontologies OntoCrystal and OntoZeolite, enabling the representation and instantiation of crystalline zeolite information into a dynamic, interoperable knowledge graph called The World Avatar (TWA). In TWA, crystalline zeolite instances are semantically interconnected with chemical species that act as guests in these materials. Information can be obtained via custom or templated SPARQL queries administered through a user-friendly web interface. Unstructured exploration is facilitated through natural language processing using the Marie System, showcasing promise for the blended large language model – knowledge graph approach in providing accurate responses on zeolite chemistry in natural language.}
}
@article{KARTHIK2021107396,
title = {A fuzzy recommendation system for predicting the customers interests using sentiment analysis and ontology in e-commerce},
journal = {Applied Soft Computing},
volume = {108},
pages = {107396},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107396},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621003197},
author = {R.V. Karthik and Sannasi Ganapathy},
keywords = {Ecommerce, Sentiment analysis, User reviews, Recommendation, Opinion, Sentiment score, Ontology, Fuzzy rule},
abstract = {In Electronic commerce, customer reviews play a significant role in purchase making decision. Most of the existing recommendation systems consider the customer reviews, user purchase history and product rating for predicting the recommended product. Since the users interest are varying over time, the existing recommendation systems lack in finding the current relevant items to the customers. To overcome this problem, this article proposes a new fuzzy logic-based product recommendation system which dynamically predicts the most relevant products to the customers in online shopping according to the users’ current interests. A novel algorithm has been proposed in this paper for computing the sentimental score of the product with associated end user target category. Finally, the proposed fuzzy rules and ontology-based recommendation system uses ontology alignment for making decisions that are more accurate and predict dynamically based on the search context. The experimental results of the proposed recommendation system show better performance than the existing product recommendation systems in terms of prediction accuracy of the relevant products for target users and in the time taken to provide such recommendations.}
}
@article{KRUTAK201999,
title = {Therapeutic tattooing in the Arctic: Ethnographic, archaeological, and ontological frameworks of analysis},
journal = {International Journal of Paleopathology},
volume = {25},
pages = {99-109},
year = {2019},
issn = {1879-9817},
doi = {https://doi.org/10.1016/j.ijpp.2018.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S1879981717300785},
author = {Lars Krutak},
keywords = {Therapeutic tattooing, Tattooed mummies, Indigenous ontologies, Indices of care, Arctic},
abstract = {This essay describes the potential for using ethnographic evidence and mummified tattooed skin to reflect on past therapeutic tattoo practice in the Arctic. It also considers the ways in which circumpolar concepts of disease emerged in relation to the agency of nonhuman entities. I argue that specific forms of curative tattooing offer interpretive models for the paleopathological and bioarchaeological study of care through an ontological framework of analysis.}
}
@article{TIWARI2022,
title = {Adaptive Ontology-Based IoT Resource Provisioning in Computing Systems},
journal = {International Journal on Semantic Web and Information Systems},
volume = {18},
number = {1},
year = {2022},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.306260},
url = {https://www.sciencedirect.com/science/article/pii/S1552628322000345},
author = {Ashish Tiwari and Ritu Garg},
keywords = {High-Performance Computing, Internet of Things, Ontology Method, Ontology Tools, Rough Set Theory, Scheduling Technique, Service Providers, Simulator},
abstract = {ABSTRACT
The eagle expresses of cloud computing plays a pivotal role in the development of technology. The aim is to solve in such a way that it will provide an optimized solution. The key role of allocating these efficient resources and making the algorithms for its time and cost optimization. The approach of the research is based on the rough set theory RST. RST is a great method for making a large difference in qualitative analysis situations. It’s a technique to find knowledge discovery and handle the problems such as inductive reasoning, automatic classification, pattern recognition, learning algorithms, and data reduction. The rough set theory is the new method in cloud service selection so that the best services provide for cloud users and efficient service improvement for cloud providers. The simulation of the work is finished at intervals with the merchandise utilized for the formation of the philosophy framework. The simulation shows the IoT services provided by the IoT service supplier to the user are the best utilization with the parameters and ontology technique.}
}
@article{DAVARPANAH2024100167,
title = {Knowledge-based query system for the critical minerals},
journal = {Applied Computing and Geosciences},
volume = {22},
pages = {100167},
year = {2024},
issn = {2590-1974},
doi = {https://doi.org/10.1016/j.acags.2024.100167},
url = {https://www.sciencedirect.com/science/article/pii/S2590197424000144},
author = {Armita Davarpanah and Hassan A. Babaie and W. Crawford Elliott},
keywords = {Basic formal ontology, BFO, Ontology, Critical minerals, Query system},
abstract = {Critical minerals are increasingly used in advanced, modern technologies. Exploration for these minerals require efficient mechanisms to search for the latest geological knowledge about the petrogenesis and spatial distribution of these essential resources. Although the current text-based deposit classification schemes help geoscientists to understand how and where these critical minerals form, they cannot easily be queried by software without extensive natural language processing and knowledge modeling. Ontologies can explicitly specify the knowledge scattered in the texts and tables of these schemes and the Critical Minerals Mapping Initiative (CMMI) database by way of logical structures whose results can automatically be processed and queried. They can also draw new knowledge by inference from the ones that are explicitly specified in them. These qualities make ontologies a perfect choice for digital knowledge storage, search, and extraction. The Critical Minerals Ontology (CMO) is described herein by reusing the logical class and property structures of the top-level Basic Formal Ontology (BFO) and mid-level Common Core Ontologies (CCO) and Relation Ontology (RO). The CMO formally models the knowledge about the critical mineral systems using the latest deposit classification scheme and the CMMI database schema. The ontology specifies the geochemical and geological processes that operate in various geotectonic environments of mineral systems to form the critical minerals in different deposit types. It models the properties of both the host minerals that contain the rare-earth elements and those that bear other types of elements. The CMO also represents uses of specific critical minerals in the manufacturing of industrial products, their alternate substitutes, and countries that produce, import, and export them. A query system, applying the Python programming language, accesses the knowledge modeled in the CMO and allows users through interactive web pages to query the ontology and extract different types of information from it. The ontology and the query system are useful for research in ore mineralogy and critical mineral prospecting. The information modeled by the ontology and served by the query system allows users to classify their ore specimen data into specific deposit types.}
}
@article{THAKAR2018762,
title = {Enterprise Level Integration of Ontology Engineering and Process Mining for Management of Complex Data and Processes to improve Decision System},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {30},
pages = {762-767},
year = {2018},
note = {18th IFAC Conference on Technology, Culture and International Stability TECIS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.11.200},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318328659},
author = {Tejan Thakar and Tenzin Tsultrim and Larry Stapleton and Liam Doyle},
keywords = {Enterprise integration, Systems interoperability, Enterprise network design, implementation, Enterprise Systems, business process analysis, complex systems},
abstract = {Software development may involve international-scale, dynamic business processes that consume and generate data in complex ways which may not be obvious to management. This loads risks for global data management projects. This paper investigates an approach combining process mining and knowledge engineering to help manage complex data assets in an international software development process. The research engaged a data management team and other stakeholders over a critical one year period during which the company was involved in an acquisition of another similar sized company. This added significantly to the overall complexity of the enterprise context and decision process. Serious challenges existed with systemic complexity, including the silo-ed nature of IT assets which were not readily amenable to modelling dynamic networks of processes. Preliminary results presented here showed that some features of the combined process mining/ontology development framework would address process management complexity, aiding control of that complex data environment.}
}
@article{COLOMBO2025104082,
title = {An LLM-assisted ETL pipeline to build a high-quality knowledge graph of the Italian legislation},
journal = {Information Processing & Management},
volume = {62},
number = {4},
pages = {104082},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104082},
url = {https://www.sciencedirect.com/science/article/pii/S030645732500024X},
author = {Andrea Colombo and Anna Bernasconi and Stefano Ceri},
keywords = {Law, Knowledge graph, Property graph, Large language models, Data quality},
abstract = {The increasing complexity of legislative systems, characterized by an ever-growing number of laws and their interdependencies, has highlighted the utility of Knowledge Graphs (KGs) as an effective data model for organizing such information, compared to traditional methods, often based on relational models, which struggle to efficiently represent interlinked data, such as references within laws, hindering efficient knowledge discovery. A paradigm shift in modeling legislative data is already ongoing with the adoption of common international standards, predominantly XML-based, such as Akoma Ntoso (AKN) and the Legal Knowledge Interchange Format, which aim to capture fundamental aspects of laws shared across different legislations and simplify the task of creating Knowledge Graphs through the use of XML tags and identifiers. However, to enable advanced analysis and data discovery within these KGs, it is necessary to carefully check, complement, and enrich KG nodes and edges with properties, either metadata or additional derived knowledge, that enhance the quality and utility of the model, for instance, by leveraging the capabilities of state-of-the-art Large Language Models. In this paper, we present an ETL pipeline for modeling and querying the Italian legislation in a Knowledge Graph, by adopting the property graph model and the AKN standard implemented in the Italian system. The property graph model offers a good compromise between knowledge representation and the possibility of performing graph analytics, which we consider essential for enabling advanced pattern detection. Then, we enhance the KG with valuable properties by employing carefully fine-tuned open-source LLMs, i.e., BERT and Mistral-7B models, which enrich and augment the quality of the KG, allowing in-depth analysis of legislative data.}
}
@article{CHIESA2018e299,
title = {PRE.M.I.S.E. (PREdiction Models in Stereotactic External Radiation Therapy). Could the Creation of an Ontology Pave the Way for a Prediction Model for Stereotactic Radiation Therapy?},
journal = {International Journal of Radiation Oncology*Biology*Physics},
volume = {102},
number = {3, Supplement },
pages = {e299},
year = {2018},
note = {Proceedings of the Amercian Society for Radiation Oncology},
issn = {0360-3016},
doi = {https://doi.org/10.1016/j.ijrobp.2018.07.944},
url = {https://www.sciencedirect.com/science/article/pii/S0360301618323988},
author = {S. Chiesa and S. Longo and F. Bianciardi and B. Tolu and B. Nardiello and F. Rea and G. Stimato and L. Capone and D. {Di Carlo} and F. Cellini and C. Masciocchi and M. Massaccesi and G. Minniti and A. Pacchiarotti and V. Lanzotti and M. Balducci and A. Damiani and V. Valentini and P. Gentile}
}
@article{OMAR2020101796,
title = {Semi-automated development of conceptual models from natural language text},
journal = {Data & Knowledge Engineering},
volume = {127},
pages = {101796},
year = {2020},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2020.101796},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X19301429},
author = {Mussa Omar and George Baryannis},
keywords = {Conceptual modelling, Information extraction, Natural language processing, Ontologies, Semi-structured data},
abstract = {The process of converting natural language specifications into conceptual models requires detailed analysis of natural language text, and designers frequently make mistakes when undertaking this transformation manually. Although many approaches have been used to partly automate this process, one of the main limitations is the lack of a domain-independent ontology that can be used as a repository for entities and relationships, thus guiding the transformation process. In this paper, a semi-automated system for mapping natural language text into conceptual models is proposed. The system, called SACMES, combines a linguistic approach with an ontological approach and human intervention to achieve the task. SACMES learns from the natural language specifications that it processes and stores the information that is learnt in a conceptual model ontology and a user history knowledge database. It then uses the stored information to improve performance and reduce the need for human intervention. The evaluation conducted on SACMES demonstrates that: (1) by using the system, precision and recall for users identifying entities of conceptual models is increased by 6% and 13%, respectively, while for relationships, increases are even higher, 14% for precision and 23% for recall; (2) the performance of the system is improved by processing more natural language requirements, and thus, the need for human intervention is decreased.}
}
@article{FAZZINGA2018354,
title = {Ontological query answering under many-valued group preferences in Datalog+/–},
journal = {International Journal of Approximate Reasoning},
volume = {93},
pages = {354-371},
year = {2018},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2017.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X17301068},
author = {Bettina Fazzinga and Thomas Lukasiewicz and Maria Vanina Martinez and Gerardo I. Simari and Oana Tifrea-Marciuska},
keywords = {Top-k query answering, Preferences, Social choice, Ontological query answering, Datalog+/–, Existential rules},
abstract = {The Web has recently been changing more and more to what is called the Social Semantic Web. As a consequence, the ranking of search results no longer depends solely on the structure of the interconnections among Web pages. In this paper, we argue that such rankings can be based on user preferences from the Social Web and on ontological background knowledge from the Semantic Web. We propose an approach to top-k query answering under user preferences in Datalog+/– ontologies, where the queries are unions of conjunctive queries with safe negation, and the preferences are defined via numerical values. To this end, we also generalize the previous RankJoin algorithm to our framework. Furthermore, we explore the generalization to the preferences of a group of users. Finally, we provide experimental results on the performance and quality of our algorithms.}
}
@article{STAVROPOULOS201987,
title = {SemaDrift: A hybrid method and visual tools to measure semantic drift in ontologies},
journal = {Journal of Web Semantics},
volume = {54},
pages = {87-106},
year = {2019},
note = {Managing the Evolution and Preservation of the Data Web},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300258},
author = {T.G. Stavropoulos and S. Andreadis and E. Kontopoulos and I. Kompatsiaris},
keywords = {Semantic drift, Concept drift, Semantic change, Ontologies, Versioning},
abstract = {Semantic drift is an active field of research, aiming to identify and measure changes in ontologies across versions in time, closely related to several fields such as ontology evolution and versioning. However, practical and widely adopted methods for measuring semantic drift are mostly either not directly applied to Semantic Web formalisms or largely dependent on specific models and domains. This paper presents a novel hybrid method, which combines existing identity-based and morphing-based approaches, and, in turn, introduces further insights such as hybrid chains of concepts across ontology versions and concept stability ranking. Furthermore, it introduces the SemaDrift Application Suite, which integrates both new and existing methods, including structural and text similarity measures, provided as open-source. The applications offer a graphical user interface to calculate and visually explore semantic drift metrics, in order to support and promote their use to Semantic Web and domain experts alike. Specifically, the SemaDrift plugin for the popular Protègè platform is intended for ontology engineers, while SemaDriftFx allows a wider audience of users to visually investigate drift either as numeric output or visually through graphs. Two use case scenarios demonstrate the applicability and usefulness of the methods and tools in the domain of digital preservation and Web Services, with insights previously hard to obtain. Standard scale and concrete end-user evaluation tasks were used to acquire positive preliminary feedback on the usability of the SemaDriftFx tool.}
}
@article{SONNENBURG2024153933,
title = {Artificial intelligence-based data extraction for next generation risk assessment: Is fine-tuning of a large language model worth the effort?},
journal = {Toxicology},
volume = {508},
pages = {153933},
year = {2024},
issn = {0300-483X},
doi = {https://doi.org/10.1016/j.tox.2024.153933},
url = {https://www.sciencedirect.com/science/article/pii/S0300483X24002142},
author = {Anna Sonnenburg and Benthe {van der Lugt} and Johannes Rehn and Paul Wittkowski and Karsten Bech and Florian Padberg and Dimitra Eleftheriadou and Todor Dobrikov and Hans Bouwmeester and Carla Mereu and Ferdinand Graf and Carsten Kneuer and Nynke I. Kramer and Tilmann Blümmel},
keywords = {Artificial intelligence, Risk Assessment, Systematic literature review, Automated data extraction, Large Language models, Fine-tuning},
abstract = {To underpin scientific evaluations of chemical risks, agencies such as the European Food Safety Authority (EFSA) heavily rely on the outcome of systematic reviews, which currently require extensive manual effort. One specific challenge constitutes the meaningful use of vast amounts of valuable data from new approach methodologies (NAMs) which are mostly reported in an unstructured way in the scientific literature. In the EFSA-initiated project ‘AI4NAMS’, the potential of large language models (LLMs) was explored. Models from the GPT family, where GPT refers to Generative Pre-trained Transformer, were used for searching, extracting, and integrating data from scientific publications for NAM-based risk assessment. A case study on bisphenol A (BPA), a substance of very high concern due to its adverse effects on human health, focused on the structured extraction of information on test systems measuring biologic activities of BPA. Fine-tuning of a GPT-3 model (Curie base model) for extraction tasks was tested and the performance of the fine-tuned model was compared to the performance of a ready-to-use model (text-davinci-002). To update findings from the AI4NAMS project and to check for technical progress, the fine-tuning exercise was repeated and a newer ready-to-use model (text-davinci-003) served as comparison. In both cases, the fine-tuned Curie model was found to be superior to the ready-to-use model. Performance improvement was also obvious between text-davinci-002 and the newer text-davinci-003. Our findings demonstrate how fine-tuning and the swift general technical development improve model performance and contribute to the growing number of investigations on the use of AI in scientific and regulatory tasks.}
}
@article{SANFILIPPO2019174,
title = {Editorial: Formal Ontologies meet Industry},
journal = {Procedia Manufacturing},
volume = {28},
pages = {174-176},
year = {2019},
note = {7th International conference on Changeable, Agile, Reconfigurable and Virtual Production (CARV2018)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.12.028},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918313702},
author = {Emilio Sanfilippo and Walter Terkaj}
}
@incollection{ABTAHI2021733,
title = {ProMo variable/equation Ontology-based systemmodelling},
editor = {Metin Türkay and Rafiqul Gani},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {50},
pages = {733-738},
year = {2021},
booktitle = {31st European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-88506-5.50115-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323885065501157},
author = {Niloufar Abtahi and Heinz A. Preisig},
keywords = {Multidisciplinary and multi-scale simulations, Ontology-based framework, ProMovariable/equation builder},
abstract = {The description of materials in the context of thermodynamics is an essential part of the chemical engineer’s models. The representation of thermodynamic relations is the objective of this study. By making use of ProMo variable/equation builder, we construct all the thermodynamics quantities based on the configuration space of contact geometry. The configuration space builds on the quantities internal energy (U), entropy (S), Volume (V) and mass species (n) adding the equations of state being the derivatives yielding the temperature, the negative pressure and the chemical potential. ProMo expert domain provides the facilities to construct variable/equation sets from the conserved basic physical quantities, namely the ones defined above. It follows the strict rule of defining new variables as a function of existing variables. This ProMo building regulation is an excellent match to the contact geometry’s fundamental approach to define thermodynamics. It generates a consistent set of variables and equations and provides a formal description that builds on pure mathematical terms. This mathematical representation also facilitates and simplifies automatic multiscale simulation modelling. The use of well-defined variables and equations for the base ontologies system domain, facilitate an integrating factor for the application domain both for chemical engineers as well as the material multidisciplinary modelling community.}
}
@incollection{JIN201869,
title = {Chapter 5 - Domain Environment Ontology Construction},
editor = {Zhi Jin},
booktitle = {Environment Modeling-Based Requirements Engineering for Software Intensive Systems},
publisher = {Morgan Kaufmann},
address = {Oxford},
pages = {69-84},
year = {2018},
isbn = {978-0-12-801954-2},
doi = {https://doi.org/10.1016/B978-0-12-801954-2.00005-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128019542000054},
author = {Zhi Jin},
keywords = {Basic state machine, Causality, Domain environment ontology, Tree-like hierarchical state machine},
abstract = {This chapter mainly introduces the representation and construction of the environment ontology. For environment ontology, extension to the normal ontology structure is about the state machine–based behavior representation of causal entities. The chapter is devoted to presenting techniques for building domain environment ontologies, i.e., application domain-dependent ontologies, for the purpose of specifying environment modeling system capabilities.}
}
@article{BROWNING2018106,
title = {Geostrategies, geopolitics and ontological security in the Eastern neighbourhood: The European Union and the ‘new Cold War’},
journal = {Political Geography},
volume = {62},
pages = {106-115},
year = {2018},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2017.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0962629816302967},
author = {Christopher S. Browning},
keywords = {European neighbourhood policy, Geopolitics, Geostrategies, Ontological security, New Cold War},
abstract = {Recent years have seen the EU criticised for its naïve idealism, in particular in its failure to counter Russia's increasingly assertive manoeuvres. While Russia is presented as an inherently geopolitical actor, the EU's emphasis on a normative post-geopolitical agenda is depicted as a losing strategy. The EU, it is argued, must become ‘more geopolitical’ in what is presented as an emerging ‘new Cold War’. However, post-geopolitical depictions of the EU are problematic, but derive from an overly narrow conflation of geopolitics with modernist geopolitical practices. In contrast, the article argues that the EU's actions are no less impregnated with geopolitical visions aimed at ordering and organising the space beyond its borders, but also argues that the EU's geopolitical visions – and the geostrategies adopted to implement them – are also underpinned by a need to preserve and protect the Union's sense of ontological security. This connection between its geopolitical visions, geostrategies and sense of ontological security is important, as it means challenges to the former can generate considerable anxieties in regard to the latter; anxieties that need a response. The article argues that the return of traditional geopolitical language can be understood in these terms, calming emerging anxieties by reaffirming a sense of order and stability in terms of an historically known set of coordinates. Although seductive, this move of (mis)recognising contemporary events in terms of historical analogy is also potentially problematic.}
}
@article{DENKENA2021547,
title = {Ontology-based production planning under the consideration of system robustness},
journal = {Procedia CIRP},
volume = {104},
pages = {547-552},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.092},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121009902},
author = {Berend Denkena and Marc-André Dittrich and Gina Vibora Münch},
keywords = {Robustness, Production planning, Ontology, Manufacturing systems},
abstract = {Volatile markets and high customer requirements regarding schedule reliability increase the relevance of robust production planning. To achieve robustness in planning, system-inherent buffers are used. Buffers include resource capacities that are kept free to respond to changes. Targets other than achieving the production plan are not considered, so a trade-off between reliability and the further development of the manufacturing process is not possible. This paper presents a new approach for production planning based on robustness analysis that enables a multi-criteria optimization. An information system enables the company-specific design of the robustness analysis.}
}
@article{SIVAMAYILVELAN2025111622,
title = {Building explainable artificial intelligence for reinforcement learning based debt collection recommender system using large language models},
journal = {Engineering Applications of Artificial Intelligence},
volume = {159},
pages = {111622},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.111622},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625016240},
author = {Keerthana Sivamayilvelan and Elakkiya Rajasekar and Subramaniyaswamy Vairavasundaram and Santhi Balachandran and Vishnu Suresh},
keywords = {Interpretability, Large language models, Prompt engineering, Recommender systems, Reinforcement learning, Text explanations, And zero-shot prompting},
abstract = {Managing debt collections can be a challenging task. Managing the collection process and customer details requires human effort and time. We built a Reinforcement Learning (RL) based recommendation system to optimize this collection process. The model has to generate suggestions for boosting the collection rate, thereby saving time for manual tasks. The suitable actions were developed according to customers' risk levels and debt collector performance. The reinforcement Learning algorithm works based on the principle of exploration and exploitation. Initially, the model generates the recommendations based on the fixed policies for observed cases. When the model encounters a new scenario during the explorations, it has to create the recommendations by itself. It is necessary to understand why the model generated the particular recommendation. To build such an explainable recommendation system, we used Google Vertex Artificial Intelligence (AI) to generate text-based explanations for decisions made by the model. Moreover, we compared the different large language models through different prompting methods. We used prompt engineering concepts to get the desired text explanations. Bilingual Evaluation Understudy (BLEU) and Recall Oriented Understudy For Gisting Evaluation (ROUGE) were the accuracy metrics used in this work to analyze the performance of the text generation. Our contribution not only advances interpretability in the debt collection process but can also be implemented in various real-world settings such as healthcare, e-commerce recommendations, etc. The proposed method achieved a higher collection rate in three risk categories, such as very low risk, low risk, and medium risk.}
}
@article{MCGARRY201834,
title = {RESKO: Repositioning drugs by using side effects and knowledge from ontologies},
journal = {Knowledge-Based Systems},
volume = {160},
pages = {34-48},
year = {2018},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2018.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S0950705118303332},
author = {Ken McGarry and Yitka Graham and Sharon McDonald and Anuam Rashid},
keywords = {Side-effects, Graph theory, Pattern matching, Protein targets, Ontologies},
abstract = {The objective of drug repositioning is to apply existing drugs to different diseases or medical conditions than the original target, and thus alleviate to a certain extent the time and cost expended in drug development. Our system RESKO, REpositioning drugs using Side Effects and Knowledge from Ontologies, identifies drugs with similar side-effects which are potential candidates for use elsewhere, the supposition is that similar side-effects may be caused by drugs targeting similar proteins and pathways. RESKO, integrates drug chemical data, protein interaction and ontological knowledge. The novel aspects of our system include a high level of biological knowledge through the use of pathway and biological ontology integration. This provides a explanation facility lacking in most of the existing methods and improves the repositioning process. We evaluate the shared side effects from the eight conventional Alzheimer drugs, from which sixty-seven candidate drugs based on a side-effect commonality were identified. The top 25 drugs on the list were further investigated in depth for their suitability to be repositioned, the literature revealed that many of the candidate drugs appear to have been trialed for Alzheimer’s disease. Thus verifying the accuracy of our system, we also compare our technique with several competing systems found in the literature.}
}
@article{ABDELLATIF20187,
title = {Overcoming business process reengineering obstacles using ontology-based knowledge map methodology},
journal = {Future Computing and Informatics Journal},
volume = {3},
number = {1},
pages = {7-28},
year = {2018},
issn = {2314-7288},
doi = {https://doi.org/10.1016/j.fcij.2017.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S2314728817300296},
author = {Mahmoud AbdEllatif and Marwa Salah Farhan and Naglaa Saeed Shehata},
keywords = {Business process reengineering, Knowledge map, Ontology, Analytic hierarchical processing},
abstract = {Business process reengineering (BPR) is identified as one of the most important solutions for organizational improvements in all performance measures of business processes. However, high failure rates 70% is reported about using it the most important reason that caused the failure is the focus on the process itself; regardless of the surrounding environment, and the knowledge of the organization. The other reasons are due to the lack of tools to determine the causes of the inconsistencies and inefficiencies. This paper proposes Process Reengineering Ontology-based knowledge Map Methodology (PROM) to reduce the failure ratio, solve BPR problems, and overcome their difficulties. Using an organizational ontology to show the structure and environment surrounding to organization's processes, using knowledge maps as an inference that succeeds to identify and find out the causes that lead to contradictions and inefficiencies, and using Analytical hierarchy processing to identify and prioritize processes of the business to be re-designed. Through the proposed methodology, all organizational processes are completely analyzed. Moreover, Analytical Hierarchy Processing technique is used to show the most important processes with high priority to be reengineered first then it is easy to discover any errors occurred during reengineering process through knowledge map so BPR is done successfully. Finally, Apply the proposed methodology to inventory management shows how processes reengineering are done successfully and helping the organization to achieve its objectives.}
}
@article{FERREIRA201835,
title = {Codex: A metamodel ontology to guide the execution of coding experiments},
journal = {Computer Standards & Interfaces},
volume = {59},
pages = {35-44},
year = {2018},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2018.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0920548917303197},
author = {Waldemar Ferreira and Maria Teresa Baldassarre and Sergio Soares},
keywords = {Metamodel, Experiment, Software engineering, Coding experiment, Ontology},
abstract = {Background: Experiments have been conducted in many domains of software engineering (SE). Objective: This paper presents a metamodel, the Codex metamodel, containing standard concepts found in any coding experiment. We classify coding experiments as any SE experiment where participants have to carry out coding activities (construct, test, debug, and forth). Method: The paper presents results of an exploratory study that proposes a metamodel for the domain of coding experiments. Besides, we present how our metamodel specifies a real coding experiment. Results: Our metamodel for coding experiments was modeled with few elements, and it can precisely describe all coding activities. Conclusions: Our metamodel facilitates the development of a tool to support executing a coding experiment.}
}
@article{MAALEL2019193,
title = {Towards a Collaborative Approach to Decision Making Based on Ontology and Multi-Agent System Application to crisis management},
journal = {Procedia Computer Science},
volume = {164},
pages = {193-198},
year = {2019},
note = {CENTERIS 2019 - International Conference on ENTERprise Information Systems / ProjMAN 2019 - International Conference on Project MANagement / HCist 2019 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.172},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919322112},
author = {Ahmed Maalel and Henda Ben Ghézala},
keywords = {Decision Making, Crisis Management, Ontology, Multi-Agent System, Collaboration},
abstract = {The coordination and cooperation of all the stakeholders involved is a decisive point for the control and the resolution of problems. In the insecurity events, the resolution should refer to a plan that defines a general framework of the procedures to be undertaken and the instructions to be complied with; also, a more precise process must be defined by the actors to deal with the case represented by the particular problem of the current situation. Indeed, this process has to cope with a dynamic, unstable and unpredictable environment, due to the heterogeneity and multiplicity of stakeholders, and finally due to their possible geographical distribution. In this article, we will present the first steps of validation of a collaborative decision-making approach in the context of crisis situations such as road accidents. This approach is based on ontologies and multi-agent systems.}
}
@article{YIN2025107388,
title = {A novel approach to unlocking the synergy of large language models and chemical knowledge in biomedical signal applications},
journal = {Biomedical Signal Processing and Control},
volume = {103},
pages = {107388},
year = {2025},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.107388},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424014460},
author = {Zilong Yin and Haoyu Wang and Bin Chen and Hangling Sun and Anji Li and Chenyu Zhou},
keywords = {Biomedical signal processing, Supervised chemical knowledge, Large language models, Molecular property prediction},
abstract = {This work explores the potential of using the pre-trained large language model Llama2 to address challenges in biomedical signal processing and control (BSPC), particularly in predicting the electronic and functional properties of organic molecules, an area of growing importance in fields such as drug discovery and materials science. Current approaches in BSPC often rely on specialized graph neural network models, which can be limited in their ability to capture the complex relationships inherent in molecular structures. To address this, we demonstrate that a fine-tuned Llama2 model can accurately predict the highest occupied molecular orbital (HOMO) and lowest unoccupied molecular orbital (LUMO) energies of organic semiconductor molecules, with performance comparable to state-of-the-art specialized models. To further enhance the model’s robustness and generalization, we propose several key innovations, including optimized simplified molecular input line entry system (SMILES) tokenization, incorporation of chemical knowledge as auxiliary supervised tasks, and a low-rank adaptation (LORA) based fine-tuning strategy. These techniques enable the language model to simultaneously learn SMILES prediction and acquire relevant chemical knowledge, while also improving its handling of incomplete structural information and ability to generalize to ”unseen” molecular classes. The work also discusses the limitations of using large language models for molecular property prediction, such as the lack of interpretability and the need for improved handling of non-standard SMILES representations, highlighting the potential of this approach in BSPC while identifying areas for further improvement.}
}
@article{LENHARD2024105,
title = {A child of prediction. On the History, Ontology, and Computation of the Lennard-Jonesium},
journal = {Studies in History and Philosophy of Science},
volume = {103},
pages = {105-113},
year = {2024},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2023.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0039368123001668},
author = {Johannes Lenhard and Simon Stephan and Hans Hasse},
keywords = {Molecular dynamics, Simulation, Computation, Modeling, Prediction},
abstract = {The Lennard-Jones (LJ) fluid, named after mathematician-physicist-chemist Sir John Lennard-Jones (1894–1954), occupies a special place among fluids. It is an ideal entity, defined as the fluid whose particles interact according to the Lennard-Jones potential. This paper expounds the history of the LJ fluid to throw light on the tensions between theory and computational practice. The paper argues for the following claims. Firstly, the computational approach—even prior to the computer—pragmatically aims at prediction, not truth. Secondly, computer simulation methods, especially “molecular dynamics” (MD), triggered a change in epistemology. Now, simulated model fluids became targets of investigation in their own right. The urge for prediction turned the LJ fluid into the most investigated fluid in engineering thermodynamics. Thirdly, MD took a huge upswing in the 1990s, due to exploratory options in simulation. We discuss how, under these conditions, predictive success might be fraught with problems of reproducibility.}
}
@article{BARBOSA2022,
title = {A Context-Independent Ontological Linked Data Alignment Approach to Instance Matching},
journal = {International Journal on Semantic Web and Information Systems},
volume = {18},
number = {1},
year = {2022},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.295977},
url = {https://www.sciencedirect.com/science/article/pii/S1552628322000448},
author = {Armando Barbosa and Ig I. Bittencourt and Sean W. Siqueira and Diego Dermeval and Nicholas J. T. Cruz},
keywords = {Domain-Independent, Instance Matching, Linked Data, Linked Open Data, Ontology Alignment, Schema-Independent, Semantic Web},
abstract = {ABSTRACT
Linking data by finding matching instances in different datasets requires considering many characteristics, such as structural heterogeneity, implicit knowledge, and uniform resource identifier-oriented (URI-oriented) identification. The authors propose a context-independent approach to align linked data through an alignment process based on the ontological model’s components and considering data’s multidimensionality. The researchers experimented with the proposed approach against two methods for aligning linked data in two datasets and evaluated precision, recall, and f-measure metrics. The authors also conducted a case study in a real scenario considering a Brazilian publication dataset on computers and education. This study’s results indicate that the proposed approach overcomes the other methods (regarding the precision, recall, and f-measure metrics), requiring less work when changing the dataset domain. This work’s main contributions include enabling real datasets to be semi-automatically linked and presenting an approach capable of calculating resource similarity.}
}
@article{DHANDA2025102937,
title = {Reviewing human-robot collaboration in manufacturing: Opportunities and challenges in the context of industry 5.0},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {93},
pages = {102937},
year = {2025},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2024.102937},
url = {https://www.sciencedirect.com/science/article/pii/S0736584524002242},
author = {Mandeep Dhanda and Benedict Alexander Rogers and Stephanie Hall and Elies Dekoninck and Vimal Dhokia},
keywords = {Human Robot Collaboration, Digital Manufacturing, Industry 5.0, Artificial Intelligence, Ontology},
abstract = {Industry 4.0 (I4.0) has been characterized by the increasing use of automation, artificial intelligence, and big data in manufacturing. It has brought different machines, tools, robots and devices together through integration with cyber-physical systems as well as Internet of Things and computer systems. This has dramatically improved efficiency, productivity, and flexibility of automated systems, but it has also raised concerns about the impact of automation on jobs, the ethical considerations and the future of work in general. Industry 5.0 (I5.0) is the next manufacturing paradigm evolution and builds on I4.0 with the addition of ‘people’, in which robots will be designed to work alongside humans in a safe and efficient manner. Human-robot collaboration (HRC) is its key enabler. In manufacturing, HRC has the potential to improve safety, efficiency, and productivity by allowing humans to focus on tasks that require creativity, judgment, and flexibility, while robots perform more repetitive and dangerous tasks. This paper explores the concept of HRC and its advancement within 21st century industry. It identifies the opportunities and challenges arising from the interactions between robots and humans in manufacturing applications, assembly, and inspection. It also highlights the significance of HRC in I4.0 and its potential in I5.0. In addition, the role of artificial intelligence, machine learning, large language models, information modelling (ontologies) and new emerging digital technologies (augmented reality, virtual reality, digital twins, cyber-physical system) in the development of HRC and I5.0 is documented and discussed adding new perspectives to the growing literature in this area. This investigation sheds light on the emerging paradigms that have come about as parts of I5.0 and the transformative role of human-robot interaction in shaping the future of manufacturing. This critical review provides a realistic picture of manufacturing automation and the benefits and weaknesses of current HRC systems. It presents a researched view on the concept, needs, enabling technologies and system frameworks of human-robot interaction in manufacturing, providing a practical vision and research agenda for future work in this area and its associated systems.}
}
@article{CUI2025104861,
title = {A review on knowledge graphs for healthcare: Resources, applications, and promises},
journal = {Journal of Biomedical Informatics},
volume = {169},
pages = {104861},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2025.104861},
url = {https://www.sciencedirect.com/science/article/pii/S1532046425000905},
author = {Hejie Cui and Jiaying Lu and Ran Xu and Shiyu Wang and Wenjing Ma and Yue Yu and Shaojun Yu and Xuan Kan and Chen Ling and Liang Zhao and Zhaohui S. Qin and Joyce C. Ho and Tianfan Fu and Jing Ma and Mengdi Huai and Fei Wang and Carl Yang},
keywords = {Knowledge graph, Healthcare, language models, Multimodality, Interpretable AI},
abstract = {Objective:
This comprehensive review aims to provide an overview of the current state of Healthcare Knowledge Graphs (HKGs), including their construction, utilization models, and applications across various healthcare and biomedical research domains.
Methods:
We thoroughly analyzed existing literature on HKGs, covering their construction methodologies, utilization techniques, and applications in basic science research, pharmaceutical research and development, clinical decision support, and public health. The review encompasses both model-free and model-based utilization approaches and the integration of HKGs with large language models (LLMs).
Results:
We searched Google Scholar for relevant papers on HKGs and classified them into the following topics: HKG construction, HKG utilization, and their downstream applications in various domains. We also discussed their special challenges and the promise for future work.
Discussion:
The review highlights the potential of HKGs to significantly impact biomedical research and clinical practice by integrating vast amounts of biomedical knowledge from multiple domains. The synergy between HKGs and LLMs offers promising opportunities for constructing more comprehensive knowledge graphs and improving the accuracy of healthcare applications.
Conclusions:
HKGs have emerged as a powerful tool for structuring medical knowledge, with broad applications across biomedical research, clinical decision-making, and public health. This survey serves as a roadmap for future research and development in the field of HKGs, highlighting the potential of combining knowledge graphs with advanced machine learning models for healthcare transformation.}
}
@article{GADSBY2019102772,
title = {Body representations and cognitive ontology: Drawing the boundaries of the body image},
journal = {Consciousness and Cognition},
volume = {74},
pages = {102772},
year = {2019},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2019.102772},
url = {https://www.sciencedirect.com/science/article/pii/S1053810018305853},
author = {Stephen Gadsby},
keywords = {Body image, Body schema, Long-term, Anorexia nervosa, Alice in wonderland syndrome, Body representation, Cognitive ontology},
abstract = {The distinction between body image and body schema has been incredibly influential in cognitive neuroscience. Recently, researchers have begun to speculate about the relationship between these representations (Gadsby, 2017, 2018; Pitron & de Vignemont, 2017; Pitron et al., 2018). Within this emerging literature, Pitron et al. (2018) proposed that the long-term body image and long-term body schema co-construct one another, through a process of reciprocal interaction. In proposing this model, they make two assumptions: that the long-term body image incorporates the spatial characteristics of tools, and that it is distorted in the case of Alice in wonderland syndrome. Here, I challenge these assumptions, with a closer examination of what the term “long-term body image” refers to. In doing so, I draw out some important taxonomic principles for research into body representation.}
}
@article{ASTOBIZA2025101617,
title = {The role of LLMs in theory building},
journal = {Social Sciences & Humanities Open},
volume = {11},
pages = {101617},
year = {2025},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2025.101617},
url = {https://www.sciencedirect.com/science/article/pii/S2590291125003456},
author = {Aníbal M. Astobiza},
keywords = {large linguistic models, Data-driven epistemology, Meaning, Theories, Dataism},
abstract = {Large linguistic models, such as GPT-3.5 and subsequent versions (e.g. GPT-4 or GPT-4o), have shown impressive abilities in generating human-like text and performing a variety of natural language processing tasks. However, a fundamental question in the field of artificial intelligence is whether these models can truly represent meaning and assist scientists in building scientific theories. This paper aims to address this question by conducting a thorough conceptual analysis of existing large linguistic models and their capabilities in representing and reasoning about meaning for the purpose of theory building. My conclusions suggest that while these models have made significant progress in representing and manipulating language, they still face limitations in their ability to represent abstract and complex concepts, and the application of these models in building scientific theories should be guided by specific research questions and informed hypotheses that can be tested and developed into robust theories.}
}
@article{QIN2018129,
title = {Towards an ontology-supported case-based reasoning approach for computer-aided tolerance specification},
journal = {Knowledge-Based Systems},
volume = {141},
pages = {129-147},
year = {2018},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2017.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S0950705117305348},
author = {Yuchu Qin and Wenlong Lu and Qunfen Qi and Xiaojun Liu and Meifa Huang and Paul J Scott and Xiangqian Jiang},
keywords = {Computer-aided tolerance specification, Tolerance specification scheme, Tolerance specification problem, Case-based reasoning, Ontology, Similarity measure},
abstract = {In this paper, an ontology-supported case-based reasoning approach for computer-aided tolerance specification is proposed. This approach firstly considers the past tolerance specification problems and their schemes as previous cases and the new tolerance specification problems as target cases and uses an ontology to represent previous and target cases. Then certain ontology-based similarity measure is used to assess the similarity between the toleranced features of target and previous cases, the similarity between the part features of target and previous cases, and the similarity between the topological relations of target and previous cases. Based on these similarities, an ontology-based similarity measure for computing the similarity between target and previous cases is designed, and an algorithm for establishing such similarity measure with high accuracy and retrieving similar previous cases for a target case with this similarity measure is presented. This algorithm shows how to linearly combine the similarity of toleranced features, the similarity of part features, and the similarity of topological relations to assess the similarity between target and previous cases to implement retrieval of previous cases under the prerequisite of ensuring the highest accuracy of the similarity measure. The paper also reports a prototype implementation of the proposed approach, provides an example to illustrate how the approach works, and evaluates the approach via theoretical and experimental comparisons.}
}
@article{BI201839,
title = {Research on ontology for geospatial Web services11Supported by National key research and development program of China: Key International Standard Research of Strategic Emerging Industries (First stage), 2016YFF0202700.},
journal = {Geomatica},
volume = {72},
number = {2},
pages = {39-57},
year = {2018},
issn = {1195-1036},
doi = {https://doi.org/10.1139/geomat-2018-0018},
url = {https://www.sciencedirect.com/science/article/pii/S1195103624000570},
author = {Jiantao Bi and Jean Brodeur and Jiankun Guo and Xingxing Wang},
keywords = {ontology, Semantic Web, geospatial information, web service, interoperability, ontologie, web sémantique, information géospatiale, service Web, interopérabilité},
abstract = {Discovery and access of Web services for geographic information on the Semantic Web has not been addressed yet by the Semantic Web community or by the geographic information community. However, ISO/TC 211 in the ISO Technical Specification ISO/TS 19150-1:2012, Geographic information — Ontology — Part 1: Framework provides a plan to cover this purpose. Recently, ISO/TC 211 approved a new project ISO 19150-4, Geographic information — Ontology — Part 4: Service ontology, for the development of a new ISO standard that deals with Web services for geographic information. This ISO standard has reached the draft international standard stage. This paper aims at providing an overall description of the standard including the ontological framework for geographic information services and a crosswalk with other frameworks for Web services (such as OWL-S, SWSO, WSMO) to support interoperability with them.
Résumé
La découverte et l’accès aux services Web pour l’information géographique sur le Web sémantique n’ont pas encore été abordés par la communauté du Web sémantique ni par la communauté de l’information géographique. Toutefois, l’ISO/TC 211, dans la spécification technique ISO/TS 19150-1:2012, Information géographique — Ontologie – Partie 1 : Cadre de travail, fournit un plan pour atteindre cet objectif. Récemment, l’ISO/TC 211 a approuvé un nouveau projet ISO 19150-4, Information géographique — Ontologie – Partie 4 : Ontologie de service, pour le développement d’une nouvelle norme ISO qui traite des services Web pour l’information géographique. Cette norme ISO a atteint le stade d’ébauche de norme internationale. La présente communication vise à donner une description générale de la norme, y compris le cadre ontologique pour les services d’information géographique et une correspondance avec d’autres cadres de travail pour les services Web (par exemple OWL-S, SWSO, WSMO) afin d’appuyer l’interopérabilité avec eux. [Traduit par la Rédaction]}
}
@incollection{OLABANJO2025157,
title = {Chapter 9 - Natural Language Processing for Earth resource management: a case of H2 Golden Retriever research},
editor = {Deepak Kumar and Tavishi Tewary and Sulochana Shekhar},
booktitle = {Data Analytics and Artificial Intelligence for Earth Resource Management},
publisher = {Elsevier},
pages = {157-183},
year = {2025},
isbn = {978-0-443-23595-5},
doi = {https://doi.org/10.1016/B978-0-443-23595-5.00009-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443235955000097},
author = {Olusola Olabanjo and Paul Seurin and Joseph Wiggins and Lorien Pratt and Loveneesh Rana and Rozhin Yasaei and Gregory Renard},
keywords = {Hydrogen water electrolysis, ontology, knowledge graph, natural language processing, decision intelligence},
abstract = {Hydrogen is recognized as a crucial element in the future of green energy, playing a pivotal role in decarbonizing the economy. The pursuit of low-cost, high-performance, and durable materials for electrolysis has underscored the need for advanced tools to facilitate evidence-based decision-making in hydrogen research funding. This study introduces the H2 Golden Retriever (H2GR) system, a comprehensive platform leveraging Natural Language Processing (NLP), Knowledge Graph (KG), and Decision Intelligence for effective hydrogen knowledge discovery and representation. Hydrogen-related papers were systematically gathered from the web and subjected to preprocessing techniques such as noise and stop-word removal, language and spell checks, stemming, and lemmatization. NLP tasks included Named Entity Recognition using Stanford and Spacy NER, as well as topic modeling through Latent Dirichlet Allocation and term frequency-inverse document frequency (TF-IDF) analysis. The KG module facilitated the identification of meaningful entities, relationships, trends, and patterns within the realm of hydrogen research. The Decision Intelligence component created a simulation environment, capturing cost and quantity dependencies. The PageRank algorithm was employed to rank papers based on relevance. The H2GR system underwent random searches, yielding results that comprised a ranked list of papers, relevant entities, relationship graphs, an ontology of H2 production, and Causal Decision Diagrams illustrating component interactivity. Qualitative assessments by experts confirmed the satisfactory functionality of H2GR. This study demonstrates the significant potential of combining NLP and human-in-the-loop AI for accelerated knowledge discovery in hydrogen research. The adoption of an ontology centered on hydrogen production enables the identification of papers not highlighted by traditional citation-based metrics. H2GR presents a promising solution to alleviate the workload of experts in navigating the vast landscape of daily released hydrogen-related literature.}
}
@incollection{JAIN2021117,
title = {Chapter 9 - Ontology-supported rule-based reasoning for emergency management},
editor = {Sarika Jain and Vishal Jain and Valentina Emilia Balas},
booktitle = {Web Semantics},
publisher = {Academic Press},
pages = {117-128},
year = {2021},
isbn = {978-0-12-822468-7},
doi = {https://doi.org/10.1016/B978-0-12-822468-7.00017-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128224687000171},
author = {Sarika Jain and Sonia Mehla and Jan Wagner},
keywords = {Ontology, rules, emergency situations, inference, earthquake, RDF, OWL, advisory system},
abstract = {Emergency situations (disasters) occur regularly, and they present a vast risk for human lives and infrastructures. Reactions to disasters can vary depending upon the availability of resources and the location of the emergency. Planning is essential for quick and efficient responses to emergencies in every location. In this chapter, a rule-based reasoning approach to emergency management has been formulated, on the basis of which we will make recommendations for recovery from emergencies. All the required facts should be collected first and then represented in the appropriate form. Both of these tasks take time, and time can be a determinant between death and life for the affected persons. We propose ontology-supported rule-based reasoning to automate this process of decision support in order to recommend actions faster than a human beings are capable of doing and under any circumstances. An extensible knowledge model (Resource Description Framework) has been framed using semantic technology, in which the data are described in triplet (subject, predicate, object) form. Ontology has been used to build a knowledge base that can store procedures and facts about emergencies, and experts’ advice is stored in the form of rules.}
}
@article{VIGO2019100473,
title = {Comparing ontology authoring workflows with Protégé: In the laboratory, in the tutorial and in the ‘wild’},
journal = {Journal of Web Semantics},
volume = {57},
pages = {100473},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300477},
author = {Markel Vigo and Nicolas Matentzoglu and Caroline Jay and Robert Stevens},
keywords = {Empirical studies, Ontologies, Usability, Semantic web, Authoring tools, Engineering},
abstract = {The development of ontology engineering tools has traditionally lacked a user-centred perspective, instead being guided by the need to address particular gaps indicated by anecdotal evidence. This has typically resulted in prototypes that do not obtain traction beyond a narrow scope. Understanding the authoring patterns of ontology engineers is crucial to informing the development of ontology engineering tools that cater for the activity workflows of the users and, consequently, boosting the adoption of these tools. We report evidence about how Protégé is used across three different authoring settings, addressing the threats to validity of relying on a single user study. These settings address the continuum of expertise (from intermediate to expert users), the type of tasks (whether they are free-form or prescriptive) and the effect of the location (laboratory, tutorial or on their own) and how the studies are administered (whether or not there is a close supervision). While there are activity workflows that are particular to settings, the results indicate a number of core workflows that are common to all of them. We discuss actionable recommendations for ontology engineering tools in light of these results.}
}
@article{ZHANG2025103206,
title = {An LLM-based knowledge and function-augmented approach for optimal design of remanufacturing process},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103206},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103206},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625000990},
author = {Haiyang Zhang and Wei Yan and Huicong Hu and Xumei Zhang and Qingtao Liu and Hong Xia and Yingguang Zhang and Yuhao Lin},
keywords = {Optimal design of remanufacturing process (ODRP), Knowledge and function-augmented, Large language models (LLMs), Retrieval augmented generation (RAG), Function learning, Decision-making},
abstract = {Remanufacturing that returns used products to a like-new condition, is essential for promoting the circular economy and reducing carbon emissions. The optimal design of remanufacturing process (ODRP), as a knowledge-intensive complex decision-making task, plays a vital role in the success of remanufacturing. However, insufficient utilization of remanufacturing knowledge, and the trade-offs among multi-objectives in decision-making scenarios make ODRP time-consuming and labor-intensive. With the development of next-generation artificial intelligence (AI) technologies, large language models (LLMs) provide an important enabling tool for complex decision-making tasks. However, existing LLMs still face significant challenges in ODRP due to a lack of remanufacturing knowledge and computational capabilities. To address this issue, an LLM-based approach augmented with knowledge and function is proposed in this paper. Firstly, based on the establishment of remanufacturing process database, a retrieval augmented generation (RAG)-based knowledge-augmented strategy is designed to retrieve failure information (e.g., failure form, failure degree, etc.) of returned products through the interaction with LLMs, and generate the feasible remanufacturing schemes. Secondly, a function-augmented mechanism with function learning is also proposed to calculate each objective value and combined assessed value of the generated remanufacturing schemes with LLMs, assisting process designers in designing optimal remanufacturing scheme and process parameters. Finally, the proposed approach is validated using a case study on automobile gearbox remanufacturing. The results indicate that the proposed knowledge-augmented strategy improves the average accuracy from 65% to 79% when using ChatGLM3-6B as the base LLMs. Additionally, the proposed function-augmented mechanism can calculate the minimum combined assessed value and make more realistic results for ODRP. Meanwhile, the proposed integrated approach provides a solution to knowledge-intensive and complex decision-making tasks, which has a broad application prospect.}
}
@article{WANG2025100268,
title = {A lightweight knowledge graph-driven question answering system for field-based mineral resource survey},
journal = {Applied Computing and Geosciences},
volume = {27},
pages = {100268},
year = {2025},
issn = {2590-1974},
doi = {https://doi.org/10.1016/j.acags.2025.100268},
url = {https://www.sciencedirect.com/science/article/pii/S2590197425000503},
author = {Mingguo Wang and Chengbin Wang and Jianguo Chen and Bo Wang and Wei Wang and Xiaogang Ma and Jiangtao Ren and Zichen Li and Yicai Ye and Jiakai Zhang and Yue Wang},
keywords = {Knowledge graph-driven question answering, Question answering, Sentence transformer, Mineral resource survey, Intelligent service},
abstract = {Geoscience data associated with mineral resource surveys have become essential digital assets for governments and mining companies. The rapid increase in the volume of geoscience data makes it challenging to acquire knowledge quickly. In this study, we proposed and built a workflow that employs knowledge graph techniques, deep learning, question templates, and matching algorithms to provide a lightweight question-answering service for field-based geologists involved in mineral resource surveys. Initially, we utilized deep-learning-based geological entities and their semantic relation recognition, along with relational data mapping, to construct the mineral resource survey knowledge graph based on the ontology model. We then employed question template matching, a geological entity recognition model, and a sentence transformer to determine the optimal question template and generate a query statement for knowledge acquisition from a knowledge graph based on the Cypher language. Subsequently, we utilized a subgraph and a short abstract to express the results. The comparison with large language models and retrieval-augmented generation indicates that our solution is suitable for field-based mineral source surveys in a poor network environment with low-performance devices, data privacy concerns, and narrowly focused topics. The results also suggest that further studies on geoscience pre-trained models, an informative library of question templates, and multimodal knowledge graphs are necessary to improve the performance of the knowledge graph-driven question-answering system.}
}
@article{CHAKRABORTY2025109236,
title = {Building hybrid AI models in chemical engineering: A tutorial review},
journal = {Computers & Chemical Engineering},
volume = {201},
pages = {109236},
year = {2025},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2025.109236},
url = {https://www.sciencedirect.com/science/article/pii/S0098135425002406},
author = {Arijit Chakraborty and Naz Pinar Taskiran and Rishab Kottooru and Vipul Mann and Venkat Venkatasubramanian},
keywords = {Hybrid AI, First-principles, Machine learning, Artificial intelligence},
abstract = {Modern machine learning (ML) methods excel at learning from vast datasets and have demonstrated exceptional performance in conventional applications like text prediction, recommender systems, and chatbots. However, their application in science and engineering is constrained by challenges such as limited explainability, susceptibility to hallucinations, and a lack of grounding in first-principles knowledge. These limitations could be overcome by incorporating symbolic or classical artificial intelligence (AI) methods, which have been applied in chemical engineering for more than four decades. This paper outlines a systematic approach to incorporating domain knowledge into the AI/ML workflow, resulting in the development of hybrid AI models. Our proposed four-stage process includes (1) knowledge assessment, (2) domain-informed problem formulation, (3) selection of an appropriate AI/ML model, and (4) model validation. Additionally, we present six commonly used templates for hybrid AI development: feature engineering, customized knowledge representation, imposition of additional constraints, integration of these approaches, custom model architecture design, and end-to-end domain-specific AI models. These templates are organized by increasing levels of “hybridization”, reflecting progressively more advanced integration of domain knowledge. The goal is to migrate from the paradigm of large language models (LLMs) to large knowledge models (LKMs), which are better positioned to meet the unique demands of science and engineering applications.}
}
@article{WANG201989,
title = {Ontology-based semantic mapping of chemical toxicities},
journal = {Toxicology},
volume = {412},
pages = {89-100},
year = {2019},
issn = {0300-483X},
doi = {https://doi.org/10.1016/j.tox.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0300483X18302920},
author = {Rong-Lin Wang and Stephen Edwards and Cataia Ives},
keywords = {Chemical toxicity, Ontology, Phenotype, Phenomics, Semantic analysis},
abstract = {This study was undertaken to evaluate the use of ontology-based semantic mapping (OS-Mapping) in chemical toxicity assessment. Nineteen chemical-species phenotypic profiles (CSPPs) were constructed by ontologically annotating the toxicity responses reported in more than seven hundred published studies of ten chemicals on six vertebrate species. The CSPPs were semantically compared to more than 29,000 publicly available phenotypic profiles of genes, KEGG (Kyoto Encyclopedia of Genes and Genomes) pathways, and diseases based on a cross-species phenotype ontology. OS-Mapping was shown to differentiate chemical toxicities among themselves as well as within and across species. It also revealed cases of chemical by species interactions. In addition to confirming similar MOAs (mechanisms of action) for a few chemicals, OS-Mapping also generated novel insights into the MOAs underlying some seemingly different, yet phenotypically similar, classes of chemicals. The nature of a unified cross-species phenotype ontology and its representation of diverse knowledge domains allowed the construction of a complete phenotypic continuum for the 17α-ethynylestradiol_fathead minnow across the biological levels of organization, which complemented a similar one derived from the Comparative Toxicogenomics Database but based primarily on 17α-ethynylestradiol-induced molecular phenotypes. Overall, OS-Mapping has been demonstrated to offer a powerful approach to help bridge the gap between the molecular and non-molecular phenotypes of chemicals characterized by using high throughput or traditional omics methods and their apical endpoints of greater regulatory relevance, which are typically phenotypes found at the higher levels of biological organization. OS-Mapping also enables comparative toxicity assessment among chemicals, both within and across species. Furthermore, the semantic analysis of phenotypes can reveal additional novel MOAs for some well-known chemicals and discover candidate MOAs for chemicals that are less molecularly characterized. A full phenotypic continuum based on OS-Mapping will also be conducive to the future development of adverse outcome pathways. As phenomics continues to advance and the ontological annotation of literature becomes more automated, the power of OS-Mapping will be further enhanced.}
}
@article{MERDLERRABINOWICZ2025109098,
title = {The role of large language models in medical genetics},
journal = {Molecular Genetics and Metabolism},
volume = {145},
number = {1},
pages = {109098},
year = {2025},
issn = {1096-7192},
doi = {https://doi.org/10.1016/j.ymgme.2025.109098},
url = {https://www.sciencedirect.com/science/article/pii/S1096719225000897},
author = {Rona Merdler-Rabinowicz and Mahmud Omar and Jaya Ganesh and Eva Morava and Girish N. Nadkarni and Eyal Klang}
}
@article{KAMSUFOGUEM201914,
title = {Graph-based ontology reasoning for formal verification of BREEAM rules},
journal = {Cognitive Systems Research},
volume = {55},
pages = {14-33},
year = {2019},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2018.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S1389041718301608},
author = {B. Kamsu-Foguem and F.H. Abanda and M.B. Doumbouya and J.F. Tchouanguem},
keywords = {Data, Information, Knowledge, Reasoning, Building, Sustainability},
abstract = {Globally, the need to check regulation compliance for sustainability has become central in the delivery of construction projects. This is partly due to policies by various governments requiring existing and new buildings to comply with certain standards or regulations. However, the verification of whether a building complies with any particular standard or regulation has proven challenging in practice. The purpose of formal verification is to prove that under a certain set of assumptions, a building will adhere to a certain set of requirements, for example the minimum performance standards of key environmental issues. Compliance checking requires different criteria often difficult to straightforwardly define and combine in an integrated fashion for providing holistic interpretation to facilitate easy decision-making. Such criteria, their various flows and combinations can easily be dealt with using conceptual graph theories and Semantic Web concepts which allow rules to be imbued to facilitate reasoning. The aim of this study is to tap on conceptual graphs and Semantic Web concepts to develop a system for checking Building Research Establishment Environmental Assessment Methodology (BREEAM) sustainability standard compliance in the French construction industry. A conceptual graph-based framework that formally describes BREEAM requirements and visually analyse compliance checking processes has been proposed. When implemented in a software that integrates conceptual graphs and Semantic Web knowledge, automatic reasoning allows both the logical specification and the visual interpretation to be displayed and further provides a semantic support for compliance checking information.}
}
@article{LEONG2025102331,
title = {MERMaid: Universal multimodal mining of chemical reactions from PDFs using vision-language models},
journal = {Matter},
pages = {102331},
year = {2025},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2025.102331},
url = {https://www.sciencedirect.com/science/article/pii/S2590238525003741},
author = {Shi Xuan Leong and Sergio Pablo-García and Brandon Wong and Alán Aspuru-Guzik},
keywords = {data mining, digitization, database, electro-organic synthesis, knowledge graphs, organic synthesis, photocatalysis, retrieval-augmented generation, vision large language models},
abstract = {Summary
Data digitization of scientific literature is essential for creating machine-actionable knowledge bases to advance data-driven research and integrate with self-driving laboratories. It is especially critical to extract, interpret, and structure data from graphical elements, the primary medium for conveying complex scientific insights. However, this remains challenging due to the inherent lack of semantic structure in the prevalent PDF format, the complexity of visual content, and the need for multimodal integration. We present MERMaid (multimodal aid for reaction mining), an end-to-end pipeline that converts disparate visual data across PDFs into a coherent knowledge graph. Leveraging the emergent visual cognition and reasoning capabilities of vision-language models, MERMaid demonstrates chemical context awareness, self-directed context completion, and robust coreference resolution to achieve 87% end-to-end accuracy across three chemical domains. Its modular design facilitates future application to diverse data beyond reaction mining, promising to unlock the full potential of scientific literature for knowledge-intensive applications.}
}
@article{HONG2019230,
title = {Automated management of green building material information using web crawling and ontology},
journal = {Automation in Construction},
volume = {102},
pages = {230-244},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S0926580518303571},
author = {Sim-Hee Hong and Seul-Ki Lee and Jung-Ho Yu},
keywords = {Green Building Material Information (GBMI), Information collection, Information classification, Web-crawling, Ontology},
abstract = {Various green building certifications have been discussed as a part of efforts to realize sustainable development. In some countries, it is mandatory to acquire certifications for buildings above a certain scale. As a result, the demand for green building certifications has increased. Various studies have been conducted on the efficient performance of green building certification tasks. To improve the tasks for material information management, the following problems should be addressed: 1) Evaluation of material selection is difficult because of limitations on the amount and quality of the collected information; 2) Unnecessary duplication of work occurs because the important information created at each stage of a project is not delivered efficiently to the next step; 3) Information management for material information, which requires continuous updating, is not sufficient. Therefore, this study proposes an automated process of collecting and classifying Green Building Material Information (GBMI) using “web crawling” and “ontology” to improve the work efficiency of material information management. The proposed process is verified for interior finishing materials, which are a part of green building certification tasks. The proposed process can reduce the time required for the information management of building materials and eliminate human errors.}
}
@article{JAHAN2024108189,
title = {A comprehensive evaluation of large Language models on benchmark biomedical text processing tasks},
journal = {Computers in Biology and Medicine},
volume = {171},
pages = {108189},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108189},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524002737},
author = {Israt Jahan and Md Tahmid Rahman Laskar and Chun Peng and Jimmy Xiangji Huang},
keywords = {Large language models, ChatGPT, PaLM, LLaMA, Claude, Transformer, Natural language processing, LLM evaluation},
abstract = {Recently, Large Language Models (LLMs) have demonstrated impressive capability to solve a wide range of tasks. However, despite their success across various tasks, no prior work has investigated their capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of LLMs on benchmark biomedical tasks. For this purpose, a comprehensive evaluation of 4 popular LLMs in 6 diverse biomedical tasks across 26 datasets has been conducted. To the best of our knowledge, this is the first work that conducts an extensive evaluation and comparison of various LLMs in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot LLMs even outperform the current state-of-the-art models when they were fine-tuned only on the training set of these datasets. This suggests that pre-training on large text corpora makes LLMs quite specialized even in the biomedical domain. We also find that not a single LLM can outperform other LLMs in all tasks, with the performance of different LLMs may vary depending on the task. While their performance is still quite poor in comparison to the biomedical models that were fine-tuned on large training sets, our findings demonstrate that LLMs have the potential to be a valuable tool for various biomedical tasks that lack large annotated data.}
}
@article{BLAIR2020,
title = {Towards a catalogue of biodiversity databases: An ontological case study},
journal = {Biodiversity Data Journal},
volume = {8},
year = {2020},
issn = {1314-2836},
doi = {https://doi.org/10.3897/BDJ.8.e32765},
url = {https://www.sciencedirect.com/science/article/pii/S131428362000189X},
author = {Jarrett Blair and Rodger Gwiazdowski and Andrew Borrelli and Michelle Hotchkiss and Candace Park and Gleannan Perrett and Robert Hanner},
keywords = {Ontology, Database, Databases, Database of Databases, Metadata, Biodiversity, Indexing, Information Resource Discover},
abstract = {Biodiversity informatics depends on digital access to credible information about species. Many online resources host species’ data, but the lack of categorisation for these resources inhibits the growth of this entire field. To explore possible solutions, we examined the (now retired) Biodiversity Information Projects of the World (BIPW) dataset created by the Biodiversity Information Standards (TDWG); this project, which ran from 2007-2015 (officially removed from the TDWG website in 2018) was an attempt at organising the Web's biodiversity databases into an indexed list. To do this, we applied a simple classification scheme to score databases within BIPW based on nine data categories, to characterise trends and current compositions of this biodiversity e-infrastructure. Primarily, we found that of 600 databases investigated from BIPW, only 315 (~53%) were accessible at the time of this writing, underscoring the precarious nature of the biodiversity information landscape. Many of these databases are still available, but suffer accessibility issues such as link rot, thus putting the information they contain in danger of being lost. We propose that a community-driven database of biodiversity databases with an accompanying ontology could facilitate efficient discovery of relevant biodiversity databases and support smaller databases – which have the greatest risk of being lost.}
}
@article{GHORBEL2019101719,
title = {Ontology-based representation and reasoning about precise and imprecise temporal data: A fuzzy-based view},
journal = {Data & Knowledge Engineering},
volume = {124},
pages = {101719},
year = {2019},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2019.101719},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X19300382},
author = {Fatma Ghorbel and Fayçal Hamdi and Elisabeth Métais and Nebrasse Ellouze and Faïez Gargouri},
keywords = {Precise Temporal Data, Imprecise Temporal Data, Temporal Representation, Temporal Reasoning, Fuzzy Ontology},
abstract = {Temporal representation and reasoning are important facets in the design of many Semantic Web applications. Several approaches exist to represent and reason about precise temporal data in ontology. However, most of them handle only time intervals and associated qualitative relations. Besides, to the best of our knowledge, there is no approach devoted to handle imprecise temporal data (e.g., “late 1970s”). In this paper, we propose an ontology-based approach for representing and reasoning about precise and imprecise temporal data. Quantitative temporal data (i.e., time intervals and points) and qualitative ones (i.e., relations between time intervals, relations between a time interval and a time point and relations between time points) are taken into consideration. Our approach is three folds: (i) extending the 4D-fluents approach with new crisp and fuzzy components, to represent precise and imprecise temporal data, (ii) extending the Allen’s interval algebra to enable reasoning about precise and imprecise temporal data, and (iii) creating a Fuzzy-OWL 2 ontology TimeOnto that, based on the extended Allen’s interval algebra, instantiates our 4D-fluents-based representation. The extension that we propose for the Allen’s interval algebra handles precise and imprecise time intervals. Indeed, it enables expressing precise (e.g., “before”) and imprecise (e.g., “just before”) temporal relations. Compared to related work, our imprecise relations are personalized, in the sense that they are not limited to a defined set of interval relations and their meanings are determined by the domain expert. For instance, the classic Allen’s relation “Before” may be generalized in 5 imprecise relations, where “Before(1)” means “just before” and gradually the time gap between the two intervals increases until “Before(5)” which means “very long before”. To enable this representation, we propose an extension of the Vilain and Kautz’s point algebra and redefined the Allen’s relations by means of this extended algebra. We show in this paper that, unlike most related work, the resulting relations preserve many of the desirable properties of the Allen’s interval algebra. The definitions of the resulting interval relations are adapted to allow relating a time interval and a time point, and two time points, where time intervals and points maybe both precise or both imprecise. These relations can be used for temporal reasoning by means of four transitivity tables. Finally, we describe a prototype based on “TimeOnto” that infers new relations using a set of SWRL and fuzzy IF-THEN rules. This prototype was integrated in an ontology-based memory prosthesis for Alzheimer’s patients.}
}
@article{GAYATHRI20181974,
title = {Ontology Based Indian Medical System},
journal = {Materials Today: Proceedings},
volume = {5},
number = {1, Part 1},
pages = {1974-1979},
year = {2018},
note = {International Conference on Processing of Materials, Minerals and Energy (July 29th – 30th) 2016, Ongole, Andhra Pradesh, India},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2017.11.301},
url = {https://www.sciencedirect.com/science/article/pii/S2214785317325695},
author = {M. Gayathri and R. {Jagadeesh Kannan}},
keywords = {Traditional medicine, Ontology, Text mining},
abstract = {India is known for its traditional medicines like Ayurveda, siddha, unnani, homeopathy and yoga. Publishing of literatures on traditional medicines are expanding rapidly. Due to the exponential growth of articles and literatures existence, it is difficult for the user or the practitioners to find the relevant and useful information from this large amount of data. Text mining is applied on this data to find the useful facts. In this study, we concentrate on ayurvedic medical system and we discussed on how to apply text mining techniques on ayurvedic literatures to find the most relevant ayurvedic facts. Moreover we constructed ontology for the medicinal plants for better understanding of relations among such herbal plants with its usage in curing the diseases.}
}
@article{MONSEN2018e109,
title = {The Omaha system as an ontology and meta-model for nursing and healthcare in an era of Big Data},
journal = {Kontakt},
volume = {20},
number = {2},
pages = {e109-e110},
year = {2018},
issn = {1212-4117},
doi = {https://doi.org/10.1016/j.kontakt.2018.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1212411718300060},
author = {Karen A. Monsen}
}
@article{SCHROEDER2019198,
title = {Bridging knowledge divides: The case of indigenous ontologies of territoriality and REDD+},
journal = {Forest Policy and Economics},
volume = {100},
pages = {198-206},
year = {2019},
issn = {1389-9341},
doi = {https://doi.org/10.1016/j.forpol.2018.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S1389934118301321},
author = {Heike Schroeder and Nidia C. González},
keywords = {Indigenous peoples, Territoriality, Deforestation, REDD+ governance, Bolivia, Colombia},
abstract = {This study examines traditional indigenous ontologies of territoriality based on a number of indigenous communities in Bolivia and Colombia to show how they can inform effective implementation of REDD+ (Reducing Emissions from Deforestation and forest Degradation plus sustainable forest management, forest conservation and enhancing forest carbon stock). This could help address concerns that REDD+ interventions oversimplify local dynamics and complexities. The concept of territoriality subsumes a variety of definitions and conceptions, some of which are embedded in Traditional Ecological Knowledge and represented in the multiple expressions of collective indigenous identity. We compare and contrast Western and indigenous ontologies of territoriality and identify three ways in which engagement with territoriality can enhance REDD+ implementation and effective non-state actor participation.}
}
@article{HURTADO2025100915,
title = {Self-configurable Manufacturing Industrial Agents (SMIA): a standardized approach for digitizing manufacturing assets},
journal = {Journal of Industrial Information Integration},
volume = {47},
pages = {100915},
year = {2025},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2025.100915},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X25001384},
author = {Ekaitz Hurtado and Arantzazu Burgos and Aintzane Armentia and Oskar Casquero},
keywords = {Standardization, Asset administration shell (AAS), Capability-skill-service (CSS) model, Digital Twin, Multi-agent systems, Flexible manufacturing},
abstract = {The integration of heterogeneous industrial assets into flexible and distributed manufacturing environments remains a fundamental challenge in the evolution of Industry 4.0. Although standardization efforts – such as the Asset Administration Shell (AAS) and the Capability-Skill-Service (CSS) model promoted by the Plattform Industrie initiative, the OWL ontology language promoted by W3C, and the FIPA Agent Communication Language promoted by IEEE – offer structured frameworks for interoperability, their combined application still presents unresolved implementation challenges. This paper introduces SMIA (Self-configurable Manufacturing Industrial Agents), a dual-layer solution that operationalizes these standards by converging semantic modeling and distributed software execution. First, SMIA proposes a methodology for characterizing proactive AAS by enriching their descriptions with an OWL ontology based on the CSS model. Second, it provides a software toolchain that automatically generates Digital Twins –implemented as FIPA-compliant industrial agents using SPADE – from these enriched AAS descriptions. The resulting agents can react to external events, coordinating with peers and executing domain-specific skills within a standardized I4.0 communication framework. Built following an Open Source Software Engineering approach, SMIA leverages mature tools such as Eclipse BaSyx, OWLReady2 and SPADE, showcasing a replicable and extensible approach to the adoption of industrial standards in practice. A preliminary validation in a robotic logistics scenario demonstrates its feasibility and adaptability.}
}
@article{VANOMMESLAEGHE2021104151,
title = {Ontological reasoning in the design space exploration of advanced cyber–physical systems},
journal = {Microprocessors and Microsystems},
volume = {85},
pages = {104151},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2021.104151},
url = {https://www.sciencedirect.com/science/article/pii/S0141933121003197},
author = {Yon Vanommeslaeghe and Joachim Denil and Jasper {De Viaene} and David Ceulemans and Stijn Derammelaere and Paul {De Meulenaere}},
keywords = {Embedded systems, Cyber–physical systems, Co-design, Design space exploration, Ontological reasoning},
abstract = {Cyber–physical systems are becoming increasingly complex. In these advanced systems, the different engineering domains involved in the design process become more and more intertwined. Therefore, a traditional (sequential) design process becomes inefficient in finding good design options. Instead, an integrated approach is needed where parameters in multiple different engineering domains can be chosen, evaluated, and optimized to achieve a good overall solution. However, in such an approach, the combined design space becomes vast. As such, methods are needed to mitigate this problem. In this paper, we show a method for systematically capturing and updating domain knowledge in the context of a co-design process involving different engineering domains, i.e. control and embedded. We rely on ontologies to reason about the relationships between parameters in the different domains. This allows us to derive a stepwise design space exploration workflow where this domain knowledge is used to quickly reduce the design space to a subset of likely good candidates. We illustrate our approach by applying it to the design space exploration process for an advanced electric motor control system and its deployment on embedded hardware.}
}
@article{STEVENS2020102723,
title = {Critical realism and the ‘ontological politics of drug policy’},
journal = {International Journal of Drug Policy},
volume = {84},
pages = {102723},
year = {2020},
issn = {0955-3959},
doi = {https://doi.org/10.1016/j.drugpo.2020.102723},
url = {https://www.sciencedirect.com/science/article/pii/S0955395920300645},
author = {Alex Stevens},
keywords = {Critical realism, Ontology, Constructionism, Data science, Drug policy},
abstract = {This article explores the question of what we can consider to be real in drug policy. It examines two increasingly common aspects of drug policy analysis; radical constructionist critique and successionist data science. It shows how researchers using these assumptions have produced interesting findings, but also demonstrates their theoretical incoherence, based on their shared ‘flat ontology’. The radical constructionist claim that reality is produced within research methods – as seen in some qualitative studies - is shown to be unsustainably self-defeating. It is analytically ‘paralyzing’. This leads to two inconsistencies in radical constructionist studies; empirical ambivalence and ersatz epistemic egalitarianism. The Humean successionist approach of econometric data science is also shown to be unsustainable, and unable to provide explanations of identified patterns in data. Four consequent, limiting characteristics of this type of drug policy research are discussed: causal inference at a distance, monofinality, limited causal imagination, and overly confident causal claims. The article goes on to describe the critical realist approach towards ‘depth ontology’ and ‘generative causation’. It provides examples of how this approach is deployed in critical realist reviews and discourse analysis of drug policy. It concludes by arguing that critical realism enables more deeply explanatory, methodologically eclectic and democratically inclusive analysis of drug policy development and effects.}
}
@article{SHEVELEVA2020107,
title = {Development of a Domain-Specific Ontology to Support Research Data Management for the Tailored Forming Technology},
journal = {Procedia Manufacturing},
volume = {52},
pages = {107-112},
year = {2020},
note = {System-Integrated Intelligence – Intelligent, Flexible and Connected Systems in Products and ProductionProceedings of the 5th International Conference on System-Integrated Intelligence (SysInt 2020), Bremen, Germany},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.11.020},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920321624},
author = {Tatyana Sheveleva and Oliver Koepler and Iryna Mozgova and Roland Lachmayer and Sören Auer},
keywords = {Manufacturing Process Chains, Digitisation of Scientific Data, Ontology Development, FAIR Data Principles, Research Data Management},
abstract = {The global trend towards the comprehensive digitisation of technologies in product manufacturing is leading to radical changes in engineering processes and requires a new extended understanding of data handling. The amounts of data to be considered are becoming larger and more complex. Data can originate from process simulations, machines used or subsequent analyses, which together with the resulting components serve as a complete and reproducible description of the process. Within the Collaborative Research Centre "Process Chain for Manufacturing of Hybrid High Performance Components by Tailored Forming", interdisciplinary work is being carried out on the development of process chains for the production of hybrid components. The management of the generated data and descriptive metadata, the support of the process steps and preliminary and subsequent data analysis are fundamental challenges. The objective is a continuous, standardised data management according to the FAIR Data Principles so that process-specific data and parameters can be transferred together with the components or samples to subsequent processes, individual process designs can take place and processes of machine learning can be accelerated. A central element is the collaborative development of a domain-specific ontology for a semantic description of data and processes of the entire process chain.}
}
@article{WANG2025385,
title = {WT-Fault-LLM:Knowledge Graph Enhanced Large Model for Wind-turbine Faults},
journal = {Procedia Computer Science},
volume = {266},
pages = {385-390},
year = {2025},
note = {The 12th International Conference on Information Technology and Quantitative Management (ITQM 2025)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.08.049},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925023543},
author = {Feng Wang and Xizhen Zhang and Rui Tang and Jiading Jiang},
keywords = {wind turbine faults, fault diagnosis, large language modeling, knowledge graph, retrieval-enhanced generation},
abstract = {This study introduces an innovative framework integrating Large Language Models (LLMs) with knowledge graphs to significantly enhance wind turbine fault diagnosis, specifically addressing critical LLM limitations in industrial settings, such as knowledge inaccuracies and deficiencies in symbolic reasoning. While LLMs exhibit proficiency in general domains, their application in specialized vertical fields remains constrained. Knowledge graphs offer a solution by providing structured, interpretable reasoning through networks of triplets. The proposed architecture employs a three-layer "knowledge storage-retrieval-generation reasoning" structure to integrate heterogeneous data sources—including wind turbine manuals and maintenance records—into a domain-specific knowledge graph that explicitly models fault entities, their attributes, and causal relationships. A novel dual-channel semantic retrieval mechanism is implemented, concurrently leveraging graph-based reasoning paths within the knowledge graph and semantic similarity matching via a vector database, thereby substantially improving adaptability to complex operational scenarios. Utilizing local deployment with the DeepSeek-R1 model and Retrieval-Augmented Generation (RAG) techniques, the framework transforms both structured and unstructured data into contextual constraints through dynamic prompt engineering, resulting in markedly improved fault diagnosis accuracy. Case studies establish the system’s capacity for sophisticated multi-hop causal reasoning, such as identifying that "gearbox noise" can originate from underlying causes like lubricant contamination. Future research will prioritize the integration of multi-modal data, including vibration spectrums, to accelerate the transformation of industrial operations from experience-based practices to knowledge-driven methodologies. This work provides a valuable technical paradigm for the reliable application of LLMs in vertical industrial domains.}
}
@article{CANCINO201831,
title = {Technological innovation for sustainable growth: An ontological perspective},
journal = {Journal of Cleaner Production},
volume = {179},
pages = {31-41},
year = {2018},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.01.059},
url = {https://www.sciencedirect.com/science/article/pii/S0959652618300672},
author = {Christian A. Cancino and Ariel I. {La Paz} and Arkalgud Ramaprasad and Thant Syn},
keywords = {Ontological framework, Technological innovation, Sustainable growth, Research agenda},
abstract = {Technological innovations are seen as means to optimize the efficient and clean use of vital resources in social-biological-economic systems. However, partial theoretical perspectives and experiences of their effects can lead to significant oversight of their potential and limitations. There is a need to manage technological innovations for sustainable growth from a holistic perspective, systemically and systematically. To do so, we present and validate an ontological framework, map the current body of knowledge, and identify the emphases and gaps in the domain. The ontological framework is constructed from the common terminology of the domain. The analysis is based on a map of 375 research papers published in the most prestigious journals relevant to the domain. The results show significant gaps in the research to fulfil the potential. Future research can be directed to fill these gaps.}
}
@article{QI2020101875,
title = {Ontology-based knowledge representation of urban heat island mitigation strategies},
journal = {Sustainable Cities and Society},
volume = {52},
pages = {101875},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2019.101875},
url = {https://www.sciencedirect.com/science/article/pii/S2210670719311606},
author = {Jinda Qi and Lan Ding and Samsung Lim},
keywords = {Urban heat island mitigation strategies, Urban contexts, Performance metrics, Ontology-based representation, Knowledge base, Application framework},
abstract = {Previous studies have presented numerous urban heat island mitigation strategies (UHIMSs) and have clearly demonstrated their effectiveness. However, the lack of a prototype for representing the uncharacterised UHIMS and the absence of a link between UHI mitigation techniques and urban contexts, make it challenging to understand and manage UHIMS. To fill these gaps, this study proposes an ontology-based representation of UHIMS with a particular emphasis on the relationship between UHI mitigation techniques, performance metrics and urban contexts. Our representation is structured with three steps: the conceptualisation of terminologies, the establishment of relationships and the integration. Consequently, a prototype for the UHIMS representation is introduced covering the use of ‘UHIM techniques’ having ‘Planning and design variables’ on ‘The place of application’ to address ‘UHI problems’ in ‘Urban settings’ with the evaluation by ‘Performance metrics’. Meanwhile, a three-step framework is developed to facilitate the application of the prototype and is subsequently validated by a case example. The significance of UHIMS representation is to understand and systematically manage UHIMS knowledge, thereby further supporting performance evaluation and governmental decisions for urban heat island mitigation.}
}
@article{BLACKSTOCK2020104587,
title = {Indigenous ontology, international law and the application of the Convention to the over-representation of Indigenous children in out of home care in Canada and Australia},
journal = {Child Abuse & Neglect},
volume = {110},
pages = {104587},
year = {2020},
note = {30 Years of the Convention on the Rights of the Child: Evolving Progress and Prospects for Child Protection},
issn = {0145-2134},
doi = {https://doi.org/10.1016/j.chiabu.2020.104587},
url = {https://www.sciencedirect.com/science/article/pii/S0145213420302428},
author = {Cindy Blackstock and Muriel Bamblett and Carlina Black},
keywords = {Indigenous, Child welfare, Structural risks, Equity, Human rights, Ontology},
abstract = {This paper explores the efficacy of the United Nations Convention on the Rights of the Child (Convention, UN General Assembly, 1989) through the lens of the over-representation of First Nations children placed in out-of-home care in Canada and Aboriginal and Torres Strait Islander children in Australia. A general overview of Indigenous worldviews frames a discussion on the coherence of international human rights law and instruments, including the Convention, account for Indigenous Peoples’ ontologies. The authors argue that the United Nations Declaration on the Rights of Indigenous Peoples (UN General Assembly, 2007) and a new theoretical framework published by the Pan American Health Organization (2019) on health equity and inequity are useful tools to augment the Convention’s coherence with Indigenous ontologies. The paper discusses how the Convention can be applied to structural and systemic risks driving the over-representation of First Nations and Aboriginal and Torres Strait Islander children in out of home care in Canada and Australia. These two countries are included as First Nations and Aboriginal and Torres Strait Islander peoples in these countries have both had significant impact in advocating for their children despite experiencing similar barriers including contemporary colonialism. The advocacy work of the First Nations Child and Family Caring Society in Canada and the Victorian Aboriginal Child Care Agency in Victoria, Australia are discussed. The paper ends by outlining some of the challenges ahead that include the need to meaningfully recognize Indigenous self-determination and equitable funding and resources to enable the actualization of self-determination. Further research contrasting international human rights instruments with Indigenous ontologies could help inform possible amendments to international human rights treaties and general comments.}
}
@article{WEI2020113461,
title = {A decision support system for urban infrastructure inter-asset management employing domain ontologies and qualitative uncertainty-based reasoning},
journal = {Expert Systems with Applications},
volume = {158},
pages = {113461},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113461},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420302852},
author = {Lijun Wei and Heshan Du and Quratul-ain Mahesar and Kareem {Al Ammari} and Derek R. Magee and Barry Clarke and Vania Dimitrova and David Gunn and David Entwisle and Helen Reeves and Anthony G. Cohn},
keywords = {Smart cities, Infrastructure maintenance, Underground utilities, Rule-based system, Reasoning under uncertainty},
abstract = {Urban infrastructure assets (e.g. roads, water pipes) perform critical functions to the health and well-being of society. Although it has been widely recognised that different infrastructure assets are highly interconnected, infrastructure management in practice such as planning, installation and maintenance are often undertaken by different stakeholders without considering these dependencies due to the lack of relevant data and cross-domain knowledge, which may cause unexpected cascading social, economic and environmental effects. In this paper, we present a knowledge based decision support system for urban infrastructure inter-asset management. By considering various infrastructure assets (e.g. road, ground, cable), triggers (e.g. pipe leaking) and potential consequences (e.g. traffic disruption) as a holistic system, we model each sub-domain using a modular ontology and encapsulate the interdependence between them using a set of rules. Moreover, qualitative likelihood is assigned to each rule by domain experts (e.g. civil engineers) to encode the uncertainty of knowledge, and an inference engine is applied to predict the potential consequences of a given trigger with location specific data and the encoded rules. A web-based prototype system has been developed based on the above concept and demonstrated to a wide range of stakeholders. The system can assist in the process of decision making by aiding data collation and integration, as well as presenting potential consequences of possible triggers, advising on whether additional information is needed or suggesting ways of obtaining such information. The work shows an intelligent approach to integrate and process multi-source data to pioneer a novel way to aid a complex decision process with a high social impact.}
}
@article{PEPPES20201829,
title = {A Semantic Engine and an Ontology Visualization Tool for Advanced Crime Analysis},
journal = {Procedia Computer Science},
volume = {176},
pages = {1829-1838},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.222},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920321244},
author = {N. Peppes and T. Alexakis and E. Adamopoulou and K. Remoundou and K. Demestichas},
keywords = {Crime investigation, Semantic engine, Ontology visualisation},
abstract = {Crime has been and still is one of the major threats and issues for every government and society around the world. Nowadays, a new generation of (cyber)criminals exploits modern and state-of-the-art technology and tools, especially social media channels, in order to achieve their malicious purposes. Thus, Law Enforcement Agencies (LEAs), analysts and security practitioners are facing the need to engage new methods and tools which can support them in the fight and prevention of crime by minimizing the prediction and response time. This paper focuses on two specific functionalities of a crime prediction and fighting software framework which assists LEAs to adopt and utilize emerging technologies such as Data mining and Big Data tools, Semantic Analysis, Visual Intelligence and more in the context of their everyday operations. In this light, the authors of this paper present an innovative Semantic Engine with a person fusion tool as well as an ontology visualization tool as parts of a discussed framework. Both of these tools are presented in detail in separate sections so that the readers can understand better their architecture and their integration into this common framework alongside with other tools. These two tools use state-of-the-art components and software libraries in order to offer robust and future proof solutions for LEAs. Also, they can be integrated and expanded in future research and commercial projects both as independent tools as well as parts of other integrated frameworks.}
}
@article{LU2025104091,
title = {LLM-infused bi-level semantic enhancement for corporate credit risk prediction},
journal = {Information Processing & Management},
volume = {62},
number = {4},
pages = {104091},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104091},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325000330},
author = {Sichong Lu and Yi Su and Xiaoming Zhang and Jiahui Chai and Lean Yu},
keywords = {Corporate credit risk prediction, Semantic enhancement, Large language model, Contrastive learning, Multitask learning},
abstract = {Corporate credit risk (CCR) prediction enables investors, governments, and companies to make informed financial decisions. Existing research primarily focuses solely on the tabular feature values, yet it often overlooks the rich inherent semantic information. In this paper, a novel bi-level semantic enhancement framework for CCR prediction is proposed. Firstly, at the data-level, a large language model (LLM) generates detailed textual descriptions of companies’ financial conditions, infusing raw tabular training data with semantic information and domain knowledge. Secondly, to enable semantic perception during inference when only tabular data is available, a contrastive multimodal multitask learning model (CMML) is proposed at the model level. CMML leverages the semantically enhanced data from the previous level to acquire semantic perception capabilities during the training phase, requiring only tabular data during prediction. It aligns the representations of tabular data with textual data, enabling extracting semantically rich features from tabular data. Furthermore, a semantic alignment classifier and an MLP classifier are integrated into a weighted ensemble learner within a multitask learning architecture to enhance robustness. Empirical verification on two datasets demonstrates that CMML surpasses benchmark models in key metrics, particularly in scenarios with limited samples and high proportions of unseen corporations, implying its effectiveness in CCR prediction through bi-level semantic enhancement.}
}
@article{SCHOENFISCH2018103,
title = {Root cause analysis in IT infrastructures using ontologies and abduction in Markov Logic Networks},
journal = {Information Systems},
volume = {74},
pages = {103-116},
year = {2018},
note = {Special Issue on papers presented in the 20th IEEE International Enterprise Distributed Object Computing1 Conference, EDOC 2016},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2017.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0306437916306172},
author = {Joerg Schoenfisch and Christian Meilicke and Janno von Stülpnagel and Jens Ortmann and Heiner Stuckenschmidt},
keywords = {Root cause analysis, IT Infrastructure management, Markov Logic Network, Ontology, Abductive reasoning},
abstract = {Information systems play a crucial role in most of today’s business operations. High availability and reliability of services and hardware, and, in the case of outages, short response times are essential. Thus, a high amount of tool support and automation in risk management is desirable to decrease downtime. We propose a new approach for calculating the root cause for an observed failure in an IT infrastructure. Our approach is based on abduction in Markov Logic Networks. Abduction aims to find an explanation for a given observation in the light of some background knowledge. In failure diagnosis, the explanation corresponds to the root cause, the observation to the failure of a component, and the background knowledge to the dependency graph extended by potential risks. We apply a method to extend a Markov Logic Network in order to conduct abductive reasoning, which is not naturally supported in this formalism. Our approach exhibits a high amount of reusability and facilitates modeling by using ontologies as background knowledge. This enables users without specific knowledge of a concrete infrastructure to gain viable insights in the case of an incident. We implemented the method in a tool and illustrate its suitability for root cause analysis by applying it to a sample scenario and testing its scalability on randomly generated infrastructures.}
}
@article{ZULKARNAIN2019100003,
title = {Ultrasound reports standardisation using rhetorical structure theory and domain ontology},
journal = {Journal of Biomedical Informatics},
volume = {100},
pages = {100003},
year = {2019},
note = {Articles initially published in Journal of Biomedical Informatics: X 1-4, 2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.yjbinx.2019.100003},
url = {https://www.sciencedirect.com/science/article/pii/S2590177X19300022},
author = {Nur Zareen Zulkarnain and Farid Meziane},
keywords = {Rhetorical structure theory, Rhetorical relation, Discourse parsing, Structured reporting, Ultrasound reporting, Ontology, Discourse markers},
abstract = {Ultrasound reporting plays an important role in diagnosis as images produced during an ultrasound examination do not give the whole view of the medical conditions. However, in practice there are many issues that are inherent to ultrasound reporting and the most important was identified to be the lack of standardisation when producing these reports. There is a resistance to change from some radiologists preferring the free writing style, making any attempt to computerise the processing of these reports difficult. This paper explores the possibility of using Rhetorical Structure Theory (RST) together with a domain ontology to transform free-form ultrasound reports into a structured form. It discusses a new approach in segmenting and identifying rhetorical relations that are more applicable to ultrasound reports from classical RST relations. The approach was evaluated on a sample ultrasound reports where the system’s parsing was compared to the manual parsing performed by experts. The results show that discourse parsing using RST in ultrasound reports can be performed effectively using the support of a domain ontology. The results also demonstrate that the transformation of free-form ultrasound reports into a structured form can be performed with the support of RST relations identified and the domain ontology.}
}
@article{CELIKTEN2025114103,
title = {Medcongtm: Interpretable multi-label clinical code prediction with dual-view graph contrastive topic modeling},
journal = {Knowledge-Based Systems},
volume = {327},
pages = {114103},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114103},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125011487},
author = {Tuğba Çelikten and Aytuğ Onan},
keywords = {Multi-label classification, Clinical code prediction, Topic modeling, Contrastive learning},
abstract = {BackgroundAccurate and interpretable clinical code assignment from free-text medical records is a fundamental challenge in healthcare informatics. Traditional machine learning and language model-based methods often lack transparency and struggle with multi-label prediction across complex taxonomies such as ICD, CPT, and LOINC. Existing topic modeling techniques, while interpretable, are rarely optimized for the clinical coding task and fail to leverage the rich semantic structure inherent in medical texts and ontologies. MethodsWe propose MedConGTM, a novel dual-view graph contrastive topic modeling framework tailored for interpretable and multi-label clinical code prediction. MedConGTM constructs two semantic views of each document: a document-token semantic graph and a document-code co-assignment graph. These views are jointly optimized through a novel dual-view contrastive learning objective that maximizes the mutual information between topic distributions inferred from text and task-specific code views. We introduce a code-aware word co-occurrence graph enhanced with medical ontologies and propose a hierarchy-sensitive contrastive loss that incorporates structural relationships between clinical codes. To ensure transparency, we design a topic-to-code attention decoder that links predicted codes to interpretable latent topics and salient textual evidence. ResultsExperiments on MIMIC-III and i2b2 datasets demonstrate that MedConGTM outperforms state-of-the-art baselines in both code prediction accuracy and topic coherence. It also provides interpretable code rationales aligned with clinical semantics. ConclusionsMedConGTM offers a powerful, interpretable, and clinically grounded solution for automated ICD/CPT/LOINC code assignment, bridging the gap between topic modeling, contrastive learning, and real-world healthcare applications.}
}
@article{STAUMONT2022S77,
title = {SOC-VI-08 Physiological maps: a benchmark tool for adverse outcome pathways and a cornerstone for the development of disease ontologies},
journal = {Toxicology Letters},
volume = {368},
pages = {S77-S78},
year = {2022},
note = {Abstracts of the XVIth International Congress of Toxicology (ICT 2022) - UNITING IN TOXICOLOGY},
issn = {0378-4274},
doi = {https://doi.org/10.1016/j.toxlet.2022.07.228},
url = {https://www.sciencedirect.com/science/article/pii/S0378427422012152},
author = {B. Staumont and L.C.M. Ladeira and A. Gamba and R. Lesage and A. Verhoeven and J. Jiang and J. van Ertvelde and D.A. Barnes and M.J. Janssen and E. Kuchovska and J. Berkhout and D. Roodzant and M. Teunis and T. Bozada and T.H. Luechtefeld and R. Jover and T. Vanhaecke and M. Vinken and R. Masereeuw and E. Fritsche and A.H. Piersma and H.J. Heusinkveld and L. Geris}
}
@article{ZHANG2025115001,
title = {Data-driven building load prediction and large language models: Comprehensive overview},
journal = {Energy and Buildings},
volume = {326},
pages = {115001},
year = {2025},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.115001},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824011174},
author = {Yake Zhang and Dijun Wang and Guansong Wang and Peng Xu and Yihao Zhu},
keywords = {Data-driven approach, Building load prediction, Machine learning, Large language models, Feature engineering, Data preparation, Room-scale load prediction, Retrieval augmented generation},
abstract = {Building load forecasting is essential for optimizing the architectural design and managing energy efficiently, enhancing the performance of Heating, Ventilation, and Air Conditioning systems, and enhancing occupant comfort. With advancements in data science and machine learning, the focus on predicting building loads through data analysis has significantly intensified as a research domain. However, previous studies have typically faced challenges such as data scarcity, improper feature extraction methods, and weak model generalization capabilities. To gain a deeper understanding of these issues, a comprehensive review of data processing, feature selection, and model selection methods in previous research is conducted from the perspective of the entire load forecasting process. The aim is to identify the most suitable methods for each step of load forecasting to enhance prediction accuracy. This review surveys the research progress of statistical learning methods, traditional machine learning methods, deep learning methods, and hybrid methods in different application scenarios of building load prediction. Then, it emphasized the critical role of data preprocessing and focused on techniques like data fusion and transfer learning to overcome data shortages and bolster the models’ ability to generalize. Moreover, the obtainment of significant features from building characteristics, weather data, and operational statistics to boost prediction accuracy is explored. A notable contribution of this review is the proposed technical framework for EnergyPlus model generation using LLM-based Retrieval Augmented Generation (RAG) technology and room- level load prediction with Spatio-Temporal Graph Neural Networks. This framework utilize architectural design drawings to achieve an “end-to-end” prediction process, aiming to reduce the professional threshold of load prediction and provide technical support for fine-grained regulation of building operation. Exploratory experiment is conducted using a single-zone building model to verify the feasibility of LLM-generated EnergyPlus models, with IDF simulation file generation taking only 196 s. Room-level load forecasting with LLMs remains to be explored further. It is reasonable to believe that the methods proposed in this review hold promise for advancing data-driven building load forecasting technologies.}
}
@article{JELOKHANINIARAKI2018104,
title = {Knowledge sharing in Web-based collaborative multicriteria spatial decision analysis: An ontology-based multi-agent approach},
journal = {Computers, Environment and Urban Systems},
volume = {72},
pages = {104-123},
year = {2018},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2018.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0198971517302478},
author = {Mohammadreza Jelokhani-Niaraki},
keywords = {Collaborative decision making, Knowledge sharing, GIS-MCDA},
abstract = {The Web-based Multicriteria Spatial Decision Support Systems (MC-SDSS) enhance the collaborative/participatory spatial decision making by providing the relevant GIS-based MCDA (Multicriteria Decision Analysis) tools for active participation/collaboration. Typically, regular/novice decision makers need to acquire knowledge from expert decision makers in a participatory decision making process. Over the last decade or so, significant research efforts have been made to use Web-based GIS-MCDA tools for collaborative spatial decision making. However, these efforts as the collaborative decision making tools lack a knowledge sharing mechanism or framework that allow for exchange and sharing of decision knowledge between decision makers (decision makers' agents). In the case of providing knowledge sharing capabilities by these tools, exchange of decision knowledge relies on decision makers' common sense to manually interpret the meanings of each other's knowledge and use the right ones. To address these limitations, this study proposes an ontology-based multi-agents approach for knowledge sharing in a collaborative MC-SDSS. The decision makers' agents committed to the ontology can interoperate and exchange decision knowledge with intended and unambiguous meanings.}
}
@article{HERRERAMARTIN2025100747,
title = {A semantic-based model for the management of people with reduced mobility in airport facilities},
journal = {Egyptian Informatics Journal},
volume = {31},
pages = {100747},
year = {2025},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2025.100747},
url = {https://www.sciencedirect.com/science/article/pii/S1110866525001409},
author = {Juan José Herrera-Martín and Gonçal Costa and Iván Castilla-Rodríguez and Evelio José González},
keywords = {PRM, Ontology, OWL, Semantic web},
abstract = {The growing need for inclusive transportation systems has emphasized the importance of addressing the challenges faced by people with reduced mobility (PRM) in airport environments. Nonetheless, its correct performance is subject to different threats and uncontrollable aspects: flight delays, no-show passengers, gate changes, or a high volume of non-registered last-minute PRM passengers, among others. Increasingly, PRM service providers try to deal with such problems by relying on software tools connected to airport information systems to obtain up-to-date data (e.g., flight status, estimated time of arrival or departure of flights, etc.). However, there is no standard representation of the data within this domain that may support the development of aiding tools and enhance their features. To respond to this need, in this article we present an ontology to represent the data domain of PRM services management in airport facilities. The aim is to facilitate the access and combination of the data from different sources under a standardized approach and common understanding of the terminology. The article describes the steps that have been followed to develop the ontology, as well as some usage examples.}
}
@article{KIM2019P1449,
title = {CARE SUPPORT SYSTEM USING ONTOLOGICAL MODEL OF CARING PATIENT WITH DEMENTIA},
journal = {Alzheimer's & Dementia},
volume = {15},
number = {7, Supplement },
pages = {P1449-P1450},
year = {2019},
note = {Alzheimer’s Association International Conference 2019},
issn = {1552-5260},
doi = {https://doi.org/10.1016/j.jalz.2019.06.4060},
url = {https://www.sciencedirect.com/science/article/pii/S1552526019342268},
author = {Gyungha Kim and Hwawoo Jeon and Sungkee Park and Yoonseob Lim}
}
@article{DEGHA2019212,
title = {Intelligent context-awareness system for energy efficiency in smart building based on ontology},
journal = {Sustainable Computing: Informatics and Systems},
volume = {21},
pages = {212-233},
year = {2019},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2019.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S2210537918303457},
author = {Houssem Eddine Degha and Fatima Zohra Laallam and Bachir Said},
keywords = {Smart building, Energy efficiency, Ontology, Context-awareness, Building energy management system},
abstract = {Context-awareness is an important research area. Many energy efficiency applications of ubiquitous computing need to access some related contexts to provide the best and adequate energy saving services at the right time and at the right place. One of the big challenges is the difficulty and the complexity to make systems identify and understand situations in the building business, especially in complex situations to provide adequate energy-saving services for each of which. This issue is divided into two parts: On one hand, the smart building and its environment must be modeled in a way that provides context-awareness. On the other hand is the how to effectively exploit this context-awareness to reduce energy consumption and increase the user comfort. In this paper, we propose an intelligent context-awareness Building Energy Management System (ICA-BEMS). ICA-BEMS uses hybrid energy-saving techniques based on a Smart Context-Awareness Management (Smart-CAM). Smart-CAM uses smart building ontology to organize smart building knowledge and utilizes a new context-awareness mechanism to provide contextual information. The former exploits the contextual information in reasoning to reduce the building energy consumption, promoting users behavioral change and maximizing the user's comfort to bring about a better energy efficiency policy. To evaluate our system, we have developed a smart building simulator (Open-SBS) which simulates smart building comportments and human behaviors. We have created various scenarios of daily life in the smart building simulator. We have tested our ICA-BEMS effect on energy consumption having a positive result. Energy consumption has been decreased by 40% of total energy consumption.}
}
@article{HE2025110625,
title = {Radiology report generation using automatic keyword adaptation, frequency-based multi-label classification and text-to-text large language models},
journal = {Computers in Biology and Medicine},
volume = {196},
pages = {110625},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.110625},
url = {https://www.sciencedirect.com/science/article/pii/S001048252500976X},
author = {Zebang He and Alex Ngai Nick Wong and Jung Sun Yoo},
keywords = {Radiology report generation, Automatic keyword adaptation, Frequency-based multi-label classification, Large language model},
abstract = {Background:
Radiology reports are essential in medical imaging, providing critical insights for diagnosis, treatment, and patient management by bridging the gap between radiologists and referring physicians. However, the manual generation of radiology reports is time-consuming and labor-intensive, leading to inefficiencies and delays in clinical workflows, particularly as case volumes increase. Although deep learning approaches have shown promise in automating radiology report generation, existing methods, particularly those based on the encoder–decoder framework, suffer from significant limitations. These include a lack of explainability due to black-box features generated by encoder and limited adaptability to diverse clinical settings.
Methods:
In this study, we address these challenges by proposing a novel deep learning framework for radiology report generation that enhances explainability, accuracy, and adaptability. Our approach replaces traditional black-box features in computer vision with transparent keyword lists, improving the interpretability of the feature extraction process. To generate these keyword lists, we apply a multi-label classification technique, which is further enhanced by an automatic keyword adaptation mechanism. This adaptation dynamically configures the multi-label classification to better adapt specific clinical environments, reducing the reliance on manually curated reference keyword lists and improving model adaptability across diverse datasets. We also introduce a frequency-based multi-label classification strategy to address the issue of keyword imbalance, ensuring that rare but clinically significant terms are accurately identified. Finally, we leverage a pre-trained text-to-text large language model (LLM) to generate human-like, clinically relevant radiology reports from the extracted keyword lists, ensuring linguistic quality and clinical coherence.
Results:
We evaluate our method using two public datasets, IU-XRay and MIMIC-CXR, demonstrating superior performance over state-of-the-art methods. Our framework not only improves the accuracy and reliability of radiology report generation but also enhances the explainability of the process, fostering greater trust and adoption of AI-driven solutions in clinical practice. Comprehensive ablation studies confirm the robustness and effectiveness of each component, highlighting the significant contributions of our framework to advancing automated radiology reporting.
Conclusion:
In conclusion, we developed a novel deep-learning based radiology report generation method for preparing high-quality and explainable radiology report for chest X-ray images using the multi-label classification and a text-to-text large language model. Our method could address the lack of explainability in the current workflow and provide a clear and flexible automated pipeline to reduce the workload of radiologists and support the further applications related to Human–AI interactive communications.}
}
@article{LOPEZUBEDA2024105443,
title = {Evaluation of large language models performance against humans for summarizing MRI knee radiology reports: A feasibility study},
journal = {International Journal of Medical Informatics},
volume = {187},
pages = {105443},
year = {2024},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2024.105443},
url = {https://www.sciencedirect.com/science/article/pii/S1386505624001060},
author = {Pilar López-Úbeda and Teodoro Martín-Noguerol and Carolina Díaz-Angulo and Antonio Luna},
keywords = {Radiology report summarization, Natural Language Processing, Large Language Model, Knee MRI reports, Human expert evaluation},
abstract = {Objectives
This study addresses the critical need for accurate summarization in radiology by comparing various Large Language Model (LLM)-based approaches for automatic summary generation. With the increasing volume of patient information, accurately and concisely conveying radiological findings becomes crucial for effective clinical decision-making. Minor inaccuracies in summaries can lead to significant consequences, highlighting the need for reliable automated summarization tools.
Methods
We employed two language models — Text-to-Text Transfer Transformer (T5) and Bidirectional and Auto-Regressive Transformers (BART) — in both fine-tuned and zero-shot learning scenarios and compared them with a Recurrent Neural Network (RNN). Additionally, we conducted a comparative analysis of 100 MRI report summaries, using expert human judgment and criteria such as coherence, relevance, fluency, and consistency, to evaluate the models against the original radiologist summaries. To facilitate this, we compiled a dataset of 15,508 retrospective knee Magnetic Resonance Imaging (MRI) reports from our Radiology Information System (RIS), focusing on the findings section to predict the radiologist's summary.
Results
The fine-tuned models outperform the neural network and show superior performance in the zero-shot variant. Specifically, the T5 model achieved a Rouge-L score of 0.638. Based on the radiologist readers' study, the summaries produced by this model were found to be very similar to those produced by a radiologist, with about 70% similarity in fluency and consistency between the T5-generated summaries and the original ones.
Conclusions
Technological advances, especially in NLP and LLM, hold great promise for improving and streamlining the summarization of radiological findings, thus providing valuable assistance to radiologists in their work.}
}
@article{LIU201842,
title = {Combining ontology and reinforcement learning for zero-shot classification},
journal = {Knowledge-Based Systems},
volume = {144},
pages = {42-50},
year = {2018},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2017.12.022},
url = {https://www.sciencedirect.com/science/article/pii/S0950705117306007},
author = {Bin Liu and Li Yao and Zheyuan Ding and Junyi Xu and Junfeng Wu},
keywords = {Image classification, Zero-shot classification, Ontology, Reinforcement learning, Adaptive},
abstract = {Zero-Shot Classification (ZSC) has received much attention recently in computer vision research. Traditional classifiers are unable to handle ZSC because test data labels are significantly different from training data labels. Attribute-based methods have long dominated ZSC. However, classical attribute-based methods fail to distinguish between discriminative attributes and non-discriminative attributes and do not distinguish the different contributions each attribute makes to classification. We propose CORL (Combining Ontology and Reinforcement Learning) for ZSC. CORL first obtains hierarchical classification rules from attribute annotations of object classes based on ontology. These rules contain only discriminative attributes. Reinforcement learning is used to adaptively determine the discriminative degrees of the rules. The most discriminative rules are then selected for ZSC. Experiments on three benchmark datasets showed that CORL achieved higher accuracies than baseline classifiers. This suggests that CORL effectively discovers the most discriminative rules for ZSC.}
}
@article{MILOVANCEVIC2019752,
title = {Time and ontology for resource recommendation system},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {525},
pages = {752-760},
year = {2019},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2019.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0378437119303693},
author = {Nataša Sokolov Milovančević and Aleksandar Gračanac},
keywords = {Hybrid recommendation system, Forgetting function, Ontology, Wu & Palmer similarity},
abstract = {Nowadays social tagging systems have considerable growth. These systems help users to find their favorite resources among the large volume of information. Many ways have been proposed for recommending. Since the tags are connotations of users’ interests and also the time of assigning tags indicates the current interests of users, so the combination of semantic influence of tags and time of tag assignment information can effect on the accuracy of recommendations. In this paper, an item recommendation system is proposed that by using important available information in tagging systems, e.g. time, and also using available ontology, the accuracy of recommended results has been improved. The evaluation of the proposed system is performed on movielens.org dataset. The results in comparing with other methods demonstrated the improving quality of the proposed system.}
}
@article{UHM2025105926,
title = {Effectiveness of retrieval augmented generation-based large language models for generating construction safety information},
journal = {Automation in Construction},
volume = {170},
pages = {105926},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105926},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524006629},
author = {Miyoung Uhm and Jaehee Kim and Seungjun Ahn and Hoyoung Jeong and Hongjo Kim},
keywords = {LLMs (large language models), RAG (retrieval-augmented generation), Personalized safety, Construction safety information generation},
abstract = {While Generative Pre-Trained Transformers (GPT)-based models offer high potential for context-specific information generation, inaccurate numerical responses, a lack of detailed information, and hallucination problems remain as the main challenges for their use in assisting safety engineering and management tasks. To address the challenges, this paper systematically evaluates the effectiveness of the Retrieval-Augmented Generation-based GPT (RAG-GPT) model for generating detailed and specific construction safety information. The RAG-GPT model was compared with four other GPT models, evaluating the models' responses from three different groups––2 researchers, 10 construction safety experts, and 30 construction workers. Quantitative analysis demonstrated that the RAG-GPT model showed superior performance compared to the other models. Experts rated the RAG-GPT model as providing more contextually relevant answers, with high marks for accuracy and essential information inclusion. The findings indicate that the RAG strategy, which uses vector data to enhance information retrieval, significantly improves the accuracy of construction safety information.}
}
@article{ZHENG2024102412,
title = {An aircraft assembly process formalism and verification method based on semantic modeling and MBSE},
journal = {Advanced Engineering Informatics},
volume = {60},
pages = {102412},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102412},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624000600},
author = {Xiaochen Zheng and Xiaodu Hu and Jinzhi Lu and Rebeca Arista and Joachim Lentes and Dimitris Kiritsis},
keywords = {Semantic modeling, Ontology, Model-based systems engineering, KARMA language, Discrete-event simulation, Aircraft assembly},
abstract = {The aircraft assembly system is highly complex involving different stakeholders from multiple domains. The design of such a system requires comprehensive consideration of various industrial scenarios aiming to optimize key performance indicators. Traditional design methods heavily rely on domain expert knowledge using documents to define assembly solutions which are later verified through simulations. However, these document-centric approaches cannot provide graphical notations for engineers to efficiently understand the entire assembly process. Moreover, it is difficult to analyze the performance of the designed assembly processes using simulations since the simulation models have to be developed based on the documents manually rather than be generated automatically from the design models. In this paper, a semantic-driven approach is proposed to support aircraft assembly process formalism and performance analysis. First, meta-models of aircraft assembly processes are developed based on SysML and discrete-event simulation models using a semantic modeling language named KARMA. Then an application ontology is defined for generating semantic models from KARMA architecture models to capture domain knowledge, system requirements and simulation model information of the aircraft assembly process. A model transformer is developed to transform the KARMA models to discrete-event simulation models based on the application ontology. Then the generated simulation models are executed to obtain the simulation results for verifying the designed assembly process. Finally, the obtained simulation results are used to support decision-making of selecting the optimal aircraft assembly process. A case study is conducted to verify the proposed method.}
}
@article{ROLDAN201843,
title = {An Ontology-based Approach for Sharing, Integrating, and Retrieving Architectural Knowledge},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {339},
pages = {43-62},
year = {2018},
note = {The XLII Latin American Computing Conference},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2018.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1571066118300483},
author = {María Luciana Roldán and Silvio Gonnet and Horacio Leone},
keywords = {Software architecture, Ontologies, Software architecture knowledge, Knowledge retrieving},
abstract = {The Architectural knowledge (AK) generated during software architecture projects is a valuable asset for software organizations. Although many organizations have adopted supporting tools to capture the produced AK, still there exist some difficulties in making it available to be retrieved by the consumers. Moreover, the boundaries of knowledge of an organization can be expanded to AK repositories that are shared or made public by other organizations. Thus, organizations have to deal with heterogeneity in knowledge representation, which makes complex the integration of knowledge from several sources. There is a need of an approach for sharing, integrating, and retrieving AK from various sources, which enables organizations to revisit and retrieve both their own and others' past decisions, as a basis for upcoming decisions. This paper describes an ontology-based approach for sharing, integrating, and retrieving knowledge from different AK sources, which is based on ISO/IEC/IEEE 42010. A proof of concept was developed to apply the approach on an AK management tool, and a scenario of knowledge retrieving was carried out.}
}
@article{LOEVENICH2025111162,
title = {Design and evaluation of an Autonomous Cyber Defence agent using DRL and an augmented LLM},
journal = {Computer Networks},
volume = {262},
pages = {111162},
year = {2025},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2025.111162},
url = {https://www.sciencedirect.com/science/article/pii/S1389128625001306},
author = {Johannes Loevenich and Erik Adler and Tobias Hürten and Roberto Rigolin F. Lopes},
keywords = {Autonomous Cyber Defence, Autonomous Cyber Operation Gym, Cybersecurity Knowledge Graph, Deep Reinforcement Learning, Large Language Model, Proximal Policy Optimization, Retrieval-Augmented Generation, Performance Comparison, Hybrid AI Approach},
abstract = {In this paper, we design and evaluate an Autonomous Cyber Defence (ACD) agent to monitor and act within critical network segments connected to untrusted infrastructure hosting active adversaries. We assume that modern network segments use software-defined controllers with the means to host ACD agents and other cybersecurity tools that implement hybrid AI models. Our agent uses a hybrid AI architecture that integrates deep reinforcement learning (DRL), augmented Large Language Models (LLMs), and rule-based systems. This architecture can be implemented in software-defined network controllers, enabling automated defensive actions such as monitoring, analysis, decoy deployment, service removal, and recovery. A core contribution of our work is the construction of three cybersecurity knowledge graphs that organise and map data from network logs, open source Cyber Threat Intelligence (CTI) reports, and vulnerability frameworks. These graphs enable automatic mapping of Common Vulnerabilities and Exposures (CVEs) to offensive tactics and techniques defined in the MITRE ATT&CK framework using Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformer (GPT) models. Our experimental evaluation of the knowledge graphs shows that BERT-based models perform better, with precision (83.02%), recall (75.92%), and macro F1 scores (58.70%) significantly outperforming GPT models. The ACD agent was evaluated in a Cyber Operations Research (ACO) gym against eleven DRL models, including Proximal Policy Optimisation (PPO), Hierarchical PPO, and ensembles under two different attacker strategies. The results show that our ACD agent outperformed baseline implementations, with its DRL models effectively mitigating attacks and recovering compromised systems. In addition, we implemented and evaluated a chatbot using Retrieval-Augmented Generation (RAG) and a prompting agent augmented with the CTI reports represented in the cybersecurity knowledge graphs. The chatbot achieved high scores on generation metrics such as relevance (0.85), faithfulness (0.83), and semantic similarity (0.88), as well as retrieval metrics such as contextual precision (0.91). The experimental results suggest that the integration of hybrid AI systems with knowledge graphs can enable the automation and improve the precision of cyber defence operations, and also provide a robust interface for cybersecurity experts to interpret and respond to advanced cybersecurity threats.}
}
@article{BERKOWITZ2025104850,
title = {Biomedical text normalization through generative modeling},
journal = {Journal of Biomedical Informatics},
volume = {167},
pages = {104850},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2025.104850},
url = {https://www.sciencedirect.com/science/article/pii/S1532046425000796},
author = {Jacob S. Berkowitz and Apoorva Srinivasan and Jose Miguel {Acitores Cortina} and Yasaman Fatapour and Nicholas P Tatonetti},
keywords = {Retrieval-augmented generation, Clinical text normalization, Large language models, Prompt engineering},
abstract = {Objective
A large proportion of electronic health record (EHR) data consists of unstructured medical language text. The formatting of this text is often flexible and inconsistent, making it challenging to use for predictive modeling, clinical decision support, and data mining. Large language models’ (LLMs) ability to understand context and semantic variations makes them promising tools for standardizing medical text. In this study, we develop and assess clinical text normalization pipelines built using large-language models.
Methods
We implemented four LLM-based normalization strategies (Zero-Shot Recall, Prompt Recall, Semantic Search, and Retrieval-Augmented Generation based normalization [RAGnorm]) and one baseline approach using TF-IDF based String Matching. We evaluated performance across three datasets of SNOMED-mapped condition terms: [1] an oncology-specific dataset, [2] a representative sample of institutional medical conditions, and [3] a dataset of commonly occurring condition codes (>1000 uses) from our institution. We measured performance by recording the mean shortest path length between predicted and true SNOMED CT terms. Additionally, we benchmarked our models against the TAC 2017 drug label annotations, which normalizes terms to the Medical Dictionary for Regulatory Activities (MedDRA) Preferred Terms.
Results
We found that RAGnorm was the most effective throughout each dataset, achieving a mean shortest path length of 0.21 for the domain-specific dataset, 0.58 for the sampled dataset, and 0.90 for the top terms dataset. It achieved a micro F1 score of 88.01 on task 4 of the TAC2017 conference, surpassing all other models without viewing the provided training data.
Conclusion
We find that retrieval-focused approaches overcome traditional LLM limitations for this task. RAGnorm and related retrieval techniques should be explored further for the normalization of biomedical free text.}
}
@incollection{SHOAIP2019147,
title = {Chapter 7 - Ontology enhanced fuzzy clinical decision support system},
editor = {Nilanjan Dey and Amira S. Ashour and Simon James Fong and Surekha Borra},
booktitle = {U-Healthcare Monitoring Systems},
publisher = {Academic Press},
pages = {147-177},
year = {2019},
series = {Advances in Ubiquitous Sensing Applications for Healthcare},
issn = {25891014},
doi = {https://doi.org/10.1016/B978-0-12-815370-3.00007-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128153703000074},
author = {Nora Shoaip and Shaker El-Sappagh and Sherif Barakat and Mohammed Elmogy},
keywords = {Diabetes mellitus, Clinical decision support system, Fuzzy rule-based systems, Semantic similarity, Ontology reasoning},
abstract = {A hybrid ontology-based fuzzy decision support system (OBFDSS) has been presented to support physicians in diabetes diagnosis problems. This work combines fuzzy logic and ontology to have both capabilities of representing semantic knowledge and reasoning with vagueness. Such a CDSS encodes the medical knowledge of the diabetes experts’ backgrounds or common reasoning knowledge regarding ontology. The ontology concepts, relationships, properties, and axioms are based on literature studies and domain expert experience. Ontology is used to represent the semantic structure as well as to calculate the clinical similarity between the compared diseases, medications, and symptoms related to diabetes. Then, the level of similarity is passed to a fuzzy inference engine. Our contribution is the development of a semantically intelligent fuzzy expert system. We will enhance the inference capabilities of the regular fuzzy system with the OWL2 ontology reasoning capabilities. The methodology of the framework can be defined in four stages: knowledge acquisition and features definition, semantic modeling, fuzzy modeling, and knowledge reasoning. The knowledge acquisition and features definition stage is responsible for acquiring the knowledge for diabetes diagnosis. The diabetes knowledge is collected from medical experts, the recent literature, EHR databases, and recent CPGs. Semantic modeling is related to the ability to represent semantic features in the form of ontology and to measure the level of clinical similarity between compared medical concepts. We propose an OWL2 ontology based on SNOMED CT standard medical terminology. The fuzzy model includes the definition of the fuzzy features and its associated fuzzy terms and fuzzy rules. Three types of knowledge including CPGs, the experience of domain physicians, and the processing of EHR data are defined in the fuzzy model. There are several issues that must be resolved in knowledge reasoning to generate fuzzy rules, enhance the generated fuzzy knowledge, utilize the fuzzy inference engine, and complete defuzzification. The proposed hybrid model offers a powerful tool in the diagnosis of diabetes. It involves building a complete linguistic fuzzy rule base that can integrate knowledge from experts and CPGs with knowledge extracted from training data as well as from the semantic model. This step enhances the level of automation and interoperability of CDSS. We expect to achieve acceptable and more flexible performance when using standard medical terminology. In this work, a hybrid OBFDSS is presented. The hybrid model is the most logical step to improve the fuzzy expert system by adding a semantic reasoning process to its capabilities. We expect that our proposed hybrid system will emphasize the significance of combining ontology and Mamdani fuzzy inference for diagnosing diabetes.}
}
@article{KIM2019256,
title = {An approach for recognition of human's daily living patterns using intention ontology and event calculus},
journal = {Expert Systems with Applications},
volume = {132},
pages = {256-270},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419302349},
author = {Je-Min Kim and Myung-Joong Jeon and Hyun-Kyu Park and Seok-Hyun Bae and Sung-Hyuk Bang and Young-Tack Park},
keywords = {Event calculus, Human intention, Intention ontology, Daily living pattern, Percept sequence},
abstract = {This paper proposes a method of recognizing the intention of human activity based on percept sequences that represent the activities of daily living (ADL) in a residential space. Based on the activity intention ontology representing actions, poses, and nearby objects related to human activity intentions, the proposed method identifies a human activity intention by using the event calculus when a percept sequence is entered. It is very difficult to recognize ADL occurring in various places in a residence by using regular percept sequences without error. Furthermore, a human activity can have complex intentions. To solve these problems, this paper proposes an activity intention recognition process consisting of three steps. First, the activity intention inference step recognizes the intention of a given percept sequence based on the activity intention ontology. Second, the complex intention-identifying step determines whether to terminate the previously maintained activity intention or continue maintaining it in a complex manner based on the newly inferred activity intention. Third, the uncertainty handling step corrects an inaccurate activity intention caused by an error in the percept sequence. To evaluate the proposed method presented in this paper, activity intention recognition experiments were conducted based on the ADL data that were collected for six households of elderly persons older than 70 years. The results showed that the proposed method has a precision of 99.62% and recall of 92.83%.}
}
@article{FRANCA2022,
title = {Towards an Ontology for Impact Projection of Complex Decisions},
journal = {International Journal of Decision Support System Technology},
volume = {14},
number = {1},
year = {2022},
issn = {1941-6296},
doi = {https://doi.org/10.4018/IJDSST.286751},
url = {https://www.sciencedirect.com/science/article/pii/S1941629622000246},
author = {Juliana Baptista dos Santos França and Marcos Roberto da Silva Borges},
keywords = {Collaborative Decision-Making, Complex Decision, Decision Alternative, Impact Ontology, Naturalistic Decision-Making, OntoImpact, Ontology, Tree Design Structure},
abstract = {ABSTRACT
Complex decisions are an unusual process, composed of actions. An impact is a measure of the tangible and intangible consequences of one thing on another. Impacts are interdependent, and the environment in which they are measured generates constant change for decision making. This paper proposes the impact projection’s conceptualization, organized into a meta-ontology called OntoImpact. It comprises concepts that are crucial in supporting the understanding and representation of impact projections for complex decisions. The main contribution of OntoImpact is to support decision makers in their work tasks, besides providing bases to support the development of a complex decision system. This paper was evaluated in a case study of an emergency domain. The results show that OntoImpact provides elements that can support complex decision analysis and project impacts in a collaborative way.}
}
@article{LI2025130991,
title = {Adaptive-TOD: An LLM-driven and adaptive agent for diverse interaction modes},
journal = {Neurocomputing},
volume = {652},
pages = {130991},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.130991},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225016637},
author = {Pan Li and Qingwen Yang and Sai Xu and Xuejing Li and Zhi Li and Chen Wang and Yanyi Liu and Tiezheng Guo and Jiawei Tang and Yingyou Wen},
keywords = {TOD, Large language model, Interaction, Workflow, Agent},
abstract = {Previous studies on Task-Oriented Dialogue systems have primarily concentrated on straightforward tasks, utilizing basic strategies such as “Request and Inform" within dialogues. Although expected results have been achieved in simulated scenarios, it becomes evident that this solitary interaction mode is inadequate for complex and dynamic processes in real industrial business. Handling such intricate real-world tasks requires not only consideration of unpredictable user behaviors but also execution feedback from back-end services and workflow constraints. To tackle this challenge, we introduce, for the first time, a novel task termed Diverse Interaction Conversation (DIC) and its corresponding framework in this paper. It offers a comprehensive analysis of varied user behaviors, dynamic execution feedback, and the relationship between them and system interaction policies. Next, we specifically design a task flow schema based on a directed graph structure to illustrate these dynamic and complex task workflows. In response to DIC scenarios, we further develop an adaptive LLM-driven agent (Adaptive-TOD), which includes three modules: an Adaptive Interaction Policy Planning module for generating flexible interactive responses; a Session State Management module for maintaining dialogue status and a Task Flow Engine designed to efficiently parse dynamic task flow schema. Collaborating with industry experts, we collect DIC dataset covering eight industrial tasks to comprehensively evaluate the adaptive system. The experimental findings reveal that, even when solely utilizing an On-Device LLM with instruction-following modes, Adaptive-TOD still exhibits adaptable interaction capabilities and superior task workflow consistency. These results highlight the practical application potential and wide industrial expansion prospects.}
}
@article{CIMMINO2025104282,
title = {Open Digital Rights Enforcement framework (ODRE): From descriptive to enforceable policies},
journal = {Computers & Security},
volume = {150},
pages = {104282},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104282},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824005881},
author = {Andrea Cimmino and Juan Cano-Benito and Raúl García-Castro},
keywords = {Open digital rights language, Privacy policies, ODRL enforcement},
abstract = {From centralised platforms to decentralised ecosystems, like Data Spaces, sharing data has become a paramount challenge. For this reason, the definition of data usage policies has become crucial in these domains, highlighting the necessity of effective policy enforcement mechanisms. The Open Digital Rights Language (ODRL) is a W3C standard ontology designed to describe data usage policies, however, it lacks built-in enforcement capabilities, limiting its practical application. This paper introduces the Open Digital Rights Enforcement (ODRE) framework, whose goal is to provide ODRL with enforcement capabilities. The ODRE framework proposes a novel approach to express ODRL policies that integrates the descriptive ontology terms of ODRL with other languages that allow behaviour specification, such as dynamic data handling or function evaluation. The framework includes an enforcement algorithm for ODRL policies and two open-source implementations in Python and Java. The ODRE framework is also designed to support future extensions of ODRL to specific domain scenarios. In addition, current limitations of ODRE, ODRL, and current challenges are reported. Finally, to demonstrate the enforcement capabilities of the implementations, their performance, and their extensibility features, several experiments have been carried out with positive results.}
}
@article{VISALLI2025105456,
title = {Can natural language processing or large language models replace human operators for pre-processing word and sentence-based free comments sensory evaluation data?},
journal = {Food Quality and Preference},
volume = {127},
pages = {105456},
year = {2025},
issn = {0950-3293},
doi = {https://doi.org/10.1016/j.foodqual.2025.105456},
url = {https://www.sciencedirect.com/science/article/pii/S095032932500031X},
author = {Michel Visalli and Ronan Symoneaux and Cécile Mursic and Margaux Touret and Flore Lourtioux and Kipédène Coulibaly and Benjamin Mahieu},
keywords = {Open ended questions, ChatGPT, Textual data, Consumer test, Method comparison, Drivers of liking},
abstract = {The free comment (FC) method enables the collection of insights on products based on consumers' natural language. The primary drawback is the need for extensive data pre-processing. This study compared the results of three data pre-processing techniques applied to FC data related to the perception of six madeleines by two panels of 100 consumers: manual pre-processing by four human experts, automated pre-processing by an expert system, and automated pre-processing by the large language model ChatGPT. Two modes of data collection were used: responses only with words or short expressions (“FC words”), or responses based on complete sentences (“FC sentences”). Various indicators (number of words extracted, number of concepts retained, pre-processing time, level of repeatability/discrimination/stability of findings) were computed and compared between data collection modes and pre-processing techniques. It was shown that the automated systems performed correctly with FC words; however, they were less effective at extracting relevant words from FC sentences. The findings from statistical analyses following automated pre-processing were less repeatable and discriminative compared to those from the most proficient human operators. It was also demonstrated that, beyond the overall differences between products, the pre-processing of FC data can be a major source of non-reproducibility in findings, depending on the operators and the level of detail they consider when extracting information. Finally, the advantages and disadvantages of each pre-processing technique were summarized, along with several recommendations for pre-processing and analysing FC data at the appropriate level of granularity to draw robust conclusions.}
}
@incollection{CANNATARO2022119,
title = {Chapter 11 - Ontologies in bioinformatics},
editor = {Mario Cannataro and Pietro Hiram Guzzi and Giuseppe Agapito and Chiara Zucco and Marianna Milano},
booktitle = {Artificial Intelligence in Bioinformatics},
publisher = {Elsevier},
pages = {119-128},
year = {2022},
isbn = {978-0-12-822952-1},
doi = {https://doi.org/10.1016/B978-0-12-822952-1.00021-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128229521000218},
author = {Mario Cannataro and Pietro Hiram Guzzi and Giuseppe Agapito and Chiara Zucco and Marianna Milano},
keywords = {Biomedical ontologies, Gene ontology, Semantic similarities, Functional enrichment analysis},
abstract = {Biological data about genes, proteins, and biologically relevant molecules, that are stored in databases, may be associated to biological information (knowledge) such as experiments, properties and functions, response to drugs, etc. Such knowledge is formally structured in ontologies that provide the best formalization to organize and store knowledge. Consequently, it is possible to introduce novel analytical methodologies that are based on the use of ontologies. An example is represented by semantic similarities, i.e., the calculation of the similarity of two or more proteins starting from their annotations. For instance, semantic measures have been used for the prediction of protein complexes. Otherwise, functional enrichment analysis methods ensure the discovery of the functions of the input list of genes by using annotations. This chapter describes the main biomedical ontologies presented in the literature and their bioinformatic applications. Then, the principal semantic similarity measures are presented. Finally, the treatment of functional enrichment analysis concludes the chapter.}
}
@incollection{DEEPAK2022245,
title = {Chapter 12 - IntelliOntoRec: a knowledge infused semiautomatic approach for ontology formulation in healthcare and medical science},
editor = {Sanju Tiwari and Fernando {Ortiz Rodriguez} and M.A. Jabbar},
booktitle = {Semantic Models in IoT and eHealth Applications},
publisher = {Academic Press},
pages = {245-265},
year = {2022},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-323-91773-5},
doi = {https://doi.org/10.1016/B978-0-32-391773-5.00018-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323917735000182},
author = {Gerard Deepak and Deepak {Surya S.}},
keywords = {Intelligent systems, Ontology modeling, Linked open data, Metaheuristic, Knowledge centric},
abstract = {Linked Open Data (LOD) on the semantic structure of the World Wide Web plays a vital role in deciding the effectiveness of semantic inferencing for making application-oriented decisions. However, when working with safety-critical systems and critical domains that require scientific acceptance from a community like healthcare, the modeling of linked open data must require expert opinion. The linked open data for medical and healthcare as a domain of choice should preferably be semiautomatic with a standard scheme for automatically modeling domain specific sensitive knowledge and human interaction for verification and a community acceptance for healthcare. In this chapter, a strategic scheme that amalgamates semantic mapping of ontologies using a triadic system of semantic-similarity, Simpson's diversity index, and a novel eXplainable AI (XAI) algorithm, which imbibes the concept of fluid thrust has been proposed. The IntelliOntoRec integrates a diverse source of knowledge from medical journals, RDFs, semantic wikis and follows a three-phase verification of knowledge modeled wherein the domain experts can view the automatically modeled knowledge and contribute. The second phase is the honest review by the community of domain experts and integrates knowledge based on manual modeling and opinions of domain experts and incorporates the grey wolf metaheuristic algorithm. The third phase is the rereview an commit, where the knowledge modeled will be finalized. Further, as the modeling and curation of knowledge are successfully achieved, a rule-based axiom induction using heuristics of information is achieved to successfully organize the linked open data as hierarchies before posting it to the linked open data cloud. The ontology modeling is evaluated based on the scoring of the intelligent crowd of experts, and an overall Modeling F-Measure of 98.557583 has been achieved by the proposed IntelliOntoRec.}
}
@article{OSORIOGOMEZ20201673,
title = {Operational Risk Management in the Pharmaceutical Supply Chain Using Ontologies and Fuzzy QFD},
journal = {Procedia Manufacturing},
volume = {51},
pages = {1673-1679},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.233},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920321053},
author = {Juan Carlos {Osorio Gómez} and Katherine Torres España},
keywords = {Operational risk, ontologies, fuzzy Quality Function Deployment, pharmaceutical supply chain},
abstract = {Operational Risk Management is an important aspect of current Supply Chain Management activities. Supply Chain Risk Management Systems involve at least risk identification, risk prioritization, and risk management. Risks in the pharmaceutical supply chain can affect not only the company’s performance but also can affect human life. That is an important reason to manage risks in the chain. There are many approaches to supply chain risk management that focuses on operational risks. These approaches look to mitigate or eliminate the most critical risks in order to achieve the results that the chain expects. This paper presents a methodological approach using Ontologies and Fuzzy Quality Function Deployment (FQFD) to eliminate or mitigate the most important risks. Our proposal was applied to a pharmaceutical company in Colombia, particularly in the transport and storage of finished products for export. With this approach the company defined actions oriented to the most critical risks.}
}
@article{VALLS2018145,
title = {Using ontology-based clustering to understand the push and pull factors for British tourists visiting a Mediterranean coastal destination},
journal = {Information & Management},
volume = {55},
number = {2},
pages = {145-159},
year = {2018},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2017.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617303920},
author = {Aida Valls and Karina Gibert and Alícia Orellana and Salvador Antón-Clavé},
keywords = {Data mining, Tourism motivations, Destination meaning, Ontologies, Qualitative data tourism geography},
abstract = {This paper studies why British tourists decide to travel to a particular destination in a Catalan region. The analysis is based on a survey that includes open-ended questions. First, we propose the operationalization of the concepts of motivation and meaning as push–pull factors when choosing a destination. Second, an ontology-based clustering method is presented, which makes it possible to analyse these qualitative factors from a semantic perspective to obtain tourist segments. A benchmark confirms that the segmentation obtained is better than that generated using classic clustering methods The results show that different meanings can be associated with any single place.}
}
@article{CADEDDU2024108166,
title = {A comparative analysis of knowledge injection strategies for large language models in the scholarly domain},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108166},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108166},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624003245},
author = {Andrea Cadeddu and Alessandro Chessa and Vincenzo {De Leo} and Gianni Fenu and Enrico Motta and Francesco Osborne and Diego {Reforgiato Recupero} and Angelo Salatino and Luca Secchi},
keywords = {Knowledge injection, Knowledge graphs, Large language models, Transformers, BERT, Classification, Natural language processing},
abstract = {In recent years, transformer-based models have emerged as powerful tools for natural language processing tasks, demonstrating remarkable performance in several domains. However, they still present significant limitations. These shortcomings become more noticeable when dealing with highly specific and complex concepts, particularly within the scientific domain. For example, transformer models have particular difficulties when processing scientific articles due to the domain-specific terminologies and sophisticated ideas often encountered in scientific literature. To overcome these challenges and further enhance the effectiveness of transformers in specific fields, researchers have turned their attention to the concept of knowledge injection. Knowledge injection is the process of incorporating outside knowledge into transformer models to improve their performance on certain tasks. In this paper, we present a comprehensive study of knowledge injection strategies for transformers within the scientific domain. Specifically, we provide a detailed overview and comparative assessment of four primary methodologies, evaluating their efficacy in the task of classifying scientific articles. For this purpose, we constructed a new benchmark including both 24K labelled papers and a knowledge graph of 9.2K triples describing pertinent research topics. We also developed a full codebase to easily re-implement all knowledge injection strategies in different domains. A formal evaluation indicates that the majority of the proposed knowledge injection methodologies significantly outperform the baseline established by Bidirectional Encoder Representations from Transformers.}
}
@article{KANG2025,
title = {Detecting Redundant Health Survey Questions by Using Language-Agnostic Bidirectional Encoder Representations From Transformers Sentence Embedding: Algorithm Development Study},
journal = {JMIR Medical Informatics},
volume = {13},
year = {2025},
issn = {2291-9694},
doi = {https://doi.org/10.2196/71687},
url = {https://www.sciencedirect.com/science/article/pii/S2291969425001061},
author = {Sunghoon Kang and Hyewon Park and Ricky Taira and Hyeoneui Kim},
keywords = {person-generated health data, PGHD, bidirectional encoder representations from transformers, BERT, semantic similarity, language-agnostic BERT sentence embedding, LaBSE, sentence-bidirectional encoder representations from transformers, SBERT, interoperability},
abstract = {Background
As the importance of person-generated health data (PGHD) in health care and research has increased, efforts have been made to standardize survey-based PGHD to improve its usability and interoperability. Standardization efforts such as the Patient-Reported Outcomes Measurement Information System (PROMIS) and the National Institutes of Health (NIH) Common Data Elements (CDE) repository provide effective tools for managing and unifying health survey questions. However, previous methods using ontology-mediated annotation are not only labor-intensive and difficult to scale but also challenging for identifying semantic redundancies in survey questions, especially across multiple languages.
Objective
The goal of this work was to compute the semantic similarity among publicly available health survey questions to facilitate the standardization of survey-based PGHD.
Methods
We compiled various health survey questions authored in both English and Korean from the NIH CDE repository, PROMIS, Korean public health agencies, and academic publications. Questions were drawn from various health lifelog domains. A randomized question pairing scheme was used to generate a semantic text similarity dataset consisting of 1758 question pairs. The similarity scores between each question pair were assigned by 2 human experts. The tagged dataset was then used to build 4 classifiers featuring bag-of-words, sentence-bidirectional encoder representations from transformers (SBERT) with bidirectional encoder representations from transformers (BERT)–based embeddings, SBERT with language-agnostic BERT sentence embedding (LaBSE), and GPT-4o. The algorithms were evaluated using traditional contingency statistics.
Results
Among the 3 algorithms, SBERT-LaBSE demonstrated the highest performance in assessing the question similarity across both languages, achieving area under the receiver operating characteristic and precision-recall curves of >0.99. Additionally, SBERT-LaBSE proved effective in identifying cross-lingual semantic similarities. The SBERT-LaBSE algorithm excelled at aligning semantically equivalent sentences across both languages but encountered challenges in capturing subtle nuances and maintaining computational efficiency. Future research should focus on testing with larger multilingual datasets and on calibrating and normalizing scores across the health lifelog domains to improve consistency.
Conclusions
This study introduces the SBERT-LaBSE algorithm for calculating the semantic similarity across 2 languages, showing that it outperforms BERT-based models, the GPT-4o model, and the bag-of-words approach, highlighting its potential in improving the semantic interoperability of survey-based PGHD across language barriers.}
}
@article{LINDMAN201922,
title = {Annotations, Ontologies, and Whole Slide Images – Development of an Annotated Ontology-Driven Whole Slide Image Library of Normal and Abnormal Human Tissue},
journal = {Journal of Pathology Informatics},
volume = {10},
number = {1},
pages = {22},
year = {2019},
issn = {2153-3539},
doi = {https://doi.org/10.4103/jpi.jpi_81_18},
url = {https://www.sciencedirect.com/science/article/pii/S2153353922003856},
author = {Karin Lindman and Jerómino F. Rose and Martin Lindvall and Claes Lundstrom and Darren Treanor},
keywords = {Annotation, digital pathology, image database, ontology, whole slide images},
abstract = {Objective: Digital pathology is today a widely used technology, and the digitalization of microscopic slides into whole slide images (WSIs) allows the use of machine learning algorithms as a tool in the diagnostic process. In recent years, “deep learning” algorithms for image analysis have been applied to digital pathology with great success. The training of these algorithms requires a large volume of high-quality images and image annotations. These large image collections are a potent source of information, and to use and share the information, standardization of the content through a consistent terminology is essential. The aim of this project was to develop a pilot dataset of exhaustive annotated WSI of normal and abnormal human tissue and link the annotations to appropriate ontological information. Materials and Methods: Several biomedical ontologies and controlled vocabularies were investigated with the aim of selecting the most suitable ontology for this project. The selection criteria required an ontology that covered anatomical locations, histological subcompartments, histopathologic diagnoses, histopathologic terms, and generic terms such as normal, abnormal, and artifact. WSIs of normal and abnormal tissue from 50 colon resections and 69 skin excisions, diagnosed 2015-2016 at the Department of Clinical Pathology in Linköping, were randomly collected. These images were manually and exhaustively annotated at the level of major subcompartments, including normal or abnormal findings and artifacts. Results: Systemized nomenclature of medicine clinical terms (SNOMED CT) was chosen, and the annotations were linked to its codes and terms. Two hundred WSI were collected and annotated, resulting in 17,497 annotations, covering a total area of 302.19 cm2, equivalent to 107,7 gigapixels. Ninety-five unique SNOMED CT codes were used. The time taken to annotate a WSI varied from 45 s to over 360 min, a total time of approximately 360 h. Conclusion: This work resulted in a dataset of 200 exhaustive annotated WSIs of normal and abnormal tissue from the colon and skin, and it has informed plans to build a comprehensive library of annotated WSIs. SNOMED CT was found to be the best ontology for annotation labeling. This project also demonstrates the need for future development of annotation tools in order to make the annotation process more efficient.}
}
@article{WANG2022313,
title = {ULSA: unified language of synthesis actions for the representation of inorganic synthesis protocols},
journal = {Digital Discovery},
volume = {1},
number = {3},
pages = {313-324},
year = {2022},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d1dd00034a},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X23001109},
author = {Zheren Wang and Kevin Cruse and Yuxing Fei and Ann Chia and Yan Zeng and Haoyan Huo and Tanjin He and Bowen Deng and Olga Kononova and Gerbrand Ceder},
abstract = {ABSTRACT
Applying AI power to predict syntheses of novel materials requires high-quality, large-scale datasets. Extraction of synthesis information from scientific publications is still challenging, especially for extracting synthesis actions, because of the lack of a comprehensive labeled dataset using a solid, robust, and well-established ontology for describing synthesis procedures. In this work, we propose the first unified language of synthesis actions (ULSA) for describing inorganic synthesis procedures. We created a dataset of 3040 synthesis procedures annotated by domain experts according to the proposed ULSA scheme. To demonstrate the capabilities of ULSA, we built a neural network-based model to map arbitrary inorganic synthesis paragraphs into ULSA and used it to construct synthesis flowcharts for synthesis procedures. Analysis of the flowcharts showed that (a) ULSA covers essential vocabulary used by researchers when describing synthesis procedures and (b) it can capture important features of synthesis protocols. The present work focuses on the synthesis protocols for solid-state, sol–gel, and solution-based inorganic synthesis, but the language could be extended in the future to include other synthesis methods. This work is an important step towards creating a synthesis ontology and a solid foundation for autonomous robotic synthesis.}
}
@article{HUITZIL2019354,
title = {Gait recognition using fuzzy ontologies and Kinect sensor data},
journal = {International Journal of Approximate Reasoning},
volume = {113},
pages = {354-371},
year = {2019},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2019.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X19301331},
author = {Ignacio Huitzil and Lacramioara Dranca and Jorge Bernad and Fernando Bobillo},
keywords = {Fuzzy ontologies, Gait recognition, Machine learning},
abstract = {Gait recognition involves the automatic classification of human people from sequences of data about their movement patterns. It is an interesting problem with several applications, such as security or medicine. Even low cost sensors can be used to capture pose sequences with enough quality to make a successful classification possible. In this paper, we describe the use of fuzzy ontologies to represent sequences of Microsoft Kinect gait data and some biometric features relevant for the gait recognition computed after them, enabling more reusable and interpretable datasets. We also propose a novel recognition algorithm based on fuzzy logic that outperforms state-of-the-art methods for straight line walks. We also face the problem of the identification of unknown individuals that are not present in the system knowledge base.}
}
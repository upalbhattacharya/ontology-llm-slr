@article{MATHEW20221031,
title = {A Decade's Experience in Pediatric Chromosomal Microarray Reveals Distinct Characteristics Across Ordering Specialties},
journal = {The Journal of Molecular Diagnostics},
volume = {24},
number = {9},
pages = {1031-1040},
year = {2022},
issn = {1525-1578},
doi = {https://doi.org/10.1016/j.jmoldx.2022.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S152515782200160X},
author = {Mariam T. Mathew and Austin Antoniou and Naveen Ramesh and Min Hu and Jeffrey Gaither and Danielle Mouhlas and Sayaka Hashimoto and Maggie Humphrey and Theodora Matthews and Jesse M. Hunter and Shalini Reshmi and Matthew Schultz and Kristy Lee and Ruthann Pfau and Catherine Cottrell and Kim L. McBride and Nicholas E. Navin and Bimal P. Chaudhari and Marco L. Leung},
abstract = {Chromosomal microarray (CMA) is a testing modality frequently used in pediatric patients; however, published data on its utilization are limited to the genetic setting. We performed a database search for all CMA testing performed from 2010 to 2020, and delineated the diagnostic yield based on patient characteristics, including sex, age, clinical specialty of providers, indication of testing, and pathogenic finding. The indications for testing were further categorized into Human Phenotype Ontology categories for analysis. This study included a cohort of 14,541 patients from 29 different medical specialties, of whom 30% were from the genetics clinic. The clinical indications for testing suggested that neonatology patients demonstrated the greatest involvement of multiorgan systems, involving the most Human Phenotype Ontology categories, compared with developmental behavioral pediatrics and neurology patients being the least. The top pathogenic findings for each specialty differed, likely due to the varying clinical features and indications for testing. Deletions involving the 22q11.21 locus were the top pathogenic findings for patients presenting to genetics, neonatology, cardiology, and surgery. Our data represent the largest pediatric cohort published to date. This study is the first to demonstrate the diagnostic utility of this assay for patients seen in the setting of different specialties, and it provides normative data of CMA results among a general pediatric population referred for testing because of variable clinical presentations.}
}
@article{THALHATH2025100068,
title = {Metadata application proﬁle as a mechanism for semantic interoperability in FAIR and open data publishing},
journal = {Data and Information Management},
volume = {9},
number = {1},
pages = {100068},
year = {2025},
issn = {2543-9251},
doi = {https://doi.org/10.1016/j.dim.2024.100068},
url = {https://www.sciencedirect.com/science/article/pii/S2543925124000044},
author = {Nishad Thalhath and Mitsuharu Nagamori and Tetsuo Sakaguchi},
keywords = {Application profiles, Semantic interoperability, Data Interoperability, Open data, FAIR data, Linking data, Semantic validation, Linked Open Data, RDF},
abstract = {Application profiles, also known as metadata application profiles, are customised collections of vocabularies adapted from various namespaces and tailored for specific local applications. These profiles act as constrainers and explainers for the (meta)data. Semantic interoperability is the ability of computer systems to exchange data in a mutually understandable manner, facilitating data sharing across diverse platforms and applications without compromising its meaning. As a critical component of semantic interoperability, application profiles enforce semantics to (meta)data, enhancing its openness, interoperability, and reusability. This study assesses the feasibility of representing a comprehensive application profile in a format aligned with the semantic web, ensuring interoperability between profiles and datasets. Dublin Core Description Set Profiles (DSP) is adapted as the modeling framework for metadata application profiles, steering the associated datasets toward RDF compliance. The research outcomes include “Yet Another Metadata Application Profiles” (YAMA) as a preprocessor grounded in the DSP framework for developing and managing metadata application profiles. YAMA facilitates the generation of various standard formats of application profiles, ensuring they are represented in human-readable documentation, machine-actionable forms, and even data validation languages. A data mapping extension to YAMA is proposed to ensure the semantic interoperability of open data, bridging non-RDF data structures to RDF, thus enabling the publication of 5-star open data. This ensures smooth dataset integration and the creation of linkable, semantically rich open datasets. The work emphasizes the pivotal role of application profiles in fortifying the semantic interoperability of (meta)data, thereby elevating dataset openness.}
}
@article{ZHANG2024103468,
title = {Transcriptome analysis reveals the genes involved in spermatogenesis in white feather broilers},
journal = {Poultry Science},
volume = {103},
number = {4},
pages = {103468},
year = {2024},
issn = {0032-5791},
doi = {https://doi.org/10.1016/j.psj.2024.103468},
url = {https://www.sciencedirect.com/science/article/pii/S0032579124000476},
author = {Gaomeng Zhang and Peihao Liu and Ruiping Liang and Fan Ying and Dawei Liu and Meng Su and Li Chen and Qi Zhang and Yuhong Liu and Sha Liu and Guiping Zhao and Qinghe Li},
keywords = {broiler, semen volume, transcriptome analysis},
abstract = {ABSTRACT
Semen volume is an important economic trait of broilers and one of the important indices for continuous breeding. The objective of this study was to identify genes related to semen volume through transcriptome analysis of the testis tissue of white feather broilers. The testis samples with the highest semen volume (H group, n = 5) and lowest semen volume (L group, n = 5) were selected from 400-day-old roosters for transcriptome analysis by RNA sequencing. During the screening of differentially expressed genes (DEGs) between the H and L groups, a total of 386 DEGs were identified, among which 348 were upregulated and 38 were downregulated. Gene ontology (GO) enrichment analysis and Kyoto Encyclopedia of Genes and Genomes (KEGG) analysis revealed that the immune response, leukocyte differentiation, cell adhesion molecules and collagen binding played vital roles in spermatogenesis. The results showed that 4 genes related to spermatogenesis, namely, COL1A1, CD74, ARPC1B and APOA1, were significantly expressed in Group H, which was consistent with the phenotype results. Our findings may provide a basis for further research on the genetic mechanism of semen volume in white feather broilers.}
}
@article{VODYAHO2024350,
title = {Continuous agile cyber–physical systems architectures based on digital twins},
journal = {Future Generation Computer Systems},
volume = {153},
pages = {350-359},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23004326},
author = {Alexander Vodyaho and Nataly Zhukova and Radhakrishnan Delhibabu and Alexey Subbotin},
keywords = {Compute continuum, Digital twin, Digital twin networks, Digital threads, Model synthesis, Continuous architecture, Agile architecture, cyber-physical system},
abstract = {Modern cyber-physical systems, for the most part, are large-scale multilevel heterogeneous distributed systems that integrate subsystems of different kinds and are built on the Internet of Things platforms, where system structure and behavior are not constant. Managing such systems and keeping them in working condition throughout their lifetime is a difficult task. The proposed article discusses one of the possible approaches to solving this problem, based on the use of well-known continuous and agile architecture paradigms. However, there are currently no effective mechanisms for implementing these paradigms. The proposed article suggests a new approach to implementing continuous agile architectures by utilizing digital twins and proposes a reference architecture for a run-time dynamic digital twin. This method is unique because it builds a series of dynamic digital twins that model the system in real time, utilizing data about system events. Build the first models using the models used in earlier stages of the system lifecycle. This gives the following opportunities: i) a way to use dynamic digital twins to implement the continuous agile architecture paradigm; ii) a generalized three-level model of the life cycle of the continuous agile architecture; iii) a reference architecture for dynamic digital twins; and iv) a set of models that are all about using dynamic digital twins. The suggested approach enables the management of heterogeneous multilevel cyber-physical systems with variable structure and behavior variability.}
}
@article{INNOCENTI2020100882,
title = {An iterative process to construct an interdisciplinary ABM using MR POTATOHEAD: An application to Housing Market Models in touristic areas},
journal = {Ecological Complexity},
volume = {44},
pages = {100882},
year = {2020},
issn = {1476-945X},
doi = {https://doi.org/10.1016/j.ecocom.2020.100882},
url = {https://www.sciencedirect.com/science/article/pii/S1476945X20301628},
author = {Eric Innocenti and Claudio Detotto and Corinne Idda and Dawn Cassandra Parker and Dominique Prunetti},
keywords = {Complex systems, ABM/LUCC, MR POTATOHEAD, Conceptual Modeling, NetLogo pattern},
abstract = {This work demonstrates an interdisciplinary modeling process between economists and computer scientists to formulate an Agent-Based Model of Land Use and Cover Change (ABM/LUCC). The MR POTATOHEAD (Model Representing Potential Objects That Appear in The Ontology of Human - Environmental Actions & Decisions) generic modeling methodology for ABM/LUCC process building is used to construct the interdisciplinary ABM. In our paper, this methodology is applied to an ABM/LUCC describing a collection of tourist areas that have faced an intense development leading to huge pressure on land prices as well as to land-use conflicts (local residential market, tourist rental investments, agricultural production). The process moves from the conceptual template description, to conceptual model, to its computer model counterpart. The NetLogo Design Pattern is used to efficiently integrate the spatial aspects of the ABM. The paper highlights benefits offered by this software technique of engineering to design modular, extensible and scalable ABM/LUCC computer models. We present preliminary work aiming to better model the dynamical complex system of ABM/LUCC. Preliminary results show that the MR POTATOHEAD modeling process facilitates both interdisciplinary dialogs and computer model formulation. This conceptual template approach facilitates interdisciplinary communication and interaction between economists and computer scientists.}
}
@article{DAVID2023101223,
title = {Model consistency as a heuristic for eventual correctness},
journal = {Journal of Computer Languages},
volume = {76},
pages = {101223},
year = {2023},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2023.101223},
url = {https://www.sciencedirect.com/science/article/pii/S2590118423000333},
author = {Istvan David and Hans Vangheluwe and Eugene Syriani},
keywords = {Consistency, Correctness, Heuristics, Model consistency, Model-based systems engineering, Multi-view modeling},
abstract = {Inconsistencies between stakeholders’ views pose a severe challenge in the engineering of complex systems. The past decades have seen a vast number of sophisticated inconsistency management techniques being developed. These techniques build on the common idea of “managing consistency instead of removing inconsistency”, as put forward by Finkelstein. While it is clear what and how to do about inconsistencies, it is less clear why inconsistency is particularly useful. After all, it is the correctness of the system that should matter, as correctness is the end-user-facing quality of the product. In this paper, we analyze this question by investigating the relationship between (in)consistency and (in)correctness. We formally prove that, contrary to intuition, consistency does not imply correctness. However, consistency is still a good heuristic for eventual correctness. We elaborate on the consequences of this assertion and provide pointers as to how to make use of it in the next generation of inconsistency management techniques.}
}
@article{DOYLE2025101835,
title = {Creative art-based pedagogies with autistic students: A systematic review on stakeholders' perspectives on its delivery and implementation in secondary mainstream schools},
journal = {Thinking Skills and Creativity},
volume = {57},
pages = {101835},
year = {2025},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2025.101835},
url = {https://www.sciencedirect.com/science/article/pii/S1871187125000847},
author = {Kayleigh Doyle and Lisa E. Kim},
keywords = {Creative art, Education, Pedagogy, Autism, Systematic review, Secondary school},
abstract = {More than 70 % of autistic young people are educated in mainstream schools and some, similarly to their non-autistic peers, experience challenges in mainstream settings. Research suggests that the creative arts offer unique prospects in the education of autistic students. However, such research primarily explores the creative arts in a therapeutic or interventional context; little research has considered creative arts’ use as a pedagogical tool in mainstream secondary school classrooms with autistic students. The current narrative systematic review aims to synthesise empirical evidence concerning: (a) how creative arts-based pedagogy (CABP) is theorised, (b) creative arts-based pedagogies implementation and delivery with autistic secondary school students and (c) research gaps in the field. Identified through thematic analysis, findings indicate that creative arts-based pedagogy was perceived with greater positivity than negativity, yet a greater number of disabling factors were identified than enabling factors. The only paper to explicitly discuss a gap in research within the extracted data was Kehl (2021). However, in the concluding sections not under analysis in line with this study's data extraction approach, further gaps in research were identified. Further research is required to establish enabling factors that can help increase the effectiveness of CABP delivery and implementation in secondary educational settings with autistic students.}
}
@article{TABARI2024,
title = {State-of-the-Art Fast Healthcare Interoperability Resources (FHIR)–Based Data Model and Structure Implementations: Systematic Scoping Review},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/58445},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424001285},
author = {Parinaz Tabari and Gennaro Costagliola and Mattia {De Rosa} and Martin Boeker},
keywords = {data model, Fast Healthcare Interoperability Resources, FHIR, interoperability, modeling, PRISMA},
abstract = {Background
Data models are crucial for clinical research as they enable researchers to fully use the vast amount of clinical data stored in medical systems. Standardized data and well-defined relationships between data points are necessary to guarantee semantic interoperability. Using the Fast Healthcare Interoperability Resources (FHIR) standard for clinical data representation would be a practical methodology to enhance and accelerate interoperability and data availability for research.
Objective
This research aims to provide a comprehensive overview of the state-of-the-art and current landscape in FHIR-based data models and structures. In addition, we intend to identify and discuss the tools, resources, limitations, and other critical aspects mentioned in the selected research papers.
Methods
To ensure the extraction of reliable results, we followed the instructions of the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews) checklist. We analyzed the indexed articles in PubMed, Scopus, Web of Science, IEEE Xplore, the ACM Digital Library, and Google Scholar. After identifying, extracting, and assessing the quality and relevance of the articles, we synthesized the extracted data to identify common patterns, themes, and variations in the use of FHIR-based data models and structures across different studies.
Results
On the basis of the reviewed articles, we could identify 2 main themes: dynamic (pipeline-based) and static data models. The articles were also categorized into health care use cases, including chronic diseases, COVID-19 and infectious diseases, cancer research, acute or intensive care, random and general medical notes, and other conditions. Furthermore, we summarized the important or common tools and approaches of the selected papers. These items included FHIR-based tools and frameworks, machine learning approaches, and data storage and security. The most common resource was “Observation” followed by “Condition” and “Patient.” The limitations and challenges of developing data models were categorized based on the issues of data integration, interoperability, standardization, performance, and scalability or generalizability.
Conclusions
FHIR serves as a highly promising interoperability standard for developing real-world health care apps. The implementation of FHIR modeling for electronic health record data facilitates the integration, transmission, and analysis of data while also advancing translational research and phenotyping. Generally, FHIR-based exports of local data repositories improve data interoperability for systems and data warehouses across different settings. However, ongoing efforts to address existing limitations and challenges are essential for the successful implementation and integration of FHIR data models.}
}
@incollection{TORDAY202317,
title = {Chapter 3 - The unicell as the crucible for consciousness as Quantum Entanglement},
editor = {John S. Torday},
booktitle = {Quantum Mechanics, Cell-Cell Signaling, and Evolution},
publisher = {Academic Press},
pages = {17-26},
year = {2023},
isbn = {978-0-323-91297-6},
doi = {https://doi.org/10.1016/B978-0-323-91297-6.00015-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323912976000151},
author = {John S. Torday},
keywords = {Quantum Mechanics, Quantum Entanglement, Homeostasis, Big Bang, Newton’s Third Law of Motion, Quantum Biology},
abstract = {Quantum Mechanics is the basis for all things Cosmos. The unicell is the “crucible” for determining how to incorporate such epigenetically acquired factors, hypothetically based on Quantum Entanglements. The ultimate cellular determinant for the assimilation of such Quantum factors is homeostasis, the residual of the “equal and opposite reaction” to the Big Bang, which is Newton’s Third Law of Motion. This ontological/epistemological perspective is a frame for the burgeoning field of Quantum Biology.}
}
@article{QIU2022105938,
title = {TALE-cmap: Protein function prediction based on a TALE-based architecture and the structure information from contact map},
journal = {Computers in Biology and Medicine},
volume = {149},
pages = {105938},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105938},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522006734},
author = {Xiao-Yao Qiu and Hao Wu and Jiangyi Shao},
keywords = {Protein function prediction, Gene ontology, Protein contact map},
abstract = {Protein function prediction is one of the most critical tasks in bioinformatics. The computational predictors that can accurately predict the protein functions from their sequences are highly desired. With the development of the protein structure prediction methods, it is interesting to explore a new approach to use the predicted protein structures to improve the predictive performance of protein function prediction. TALE is a successful sequence-based method for protein function prediction. Therefore, in this study, we employed the TALE-based architecture to integrate sequence embeddings, contact map embeddings, and GO label embeddings to predict protein functions. These embeddings represent the proteins at the sequence, structure, and function levels. The TALE-cmap predictor outperforms the other state-of-the-art methods, indicating that structural information is essential for protein function prediction.}
}
@article{BABICHEV2025108115,
title = {Evaluating proximity metrics for gene expression data: A hybrid model integrating data mining and machine learning techniques for disease diagnosis systems},
journal = {Biomedical Signal Processing and Control},
volume = {110},
pages = {108115},
year = {2025},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2025.108115},
url = {https://www.sciencedirect.com/science/article/pii/S1746809425006263},
author = {Sergii Babichev and Oleg Yarema and Aleksandr Savchenko},
keywords = {Gene expression data, Proximity metrics, Hybrid model, Clustering, Classification, Personalized medicine},
abstract = {This study presents the development and application of a hybrid model for evaluating proximity metrics in high-dimensional gene expression data, integrating data mining and machine learning methods within a comprehensive framework. The research focuses on the comparative analysis of correlation distance, mutual information-based and Wasserstein metrics, assessing their effectiveness for clustering and classification tasks. In the initial modeling stage using gene expression data from over 6,000 patient samples covering 13 cancer types (TCGA dataset), the proposed model achieved classification accuracy exceeding 95.9% and a weighted F1-score above 95.8%. External validation using Alzheimer’s (GSE174367) and Type 2 Diabetes (GSE81608) datasets confirmed the model’s generalizability, with accuracy values reaching 96.28% and 97.43%, and weighted F1-scores of 96.26% and 97.41%, respectively. A stacking model was implemented to enhance classification robustness, compensating for potential clustering errors and delivering consistent performance across varying metrics and cluster structures. The proposed data processing pipeline ensures automated, standardized, and scalable analysis of large-scale gene expression datasets, aligning with the principles of personalized medicine.}
}
@article{PLAPPERT2019163,
title = {Not hedging but implying: Identifying epistemic implicature through a corpus-driven approach to scientific discourse},
journal = {Journal of Pragmatics},
volume = {139},
pages = {163-174},
year = {2019},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2018.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0378216616300674},
author = {Garry Plappert},
keywords = {Conventional implicature, Causal implicature, Relevance theory, Semantic incompleteness, Hedging},
abstract = {Whilst theorizing and analyzing implicature has become very popular, work in this area has tended to focus in the area of conversational implicature. Meanwhile, the analysis of epistemology in applied linguistics has been dominated by the study of hedging, a model which posits that it is the addition of explicit linguistic devices that chiefly alters the strength of claims. In this paper I aim to demonstrate the importance of the more implicit perspective of implicature to the analysis of epistemic signaling in academic writing, through an empirical approach. Using a corpus of scientific writing from the field of genetics, a detailed analysis of propositions containing the string mutations in the gene encoding is described and I argue that the results of this analysis are more consistent with the theory of implicature rather than that of hedging as the model of the expression and nuancing of claims in genetics. Finally, I argue that the ontologically-driven approach taken in this study, being similar to that taken in disciplines such as the philosophy or sociology of science holds great promise for interdisciplinary work between these fields and that of applied linguistics.}
}
@article{ALTURAYEIF2025125525,
title = {EASE: An enhanced active learning framework for aspect-based sentiment analysis based on sample diversity and data augmentation},
journal = {Expert Systems with Applications},
volume = {261},
pages = {125525},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125525},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424023923},
author = {Nouf Alturayeif and Irfan Ahmad},
keywords = {Aspect-based sentiment analysis, Active learning, Deep learning},
abstract = {Aspect-Based Sentiment Analysis (ABSA) has received considerable attention in recent studies. Powerful pre-trained models were proposed which can be fine-tuned for many Natural Language Processing (NLP) tasks, including ABSA. However, fine-tuning these models needs a relatively large amount of labeled data. In this research, we propose EASE; an active learning framework to minimize manual labeling effort. We extend the active learning technique by incorporating the concept of sample diversity where similar samples are not selected for labeling. Furthermore, we maximize the utility of these samples by incorporating data augmentation. EASE was evaluated on three benchmark ABSA datasets from three different domains. The results show that the reduction of the number of needed labeled samples ranges from 88% to 94% among the three datasets while maintaining accuracy. Our results show that active learning is an effective approach to reduce manual labeling effort while maintaining comparable performance. Moreover, it is possible to reduce the number of labeled data even further by incorporating sample diversity and data augmentation while maintaining performance.}
}
@article{LI2022102298,
title = {How has airport service quality changed in the context of COVID-19: A data-driven crowdsourcing approach based on sentiment analysis},
journal = {Journal of Air Transport Management},
volume = {105},
pages = {102298},
year = {2022},
issn = {0969-6997},
doi = {https://doi.org/10.1016/j.jairtraman.2022.102298},
url = {https://www.sciencedirect.com/science/article/pii/S096969972200117X},
author = {Lingyao Li and Yujie Mao and Yu Wang and Zihui Ma},
keywords = {Airport service quality (ASQ), Crowdsourcing, COVID-19, Google maps reviews, Sentiment analysis, ASQ driven factors},
abstract = {Airport service quality (ASQ) is a competitive advantage for airport management in today's airport market. Since the COVID-19 health crisis has unprecedentedly influenced airport regulations and operations, effective measurement of ASQ has become crucial for airport administrations. Surveying travelers' attitudes is useful for ASQ assessment but collecting responses could be time-consuming and costly. Therefore, this paper adopts a data-driven crowdsourcing approach to study ASQ during the COVID-19 pandemic by investigating Google Maps reviews from the 98 busiest U.S. airports. To do so, this study develops a topical ontology of keywords regarding ASQ attributes and uses a sentiment tool to derive passengers' attitudes. Through sentiment analysis, Google Maps reviews show more positive sentiment toward environment and personnel but remain constant about facilities during COVID-19. The lexical salience-valence analysis (LSVA) is then applied to explain such changes by tracking the sentiment of frequent words in reviews. Through correlation and regression analysis, this study demonstrates that rating is significantly related to check-in, environment, and personnel in pre-and post-COVID periods. Additionally, the effect of access, wayfinding, facilities, and environment on rating significantly differs between the two periods. The findings illustrate the effectiveness of leveraging online reviews and offer practical implications for what matters to air travelers, especially in the COVID-19 context.}
}
@article{HOSSEINI20183,
title = {Engineering transparency requirements: A modelling and analysis framework},
journal = {Information Systems},
volume = {74},
pages = {3-22},
year = {2018},
note = {Information Systems Engineering: selected papers from CAiSE 2016},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2017.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S0306437916305282},
author = {Mahmood Hosseini and Alimohammad Shahri and Keith Phalp and Raian Ali},
keywords = {Transparency requirements, Transparency management, Requirements engineering},
abstract = {Transparency is a requirement that denotes the communication of information that should help audience to take informed decisions. The existing research on transparency in information systems usually focuses on the party who provides transparency and its inter-relation with other requirements such as privacy, security and regulatory requirements. Engineering transparency, however, also requires the analysis of the information receivers’ situation and their transparency requirements and the medium used to communicate and present the information. A holistic consideration of transparency will enhance its management and increase its usefulness. In this paper, we provide a novel engineering framework, consisting of a modelling language and nine analytical reasonings, which is meant to represent transparency requirements and detect a set of possible side-effects. Examples of such detections include detecting information overload, information starvation, and transparency leading to biased decisions. We then evaluate the modelling language through a case study and report the results.}
}
@article{OSSBOLL2024104616,
title = {Graph neural networks for clinical risk prediction based on electronic health records: A survey},
journal = {Journal of Biomedical Informatics},
volume = {151},
pages = {104616},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104616},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000340},
author = {Heloísa {Oss Boll} and Ali Amirahmadi and Mirfarid Musavian Ghazani and Wagner Ourique de Morais and Edison Pignaton de Freitas and Amira Soliman and Farzaneh Etminani and Stefan Byttner and Mariana Recamonde-Mendoza},
keywords = {Graph neural networks, Electronic health records, Deep learning, Artificial intelligence, Graph representation learning, Keyword},
abstract = {Objective:
This study aims to comprehensively review the use of graph neural networks (GNNs) for clinical risk prediction based on electronic health records (EHRs). The primary goal is to provide an overview of the state-of-the-art of this subject, highlighting ongoing research efforts and identifying existing challenges in developing effective GNNs for improved prediction of clinical risks.
Methods:
A search was conducted in the Scopus, PubMed, ACM Digital Library, and Embase databases to identify relevant English-language papers that used GNNs for clinical risk prediction based on EHR data. The study includes original research papers published between January 2009 and May 2023.
Results:
Following the initial screening process, 50 articles were included in the data collection. A significant increase in publications from 2020 was observed, with most selected papers focusing on diagnosis prediction (n = 36). The study revealed that the graph attention network (GAT) (n = 19) was the most prevalent architecture, and MIMIC-III (n = 23) was the most common data resource.
Conclusion:
GNNs are relevant tools for predicting clinical risk by accounting for the relational aspects among medical events and entities and managing large volumes of EHR data. Future studies in this area may address challenges such as EHR data heterogeneity, multimodality, and model interpretability, aiming to develop more holistic GNN models that can produce more accurate predictions, be effectively implemented in clinical settings, and ultimately improve patient care.}
}
@article{FRADI2023101865,
title = {Category theory-based collaborative design methodology for mechatronic systems},
journal = {Advanced Engineering Informatics},
volume = {55},
pages = {101865},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101865},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622003238},
author = {Mouna Fradi and Faïda Mhenni and Raoudha Gaha and Abdelfattah Mlika and Jean-Yves Choley},
keywords = {Collaborative design, Mechatronic systems, Knowledge management, Multi-disciplinary design, Crucial knowledge identification, Conflict resolution process, Category theory},
abstract = {Recent advances in companies are characterized by highly dynamic, knowledge-intensive and collaborative process. This has become primary concern for mechatronic systems since they involve multiple disciplines and knowledge. This requires a close exchange in order to share knowledge between the different design teams. The first step in knowledge sharing is to identify the most important knowledge that need to be capitalized, which we call “crucial knowledge”. During this exchange, heterogeneous knowledge and modelling languages are involved in the design process, which can lead to conflicts. Hence, the challenge is to continuously capture and handle such conflicts between expert models. Thus, the focus of this paper is to propose a new collaborative design model suitable for mechatronic concurrent design. Our contribution lies in identifying crucial knowledge and resolving conflicts in a formal way in order to ensure efficient collaboration. Our methodology called Category Theory-based Collaborative Design (CaTCoD) is described with its associated meta-model. A demonstrator is also used to validate the proposed methodology using an example from the aeronautic field.}
}
@article{RIESENER2024573,
title = {A Systematic Literature Review for the Derivation of a Conceptual Design Framework for Platforms},
journal = {Procedia CIRP},
volume = {128},
pages = {573-578},
year = {2024},
note = {34th CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.06.031},
url = {https://www.sciencedirect.com/science/article/pii/S221282712400708X},
author = {M. Riesener and M. Kuhn and S. Schümmelfeder and G. Schuh},
keywords = {Business Ecosystem, Business Model, Business Strategy, Platform, Governance, Strategy, Architecture, Systematization},
abstract = {Business ecosystems and platforms are an omnipresent phenomenon in the current economic environment. In the course of this, market and competitive structures are undergoing a profound reinterpretation. While traditional industry structures rely on value creation within the boundaries of a company, platforms utilize an ecosystem of autonomous agents to create value. Thus, the platform and different stakeholders interact in order to incubate and coordinate an ecosystem. Shared, flexible value creation instead of a monolithic, rigid value chain, arranged to become the industry-defining premise. Platforms are the main reference point of value creation and delivery in many currently successful business models. Within different research contributions, the study of their characteristics diverge significantly. Within the framework of a design science research approach and through a systematic literature review combined with the realization of a taxonomic analysis of the term “platform” this contribution develops a systematization of the research area. This contribution develops a morphology of the conceptual design components “architecture, strategy and governance” and derives connected dimensions and characteristics in order to provide an ontology of design for companies and a unified understanding for further research initiatives. In addition, further necessary research approaches are identified as limitations.}
}
@article{KORKUT2023102247,
title = {Development of drama into teacher identity: Exchanges and experiences of English teachers},
journal = {International Journal of Educational Research},
volume = {122},
pages = {102247},
year = {2023},
issn = {0883-0355},
doi = {https://doi.org/10.1016/j.ijer.2023.102247},
url = {https://www.sciencedirect.com/science/article/pii/S0883035523001118},
author = {Perihan Korkut and Kemal Sinan Özmen},
keywords = {Creative drama, Teacher identity, Dynamic systems model of role identity, In-service teacher training, English language teaching},
abstract = {The importance of professional identity development in learning to teach has been highlighted in recent years. The present study examined the process of how English teachers develop a drama teacher role identity upon receiving in-service training on educational drama. The influence of the training was observed by means of interviews, observations, and reflective journals. The analysis, guided by Kaplan and Garner's (2017) Dynamic Systems Model of Role Identity (DSMRI), produced important hints on the conditions for the formation of a drama teacher identity. The findings indicated that the training program broadened the teachers’ perceived action possibilities and that a teacher's identification with drama depended on their institutional context, alignment of action possibilities, professional belief system, and self-perception.}
}
@article{ROBLESPINEROS2020101337,
title = {Intercultural science education as a trading zone between traditional and academic knowledge},
journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
volume = {84},
pages = {101337},
year = {2020},
issn = {1369-8486},
doi = {https://doi.org/10.1016/j.shpsc.2020.101337},
url = {https://www.sciencedirect.com/science/article/pii/S1369848620301485},
author = {Jairo Robles-Piñeros and David Ludwig and Geilsa Costa Santos Baptista and Adela Molina-Andrade},
abstract = {Intercultural science education requires negotiations between knowledge systems and of tensions between them. Building on ethnographic fieldwork and educational interventions in two farming communities in the Northeast of Brazil, we explore the potential of science education to mediate between traditional and academic knowledge. While traditional knowledge shapes agricultural practices and interactions with the environment in the villages of Coração de Maria and Retiro, academic knowledge is emphasized in biology education. On the basis of philosophical debates about “partial overlaps” between epistemologies, ontologies and value systems, we analyze relations between traditional and academic ecological knowledge in these communities and argue that they can inform reflective practices in intercultural dialogue. By investigating biology education as a “trading zone” between knowledge systems, we analyze how partial overlaps become negotiated in educational practices in rural Brazil and provide the basis for educational interventions that foster intercultural dialogue.}
}
@article{WEI2025100858,
title = {AI-Powered Problem- and Case-based Learning in Medical and Dental Education: A Systematic Review and Meta-analysis},
journal = {International Dental Journal},
volume = {75},
number = {4},
pages = {100858},
year = {2025},
issn = {0020-6539},
doi = {https://doi.org/10.1016/j.identj.2025.100858},
url = {https://www.sciencedirect.com/science/article/pii/S0020653925001479},
author = {Hongxia Wei and Yuguo Dai and Kaiting Yuan and Kar Yan Li and Kuo Feng Hung and Elaine Mingxin Hu and Angeline Hui Cheng Lee and Jeffrey Wen Wei Chang and Chengfei Zhang and Xin Li},
keywords = {Artificial intelligence, Intelligent tutoring system, Problem-based learning, Case-based learning, Meta-analysis},
abstract = {Introduction and Aims
Advances in artificial intelligence (AI) technology have generated a revolution in medical and dental education, which may offer promising solutions to tackle the challenges of traditional problem-based learning (PBL) and case-based learning (CBL). The objective of this study was to assess the available evidence concerning AI-powered PBL/CBL on students’ knowledge acquisition, clinical reasoning capability and satisfaction.
Methods
An electronic search was carried out on PubMed, MEDLINE, the Cochrane Central Register of Controlled Trials and Web of Science. Clinical trials published in English with full text available, which implemented AI technologies in PBL/CBL in the medical/dental field and evaluated knowledge acquisition, clinical reasoning and/or satisfaction were included. The quality assessment was conducted using RoB 2 by two calibrated assessors. Data synthesis and meta-analysis were performed, the standardised mean difference (SMD) or standardised mean (SM) and 95% confidence intervals (CIs) were calculated, and heterogeneity was quantified.
Results
Six randomized controlled trials were included, with an overall risk of bias judged to have ‘some concerns’. For knowledge acquisition, 4 studies were included in the meta-analysis. A low heterogeneity (I² = 20%) was detected and a fixed-effect model was utilised. Compared with the control group, the AI intervention significantly improved knowledge acquisition by 46% (95% Cls [0.18-0.73], P = .001). For clinical reasoning capability, due to methodological and measurement heterogeneity among studies, statistical analysis was not feasible. Three studies were selected for the meta-analysis of students’ satisfaction. Heterogeneity was moderate (I² = 32%), and a generic inverse variance method was selected. The pooled SM score was 0.7 (95% Cls [0.47-0.92]), and the overall effect was statistically significant (P < .00001).
Conclusion
Despite limitations such as the limited number of included studies and the overall risk of bias concerns, AI-powered PBL/CBL has the potential to enhance students’ knowledge acquisition and learner satisfaction compared to traditional learning approaches.
Clinical Relevance
Not applicable.}
}
@article{QU20243583,
title = {A Review of Knowledge Graph in Traditional Chinese Medicine: Analysis, Construction, Application and Prospects},
journal = {Computers, Materials and Continua},
volume = {81},
number = {3},
pages = {3583-3616},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.055671},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824008294},
author = {Xiaolong Qu and Ziwei Tian and Jinman Cui and Ruowei Li and Dongmei Li and Xiaoping Zhang},
keywords = {Systematic review, traditional Chinese medicine, knowledge graph, deep learning, medical applications},
abstract = {As an advanced data science technology, the knowledge graph systematically integrates and displays the knowledge framework within the field of traditional Chinese medicine (TCM). This not only contributes to a deeper comprehension of traditional Chinese medical theories but also provides robust support for the intelligent decision systems and medical applications of TCM. Against this backdrop, this paper aims to systematically review the current status and development trends of TCM knowledge graphs, offering theoretical and technical foundations to facilitate the inheritance, innovation, and integrated development of TCM. Firstly, we introduce the relevant concepts and research status of TCM knowledge graphs. Secondly, we conduct an in-depth analysis of the challenges and trends faced by key technologies in TCM knowledge graph construction, such as knowledge representation, extraction, fusion, and reasoning, and classifies typical knowledge graphs in various subfields of TCM. Next, we comprehensively outline the current medical applications of TCM knowledge graphs in areas such as information retrieval, diagnosis, question answering, recommendation, and knowledge mining. Finally, the current research status and future directions of TCM knowledge graphs are concluded and discussed. We believe this paper contributes to a deeper understanding of the research dynamics in TCM knowledge graphs and provides essential references for scholars in related fields.}
}
@article{DWYER2021103882,
title = {Commentary: An impossible dream? Integrating dietary supplement label databases: needs, challenges, next steps},
journal = {Journal of Food Composition and Analysis},
volume = {102},
pages = {103882},
year = {2021},
issn = {0889-1575},
doi = {https://doi.org/10.1016/j.jfca.2021.103882},
url = {https://www.sciencedirect.com/science/article/pii/S088915752100082X},
author = {Johanna Dwyer and Leila Saldanha and Richard Bailen and Alessandra Durazzo and Cinzia {Le Donne} and Raffaela Piccinelli and Karen Andrews and Pamela Pehrsson and Pavel Gusev and Alicia Calvillo and Emily Connor and Jeanne Goshorn and Stefania Sette and Massimo Lucarini and Laura D’Addezio and Emanuela Camilli and Luisa Marletta and Aida Turrini},
keywords = {Dietary supplement label database, Dietary supplements composition databases, Ontology, Integration, Interoperability, Harmonization, Food supplements, Natural products},
abstract = {Integrating ingredients in databases of dietary supplements (DS) involves harmonizing terminology describing ingredients, expressing amounts in a common fashion, and using interoperable description systems to link ingredients to related drug, food and other DS databases. Harmonizing ingredient descriptions facilitates calculation of bioactives in DS and allows estimation of their impact on the dietary, nutritional and health status of individuals and populations. Achieving greater integration of DS databases is not an impossible dream, but many challenges exist within and between countries including variations in legislation governing their definition, scope, regulation and labeling. Next steps begin domestically and then proceed internationally. These include harmonizing definitions of constituents and units between terminologies for DS and food databases and employing technological advances in database architecture, computer software and hardware that facilitate database integration and interoperability. Thus, data in one application can be converted into a form that can be accessed by receiving applications.}
}
@article{FIELD2022104181,
title = {Infrastructure platform for privacy-preserving distributed machine learning development of computer-assisted theragnostics in cancer},
journal = {Journal of Biomedical Informatics},
volume = {134},
pages = {104181},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104181},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422001915},
author = {Matthew Field and David I. Thwaites and Martin Carolan and Geoff P. Delaney and Joerg Lehmann and Jonathan Sykes and Shalini Vinod and Lois Holloway},
keywords = {Data mining, Decision support systems, Distributed learning, Federated learning, Machine learning, Radiation oncology},
abstract = {Introduction
Emerging evidence suggests that data-driven support tools have found their way into clinical decision-making in a number of areas, including cancer care. Improving them and widening their scope of availability in various differing clinical scenarios, including for prognostic models derived from retrospective data, requires co-ordinated data sharing between clinical centres, secondary analyses of large multi-institutional clinical trial data, or distributed (federated) learning infrastructures. A systematic approach to utilizing routinely collected data across cancer care clinics remains a significant challenge due to privacy, administrative and political barriers.
Methods
An information technology infrastructure and web service software was developed and implemented which uses machine learning to construct clinical decision support systems in a privacy-preserving manner across datasets geographically distributed in different hospitals. The infrastructure was deployed in a network of Australian hospitals. A harmonized, international ontology-linked, set of lung cancer databases were built with the routine clinical and imaging data at each centre. The infrastructure was demonstrated with the development of logistic regression models to predict major cardiovascular events following radiation therapy.
Results
The infrastructure implemented forms the basis of the Australian computer-assisted theragnostics (AusCAT) network for radiation oncology data extraction, reporting and distributed learning. Four radiation oncology departments (across seven hospitals) in New South Wales (NSW) participated in this demonstration study. Infrastructure was deployed at each centre and used to develop a model predicting for cardiovascular admission within a year of receiving curative radiotherapy for non-small cell lung cancer. A total of 10,417 lung cancer patients were identified with 802 being eligible for the model. Twenty features were chosen for analysis from the clinical record and linked registries. After selection, 8 features were included and a logistic regression model achieved an area under the receiver operating characteristic (AUROC) curve of 0.70 and C-index of 0.65 on out-of-sample data.
Conclusion
The infrastructure developed was demonstrated to be usable in practice between clinical centres to harmonize routinely collected oncology data and develop models with federated learning. It provides a promising approach to enable further research studies in radiation oncology using real world clinical data.}
}
@article{MENG2023102086,
title = {CBR-RBR fusion based parametric rapid construction method of bridge BIM model},
journal = {Advanced Engineering Informatics},
volume = {57},
pages = {102086},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102086},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623002148},
author = {Wei Meng and Hao Zhang and Qingsong Ai and Tuyu Bao and Junwei Yan},
keywords = {BIM Parametric Modeling, Bridge Modeling, CBR, RBR, MHPO Algorithm},
abstract = {The development of BIM (Building Information Modeling) parametric modeling technology has improved the reusability and modeling efficiency of building information models. Aiming at the problems of too many parameters and long time in the rapid parametric construction of the bridge BIM model, we propose a novel parametric rapid construction method of bridge BIM model based on the integration of CBR (Case Based Reasoning) and RBR (Rule Based Reasoning), which is used to realize the rapid construction of bridge BIM model. Firstly, the modeling characteristics of CBR and RBR are analyzed, and a parametric modeling framework of CBR-RBR fusion is proposed. Secondly, a MHPO (Modified Hunter-Prey Optimization) algorithm is proposed to assign the weight of feature parameters to improve the accuracy of the prediction model. Then, modeling parameter reasoning rules are designed to realize the construction of parameter reasoning model. Finally, the prototype system is developed to verify the proposed method based on Revit and C#. Compared with manual modeling and traditional parametric modeling, the fusion method of CBR and RBR greatly shortens the modeling time and improves modeling efficiency and flexibility.}
}
@article{HAGOORT2018191,
title = {Prerequisites for an evolutionary stance on the neurobiology of language},
journal = {Current Opinion in Behavioral Sciences},
volume = {21},
pages = {191-194},
year = {2018},
note = {The Evolution of Language},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2018.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S2352154617302188},
author = {Peter Hagoort},
abstract = {The neurobiology of language has to specify the cognitive architecture of complex language functions such as speaking and comprehending language, and, in addition, how these functions are mapped onto the underlying anatomical and physiological building blocks of the brain (the neural architecture). Here it is argued that the constraints provided by the classical anatomical measures (cytoarchitectonics and myeloarchitectonics) are in our current understanding only very loose constraints for detailed specifications of cognitive functions, including language learning and language processing. However, measures of the computational features of brain tissue might provide stronger constraints. For understanding cognitive specialization, for the time being we thus have to put our cards on measures of functional instead of structural neuroanatomy. The implication for an evolutionary stance on the neurobiology of language is that in a cross-species comparative perspective one needs to identify the factors that gave rise to the properties of the canonical microcircuits in the neocortex, and to the large scale network organization that created the language-readiness of the human brain.}
}
@article{WANG2025301,
title = {A deep graph neural network-based link prediction model for proactive anomaly detection in discrete manufacturing workshop},
journal = {Journal of Manufacturing Systems},
volume = {79},
pages = {301-317},
year = {2025},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2025.01.022},
url = {https://www.sciencedirect.com/science/article/pii/S0278612525000287},
author = {Shengbo Wang and Yu Guo and Shaohua Huang and Ruixi Lai and Litong Zhang and Weiwei Qian},
keywords = {Anomaly detection, Link prediction, Multi-source data, Manufacturing knowledge graph, Local graph learning},
abstract = {Production anomaly has always been one of the main influencing factors that prevent discrete manufacturing workshops from maintaining stability and agility. Proactive anomaly detection can evaluate the production state and serves as a crucial foundation for preventive maintenance decision. Knowledge graph enables the use of multi-source manufacturing data as a data foundation for proactive anomaly detection. Although rich manufacturing data can comprehensively depict complex manufacturing process, constructing an accurate proactive anomaly detection model remains challenging because of insufficient analysis of the local and temporal features of the manufacturing process. This paper presents a link prediction model based on a deep graph neural network to solve the problem. Specifically, the manufacturing knowledge graph is constructed through OPC UA information model, Bert model and OWL semantic mapping model to organize multi-source heterogeneous data. The deep autoencoder model with local graph learning and the Seq2Seq model with attention mechanism are trained to analyze the neighboring relationship and the temporal correlation of the manufacturing elements, respectively. Finally, the link prediction model is designed by integrating both local and temporal features, with a restructured loss function to improve training effectiveness. Experiments suggest that the designed link prediction model has better prediction performance and is at least 25.6 % higher than the baseline models on the mean reciprocal rank.}
}
@incollection{GUZZI2019896,
title = {Functional Enrichment Analysis Methods},
editor = {Shoba Ranganathan and Michael Gribskov and Kenta Nakai and Christian Schönbach},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology},
publisher = {Academic Press},
address = {Oxford},
pages = {896-897},
year = {2019},
isbn = {978-0-12-811432-2},
doi = {https://doi.org/10.1016/B978-0-12-809633-8.20404-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128096338204044},
author = {Pietro H. Guzzi},
keywords = {Annotation, Functional enrichment methods, Gene ontology},
abstract = {Functional Enrichment Analysis means the analysis of large gene and protein lists obtained from experiments using knowledge contained into ontologies. The main goal of such analysis is the identification of processes related to the input lists. Many algorithms have been proposed in recent years that falls into some major classes according to their schemas.}
}
@article{CAMERON2022100015,
title = {The Digital Design Basis. Demonstrating a framework to reduce costs and improve quality in early‐phase design},
journal = {Digital Chemical Engineering},
volume = {2},
pages = {100015},
year = {2022},
issn = {2772-5081},
doi = {https://doi.org/10.1016/j.dche.2022.100015},
url = {https://www.sciencedirect.com/science/article/pii/S2772508122000060},
author = {David B. Cameron and Arnljot Skogvang and Mihaly Fekete and Henrik Martinsson and Morten Strand and Marcel Castro and Kirsten Helle and Anders Gjerver and Christian Mahesh Hansen and Kristin Høines Johnsen},
keywords = {Digital twins, Standards, ISO15926, OPC, Industry 4.0, ISO/IEC81346, Systems, Concept design, Review, Semantic technologies},
abstract = {As a joint initiative, Aker BP, Lundin-Norway, Aker Solutions, TechnipFMC, Aibel and Aize, together with Sirius (the oil and gas research centre at the University of Oslo) have developed and demonstrated a common digital model representation of the information in early-phase design bases for oil & gas field developemnts. The scope of the project was to develop a proof of concept for a Digital Design Basis that supports data-centric rather than document-based engineering. The project established a standards-based data model that holds data about both the design basis and functional requirements decided by an operator. This model that can be implemented in any relevant software tools in a concept study, to ensure that information shared between operators and EPC vendors, with their different software tools, have the same meaning and understanding. The model is based on a common digitalized language for communication along the field development supply chain.}
}
@article{GONZATTO2022758,
title = {User oppression in human-computer interaction: a dialectical-existential perspective},
journal = {Aslib Journal of Information Management},
volume = {74},
number = {5},
pages = {758-781},
year = {2022},
issn = {2050-3806},
doi = {https://doi.org/10.1108/AJIM-08-2021-0233},
url = {https://www.sciencedirect.com/science/article/pii/S2050380622000059},
author = {Rodrigo Freese Gonzatto and Frederick M.C. {van Amstel}},
keywords = {Users, Human-computer interaction, Oppression, Ontological design, Social justice informatics, Userism},
abstract = {Purpose
This research theorizes the condition of human beings reduced to being users (and only users) in human-computer interaction (HCI), a condition that favors them becoming objects or targets of commercial dark patterns, racialized profiling algorithms, generalized surveillance, gendered interfaces and heteromation.
Design/methodology/approach
The reconceptualization of the users’ condition is done by confronting HCI theories on users with a dialectical-existential perspective over human ontology. The research is presented as a conceptual paper that includes analyzing and revising those theories to develop a conceptual framework for the user oppression in HCI.
Findings
Most HCI theories contribute to the user oppression with explicit or implicit ontological statements that denies their becoming-more or the possibility of users developing their handiness to the full human potential. Put together, these statements constitute an ideology called userism.
Social implications
HCI needs to acknowledge its role in structuring oppression not just in sexism, racism, classism and ableism, but also the specific relation that pertains to HCI: userism. Similar to other fields, acknowledging oppression is the first step toward liberating from oppression.
Originality/value
The user is an existential condition for HCI theories, yet few theories can explain in depth how this condition affects human development. From the dialectical-existential perspective, the user condition can be dehumanizing. Computers may intensify existing oppressions through esthetic interactions but these interactions can be subverted for liberation.}
}
@article{ARYA2024201346,
title = {Identification of potential microRNAs involved in pathogenesis of venous thromboembolism (VTE): A meta analysis},
journal = {Human Gene},
volume = {42},
pages = {201346},
year = {2024},
issn = {2773-0441},
doi = {https://doi.org/10.1016/j.humgen.2024.201346},
url = {https://www.sciencedirect.com/science/article/pii/S2773044124000901},
author = {Sunanda Arya and Rashi Khare and Iti Garg and Swati Srivastava},
keywords = {MicroRNA, Venous thromboembolism (VTE), Pulmonary embolism (PE), Meta analysis},
abstract = {Objective
MicroRNAs (MiRNAs) are master regulators of gene expression and have been suggested as potential diagnostic and therapeutic biomarkers in variety of complex diseases. Venous thromboembolism is a major cause of morbidity and mortality worldwide. Several miRNA expression studies have been conducted to identify miRNAs linked to VTE prognosis. These studies reveal that various miRNAs are significantly up-regulated and down-regulated in VTE patients in comparison to healthy controls. Present meta-analysis deliberate findings of such studies to identify most potential miRNA targets associated with VTE.
Methodology
Comprehensive literature review on Pubmed was conducted. Present analysis assessed 20 research article out of a total of 383 articles screened, based on inclusion and exclusion criteria. The up-regulated and down-regulated miRNAs obtained from selected research articles were subjected to meta-analysis. The differentially expressed miRNAs so obtained and were used for further bioinformatic analysis, including gene ontology and pathway analysis of their target genes, to study their potential involvement in VTE pathogenesis.
Results
Twenty articles selected for meta-analysis included a total of 808 patients. These included 26 up-regulated miRNAs and 11 down-regulated miRNAs. The meta-analysis based on Odds ratio suggested that one up-regulated miRNA (hsa-miR-1233-3p) and two down-regulated miRNAs (hsa-miR-103a-3p and hsa-miR-200c) returned significantly higher Odds compared to other miRNAs. Further bioinformatics analysis of their target genes revealed that these miRNAs target a large number of genes involved in cell adhesion, cell migration, endothelial activation and inflammation genes.
Conclusions
Three miRNAs, viz.; hsa-miR-1233-3p, hsa-miR-103a-3p and hsa-miR-200c may play a crucial role in VTE pathogenesis and has potential to serve as reliable diagnostic or therapeutic biomarkers for VTE.}
}
@article{WANG2025105050,
title = {Advancing food safety behavior with AI: Innovations and opportunities in the food manufacturing sector},
journal = {Trends in Food Science & Technology},
volume = {161},
pages = {105050},
year = {2025},
issn = {0924-2244},
doi = {https://doi.org/10.1016/j.tifs.2025.105050},
url = {https://www.sciencedirect.com/science/article/pii/S0924224425001864},
author = {Ke Wang and Miranda Mirosa and Yakun Hou and Phil Bremer},
keywords = {Food safety behavior, Artificial intelligence, Monitoring, Food safety culture, Decision-making, Training, Communication},
abstract = {The importance of human behavior in ensuring regulatory compliance and food safety is becoming increasing well recognized by academics, regulators and people working across all aspects of the food industry. This article explores the transformative potential of artificial intelligence (AI) in advancing food safety behaviors due to its developing capabilities to enhance critical food safety features, namely: monitoring, evaluation, and action. AI-assisted monitoring systems (Section 2.1) could enable accurate and unobtrusive observation of food safety behaviors such as hand hygiene and personal protective equipment compliance. AI-assisted evaluation tools (Section 2.2) could leverage advanced data analytics and large language models to evaluate food safety culture indicators comprehensively, enabling actionable insights and proactive decision-making. Additionally, AI interventions (Section 2.3), such as real-time feedback, personalized training, and communication assistance, will facilitate behavioral improvements and effective communication. Together, these phases have the potential to create a continuous AI-assisted cycle of improvement, in which behavior is assessed, a company's food safety culture is measured, actions based on objective evaluations are undertaken, and their effectiveness is measured. Such an adaptive approach will ensure a dynamic and responsive system capable of driving enhancements in food safety behaviors. While anticipating the transformative power of AI technologies, food manufacturers must also prepare themselves to address potential challenges related to technical limitations, ethical concerns, and a reluctance to adopt. This article highlights innovative AI applications, identifies opportunities for cross-sectoral adoption, proposes challenges and future directions to improve food safety behaviors.}
}
@article{ZHOU2022103746,
title = {Integrating NLP and context-free grammar for complex rule interpretation towards automated compliance checking},
journal = {Computers in Industry},
volume = {142},
pages = {103746},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103746},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522001439},
author = {Yu-Cheng Zhou and Zhe Zheng and Jia-Rui Lin and Xin-Zheng Lu},
keywords = {Automated rule checking (ARC), Automated compliance checking (ACC), Rule interpretation, Context-free grammar, Syntax tree, Natural language processing (NLP), Building information modeling (BIM), Smart design},
abstract = {Automated rule checking (ARC) is expected to significantly promote the efficiency and compliance of design in the construction industry. The most vital and complex stage of ARC is interpreting the regulatory text into a computer-processable format. However, existing systems and studies of rule interpretation either require considerable time-consuming manual effort or are based on exhaustive enumerations of matching patterns with a limited scope of application. To address this problem, this research integrates natural language processing (NLP) and context-free grammar (CFG) to propose a novel generalized rule interpretation framework, which can parse regulatory text like a domain-specific language. First, a syntax tree structure with several semantic elements is proposed to represent the roles and relations of concepts in regulatory text. Second, a deep learning model with the transfer learning technique is utilized to label the semantic elements in a sentence. Finally, a set of CFGs is built to parse a labeled sentence into the language-independent tree structure, from which computable checking rules can be generated. Experimental results demonstrate that our method outperforms the state-of-the-art methods: it achieves 99.6% and 91.0% accuracies for parsing single- and multi-requirement sentences, respectively, where the multi-requirement sentences are more complex and common but difficult for existing methods to deal with. This research contributes a method and framework for rule interpretation with both a high level of automation and wide scope of application, which can create computable rules from various textual regulatory documents. This research also publishes the first regulation dataset for future exploration, validation, and benchmarking in the ARC area.}
}
@article{SI2021103726,
title = {Generalized and transferable patient language representation for phenotyping with limited data},
journal = {Journal of Biomedical Informatics},
volume = {116},
pages = {103726},
year = {2021},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2021.103726},
url = {https://www.sciencedirect.com/science/article/pii/S1532046421000551},
author = {Yuqi Si and Elmer V. Bernstam and Kirk Roberts},
keywords = {Language representation, Phenotype prediction, Patient representation, Deep learning},
abstract = {The paradigm of representation learning through transfer learning has the potential to greatly enhance clinical natural language processing. In this work, we propose a multi-task pre-training and fine-tuning approach for learning generalized and transferable patient representations from medical language. The model is first pre-trained with different but related high-prevalence phenotypes and further fine-tuned on downstream target tasks. Our main contribution focuses on the impact this technique can have on low-prevalence phenotypes, a challenging task due to the dearth of data. We validate the representation from pre-training, and fine-tune the multi-task pre-trained models on low-prevalence phenotypes including 38 circulatory diseases, 23 respiratory diseases, and 17 genitourinary diseases. We find multi-task pre-training increases learning efficiency and achieves consistently high performance across the majority of phenotypes. Most important, the multi-task pre-training is almost always either the best-performing model or performs tolerably close to the best-performing model, a property we refer to as robust. All these results lead us to conclude that this multi-task transfer learning architecture is a robust approach for developing generalized and transferable patient language representations for numerous phenotypes.}
}
@article{MENNES2020102896,
title = {A critical analysis and explication of word sense disambiguation as approached by natural language processing},
journal = {Lingua},
volume = {243},
pages = {102896},
year = {2020},
issn = {0024-3841},
doi = {https://doi.org/10.1016/j.lingua.2020.102896},
url = {https://www.sciencedirect.com/science/article/pii/S0024384120301042},
author = {Julie Mennes and Stephan {van der Waart van Gulik}},
keywords = {Natural language processing, Word sense disambiguation, Word sense induction, Machine learning, Sense repository},
abstract = {This paper presents a critical analysis and explication of the approach to the problem of word sense disambiguation by the field of natural language processing. The paper is motivated by the observation that, despite the approach being interesting and promising, the actual research has some problems regarding its conceptual frameworks and methods. The methodology behind the analysis and explication is philosophical, focusing on the identification of conceptual unclarities and methodological incoherencies as well as their (practical) implications. The methodology helps to pinpoint some structural confusion in the research with respect to a central concept and an important set of methods. First, the characterization of the problem of word sense disambiguation as a computational task allows for different interpretations. When a rich interpretation is adopted, the methods developed for performing the task have certain problematic limitations. Secondly, the applicability of methods for the inductive discrimination of word senses is overestimated. The acknowledgement of this confusion makes it possible to understand the sometimes erratic course of the research and underlines the need for one, commonly shared, more refined and integrated conceptual framework and methodology. This paper may be interpreted as a humble onset of some of the necessary groundwork for developing these.}
}
@article{BERGMANN2023102172,
title = {How to find similar companies using websites?},
journal = {World Patent Information},
volume = {73},
pages = {102172},
year = {2023},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2023.102172},
url = {https://www.sciencedirect.com/science/article/pii/S0172219023000029},
author = {Jan-Peter Bergmann and Miriam Amin and Yuri Campbell and Karl Trela},
keywords = {Natural language processing, Partner selection, Semantic web, Web mining, BERT},
abstract = {The selection of industry partners for Research and Development (R&D) is a challenging task for many organizations. Present methods for partner-selection, based on patents, publications or company databases, do often fail for highly specialized SMEs. Our approach aims at calculating the technological similarity for partner discovery. We apply methods from Natural Language Processing (NLP) on companies’ website texts. We show that the deep-learning language model BERT outperforms other methods at this task. Tested against expert-proven ground truth, it achieves an F1-score up to 0.90. Our results imply that website texts are useful for the purpose of estimating the similarity between companies. We see great potential in the scalability of the semantic analysis of company website texts.}
}
@article{MORALESGARZON2025127603,
title = {Service-oriented multi-platform for food computing: A mobile application for recipe adaptation to nutrition behaviours (AI2Cuisine)},
journal = {Expert Systems with Applications},
volume = {281},
pages = {127603},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127603},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425012254},
author = {Andrea Morales-Garzón and Paola Santos Peinado and Roberto Morcillo-Jimenez and Karel Gutiérrez-Batista and Maria J. Martin-Bautista},
keywords = {Food system, Recipe adaptation, Micro-services, Healthy diet, Natural language processing},
abstract = {Supporting users in their food choices for mindful eating has become one of the spotlights for investigating modern food systems. However, building integral food platforms for this purpose is challenging due to dealing with heterogeneous data sources of different scopes, such as recipe data, food data, and user and dietary specificities. This research paper presents a versatile multi-platform architecture based on micro-services for dealing with different food-related tasks. The contributions of this research are manifold: (1) Firstly, we propose an architecture that enables us to handle various food-related tasks while managing various food data sources, providing data standardisation, scalability, and security benefits; (2) We introduce a novel recipe adaptation algorithm based on intelligent search in external resources and intelligent adaptation of the recipe preparation; (3) We include AI2Cuisine, a mobile application for recipe adaptation to meet different requirements like preferences, health and sustainable goals; (4) Finally, we perform an analysis and discussion in term of the necessity and impact of such sort of applications on the population. To demonstrate the feasibility of our proposal, we have conducted an experimental evaluation, and the results have been validated for various end-users with different expertise.}
}
@incollection{GYRARD2022171,
title = {Chapter 9 - SAREF4EHAW-compliant knowledge discovery and reasoning for IoT-based preventive health and well-being: IoT-based preventive health and well-being knowledge discovery and reasoning},
editor = {Sanju Tiwari and Fernando {Ortiz Rodriguez} and M.A. Jabbar},
booktitle = {Semantic Models in IoT and eHealth Applications},
publisher = {Academic Press},
pages = {171-198},
year = {2022},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-323-91773-5},
doi = {https://doi.org/10.1016/B978-0-32-391773-5.00015-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323917735000157},
author = {Amelie Gyrard and Antonio Kung},
keywords = {Preventive health, Smart health, Internet of things, Artificial intelligence, Health ontology, Well-being, ETSI SmartM2M SAREF, Standard, Semantic reasoning, Semantic web technologies},
abstract = {Assisting people to stay independent at home and to decrease hospital costs but also social isolation for the (elderly) people is getting more attention. The integration of heterogeneous technologies and devices require an interoperable solution to describe devices and data exchanged for preventive health and well-being. Several devices are designed to monitor patients' vital signs: Apple HealthKit, smartwatches, etc. If the companies do not develop the full-stack compatible with (1) sensors, (2) the final application for consumers, (3) and other products; the designed product can fail to succeed. It illustrates the need for interoperability between devices, applications, and sensor data. We focus on semantic interoperability on data to infer meaningful information applied to preventive health and well-being. ETSI SmartM2M SAREF is an ontology that can be extended to any IoT vertical domains such as eHealth and Ageing Well (SAREF4EHAW) and wearables (SAREF4WEAR). SAREF can be used to achieve interoperability among IoT projects, architectures, etc., and describe data sent through communication protocols or once that data must be processed (e.g., on the cloud, gateways, devices). ETSI does not provide tools yet that support the SAREF ontology to avoid engineers developing from scratch. There is a need to design a SAREF-compliant sensor dictionary for health and well-being scenarios. The health dictionary is employed with a reasoner, to infer meaningful knowledge from health sensor data. The scenarios are accessible via online demonstrators.}
}
@article{MCINNES20222043,
title = {Discovering Content through Text Mining for a Synthetic Biology Knowledge System},
journal = {ACS Synthetic Biology},
volume = {11},
number = {6},
pages = {2043-2054},
year = {2022},
issn = {2161-5063},
doi = {https://doi.org/10.1021/acssynbio.1c00611},
url = {https://www.sciencedirect.com/science/article/pii/S2161506322001681},
author = {Bridget T. McInnes and J. Stephen Downie and Yikai Hao and Jacob Jett and Kevin Keating and Gaurav Nakum and Sudhanshu Ranjan and Nicholas E. Rodriguez and Jiawei Tang and Du Xiang and Eric M. Young and Mai H. Nguyen},
keywords = {synthetic biology text processing pipeline, natural language processing, named entity recognition, relation extraction, concept grounding, topic modeling},
abstract = {Scientific articles contain a wealth of information about experimental methods and results describing biological designs. Due to its unstructured nature and multiple sources of ambiguity and variability, extracting this information from text is a difficult task. In this paper, we describe the development of the synthetic biology knowledge system (SBKS) text processing pipeline. The pipeline uses natural language processing techniques to extract and correlate information from the literature for synthetic biology researchers. Specifically, we apply named entity recognition, relation extraction, concept grounding, and topic modeling to extract information from published literature to link articles to elements within our knowledge system. Our results show the efficacy of each of the components on synthetic biology literature and provide future directions for further advancement of the pipeline.
}
}
@incollection{KITCHIN2020321,
title = {Space and Spatiality},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {321-327},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10998-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955109989},
author = {Rob Kitchin},
keywords = {Absolute space, Cognitive space, Decolonial space, Masculinist space, Ontogenesis, Ontology, Paradoxical space, Production of space, Relative space, Space, Spatiality, TimeSpace, Virtual space},
abstract = {This article provides a brief genealogy of ontological thinking about the concept of space from a geographical perspective. In particular, it focuses on philosophical debates on the nature of space post-1950 tracing the evolution of spatial thought. It is divided into three sections: absolute conceptions of space (including implicitly absolute space, absolute space, and cognitive space), relative conceptions of space (including relative space; masculinist and paradoxical space; postcolonial and decolonial space; and metaphorical space, virtual space, and TimeSpace) and ontogenetic conceptions of space.}
}
@article{JUSSLIN2022100480,
title = {Embodied learning and teaching approaches in language education: A mixed studies review},
journal = {Educational Research Review},
volume = {37},
pages = {100480},
year = {2022},
issn = {1747-938X},
doi = {https://doi.org/10.1016/j.edurev.2022.100480},
url = {https://www.sciencedirect.com/science/article/pii/S1747938X22000495},
author = {Sofia Jusslin and Kaisa Korpinen and Niina Lilja and Rose Martin and Johanna Lehtinen-Schnabel and Eeva Anttila},
keywords = {Embodied learning, First language, Second language, Foreign language, Language education},
abstract = {The notion of embodied learning has gained ground in educational sciences over the last decade and has made its way to language education with researchers acknowledging language learning as an embodied process. This mixed studies review aggregates and reviews empirical research, published from 1990 to 2020, using embodied learning approaches in language education. The review focuses on embodied approaches in learning and teaching first, second, and foreign languages at various educational levels. It encompasses 41 empirical studies with a majority published between 2019 and 2020, suggesting that the research area is growing rapidly. The results show that the studies align with two strands: (1) embodied learning through orchestrating embodied language learning and teaching, and (2) embodied learning in naturally occurring language learning interactions. The review identifies various embodied learning activities and presents how they contribute to language learning and teaching in different ways. The review proposes an understanding of embodied language learning that holds potentials to engage learners holistically, while simultaneously promoting language learning skills and adding emotional and motivational benefits to language learning.}
}
@article{GAMBO2024e36729,
title = {Identifying and resolving conflict in mobile application features through contradictory feedback analysis},
journal = {Heliyon},
volume = {10},
number = {17},
pages = {e36729},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e36729},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024127600},
author = {Ishaya Gambo and Rhodes Massenon and Roseline Oluwaseun Ogundokun and Saurabh Agarwal and Wooguil Pak},
keywords = {Natural language processing, BERT, RoBERTa, Mobile app, Sentiment analysis, iOS app store, Google play store},
abstract = {As mobile applications proliferate and user feedback becomes abundant, the task of identifying and resolving conflicts among application features is crucial for delivering satisfactory user experiences. This research, motivated to align application development with user preferences, introduces a novel methodology that leverages advanced Natural Language Processing techniques. The paper showcases the use of sentiment analysis using RoBERTa, topic modeling with Non-negative matrix factorization (NMF), and semantic similarity measures from Sentence-BERT. These techniques enable the identification of contradictory sentiments, the discovery of latent topics representing application features, and the clustering of related feedback instances. The approach detects conflicts by analyzing sentiment distributions within semantically similar clusters, further enhanced by incorporating antonym detection and negation handling. It employs majority voting, weighted ranking based on rating scores, and frequency analysis of feature mentions to resolve conflicts, providing actionable insights for prioritizing requirements. Comprehensive evaluations on large-scale iOS App Store and Google Play Store datasets demonstrate the approach's effectiveness, outperforming baseline methods and existing techniques. The research improves mobile application development and user experiences by aligning features with user preferences and providing interpretable conflict resolution strategies, thereby introducing a novel approach to the field of mobile application development.}
}
@incollection{HAYS2025352,
title = {Measurement and Modeling of Health-Related Quality of Life},
editor = {Stella R. Quah},
booktitle = {International Encyclopedia of Public Health (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Oxford},
pages = {352-364},
year = {2025},
isbn = {978-0-323-97280-2},
doi = {https://doi.org/10.1016/B978-0-323-99967-0.00217-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323999670002179},
author = {Ron D. Hays and Bryce B. Reeve},
keywords = {Administration burden, Data collection mode, Health-related quality of life, Interpretation of scores, Item banking, Preference measures, Profile measures, Psychometric properties, Respondent, PROMIS®, Translations},
abstract = {Health-related quality of life (HRQOL) includes ability to function and perceptions of physical, mental, and social health. HRQOL data are most often collected using self-report methods of assessment (e.g., questionnaires or interviews). There are generic and targeted profile measures that yield multiple HRQOL scores and preference-based measures that summarize HRQOL in a single score. Considerations for HRQOL measures include the conceptual model, reliability, validity, interpretation of scores, respondent and administrative burden, alternative assessment modalities, and availability of language translations. HRQOL measures are used to inform decisions in research, population surveillance, and clinical practice.}
}
@article{LI2025103611,
title = {Multi-modal causal hypergraph reasoning for enhancing collaborative diagnosis of equipment composite failures},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103611},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103611},
url = {https://www.sciencedirect.com/science/article/pii/S147403462500504X},
author = {Fei Li and Xinyu Li and Sijie Wen and Jinsong Bao},
keywords = {Composite fault diagnosis, CoCT-prompted LLMs, Multi-modal causal knowledge hypergraph, Causal relation cascade graph attention network, Human-machine collaborative},
abstract = {The causal relationships between equipment fault tickets and component production numerical data reflect the equipment’s fault state. However, given the complexity and diversity of equipment failures, existing graph construction and graph embedding methods overlook the long causal chain semantics between failures, the multivariate relationships of composite faults formed by connections of more than two entities, and the rich multimodal information in their triplets. To address these issues, this paper proposes a method based on multi-modal causal knowledge hypergraph (MCKH) inference to enhance the collaborative diagnosis of equipment composite faults. First, a few-shot chain of causal thought (CoCT) strategy is used to extract multivariate hyperedge relationships and long causal chain knowledge from fault investigation forms, combining fault factors, fault images, and numerical data to create multimodal hyper-nodes and form the MCKH. Second, a low-rank multimodal fusion strategy is adopted to achieve dynamic semantic modeling between hyper-node modalities. The MCKH encoder-decoder architecture is designed, where the encoder is a causal relation cascade graph attention network (CRCGAT) that includes a hyperedge-dominated node-level attention and layer-level attention mechanism. The joint decoder, ConvE, is used for the triplet link prediction of composite faults. Third, a collaborative fault diagnosis strategy is introduced through LLM-based human–machine interaction, leveraging CRCGAT + ConvE for MCKH inference and long causal chain backtracking to improve composite fault diagnosis accuracy. Finally, the method is applied to bridge crane composite fault diagnosis, with ablation experiments validating its effectiveness and providing a new perspective for multimodal fault diagnosis and root cause analysis in equipment spot inspection.}
}
@article{SU2025112001,
title = {A two-stage model for unified sentence- and document-level biomedical event extraction},
journal = {Engineering Applications of Artificial Intelligence},
volume = {161},
pages = {112001},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.112001},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625020093},
author = {Fangfang Su and Yue Zhang and Pengfei Jiao and Zhidong Zhao and Bobo Li and Fei Li and Donghong Ji},
keywords = {Biomedical event extraction, Unified modeling, Sentence-level extraction, Cross-sentence events, Multi-task model},
abstract = {Biomedical event extraction, a cornerstone of information extraction, has increasingly attracted attention within the biomedical research community. Moreover, it is a highly complex task, which not only deals with many sub-tasks but also involves nested events. Currently, the research on biomedical event extraction, whether pipelined model or joint method, needs to be processed for each sub-task. The process of processing each sub-task one by one lead to the degradation of event extraction performance. In addition, most studies focus on extracting sentence-level events and ignore cross-sentence event information. To solve these problems, we simplify the process of event extraction, reduce the processing steps, and combine the two sub-tasks of relation extraction and argument combination as one sub-task. In addition, we consider document-level event extraction, which not only extracts cross-sentence events but also considers broader context information. Experimental results indicate that our novel approach outperforms prior studies. Additionally, the document-level event extraction model attains the top performance on the BioNLP’11 test data and achieves near-leading performance on the BioNLP’13 test data.}
}
@article{LIU2020110527,
title = {SemanticGO: a tool for gene functional similarity analysis in Arabidopsis thaliana and rice},
journal = {Plant Science},
volume = {297},
pages = {110527},
year = {2020},
issn = {0168-9452},
doi = {https://doi.org/10.1016/j.plantsci.2020.110527},
url = {https://www.sciencedirect.com/science/article/pii/S0168945220301321},
author = {Wei Liu},
keywords = {Functional similarity, Gene ontology (GO), Gene vector, Pathway vector, Vector arithmetic},
abstract = {Gene or pathway functional similarities are important information for researchers. However, these similarities are often described sparsely and qualitatively. The latent semantic analysis of Arabidopsis thaliana (Arabidopsis) Gene Ontology (GO) data produced a set of 200-dimension feature vectors for each gene. Pathways were represented by summing the vectors of the pathway member genes. Thus, the similarities between genes and pathways were assessed. Additionally, the gene feature vectors were correlated with external gene data, including gene expression and gene network connectivity, to elucidate the associated functions. The gene feature vectors were decoded, and their applications were demonstrated. A simple online tool, SemanticGO (http://bioinformatics.fafu.edu.cn/semanticGO/), is herein provided to enable researchers to explore the similarities between genes and pathways in both Arabidopsis and rice.}
}
@article{LI2025107108,
title = {Temporal multi-modal knowledge graph generation for link prediction},
journal = {Neural Networks},
volume = {185},
pages = {107108},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.107108},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024010372},
author = {Yuandi Li and Hui Ji and Fei Yu and Lechao Cheng and Nan Che},
keywords = {Multimodal knowledge graph, Temporal knowledge graphs, Knowledge graph generation, Link prediction},
abstract = {Temporal Multi-Modal Knowledge Graphs (TMMKGs) can be regarded as a synthesis of Temporal Knowledge Graphs (TKGs) and Multi-Modal Knowledge Graphs (MMKGs), combining the characteristics of both. TMMKGs can effectively model dynamic real-world phenomena, particularly in scenarios involving multiple heterogeneous information sources and time series characteristics, such as e-commerce websites, scene recording data, and intelligent transportation systems. We propose a Temporal Multi-Modal Knowledge Graph Generation (TMMKGG) method that can automatically construct TMMKGs, aiming to reduce construction costs. To support this, we construct a dynamic Visual-Audio-Language Multimodal (VALM) dataset, which is particularly suitable for extracting structured knowledge in response to temporal multimodal perception data. TMMKGG explores temporal dynamics and cross-modal integration, enabling multimodal data processing for dynamic knowledge graph generation and utilizing alignment strategies to enhance scene perception. To validate the effectiveness of TMMKGG, we compare it with state-of-the-art dynamic graph generation methods using the VALM dataset. Furthermore, TMMKG exhibits a significant disparity in the ratio of newly introduced entities to their associated newly introduced edges compared to TKGs. Based on this phenomenon, we introduce a Temporal Multi-Modal Link Prediction (TMMLP) method, which outperforms existing state-of-the-art techniques.}
}
@article{GYRARD2020100083,
title = {IAMHAPPY: Towards an IoT knowledge-based cross-domain well-being recommendation system for everyday happiness},
journal = {Smart Health},
volume = {15},
pages = {100083},
year = {2020},
issn = {2352-6483},
doi = {https://doi.org/10.1016/j.smhl.2019.100083},
url = {https://www.sciencedirect.com/science/article/pii/S2352648319300479},
author = {Amelie Gyrard and Amit Sheth},
keywords = {Internet of things (IoT), Recommender systems (RS), Affective science, Emotion, Happiness, Well-being, Wellness, Rule-based reasoning, Inference engine, Knowledge directory service, Semantic ontology interoperability, Ontology validation, Reusability, Semantic web of things (SWoT), Semantic web technologies, Reusable knowledge},
abstract = {Nowadays, healthy lifestyle, fitness, and diet habits have become central applications in our daily life. Positive psychology such as well-being and happiness is the ultimate dream of everyday people's feelings (even without being aware of it). Wearable devices are being increasingly employed to support well-being and fitness. Those devices produce physiological signals that are analyzed by machines to understand emotions and physical state. The Internet of Things (IoT) technology connects (wearable) devices to the Internet to easily access and process data, even using Web technologies (aka Web of Things). We design IAMHAPPY, an innovative IoT-based well-being recommendation system to encourage every day people's happiness. The system helps people deal with day-to-day discomforts (e.g., minor symptoms such as headache, fever) by using home remedies and related alternative medicines (e.g., naturopathy, aromatherapy), activities to reduce stress, etc. To achieve this system, we build a web-based knowledge repository for emotion with a focus on happiness and well-being. The knowledge repository helps analyze data produced by IoT devices to understand users' emotions and health. The semantics-based knowledge repository is integrated with a rule-based engine to suggest recommendations to achieve everyday people's happiness. The naturopathy application scenario supports the recommendation system.}
}
@article{ELKHOURY201958,
title = {An industrial evaluation of data access techniques for the interoperability of engineering software tools},
journal = {Journal of Industrial Information Integration},
volume = {15},
pages = {58-68},
year = {2019},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2019.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X18300359},
author = {Jad El-khoury and Andrii Berezovskyi and Mattias Nyberg},
keywords = {Linked data, OSLC, Tool integration, Tool interoperability, SQL To RDF, Information management},
abstract = {New industrial initiatives such as Industrie 4.0 rely on digital end-to-end engineering across the entire product lifecycle, which in turn depends on the ability of the supporting software tools to interoperate. A tool interoperability approach based on Linked Data, and the OASIS OSLC standard, has the potential to provide such integration, where each software tool can expose its information and services to other tools using the web as a common technology base. In this paper, we report on our negative findings when attempting to use existing ontology-based Linked Data access techniques to expose and manipulate the structured content managed in engineering tools. Such techniques typically target the Data Access Layer (DAL) of a tool to access and manipulate its content, with the assumption that sufficient information and control is available within this layer to automate the process. Based on a case study with the truck manufacturer Scania CV AB, our study finds that an engineering tool controls its artefacts using business logic that is not necessarily reflected at the data layer. This renders such ontology-based access techniques inadequate. Instead we propose an alternative Linked Data extraction architecture that can mitigate the identified shortcomings. While less automated compared to the existing solutions, the proposed architecture is realised as a standalone library that can still facilitate the extraction process.}
}
@article{TRASMUNDI2024101615,
title = {Dialogical cognition},
journal = {Language Sciences},
volume = {103},
pages = {101615},
year = {2024},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2024.101615},
url = {https://www.sciencedirect.com/science/article/pii/S0388000124000044},
author = {Sarah Bro Trasmundi and Sune Vork Steffensen},
keywords = {Per Linell, Dialogical cognition, Distributed cognition, Cognitive ethnography, Distributed language},
abstract = {In this article we review Per Linell's work within the last five decades that led to his dialogism framework, which he defines as a general epistemology of language, cognition and communication. We critically discuss how his contribution on the one hand, altered and qualified existent models within language, communication and cognitive science, because dialogism removed language and cognition from their abstract and mental seat in the brain, and embedded them instead in situational contexts and embodied interaction. In that sense, his dialogism successfully replaced monological assumptions about the mind, action and thinking with more contextual and temporally distributed ones. On the other hand, we also question why Linell has not pursued a more rigorous empirical program for studying human cognition, when he did establish a theoretical apparatus for approaching cognition from a dialogical starting point. In going through Linell's arguments over the past five decades we suggest that this absence of an empirical program is due to his humanistic roots which both have sensitised him to appreciating the contingencies and dynamics of human sense making and cognition, and have impeded him from buying into a necessary condition for pursuing a cognitive analysis, even if he conceptually and methodologically accepts a distributed view on cognition. The outcome of this discussion leads to an empirical-based cognitive analysis of a medical interaction. Altogether, the purpose of this article is to show how Linell's conceptual framework can be put to use in ways that make a dialogical cognitive science achievable.}
}
@article{WANG2025102933,
title = {A posthumanist approach to AI literacy},
journal = {Computers and Composition},
volume = {76},
pages = {102933},
year = {2025},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2025.102933},
url = {https://www.sciencedirect.com/science/article/pii/S8755461525000209},
author = {Zhaozhe Wang and Chaoran Wang},
keywords = {Posthumanism, GenAI, AI literacy, literacy practices, Agency, Multilingual writers},
abstract = {How can posthumanism help us reframe AI-mediated literacy practices? And what implications does such reframing have for cultivating AI literacy in language and literacy education? This article explores these two imperative questions through a case study analyzing two multilingual undergraduate students’ meaning-making and meaning-negotiation intra-actions with AI technologies in a writing classroom. The case study reveals a productive tension between these students’ experiments with posthumanist literacy and their entrenched humanistic assumptions. Ultimately, through the case study, the authors hope to demonstrate that reframing and re-engaging with AI literacy through a posthumanist lens may offer students and educators a relational approach to developing and cultivating AI literacy.}
}
@article{DEY2024101984,
title = {An integrated bioinformatics approach for unravelling the molecular insights into psoriasis pathology and therapeutics},
journal = {Gene Reports},
volume = {36},
pages = {101984},
year = {2024},
issn = {2452-0144},
doi = {https://doi.org/10.1016/j.genrep.2024.101984},
url = {https://www.sciencedirect.com/science/article/pii/S2452014424001079},
author = {Rahul Dey and Amitava Das},
keywords = {Psoriasis, RNA-Seq, Microarray, Differentially expressed genes, Hub genes, Drug-gene interactions},
abstract = {Psoriasis is a chronic relapsing inflammatory disease of the skin that affects almost 2–3 % population worldwide. The current treatment strategies include palliative therapeutics targeting the inflammatory pathways that do not completely cure the lesioned skin. The molecular cues in the hyper-proliferative and aberrantly differentiated keratinocytes of the psoriatic lesioned skin remain unknown. Through an integrative in-silico approach, we have analyzed human psoriatic skin samples from 3 RNA-Seq and 3 microarray datasets to identify 340 differentially expressed genes (DEGs). Further, these DEGs were analyzed using gene ontology enrichment, KEGG pathways, and protein-protein interaction networks for their role in disease pathology and the identification of hub genes. The expression of the hub genes was validated in a preclinical murine model of psoriasiform dermatitis. Finally, the ten hub genes were assessed for their drugability, which revealed 74 drugs targeting 7 hub genes (CCNA2, TOP2A, BIRC5, RRM2, CDK1, AURKA, and CCNB1) that can be repurposed for psoriasis treatment. This study provides an understanding of psoriasis pathophysiology and suggests key molecular biomarkers as therapeutic targets for effective mitigation of the disease.}
}
@article{KAISER2022387,
title = {Model-based automatic generation of digital twin models for the simulation of reconfigurable manufacturing systems for timber construction},
journal = {Procedia CIRP},
volume = {107},
pages = {387-392},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.04.063},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122002797},
author = {Benjamin Kaiser and Alexander Reichle and Alexander Verl},
keywords = {AutomationML, ROS, URDF, Software Defined Manufacturing, Reconfigurable Manufacturing System, Simulation, Timber Prefabrication, IntCDC},
abstract = {Reconfigurable manufacturing systems adapt to changing requirements and therefore offer a solution to the challenges associated with increasing product variety and shorter product life cycles. This is especially helpful in the prefabrication in timber construction, as components and requirements vary widely between projects. However, reconfigurations must be quick to keep downtimes short. The planning of reconfigurations is a complex problem where digital twins and simulation models of the manufacturing systems are necessary for simulation and validation. In our timber prefabrication use case, building components and requirements change from project to project, and the manufacturing system needs to reconfigure frequently. The building components and the configuration of the manufacturing system are designed simultaneously during a co-design process to ensure the integrity of the manufacturing process in a changing manufacturing environment. Hence, this may require many iterations of the planning process. Each iteration needs validation through a simulation. However, the manual generation of simulation models is time-consuming and error-prone and thus poses an obstacle for the digital planning phase. This paper addresses this issue and presents a model-based approach to generate simulation models for reconfigurable manufacturing systems. Our system and its components are modeled in automation markup language as this domain-specific language allows us to model not only kinematics and geometry but also behavior, skills, and interfaces to check the compatibility of components. We apply the method to our use case with the IntCDC wood prefabrication system. The proposed method builds a component graph for a configuration of the manufacturing system and generates the required packages for the simulation in Gazebo and ROS. To build simulation models automatically, we apply model-to-model and model-to-text transformation techniques. The proposed method is suitable for integration in the digital co-design workflow of IntCDC and allows fast iterations with continuous simulations of machine configurations throughout the planning process.}
}
@article{COWLEY20198,
title = {Evolution, lineages and human language},
journal = {Language Sciences},
volume = {71},
pages = {8-18},
year = {2019},
note = {Simplexity, agency and language},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2018.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0388000118300391},
author = {Stephen J. Cowley and Anton Markoš},
keywords = {Life, Metaphor of language, Biosemiotics, Distributed language, Cognitive linguistics, Enactivism, Simplexity},
abstract = {In life as in language, living beings act in ways that are multiply constrained as history works through them both directly and as mediated by what we identify as structures (e.g. genes or words). Emphasising direct effects, we replace the ‘language metaphor of life’ with the view that language extends the domain of the living. Just as a living proteome system manages without central control, so does language. Both life and language enable living beings to expand into –and create – new domains or Umwelten. Pursuing the parallel, we link emphasis on fitness with Berthoz's notion of simplexity and the distributed view of life/language/cognition. The semiosphere evolved, we suggest, as systems found novel ways of tapping into the bio-ecology's energetics. Accordingly, there are striking parallels between how regulatory genes influence body structures and how, in humans, community histories re-echo during conversation. In both cases, cross-talk prompts living systems to re-enact a lineage/community's music (or ‘worldviews’). While rejecting Berthoz's residual neuro-centrism, we find ‘simplexity’ to be a powerful heuristic. Instead of proposing a single explanatory principle (e.g. computation, autonomy), lineages and communities build on meaning by altering ways of coordinating/cooperating. In all cases, life and language co-operate to bring forth new possibilities.}
}
@article{HUANG2024109100,
title = {Knowledge graph based reasoning in medical image analysis: A scoping review},
journal = {Computers in Biology and Medicine},
volume = {182},
pages = {109100},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.109100},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524011855},
author = {Qinghua Huang and Guanghui Li},
keywords = {Medical diagnosis, Medical expert systems, Knowledge graph, Medical image analysis},
abstract = {Automated computer-aided diagnosis (CAD) is becoming more significant in the field of medicine due to advancements in computer hardware performance and the progress of artificial intelligence. The knowledge graph is a structure for visually representing knowledge facts. In the last decade, a large body of work based on knowledge graphs has effectively improved the organization and interpretability of large-scale complex knowledge. Introducing knowledge graph inference into CAD is a research direction with significant potential. In this review, we briefly review the basic principles and application methods of knowledge graphs firstly. Then, we systematically organize and analyze the research and application of knowledge graphs in medical imaging-assisted diagnosis. We also summarize the shortcomings of the current research, such as medical data barriers and deficiencies, low utilization of multimodal information, and weak interpretability. Finally, we propose future research directions with possibilities and potentials to address the shortcomings of current approaches.}
}
@article{RADIO2019752,
title = {A survey of time based approaches for linked data},
journal = {Library Hi Tech},
volume = {37},
number = {4},
pages = {752-763},
year = {2019},
issn = {0737-8831},
doi = {https://doi.org/10.1108/LHT-04-2019-0084},
url = {https://www.sciencedirect.com/science/article/pii/S0737883119000010},
author = {Erik Radio},
keywords = {Information retrieval, Technology, Metadata, Ontologies, Resource description framework, Bibliographic standards},
abstract = {Purpose
Linked data technologies promise different ways of querying and retrieving information that enable individuals to have search experiences that are broader and more coordinated than those common in current library technologies. It is vital that information technologies be able to incorporate temporal capabilities or reasoning to allow for the more nuanced interactions with resources, particularly as they change over time. The purpose of this paper is to assess methods currently in use that allow for temporal querying of resources serialized as linked data.
Design/methodology/approach
This paper examines philosophical models, experimental approaches and common standards to identify areas of alignment and divergence in their orientations toward serializing time and change as linked data. By framing approaches and standards within the context of philosophical theories, a clear preference for certain models of time emerge.
Findings
While there have been several approaches to serializing time as linked data, none have found their way into a full implementation by standards in common use. Further, approaches to the issue are largely rooted in one model of philosophical thought that is particularly oriented to computational approaches. As such there is a gap between methods and standards, and a large room for further investigation into temporal models that may be applicable for different contexts. A call for investigation into a model that can cascade in to different temporal approaches is provided.
Originality/value
While there are many papers concerning serializing time as linked data, none have tried to thoroughly align these to philosophical theories of time and further to standards currently in use.}
}
@article{KIM2021101263,
title = {Automated composition and execution of web-based simulation systems through knowledge designing and reasoning},
journal = {Advanced Engineering Informatics},
volume = {48},
pages = {101263},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101263},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621000185},
author = {Dohyun Kim and Dongsu Jeong and Yoonho Seo},
keywords = {System composition and execution, Web-based simulation, Abductive event calculus, Ontology design, Data transformation, Weapon system design},
abstract = {Modeling and simulation (M&S) techniques can be used to improve cooperation among participants by clarifying the various design aspects of a devised system. Whenever an M&S system with specific functions is required, redesigning it from scratch is time and resource intensive. Thus, an approach to reuse or reconfigure existing systems with functions similar to the required ones is a potential alternative to overcome such problems. This paper proposes a method to build composite systems providing specific M&S functions through the web. The method proposed herein offers an integrated process that includes web-based reuse of existing systems, analysis of the requirements of a composite system, derivation of the logical execution order of element systems, and interconnection of the input–output interfaces between linked constituent systems. This approach provides the required tools by dynamically reconfiguring the element systems at the requested locations. The experimental results demonstrate that the proposed procedure can provide automatic generation of the required M&S system (e.g., missile-interception simulation system) and deliver the system to the target location of the cooperative work through the web.}
}
@article{MO2024102625,
title = {Semantic models and knowledge graphs as manufacturing system reconfiguration enablers},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {86},
pages = {102625},
year = {2024},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102625},
url = {https://www.sciencedirect.com/science/article/pii/S073658452300100X},
author = {Fan Mo and Jack C. Chaplin and David Sanderson and Giovanna Martínez-Arellano and Svetan Ratchev},
keywords = {Semantic models, Knowledge graphs, Reconfigurable manufacturing systems, Capability matching},
abstract = {Reconfigurable Manufacturing System (RMS) provides a cost-effective approach for manufacturers to adapt to fluctuating market demands by reconfiguring assets through automated analysis of asset utilization and resource allocation. Achieving this automation necessitates a clear understanding, formalization, and documentation of asset capabilities and capacity utilization. This paper introduces a unified model employing semantic modeling to delineate the manufacturing sector’s capabilities, capacity, and reconfiguration potential. The model illustrates the integration of these three components to facilitate efficient system reconfiguration. Additionally, semantic modeling allows for the capture of historical experiences, thus enhancing long-term system reconfiguration through a knowledge graph. Two use cases are presented: capability matching and reconfiguration solution recommendation based on the proposed model. A thorough explication of the methodology and outcomes is provided, underscoring the advantages of this approach in terms of heightened efficiency, diminished costs, and augmented productivity.}
}
@article{SCHIFFER2025111738,
title = {A four-field quantum model of life, subjectivity, consciousness, and memory: integrating dual-brain psychology and biophoton quantum interactions},
journal = {Medical Hypotheses},
volume = {202},
pages = {111738},
year = {2025},
issn = {0306-9877},
doi = {https://doi.org/10.1016/j.mehy.2025.111738},
url = {https://www.sciencedirect.com/science/article/pii/S030698772500177X},
author = {Fredric Schiffer},
keywords = {Quantum Biology, Consciousness, Biophoton Emission, Transcranial Photobiomodulation, Dual-Brain Psychology, Quantum Coherence, Epigenetic Inheritance, Quantum Fields, Subjectivity},
abstract = {Contemporary theories of life, consciousness, and memory rely on untestable theories that do not address the physical mechanisms behind the dramatic state changes that they attempt to explain, such as the change from matter (brain) to mind. Here, we propose a testable, novel, Four-Field Quantum Model comprising fundamental Life, Subjective, Awareness, and Memory Fields. These fields interact with biological systems through coherent emissions, serving as the mechanistic interface coupling coherence patterns (quantum, classical electromagnetic, and biophotonic) to the fields. I suggest that the observed qualitative state changes are due to symmetry-breaking transformations such as, by analogy, the effects of the Higgs fields’ transformation of a massless particle to one with mass, or the photoelectric effect where a photon is essentially transformed into an electron, via its electric field. Mechanistically, the Life Field promotes biochemical processes to living states; the Subjective Field transforms structured neural information into non-conscious experiential agential content, ultimately non-conscious hemispheric selves; the Awareness Field converts these non-conscious states into conscious self experiences; and the Memory Field encodes and retrieves episodic and generational developmental memories. We suggest that structured hemispheric brain information interacts with the proposed quantum subjective field, involving symmetry-breaking processes that qualitatively transform the initial informational states into subjective states. Clinical evidence from unilateral transcranial photobiomodulation, informed by Dual-Brain Psychology, supports these proposed mechanisms, demonstrating therapeutic efficacy. This model remains speculative pending proposed experimental validation of the coherence effects. The mechanistic framework does provide testable predictions suitable for interdisciplinary research into life, consciousness, memory, and psychology.}
}
@article{BOGEL2022170,
title = {An interdisciplinary perspective on scaling in transitions: Connecting actors and space},
journal = {Environmental Innovation and Societal Transitions},
volume = {42},
pages = {170-183},
year = {2022},
issn = {2210-4224},
doi = {https://doi.org/10.1016/j.eist.2021.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S2210422421001350},
author = {Paula Maria Bögel and Karoline Augenstein and Meike Levin-Keitel and Paul Upham},
keywords = {Scaling, Actors, Spatial analysis, Socio-spatial, Urban transitions},
abstract = {The question of how sustainable innovations and how niche experimentation lead to systemic changes are a core motivation of sustainability transitions research. As an inherently interdisciplinary field, although this question is addressed from different academic perspectives, the dominant understanding of relevant scaling processes is grounded in concepts of growth, diffusion and expansion. This article contributes to the discussion of more nuanced understandings of scaling, acknowledging the value of ontological levels for analytic purposes, but also drawing on knowledge from socio-psychological and spatial perspectives. Alternative understandings of spatial and agency-related scaling approaches are discussed and compared. An integrative socio-spatial framework is developed, providing a mid-range framework capable of supporting analysis of transitions that connects different disciplinary perspectives within a level-based ontology. We use an illustrative case study and derive implications for how this can inform questions of scaling and particularly spatial upscaling of new ways of doing, thinking & organizing}
}
@article{RANJAN2018994,
title = {LFNN: Lion fuzzy neural network-based evolutionary model for text classification using context and sense based features},
journal = {Applied Soft Computing},
volume = {71},
pages = {994-1008},
year = {2018},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2018.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S1568494618304046},
author = {Nihar M. Ranjan and Rajesh S. Prasad},
keywords = {Text classification, Connectionist model, Incremental learning, Semantic word processing, Contextual word computation, Neural network, Fuzzy bounding},
abstract = {Text classification is one of the popular techniques of text mining that labels the documents based on a set of topics defined according to the requirements. Among various approaches used for text categorization, incremental learning techniques are important due to its widespread applications. This paper presents a connectionist classification approach using context-semantic features and LFNN-based incremental learning algorithm for the text classification. The proposed technique considers a dynamic database for the classification so that the classifier can learn the model dynamically. This incremental learning process adopts Back Propagation Lion (BPLion) Neural Network, where it includes fuzzy bounding and Lion Algorithm (LA), for the feasible selection of weights. The effectiveness of the proposed method is analyzed by comparing it with the existing techniques, I-BP, FI-BP, and I-BPLion regarding accuracy and error, in a comparative analysis. As a result of the comparison, classification accuracies of 81.49%, 83.41%, 88.76%, and 95%; and minimum error values of 8.11, 7.49, 3.02, and 4.92 are possible to attain in LFNN, for 20 Newsgroup, Reuter datasets, WebKB, and RCV1 respectively.}
}
@article{DECASTROLIPPI2024173393,
title = {Intake of imidacloprid in lethal and sublethal doses alters gene expression in Apis mellifera bees},
journal = {Science of The Total Environment},
volume = {940},
pages = {173393},
year = {2024},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2024.173393},
url = {https://www.sciencedirect.com/science/article/pii/S004896972403540X},
author = {Isabella Cristina {de Castro Lippi} and Jaine {da Luz Scheffer} and Yan Souza {de Lima} and Juliana Sartori Lunardi and Aline Astolfi and Samir Moura Kadri and Marcus Vinícius Niz Alvarez and Ricardo {de Oliveira Orsi}},
keywords = {Beekeeping, Gene expression, Insecticide-induced hormesis, Pesticide},
abstract = {Bees are important pollinators for ecosystems and agriculture; however, populations have suffered a decline that may be associated with several factors, including habitat loss, climate change, increased vulnerability to diseases and parasites and use of pesticides. The extensive use of neonicotinoids, including imidacloprid, as agricultural pesticides, leads to their persistence in the environment and accumulation in bees, pollen, nectar, and honey, thereby inducing deleterious effects. Forager honey bees face significant exposure to pesticide residues while searching for resources outside the hive, particularly systemic pesticides like imidacloprid. In this study, 360 Apis mellifera bees, twenty-one days old (supposed to be in the forager phase) previously marked were fed syrup (honey and water, 1:1 m/v) containing a lethal dose (0.081 μg/bee) or sublethal dose (0.00081 μg/bee) of imidacloprid. The syrup was provided in plastic troughs, with 250 μL added per trough onto each plastic Petri dish containing 5 bees (50 μL per bee). The bees were kept in the plastic Petri dishes inside an incubator, and after 1 and 4 h of ingestion, the bees were euthanised and stored in an ultra-freezer (−80 °C) for transcriptome analysis. Following the 1-h ingestion of imidacloprid, 1516 genes (73 from lethal dose; 1509 from sublethal dose) showed differential expression compared to the control, while after 4 h, 758 genes (733 from lethal dose; 25 from sublethal) exhibited differential expression compared to the control. All differentially expressed genes found in the brain tissue transcripts of forager bees were categorised based on gene ontology into functional groups encompassing biological processes, molecular functions, and cellular components. These analyses revealed that sublethal doses might be capable of altering more genes than lethal doses, potentially associated with a phenomenon known as insecticide-induced hormesis. Alterations in genes related to areas such as the immune system, nutritional metabolism, detoxification system, circadian rhythm, odour detection, foraging activity, and memory in bees were present after exposure to the pesticide. These findings underscore the detrimental effects of both lethal and sublethal doses of imidacloprid, thereby providing valuable insights for establishing public policies regarding the use of neonicotinoids, which are directly implicated in the compromised health of Apis mellifera bees.}
}
@incollection{CARDIER202093,
title = {Chapter 5 - A narrative modeling platform: Representing the comprehension of novelty in open-world systems},
editor = {William F. Lawless and Ranjeev Mittu and Donald A. Sofge},
booktitle = {Human-Machine Shared Contexts},
publisher = {Academic Press},
pages = {93-134},
year = {2020},
isbn = {978-0-12-820543-3},
doi = {https://doi.org/10.1016/B978-0-12-820543-3.00005-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128205433000055},
author = {Beth Cardier and John Shull and Alex Nielsen and Saikou Diallo and Niccolo Casas and Larry D. Sanford and Patrick Lundberg and Richard Ciavarra and H.T. Goranson},
keywords = {Cognitive modeling, Dynamic knowledge representation, Contextual influence, Narrative inference, Representing open-world systems, Virtual reality},
abstract = {Reasoning is dynamic. Human understanding evolves across changing and unexpected circumstances, whether escaping a war zone or watching a suspenseful movie. General and past knowledge is often insufficient to make sense of these unfolding situations; it must be adapted to account for unexpected inputs. This chapter describes an early-stage prototype of a modeling platform that tracks how humans adjust old information toward new contexts, revising their understanding of real-world situations. The process is modeled as streams of narrative—as an interplay of semantic networks that represent mental, social, and cultural webs of information. Its taxonomy is newly realized in a Unity 3D environment so that annotations can be spatially captured and anchored in inhabitable spaces. The result is a new method for modeling the emergence of unexpected entities and contexts, including shared contexts. This can endlessly connect artifacts of different media and sources in a unifying immersive space. A long-term plan for these models is to inform artificial intelligence that can reason about situations that unfold past the ontological boundaries of their general reference frameworks.}
}
@article{WANG2023e18956,
title = {Nomogram of uveal melanoma as prediction model of metastasis risk},
journal = {Heliyon},
volume = {9},
number = {8},
pages = {e18956},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e18956},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023061649},
author = {Yimin Wang and Minyue Xie and Feng Lin and Xiaonan Sheng and Xiaohuan Zhao and Xinyue Zhu and Yuwei Wang and Bing Lu and Jieqiong Chen and Ting Zhang and Xiaoling Wan and Wenjia Liu and Xiaodong Sun},
keywords = {Uveal melanoma, Nomogram},
abstract = {Background
Since the poor prognosis of uveal melanoma with distant metastasis, we intended to screen out possible biomarkers for uveal melanoma metastasis risk and establish a nomogram model for predicting the risk of uveal melanoma (UVM) metastasis.
Methods
Two datasets of UVM (GSE84976, GSE22138) were selected. Data was analyzed by R language, CTD database and GEPIA.
Results
The co-upregulated genes of two datasets, HTR2B, CHAC1, AHNAK2, and PTP4A3 were identified using a Venn diagram. These biomarkers are combined with clinical characteristics, and Lasso regression was conducted to filter the metastasis-related biomarkers. HTR2B, CHAC1, AHNAK2, PTP4A3, tumor thickness, and retinal detachment (RD) were selected to establish the nomogram.
Conclusion
Our study provides a comprehensive predictive model and personalized risk estimation tool for assessment of 3-year metastasis risk of UVM with a better accuracy.}
}
@article{SLATER2021104360,
title = {Towards similarity-based differential diagnostics for common diseases},
journal = {Computers in Biology and Medicine},
volume = {133},
pages = {104360},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104360},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521001542},
author = {Karin Slater and Andreas Karwath and John A. Williams and Sophie Russell and Silver Makepeace and Alexander Carberry and Robert Hoehndorf and Georgios V. Gkoutos},
keywords = {Semantic web, Ontology, Differential diagnosis, Mimic-iii, Semantic similarity},
abstract = {Ontology-based phenotype profiles have been utilised for the purpose of differential diagnosis of rare genetic diseases, and for decision support in specific disease domains. Particularly, semantic similarity facilitates diagnostic hypothesis generation through comparison with disease phenotype profiles. However, the approach has not been applied for differential diagnosis of common diseases, or generalised clinical diagnostics from uncurated text-derived phenotypes. In this work, we describe the development of an approach for deriving patient phenotype profiles from clinical narrative text, and apply this to text associated with MIMIC-III patient visits. We then explore the use of semantic similarity with those text-derived phenotypes to classify primary patient diagnosis, comparing the use of patient-patient similarity and patient-disease similarity using phenotype-disease profiles previously mined from literature. We also consider a combined approach, in which literature-derived phenotypes are extended with the content of text-derived phenotypes we mined from 500 patients. The results reveal a powerful approach, showing that in one setting, uncurated text phenotypes can be used for differential diagnosis of common diseases, making use of information both inside and outside the setting. While the methods themselves should be explored for further optimisation, they could be applied to a variety of clinical tasks, such as differential diagnosis, cohort discovery, document and text classification, and outcome prediction.}
}
@article{DYNOMANT2019,
title = {Word Embedding for the French Natural Language in Health Care: Comparative Study},
journal = {JMIR Medical Informatics},
volume = {7},
number = {3},
year = {2019},
issn = {2291-9694},
doi = {https://doi.org/10.2196/12310},
url = {https://www.sciencedirect.com/science/article/pii/S2291969419000796},
author = {Emeric Dynomant and Romain Lelong and Badisse Dahamna and Clément Massonnaud and Gaétan Kerdelhué and Julien Grosjean and Stéphane Canu and Stefan J Darmoni},
keywords = {natural language processing, data mining, data curation},
abstract = {Background
Word embedding technologies, a set of language modeling and feature learning techniques in natural language processing (NLP), are now used in a wide range of applications. However, no formal evaluation and comparison have been made on the ability of each of the 3 current most famous unsupervised implementations (Word2Vec, GloVe, and FastText) to keep track of the semantic similarities existing between words, when trained on the same dataset.
Objective
The aim of this study was to compare embedding methods trained on a corpus of French health-related documents produced in a professional context. The best method will then help us develop a new semantic annotator.
Methods
Unsupervised embedding models have been trained on 641,279 documents originating from the Rouen University Hospital. These data are not structured and cover a wide range of documents produced in a clinical setting (discharge summary, procedure reports, and prescriptions). In total, 4 rated evaluation tasks were defined (cosine similarity, odd one, analogy-based operations, and human formal evaluation) and applied on each model, as well as embedding visualization.
Results
Word2Vec had the highest score on 3 out of 4 rated tasks (analogy-based operations, odd one similarity, and human validation), particularly regarding the skip-gram architecture.
Conclusions
Although this implementation had the best rate for semantic properties conservation, each model has its own qualities and defects, such as the training time, which is very short for GloVe, or morphological similarity conservation observed with FastText. Models and test sets produced by this study will be the first to be publicly available through a graphical interface to help advance the French biomedical research.}
}
@article{BHARTI2020106693,
title = {Optimal resource selection framework for Internet-of-Things},
journal = {Computers & Electrical Engineering},
volume = {86},
pages = {106693},
year = {2020},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2020.106693},
url = {https://www.sciencedirect.com/science/article/pii/S0045790620305486},
author = {Monika Bharti and Rajesh Kumar and Sharad Saxena and Himanshu Jindal},
keywords = {Internet-of-Things, Discovery and selection, Optimization, Shared ontology, Fuzzy based rules},
abstract = {The fundamental requirement for communication and computation across distinct application areas on Internet-of-Things is the resource discovery that demands appropriate reasoning for the optimal selection. With exponential growth of resources and their produced huge amount of heterogeneous data, various activities with respect to foraging and sense-making loops face challenges due to interoperability. Hence, interoperability emerges as a major bottleneck for the requirement. Therefore, to eliminate the challenge, the paper has proposed an “Optimal Resource Selection Framework for Internet-of-Things” that deals with the interoperability and ease the resource discovery and selection. The framework facilitates formation of semantic knowledge base as Shared Virtual Composite Ontology for capturing dynamic IoT heterogeneous data. Moreover, it supports optimal resource selection through the proposed algorithms, namely, Resource discovery Algorithm and Improved Firefly Algorithm. Both algorithms target coordination and optimization with Shared Ontology, respectively. The feasibility of the framework is checked against data collected from Sutlej river, Ludhiana, Punjab, India. The proposed framework is evaluated using benchmark functions with respect to metrics such as mean, standard deviation, processing and execution time. The obtained results are compared with the existing Nature-Inspired algorithms to confirm the efficiency of the proposed framework.}
}
@article{TRAN2025106127,
title = {Visual Question Answering-based Referring Expression Segmentation for construction safety analysis},
journal = {Automation in Construction},
volume = {174},
pages = {106127},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106127},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525001670},
author = {Dai Quoc Tran and Armstrong Aboah and Yuntae Jeon and Minh-Truyen Do and Mohamed Abdel-Aty and Minsoo Park and Seunghee Park},
keywords = {Visual Question Answering, Referring Expression Segmentation, Construction safety analysis},
abstract = {Despite advancements in computer vision techniques like object detection and segmentation, a significant gap remains in leveraging these technologies for hazard recognition through natural language processing. To address this gap, this paper proposes VQA-RESCon, an approach that combines Visual Question Answering (VQA) and Referring Expression Segmentation (RES) to enhance construction safety analysis. By leveraging the visual grounding capabilities of RES, our method not only identifies potential hazards through VQA but also precisely localizes and highlights these hazards within the image. The method utilizes a large “scenario-questions” dataset comprising 200,000 images and 16 targeted questions to train a vision-and-language transformer model. In addition, post-processing techniques were employed using the ClipSeg and Segment Anything Model. The validation results indicate that both the VQA and RES models demonstrate notable reliability and precision. The VQA model achieves an F1 score surpassing 90%, while the segmentation models achieve a Mean Intersection over Union of 57%.}
}
@article{JULIANIRANZO2020139,
title = {The Fuzzy Logic Programming language FASILL: Design and implementation},
journal = {International Journal of Approximate Reasoning},
volume = {125},
pages = {139-168},
year = {2020},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2020.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X2030181X},
author = {Pascual Julián-Iranzo and Ginés Moreno and José Antonio Riaza},
keywords = {Fuzzy Logic Programming, Similarity relation, Operational semantics, High level implementation techniques},
abstract = {The FASILL programming language (acronym of “Fuzzy Aggregators and Similarity Into a Logic Language”) combines a weak unification algorithm, based on similarity relations, along with a rich repertoire of fuzzy connectives and aggregators, whose truth functions can be defined on a complete lattice. In this work, we want to provide a unified view of the fundamental concepts and ideas that inspired its construction, jointly with the implementation techniques that made it possible. We detail the implementation of the operational semantics and we describe the overall structure of the FASILL system. The FASILL language is well-suited for a wide range of applications and, specifically, it can help the development of knowledge based systems where to deal with uncertainty is important. After ten years designing and implementing fuzzy logic systems, this work culminates and agglutinates the experience acquired in our research group on the development of this kind of modern programming languages.}
}
@article{HUNTER2022102933,
title = {Can language learners hear their own errors? The identification of grammaticality in one’s own production},
journal = {System},
volume = {111},
pages = {102933},
year = {2022},
issn = {0346-251X},
doi = {https://doi.org/10.1016/j.system.2022.102933},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X22002159},
author = {James Hunter},
keywords = {Complexity, Accuracy, Fluency, Individual differences, Error analysis, CALL, Computer assisted language learning, Delayed corrective feedback},
abstract = {This exploratory study investigated whether learners can correctly identify the grammaticality of items drawn from corrective feedback (CF) on their own oral production or on that of their peers. It was hypothesized that participants would judge less well-established items more slowly, and conversely that entrenched items, whether target-like or not, would be judged more quickly. 20 learners at two proficiency levels judged audio recordings of themselves reformulating errors they had made in small-group conversations. Items had been categorized according to reformulation accuracy and fluency, and the analysis investigated whether judgment accuracy and speed mirrored these categories. Results indicate clear parallels in reformulation and judgment accuracy, but a weak relationship between fluency of production and recognition. The categorization of errors occurring in both production and recognition, perhaps representing “attempts” at meaning-making (Edge, 1989; Willis, 2003), is proposed as the focus of future pedagogical research investigation. To this end, a pedagogical application of the self-judgment methodology is described.}
}
@article{GAO2025,
title = {Semantic Annotation Model and Method Based on Internet Open Dataset},
journal = {International Journal of Intelligent Information Technologies},
volume = {21},
number = {1},
year = {2025},
issn = {1548-3657},
doi = {https://doi.org/10.4018/IJIIT.370966},
url = {https://www.sciencedirect.com/science/article/pii/S1548365724000074},
author = {Xin Gao and Yansong Wang and Fang Wang and Baoqun Zhang and Caie Hu and Jian Wang and Longfei Ma},
keywords = {Internet Open Dataset, Semantic Annotation Method, Semantic Annotation Model, Topic Coverage},
abstract = {ABSTRACT
Traditional semantic annotation faces the problem of dataset diversity. Different fields and scenarios need to be specially annotated, and annotation work usually requires a lot of manpower and time investment. To meet these challenges, this paper deeply studies the semantic annotation model and method based on internet open datasets, aiming to improve annotation efficiency and accuracy and promote data resource sharing and utilization. This paper selects Common Crawl dataset to provide sufficient training samples; methods such as removing stop words and deduplication are used to preprocess data to improve data quality; a keyword extraction model based on heuristic rules and text context is constructed. In terms of semantic annotation model, this paper constructs a model based on Bidirectional Long Short-Term Memory (BiLSTM), which can make full use of the part-of-speech information of the corpus context, capture the part-of-speech features of the corpus, and generate semantic tags through supervised learning.}
}
@article{MITCHELL20222523,
title = {pySBOL3: SBOL3 for Python Programmers},
journal = {ACS Synthetic Biology},
volume = {11},
number = {7},
pages = {2523-2526},
year = {2022},
issn = {2161-5063},
doi = {https://doi.org/10.1021/acssynbio.2c00249},
url = {https://www.sciencedirect.com/science/article/pii/S2161506322002480},
author = {Tom Mitchell and Jacob Beal and Bryan Bartley},
keywords = {synthetic biology, standards, ontology, design automation, Synthetic Biology Open Language, Python},
abstract = {The Synthetic Biology Open Language version 3 (SBOL3) provides a data model for representation of synthetic biology information across multiple scales and throughout the design-build-test-learn workflow. To support practical use of this data model, we have developed pySBOL3, a Python library that allows programmers to create and edit SBOL3 documents. Here we describe this library and key engineering decisions in its design. The resulting implementation is a compact and maintainable core that provides both a familiar, pythonic interface for manipulating SBOL3 objects as well as mechanisms for building additional extensions and representations on this base.
}
}
@article{PICOVALENCIA2019250,
title = {A systematic method for building Internet of Agents applications based on the Linked Open Data approach},
journal = {Future Generation Computer Systems},
volume = {94},
pages = {250-271},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.11.042},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17321234},
author = {Pablo Pico-Valencia and Juan A. Holgado-Terriza and Patricia Paderewski},
keywords = {Internet of Things, Linked Open Data, Semantic agent contract, Software agile methodology, Agent-based method},
abstract = {The Internet of Agents (IoA) is an emerging paradigm whose objective is to mitigate the deficiencies of devices of Internet of Things (IoT) in terms of reasoning and social capabilities, in order to improve proactivity, intelligence and interoperability. This paper presents the guidelines to develop IoA applications based on described semantic agents following the Linked Open Data (LOD) approach and the specifications of the IoA-OWL ontology—a specialized full ontology that formally defines the main aspects related to a novel approach such as IoA. These guidelines have been drawn up via a systematic method created from the best practices of Agile Software Development Methodologies. This method creates smart, autonomous, collaborative IoA applications based on novel Linked Open Agents (LOAs) that are driven by Linked-Agent Contracts (LACs) and Workflows for Agent Control (WACs). From a practical perspective, our method separates the modeling of components using two levels, microscopic (at agent level) and macroscopic (at agent society level) facilitated the planning, configuring and implementation of each agent in the IoA ecosystem. Moreover, the method facilitates the agent creation automation process, reducing the time required for its development and simplifying the design complexity. These achievements were demonstrated through the modeling of an Ambient Intelligence (AmI) scenario on an office composed by a set of collaborative agents in order to provide smart comfort.}
}
@article{ESPARZAPEIDRO2024112041,
title = {Modeling microservice architectures},
journal = {Journal of Systems and Software},
volume = {213},
pages = {112041},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112041},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224000840},
author = {Javier Esparza-Peidro and Francesc D. Muñoz-Escoí and José M. Bernabéu-Aubán},
keywords = {DSL, Microservice, Architecture, Modeling, Language, Hypergraph},
abstract = {Modern microservice architectures demand new features from traditional architecture description languages, many of them related to the complexity of the modeled systems. This paper first identifies common concerns found in microservice architectures. Then it presents the features required by a suitable architecture modeling language in order to face many of these concerns. Existent modeling languages get evaluated and a lightweight high-level platform-independent modeling language is proposed. The language is general enough for describing many interactive microservice architectures, bringing together most of the features found in a scattered way in previous contributions. The language is presented in an ordered way, first defining its syntax using MOF and describing informally its underlying concepts, and later proposing an alternative hypergraph-based mechanism for describing its semantics. Regarding this methodology, an architectural style gets defined using a hierarchical type hypergraph, which contains all the information about all valid software architectures in an intuitive and compact way. The feasibility of the language is then demonstrated by providing an experimental tool which translates models to different container orchestration systems. Finally, the language is evaluated against the identified features in the context of the TeaStore reference application.}
}
@article{BAWACK2020102179,
title = {The role of digital information use on student performance and collaboration in marginal universities},
journal = {International Journal of Information Management},
volume = {54},
pages = {102179},
year = {2020},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2020.102179},
url = {https://www.sciencedirect.com/science/article/pii/S0268401219317700},
author = {Ransome Epie Bawack and Jean Robert {Kala Kamdjoug}},
keywords = {Information seeking, Information need, Information source, Information use, Digital literacy},
abstract = {Using technology to facilitate learning in universities and other higher education institutions (HEIs) has become common practice due to its ability to reduce barriers related to time and space in traditional learning environments. However, current literature mostly focuses on the use of the technology and not on the use of the information it conveys. Also, very few studies focus on technology adoption in universities and HEIs in developing countries, especially those in Africa. Thus, we propose a model that explains the changing information behaviors of students in this digital age and the effect this has on their learning outcomes. We collected questionnaire data from 303 students and analyzed the data using structural equation modelling partial least squares (SEM-PLS). We found that our proposed model explains 60.2 % of student satisfaction, 24.2 % of academic performance, 24.1 % of information sharing, and 19.8 % of their information exchange behavior. This study confirms that the use of digital information and its antecedent factors have significant effects on the college experience of students. This has several implications for information systems research and practice, especially in the design and assessment of technology use in learning environments.}
}
@article{LAGOARDESEGOT20191,
title = {Sustainable finance. A critical realist perspective},
journal = {Research in International Business and Finance},
volume = {47},
pages = {1-9},
year = {2019},
issn = {0275-5319},
doi = {https://doi.org/10.1016/j.ribaf.2018.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0275531917306414},
author = {Thomas Lagoarde-Segot},
keywords = {Finance, Sustainability, Epistemology},
abstract = {This article seeks to respond to certain epistemological challenges which are posed to the traditional methods of inquiry used in finance by the emergence of sustainable finance. We first underline the qualitative changes that have been induced in the finance function by the imperatives of sustainable finance. Then, through the prism of critical realism, we analyse it to reveal a contradiction, which we attribute to a misconceived social ontology in academic finance. Finally, we outline methodological avenues which might allow us to overcome these difficulties and pave the way for a more realist and pluralist approach to finance research in the 21st century.}
}
@article{FOUGERES2021100025,
title = {Fuzzy engineering design semantics elaboration and application},
journal = {Soft Computing Letters},
volume = {3},
pages = {100025},
year = {2021},
issn = {2666-2221},
doi = {https://doi.org/10.1016/j.socl.2021.100025},
url = {https://www.sciencedirect.com/science/article/pii/S2666222121000149},
author = {Alain-Jérôme Fougères and Egon Ostrosi},
keywords = {Fuzzy collaborative design, Fuzzy collaborative system, Natural language processing, Fuzzy requirement engineering, Fuzzy engineering design platform},
abstract = {Product design activities are predicated on fuzzy modelling, given that verbalising and interpreting engineering requirements are inherently fuzzy processes. The aim of this paper is to present a method for fuzzy intelligent requirement engineering from natural language to Computer-Aided Design (CAD) models. The field exploring the dynamics of computational processes from fuzzy linguistic modelling to fuzzy design modelling is complex and remains under-explored. No existing research has been identified which focuses specifically on fuzzy requirements engineering from natural language to CAD modelling. This paper seeks to address this by providing a design formalisation system based on five key principles. These principles are used to set out a computing procedure which follows a method broken up into six phases. The results of these six phases are fuzzy semantic graphs, which provide engineering requirements according to reliable design information. The approach is put into practice using the fuzzy agent-based tool developed by the authors, called F-EGEON (Fuzzy Engineering desiGn sEmantics elabOration and applicatioN). The proposed method is illustrated through an application from the automotive industry.}
}
@article{GATTI2023107173,
title = {A hybrid approach for artwork recommendation},
journal = {Engineering Applications of Artificial Intelligence},
volume = {126},
pages = {107173},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.107173},
url = {https://www.sciencedirect.com/science/article/pii/S095219762301357X},
author = {Ignacio Gatti and J. Andres Diaz-Pace and Silvia Schiaffino},
keywords = {Recommender systems, Artwork, Deep autoencoder, Ontology},
abstract = {Museums usually exhibit thousands of artworks, and nowadays, they often have their collections online for visitors. In these collections, the curators are responsible for organizing the artworks seeking a delicate balance between emotion and reason. Given an initial artwork, however, a visitor is likely to select and admire a set of related artworks that match her interests. This setting can be seen as a recommendation problem in the art domain. Although image recommendation systems have been previously developed, considering the artwork nature is a fundamental aspect when designing a recommender system in this domain. Thus, we propose a hybrid recommendation approach that combines deep autoencoders with a social influence graph in order to capture the visual aspects and context of artworks (represented by images). These mechanisms inform the generation of rankings of related artworks. In this context, we report on a case-study with a group of art experts who assessed the rankings of artworks recommended by our approach. Although preliminary, the results showed a better precision than traditional strategies based solely on image features or metadata. Furthermore, the recommendations exhibited diversity properties, avoiding typical over-specialization problems of content-based techniques.}
}
@article{KIM2025110290,
title = {Modulating vascular smooth muscle cell phenotype via Wnt-Independent FRZB pathways},
journal = {Archives of Biochemistry and Biophysics},
volume = {764},
pages = {110290},
year = {2025},
issn = {0003-9861},
doi = {https://doi.org/10.1016/j.abb.2025.110290},
url = {https://www.sciencedirect.com/science/article/pii/S0003986125000037},
author = {Hyomin Kim and Eun Kyoung Kim and Yeuni Yu and Hye Jin Heo and Dokyoung Kim and Su-Yeon Cho and Yujin Kwon and Won Kyu Kim and Kihun Kim and Dai Sik Ko and Yun Hak Kim},
keywords = {Frizzled-related protein, Vascular smooth muscle cell, Atherosclerosis, Phenotype modulation, Focal adhesion},
abstract = {Background and aims
Vascular smooth muscle cells are pivotal in atherosclerosis, transitioning from a contractile to a synthetic phenotype, which is associated with increased proliferation and inflammation. FRZB, a Wnt signaling modulator, has been implicated in vascular pathology, but its specific role in vascular smooth muscle cell phenotype modulation is not well understood. This study investigates the role of FRZB in regulating vascular smooth muscle cell phenotypes.
Methods
Vascular smooth muscle cell regions were categorized based on FRZB expression levels, and various analyses, including differential gene expression, KEGG pathway analysis, and Disease Ontology analysis, were conducted. FRZB knockdown in human aortic vascular smooth muscle cell was performed using siRNA, followed by assessments of cell migration, proliferation, and phenotype marker expression.
Results
FRZB expression was significantly reduced in synthetic type compared to contractile type in both mouse models and human samples. FRZB knockdown in human vascular smooth muscle cells led to increased cell migration and proliferation, alongside decreased expression of contractile markers and increased synthetic markers. Unexpectedly, FRZB knockdown suppressed Wnt signaling. Pathway analysis revealed associations with the PI3K-Akt signaling pathway, focal adhesion, and ECM interactions.
Conclusions
Our study highlights FRZB's role in Vascular smooth muscle cell phenotype modulation, showing that reduced FRZB expression correlates with a synthetic phenotype and increased disease markers. FRZB does not enhance Wnt signaling but may regulate vascular smooth muscle cell behavior through alternative pathways. These findings suggest FRZB as a potential therapeutic target for stabilizing vascular smooth muscle cells and managing atherosclerosis.}
}
@article{PRESUTTI2025100867,
title = {Opportunities for Knowledge Graphs in the AI landscape — An application-centric perspective},
journal = {Journal of Web Semantics},
pages = {100867},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2025.100867},
url = {https://www.sciencedirect.com/science/article/pii/S1570826825000083},
author = {Valentina Presutti and Enrico Motta and Marta Sabou},
keywords = {Knowledge Graphs, Artificial Intelligence, Applications},
abstract = {Artificial Intelligence (AI), as a research field, is experiencing an intensive, dynamic period with major new trends and technologies emerging at an unprecedented pace. Generative AI systems, neuro-symbolic AI, agentic AI, AI regulations are just a few of the ground-breaking, ongoing changes in the field. Against this backdrop, it is natural for each community to embark in a “soul-searching” and strategic positioning activity: What is our role in AI? What are major current and long-term developments? In this introduction to the special issue “Opportunities for Knowledge Graphs in the AI Landscape - An Application-Centric Perspective”, we report on such a soul-searching activity for the knowledge graph community: authors of 17 papers reported on how they used KGs in the context of AI applications/tasks and reflected on the challenges encountered. Collectively, as will be evident from this short introductory piece, the papers reflect a vibrant and varied space of how KGs are used as a key component in the DNA of modern AI systems. They also identify several cross-cutting challenges, that lend themselves for an ambitious research agenda on KGs.}
}
@article{XIA2023313,
title = {Enhancing intelligent IoT services development by integrated multi-token code completion},
journal = {Computer Communications},
volume = {212},
pages = {313-323},
year = {2023},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2023.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S0140366423003791},
author = {Yu Xia and Tian Liang and WeiHuan Min and Li Kuang and Honghao Gao},
keywords = {Intelligent device-free sensing, Code completion, Language model, Graph neural network},
abstract = {The Internet of Things (IoT) is a revolutionary network of interconnected devices embedded with sensors and software that enables seamless communication, data sharing, and intelligent decision-making in the form of IoT services. To facilitate the efficient development of IoT services, code completion technique provides a promising solution by providing suggestions for missing code snippets. The development trend of IoT services is to support more mobile device terminals. Mobile devices are portable and easy to use, allowing IoT device operation and management anytime and anywhere. However, the current multi-token completion methods struggle to guarantee code generation quality under the constraints of low resources and low latency, making it difficult to fully support IoT service development. We propose a multi-token code completion framework, S2RCC, which completes code from skeleton to refinement with dual encoder and dual decoder. The framework consists of two phases: first, the code skeleton, which is the simplification of code containing structure-sensitive tokens, is predicted based on the semantics of the code context; second, the broken context is repaired with the predicted skeleton, and then parsed into the code structure so that the specific tokens can be generated combining the semantics and structure of context. Furthermore, we then provide an implementation of the framework, representing the repaired code as an improved Heterogeneous code graph and fusing the semantics and structure of code context by the three-layer stacked attention. We conducted experiments on multi-token completion datasets, showing that our model has achieved the state-of-the-art with the smallest possible scale and the fastest generation speed.}
}
@article{FATEMIMOGHADDAM202038,
title = {A multi-layered policy generation and management engine for semantic policy mapping in clouds},
journal = {Digital Communications and Networks},
volume = {6},
number = {1},
pages = {38-50},
year = {2020},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2019.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352864817301931},
author = {Faraz {Fatemi Moghaddam} and Philipp Wieder and Ramin Yahyapour},
keywords = {Cloud computing, Security, Security management, Policy management, Access control, Policy mapping},
abstract = {The long awaited cloud computing concept is a reality now due to the transformation of computer generations. However, security challenges have become the biggest obstacles for the advancement of this emerging technology. A well-established policy framework is defined in this paper to generate security policies which are compliant to requirements and capabilities. Moreover, a federated policy management schema is introduced based on the policy definition framework and a multi-level policy application to create and manage virtual clusters with identical or common security levels. The proposed model consists in the design of a well-established ontology according to security mechanisms, a procedure which classifies nodes with common policies into virtual clusters, a policy engine to enhance the process of mapping requests to a specific node as well as an associated cluster and matchmaker engine to eliminate inessential mapping processes. The suggested model has been evaluated according to performance and security parameters to prove the efficiency and reliability of this multi-layered engine in cloud computing environments during policy definition, application and mapping procedures.}
}
@article{GERARDIN2022,
title = {Construction of Cohorts of Similar Patients From Automatic Extraction of Medical Concepts: Phenotype Extraction Study},
journal = {JMIR Medical Informatics},
volume = {10},
number = {12},
year = {2022},
issn = {2291-9694},
doi = {https://doi.org/10.2196/42379},
url = {https://www.sciencedirect.com/science/article/pii/S2291969422000564},
author = {Christel Gérardin and Arthur Mageau and Arsène Mékinian and Xavier Tannier and Fabrice Carrat},
keywords = {natural language processing, similar patient cohort, phenotype, systemic disease, NLP, algorithm, automatic extraction, automated extraction, named entity, MeSH, medical subject heading, data extraction, text extraction},
abstract = {Background
Reliable and interpretable automatic extraction of clinical phenotypes from large electronic medical record databases remains a challenge, especially in a language other than English.
Objective
We aimed to provide an automated end-to-end extraction of cohorts of similar patients from electronic health records for systemic diseases.
Methods
Our multistep algorithm includes a named-entity recognition step, a multilabel classification using medical subject headings ontology, and the computation of patient similarity. A selection of cohorts of similar patients on a priori annotated phenotypes was performed. Six phenotypes were selected for their clinical significance: P1, osteoporosis; P2, nephritis in systemic erythematosus lupus; P3, interstitial lung disease in systemic sclerosis; P4, lung infection; P5, obstetric antiphospholipid syndrome; and P6, Takayasu arteritis. We used a training set of 151 clinical notes and an independent validation set of 256 clinical notes, with annotated phenotypes, both extracted from the Assistance Publique-Hôpitaux de Paris data warehouse. We evaluated the precision of the 3 patients closest to the index patient for each phenotype with precision-at-3 and recall and average precision.
Results
For P1-P4, the precision-at-3 ranged from 0.85 (95% CI 0.75-0.95) to 0.99 (95% CI 0.98-1), the recall ranged from 0.53 (95% CI 0.50-0.55) to 0.83 (95% CI 0.81-0.84), and the average precision ranged from 0.58 (95% CI 0.54-0.62) to 0.88 (95% CI 0.85-0.90). P5-P6 phenotypes could not be analyzed due to the limited number of phenotypes.
Conclusions
Using a method close to clinical reasoning, we built a scalable and interpretable end-to-end algorithm for extracting cohorts of similar patients.}
}
@article{CARRERASGUZMAN2021105458,
title = {An integrated safety and security analysis for cyber-physical harm scenarios},
journal = {Safety Science},
volume = {144},
pages = {105458},
year = {2021},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2021.105458},
url = {https://www.sciencedirect.com/science/article/pii/S0925753521003015},
author = {Nelson H. {Carreras Guzman} and Igor Kozine and Mary Ann Lundteigen},
keywords = {Cyber-Physical Systems (CPSs), Autonomous Systems, Cyber-Physical Harm Analysis for Safety and Security (CyPHASS), Bowtie Method},
abstract = {Increasing digitalization and autonomous solutions in physical systems promise to enhance their performance, cost-efficiency and reliability. However, the integration of novel information technologies with safety-related systems also brings new vulnerabilities and risks that challenge the traditional field of safety analysis. Particularly, cyber security threats are becoming key factors in complex accident scenarios in cyber-physical systems (CPSs), where unintentional errors and design flaws overlap with cyber security vulnerabilities that could lead to harm to humans and assets. This overlap between safety and security analysis is still a loosely defined domain without established theories and methods, leading to complications during the risk analysis of CPSs. In this paper, we first describe how the domain of safety science increasingly overlaps with security analysis. Subsequently, based on this overlapping, we illustrate and complement an integrated method for the identification of harm scenarios in CPSs. This method, coined Uncontrolled Flows of Information and Energy (UFoI-E), offers a distinct theoretical foundation rooted in accident causation models and a framework to design diagrammatic representations of CPSs during the analysis. After summarizing these features of the UFoI-E method, we present our original contribution to the method, which is a new practical toolkit for risk identification composed of an ontology of harm scenarios and a database of checklists built from lessons learned analysis and expert knowledge. Finally, we demonstrate an application of the method in an illustrative case and show representative fields for future work.}
}
@article{TANVINEWAZ2022105618,
title = {A review and assessment of technologies for addressing the risk of falling from height on construction sites},
journal = {Safety Science},
volume = {147},
pages = {105618},
year = {2022},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2021.105618},
url = {https://www.sciencedirect.com/science/article/pii/S0925753521004586},
author = {Mohammad {Tanvi Newaz} and Mahmoud Ershadi and Luke Carothers and Marcus Jefferies and Peter Davis},
keywords = {Falling from height, Safety technology, Construction industry, Feasibility assessment},
abstract = {Falling from height (FFH) is blamed for causing significant injuries and deaths on construction sites. Previous research has outlined a broad range of technological advances facilitating the management of the FFH safety risk. However, the extant literature lacks a comprehensive assessment to investigate the contribution of various FFH technologies, as well as their implementation feasibility on construction sites, which provides rationale for this study. The study aims to assess recent safety technologies which can be used to control the risk of FFH on construction sites, especially in urban building construction projects. A scoping review was conducted to identify such technologies and provide insight into their application in the construction industry. As a result of searching Scopus, Web of Science, and Google Scholar databases between 2010 and 2021, a total of 86 representative studies were selected and reviewed. Following this stage, an assessment of their feasibility was carried out based on a set of criteria from the literature. A total of 7 FFH technologies were identified, characterising the contribution of recent technologies to the prediction, prevention, and mitigation of FFH risks. These technologies include (1) Safety risk assessment and propagation, (2) real-time sensing and monitoring, (3) automated prevention through design, (4) ontology and knowledge modelling, (5) virtual reality for FFH training, (6) personal fall arrest systems, and (7) collective fall protection systems. This research contributes to an improved understanding of the status of FFH technologies. The feasibility assessment provides insight into suitable technologies for construction projects of various sizes and features.}
}
@article{LOVERA2025100624,
title = {An analytics approach to extracting location and sentiment insights from classified social media data},
journal = {Decision Analytics Journal},
volume = {16},
pages = {100624},
year = {2025},
issn = {2772-6622},
doi = {https://doi.org/10.1016/j.dajour.2025.100624},
url = {https://www.sciencedirect.com/science/article/pii/S2772662225000803},
author = {Fernando Lovera and Yudith Cardinale},
keywords = {Social media analytics, Topic detection, Sentiment extraction, Location inference, Text classification, Natural disasters managing},
abstract = {Social networks are becoming vital for people to interact with each other, which in turn represent the production of a huge amount of information that can be useful in many contexts (e.g., medicine, natural disasters, commercial purposes, tourism). Nevertheless, analyzing such Big Data for insights might be difficult if the appropriate processing and analysis tools are not available. In this sense, we propose a framework to analyze social network content by integrating Topic Detection, Sentiment Analysis, and Geolocation. The gathered information is processed using Natural Language Processing methods to extract textual elements that make it possible for each framework component to function as intended. After reading through a stream of posts, the Topic Detection method classifies them and removes any that have nothing to do with the subject being analyzed. Sentiment Analysis component combines Machine Learning, Knowledge Graphs, and Semantic Web techniques, using SPARQL in conjunction with DBpedia and Nominatim. The Geolocation component scans posts and attempts to determine their geographical position. In this study, we implement a proof-of-concept on X (formerly Twitter), called XAF (X Analyzer Framework), to work in the context of natural disasters, to show the efficiency of combining Sentiment Analysis, Geolocation, and Topic Detection, and the possibility to be used in other contexts. We describe the general architecture of XAF and show the performance of each module as well as the holistic solution. Results show that XAF provides a platform to analyze X posts from different perspectives that allows implementing applications able to respond in real time.}
}
@article{ZHAO2021779,
title = {Enabling situational awareness of business processes},
journal = {Business Process Management Journal},
volume = {27},
number = {3},
pages = {779-795},
year = {2021},
issn = {1463-7154},
doi = {https://doi.org/10.1108/BPMJ-07-2020-0331},
url = {https://www.sciencedirect.com/science/article/pii/S1463715421000273},
author = {Xiaohui Zhao and Sira Yongchareon and Nam-Wook Cho},
keywords = {Business process modelling, Context awareness, Event processing},
abstract = {Purpose
The purpose of this research is to explore the ways of integrating situational awareness into business process management for the purpose of realising hyper automated business processes. Such business processes will help improve their customer experiences, enhance the reliability of service delivery and lower the operational cost for a more competitive and sustainable business.
Design/methodology/approach
Ontology has been deployed to establish the context modelling method, and the event handling mechanisms are developed on the basis of event calculus. An approach on performance of the proposed approach has been evaluation by checking the cost savings from the simulation of a large number of business processes.
Findings
In this research, the authors have formalised the context presentation for a business process with a focus on rules and entities to support context perception; proposed a system architecture to illustrate the structure and constitution of a supporting system for intelligent and situation aware business process management; developed real-time event elicitation and interpretation mechanisms to operationalise the perception of contextual dynamics and real-time responses; and evaluated the applicability of the proposed approaches and the performance improvement to business processes.
Originality/value
This paper presents a framework covering process context modelling, system architecture and real-time event handling mechanisms to support situational awareness of business processes. The reported research is based on our previous work on radio frequency identification-enabled applications and context-aware business process management with substantial extension to process context modelling and process simulation.}
}
@article{WU2024e25691,
title = {Construction of molecular subtype model of osteosarcoma based on endoplasmic reticulum stress and tumor metastasis-related genes},
journal = {Heliyon},
volume = {10},
number = {3},
pages = {e25691},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e25691},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024017225},
author = {Wang-Qiang Wu and Cheng-Da Zou and Di Wu and Hou-Xin Fu and Xiao-Dong Wang and Feng Yao},
keywords = {Osteosarcoma, Metastasis, Endoplasmic reticulum stress, Subtype analysis},
abstract = {Introduction
Osteosarcoma, the prevailing primary bone malignancy among children and adolescents, is frequently associated with treatment failure primarily due to its pronounced metastatic nature.
Methods
This study aimed to establish potential associations between hub genes and subtypes for the treatment of metastatic osteosarcoma. Differentially expressed genes were extracted from patients diagnosed with metastatic osteosarcoma and a control group of non-metastatic patients, using the publicly available gene expression profile (GSE21257). The intersection of these gene sets was determined by focusing on endoplasmic reticulum (ER) stress-related genes sourced from the GeneCards database. We conducted various analytical techniques, including functional and pathway enrichment analysis, WGCNA analysis, protein-protein interaction (PPI) network construction, and assessment of immune cell infiltration, using the intersecting genes. Through this analysis, we identified potential hub genes.
Results
Osteosarcoma subtype models were developed using molecular consensus clustering analysis, followed by an examination of the associations between each subtype and hub genes. A total of 138 potential differentially expressed genes related to endoplasmic reticulum (ER) stress were identified. These genes were further investigated using Gene Ontology (GO), Kyoto Encyclopedia of Genes and Genomes (KEGG), and Gene Set Enrichment Analysis (GSEA) pathways. Additionally, the PPI interaction network revealed 38 interaction relationships among the top ten hub genes. The findings of the analysis revealed a strong correlation between the extent of immune cell infiltration and both osteosarcoma metastasis and the expression of hub genes. Notably, the differential expression of the top ten hub genes was observed in osteosarcoma clusters 1 and 4, signifying their significant association with the disease.
Conclusion
The identification of ten key genes linked to osteosarcoma metastasis and endoplasmic reticulum stress bears potential clinical significance. Additionally, exploring the molecular subtype of osteosarcoma has the capacity to guide clinical treatment decisions, necessitating further investigations and subsequent clinical validations.}
}
@article{BANGOU201982,
title = {Experimenting with creativity, immigration, language, power, and technology: a research agencement},
journal = {Qualitative Research Journal},
volume = {19},
number = {2},
pages = {82-92},
year = {2019},
issn = {1443-9883},
doi = {https://doi.org/10.1108/QRJ-D-18-00015},
url = {https://www.sciencedirect.com/science/article/pii/S1443988319000274},
author = {Francis Bangou},
keywords = {Language, Power, Creativity, Technology, Agencement, Citizenship, Hacking, New materialism},
abstract = {Purpose
This paper is the actualization of a researcher’s attempt to engage, both conceptually and methodologically, with the dynamic and ever-creative connections and forces associated with the schooling experiences of immigrant students. The research reported that in this paper comprises part of a three-year research project funded by Canada’s Social Sciences and Humanities Research Council and focuses on the interrelationships between immigration, technology and pop culture in a Canadian French-language secondary school. The paper aims to discuss this issue.
Design/methodology/approach
Drawing from new materialist thought, the experience of one immigrant student is put to work with(in) the Deleuzo–Guattarian concepts of agencement, machines, language and power (pouvoir, puissance) with(in) the rhizoanalysis of a short video clip provided by the student.
Findings
With(in) the rhizoanalysis, the publication machine emerges as a force that could potentially affect the expression of one’s becoming citizen, and hacking emerges as a force that could contribute to the destabilization of the publication machine’s power (pouvoir).
Originality/value
The originality of this paper is that readers are also invited to contribute to this experimentation in contact with the real.}
}
@incollection{BARILLARO2025167,
title = {Deep Learning Platforms: PyTorch},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {167-170},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00093-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027000932},
author = {Luca Barillaro},
keywords = {Artificial intelligence, Artificial neural networks, Computer vision, Deep learning tool, Machine learning},
abstract = {PyTorch is an open-source machine learning library developed primarily by Facebook׳s AI Research Lab (FAIR). It is designed to provide a flexible and dynamic framework for building deep learning models. PyTorch is built on top of Torch, a scientific computing framework and script language widely used in machine learning research. Released in October 2016, PyTorch has gained significant popularity among researchers, engineers, and enthusiasts due to its intuitive interface, dynamic computation graph, and strong community support. In bioinformatics, PyTorch is widely used due to the rapid increase in the application of deep learning techniques for analyzing complex biological data. This chapter presents an overview of PyTorch, highlighting technical details and applications with a focus on bioinformatics.}
}
@article{ECKHART2019156,
title = {Securing the testing process for industrial automation software},
journal = {Computers & Security},
volume = {85},
pages = {156-180},
year = {2019},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2019.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S0167404818314482},
author = {Matthias Eckhart and Kristof Meixner and Dietmar Winkler and Andreas Ekelhart},
keywords = {Security analysis, Threat modeling, Risk assessment, Security ontology, Software testing, Industrial automation software, Cyber-Physical Systems, Industrial control systems, VDI/VDE 2182, ISO/IEC/IEEE 29119},
abstract = {The testing of automation applications has become a crucial pillar of every production systems engineering (PSE) project with the proliferation of cyber-physical systems (CPSs). In light of new attack vectors against CPSs, caused, inter alia, by increased connectivity, security aspects must be considered throughout the PSE process. In this context, software testing represents a critical activity, as a lack of adequate security mechanisms puts a variety of valuable assets (e.g., system configurations and production details) at risk of information theft and sabotage. Thus, organizations must analyze the security of their software testing process on a regular basis in order to counter these threats. Yet, due to the required security knowledge or budget constraints for security-related expenses, these undertakings may be destined to fail. In this work, we present a framework that supports the semi-automated security analysis of an organization’s software testing process for industrial automation software. This framework is based on the VDI/VDE 2182 guideline and integrates an ontological approach to model the necessary background knowledge, including, e.g., data flows, assets, entities, threats, and countermeasures. The framework comprises a default model of the testing process, which users can adapt so that the target of inspection accurately reflects their software testing environment. In particular, the testing process considered for creating the default model is based on best practices observed at a major system integrator, aligned with the ISO/IEC/IEEE 29119 series of software testing standards. Moreover, we developed a tool that enables the automatic generation of attack–defense trees from such formal models of the organization’s software testing process. We demonstrate how the proposed framework can be applied to a generic software testing process to answer essential questions in conducting a security risk analysis. The results of the exemplary security analysis provide guidance, should raise awareness in the industrial domain, and support effective, yet cost- and time-efficient security analyses. Finally, we evaluate the presented framework by performing a comprehensive comparison of suitable security analysis tools.}
}
@incollection{DELFIOL2023529,
title = {Chapter 18 - Knowledge resources},
editor = {Robert A. Greenes and Guilherme {Del Fiol}},
booktitle = {Clinical Decision Support and Beyond (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Oxford},
pages = {529-536},
year = {2023},
isbn = {978-0-323-91200-6},
doi = {https://doi.org/10.1016/B978-0-323-91200-6.00018-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323912006000188},
author = {Guilherme {Del Fiol} and David A. Cook},
keywords = {Clinical decision support, Knowledge resource, Knowledge repository, Knowledge asset, Knowledge publisher, Electronic health record system},
abstract = {Knowledge resources are repositories of medical knowledge developed and maintained by knowledge publishers. Knowledge resources contain knowledge assets in various representation formats such as narrative text, terminology value sets, ontologies, order sets, decision rules, and machine learning algorithms. Knowledge assets can be directly delivered to target users for decision support, or used as building blocks for clinical decision support tools. In this chapter, we describe the role of knowledge resources in clinical decision support. In particular, we describe knowledge resources in the context of decentralized knowledge distribution architectures, in which knowledge assets can be created, managed and distributed by different entities and shared among different client systems and healthcare organizations.}
}
@article{UNGUREANU2024102953,
title = {The green, green grass of the nation. A new far-right ecology in Spain},
journal = {Political Geography},
volume = {108},
pages = {102953},
year = {2024},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2023.102953},
url = {https://www.sciencedirect.com/science/article/pii/S0962629823001312},
author = {Camil Ungureanu and Lucia Alexandra Popartan},
abstract = {The Spanish far-right party Vox has undergone, together with other similar forces in Europe, an “ecological turn”. To analyze this case, the article starts from the premise of the need to understand better the far-right parties’ discursive mechanisms and their engagement with environmental issues. Theoretically, we criticize the ideological and ontological approaches to the current wave of national populism; instead, we argue that the dominant form of authoritarian national populism is more usefully analyzed in terms of embodied narratives, metaphorical imagination, mythmaking, and affective intensification. Empirically, using discourse analysis (Laclau, 2005) and drawing on the affective turn in political ecology, first, we argue that Vox's eco-narrative is interpretable as a combination of national populism and biopolitical imaginary pitting a “culture of death” against the “culture of life”. Vox's key rhetorical mechanisms, we intimate, are hyperbolical metaphorization and rhetorical inversion, have an intensifying effect at two interconnected levels: fear and indignation concerning the degeneration of Spain, pride and hope regarding Vox's and its male hero's promise of salvation and the Reconquista of the Spanish way of life. Second, by looking into key policy areas (energy and water), we argue that Vox has systematically chosen the former in conflicts between capitalist interests and environmental issues.}
}
@article{ZAITON2024353,
title = {Leveraging Transformer Summarizer to Extract Sentences for Arabic Text Summarization},
journal = {Procedia Computer Science},
volume = {244},
pages = {353-362},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.209},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924030102},
author = {Hoda Zaiton and Amany Fashwan and Sameh Alansary},
keywords = {Arabic Text Summarization, Extractive Arabic Text Summarization, Transformer Summarizer, Natural Language Processing},
abstract = {Automatic Text Summarization (ATS) is one of the fastest-growing areas of Artificial Intelligence (AI), Machine Learning (ML), and Natural Language Processing (NLP). Automatic text summarizing aims to create summaries by extracting relevant information from input texts. Being able to study text non-sequentially allowed for large model training, making the Transformer the most widely used NLP model. This study focuses on extractive Arabic text summarization using various transformer-based summarization models. Two distinct scenarios are explored: summarization extracted from scored sentences and summarization generated from ranked (reversed) sentences. Experimental results demonstrate the effectiveness and performance of the transformer-based summarization models in the context of Arabic text summarization. The EASC corpus is used to evaluate our proposed summarization approach and ROUGE evaluation method is applied to determine the accuracy of the proposed approach. The findings of the study demonstrate notable performance when compared to other relevant works. The comparison between the two scenarios provides insights into the strengths and weaknesses of each approach. The scored sentence extraction scenario offers a more direct and content-focused summary generation, while the ranked sentence scenario provides flexibility in summarizing diverse text sources.}
}
@article{DU2025,
title = {Chufeng Yisun Decoction treat dry eye syndrome by inhibiting the PI3K/Akt pathway},
journal = {Electronic Journal of Biotechnology},
year = {2025},
issn = {0717-3458},
doi = {https://doi.org/10.1016/j.ejbt.2025.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0717345825000284},
author = {Yue Du and Xue Jiang and Yanyan Zhang and Quanyong Yi},
keywords = {Active ingredients, Chinese medicine, Dry eye syndrome, Ekaempferol, HCE-T cells, IL-1β, Molecular docking, Network pharmacology, PI3K/Akt pathway, Quercetin, Wogonin},
abstract = {Background
Dry eye disease seriously affects people’s work and life. Chufeng Yisun Decoction is a traditional Chinese medicine decoction used in treating dry eye disease. This study aims to explore the core active ingredients, targets, and mechanisms of CFYSD in dry eye disease, providing new insights for the dry eye disease treatment.
Results
A total of 196 target genes were screened from Chufeng Yisun Decoction and 170 genes were related to dry eye disease. Gene Ontology and KEGG enrichment analysis showed that Chufeng Yisun Decoction influenced dry eye disease through “Lipid and atherosclerosis”, “Fluid shear stress and atherosclerosis” and “PI3K-Akt”. The core targets of Chufeng Yisun Decoction in treating dry eye disease were Akt1 and IL-1β. The core active ingredients were kaempferol, wogonin, and quercetin. Molecular docking results showed that the binding energies of kaempferol and Akt1, wogonin and Akt1, quercetin and Akt1, and quercetin and IL-1β were -6.1, -6.1, -6.1, and -7.9 kcal/mol, respectively. Chufeng Yisun Decoction significantly alleviated cell damage and reduced PI3K/Akt pathway-related protein expression. PI3K activation partially reversed the therapeutic effect of Chufeng Yisun Decoction on dry eye disease.
Conclusion
Chufeng Yisun Decoction treats dry eye disease by inactivating the PI3K/Akt pathway through multi-ingredients and multi-targets.}
}
@article{CPEREIRA2024102814,
title = {Automated image label extraction from radiology reports — A review},
journal = {Artificial Intelligence in Medicine},
volume = {149},
pages = {102814},
year = {2024},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.102814},
url = {https://www.sciencedirect.com/science/article/pii/S0933365724000563},
author = {Sofia {C. Pereira} and Ana Maria Mendonça and Aurélio Campilho and Pedro Sousa and Carla {Teixeira Lopes}},
keywords = {Natural language processing, Information extraction, Image annotation, Medical imaging, Medical reports, Literature survey},
abstract = {Machine Learning models need large amounts of annotated data for training. In the field of medical imaging, labeled data is especially difficult to obtain because the annotations have to be performed by qualified physicians. Natural Language Processing (NLP) tools can be applied to radiology reports to extract labels for medical images automatically. Compared to manual labeling, this approach requires smaller annotation efforts and can therefore facilitate the creation of labeled medical image data sets. In this article, we summarize the literature on this topic spanning from 2013 to 2023, starting with a meta-analysis of the included articles, followed by a qualitative and quantitative systematization of the results. Overall, we found four types of studies on the extraction of labels from radiology reports: those describing systems based on symbolic NLP, statistical NLP, neural NLP, and those describing systems combining or comparing two or more of the latter. Despite the large variety of existing approaches, there is still room for further improvement. This work can contribute to the development of new techniques or the improvement of existing ones.}
}
@article{STEINBERGER2018208,
title = {Cross lifecycle variability analysis: Utilizing requirements and testing artifacts},
journal = {Journal of Systems and Software},
volume = {143},
pages = {208-230},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.04.062},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218300864},
author = {Michal Steinberger and Iris Reinhartz-Berger and Amir Tomer},
keywords = {Variability analysis, Ontology, Software reuse, Software product lines, Application lifecycle management},
abstract = {Variability analysis is an essential activity that supports increasing and systemizing reuse across similar software products. Current studies use different types of artifacts for analyzing variability, most notably are architecture or design, requirements, and code. While architecture, design, and code help understand and model the differences in solutions and realizations, requirements enable capturing differences in a higher level of abstraction through the intended use of the software products or their behavior. However, analyzing variability based on requirements may result in inaccurate outcomes, due to the informal and incomplete nature of requirements. To tackle this deficiency, we call for augmenting requirements-based variability analysis with other behavior-related cross-lifecycle artifacts. Particularly, we extend an approach that compares and analyzes software behaviors based on requirements taking into account both ontological and semantic considerations. Using test cases and their relations to requirements, our extension, named SOVA R-TC, extract software behaviors more comprehensively, including their preconditions, post-conditions, and expected results. The outputs of SOVA R-TC are feature diagrams, which group similar behaviors and present variability of software products in a tree structure. Empirically evaluating outcomes of SOVA R-TC, they seem to be perceived as significantly better than outcomes generated based on requirements only.}
}
@article{XU2022116964,
title = {A novel framework of knowledge transfer system for construction projects based on knowledge graph and transfer learning},
journal = {Expert Systems with Applications},
volume = {199},
pages = {116964},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.116964},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422003906},
author = {Jin Xu and Mengqi He and Ying Jiang},
keywords = {Construction project, Knowledge transfer, Project similarity, Knowledge graph, Transfer learning},
abstract = {For construction enterprises, efficient knowledge sharing among projects not only effectively improves enterprise technology, level of management and competitiveness, but also promotes their sustainable development. Given the many benefits of knowledge management, enterprises have an urgent need for project knowledge sharing methods and tools. In this study, we build an automated and intelligent framework for a construction project knowledge transfer system based on knowledge graph and transfer learning. This framework aims to solve the problem of ineffective knowledge transfer that is encountered in the management of construction project knowledge sharing. First, to discover the relationship among knowledge and further obtain the relationships among projects, we design a domain knowledge graph ontology for construction projects and build an example. Then, based on this domain knowledge graph and combining construction data distance distribution with construction project knowledge background, we design a new construction project similarity measurement algorithm (PBG-MMD), which can guide the selection of the knowledge transfer source domain. Finally, a new transfer learning method is developed to automatically select the transfer source domain according to the domain context. The framework provides an effective answer for the problem of “what to transfer” in transfer learning and provides an effective solution to address the problem of “how to transfer” during knowledge transfer. Through the verification of practical case data, the proposed framework successfully realizes knowledge transfer among construction projects and provides an automated and intelligent knowledge sharing approach for construction enterprises.}
}
@article{LI2025107533,
title = {Enhancing safety and operation of hydrogen fueling stations: A model-based method for complex failure scenario analysis},
journal = {Process Safety and Environmental Protection},
volume = {201},
pages = {107533},
year = {2025},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2025.107533},
url = {https://www.sciencedirect.com/science/article/pii/S0957582025008006},
author = {Ruixue Li and Jing Wu and Suroosh Mosleh and Katrina Groth and Mohammad Modarres and Morten Lind and Ole Ravn and Xinxin Zhang},
keywords = {Risk assessment, Functional modeling, Hydrogen fueling station, Accident prevention, Renewable energy, Failure mode identification},
abstract = {As a zero-emission fuel, hydrogen provides a promising solution with significant potential to meet the increasing demand for clean energy alternatives. Hydrogen fueling stations are essential infrastructure for the commercialization of hydrogen fuel cells, but the flammability of hydrogen poses safety challenges throughout its lifecycle. Past incidents highlight the need for robust risk assessments, starting with comprehensive hazard identification and failure scenario analysis. This paper proposes using Multilevel Flow Modelling (MFM), a functional modeling method integrated with reasoning capability, to support safety evaluations. MFM enables the structured representation of system functions and supports tasks such as fault diagnosis and hazard analysis. Previously applied in nuclear, offshore, and chemical systems, MFM is here used to model a liquid hydrogen fueling station. This paper demonstrates that a developed MFM model identifies failure scenarios related to hydrogen leaks, overpressure, and operational reliability issues. This paper conducts a comparison between MFM and traditional methods, FMEA and FTA, and demonstrates MFM's strength in handling the key challenges rooted from complex failure interactions. Results suggest MFM is complementary to traditional methods and can enhance risk assessments. MFM also contributes to digitalization in safety assessment and monitoring systems, ultimately improving hydrogen fueling station reliability and safety.}
}
@article{SARROUTI2020101767,
title = {SemBioNLQA: A semantic biomedical question answering system for retrieving exact and ideal answers to natural language questions},
journal = {Artificial Intelligence in Medicine},
volume = {102},
pages = {101767},
year = {2020},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2019.101767},
url = {https://www.sciencedirect.com/science/article/pii/S0933365718302756},
author = {Mourad Sarrouti and Said {Ouatik El Alaoui}},
keywords = {Biomedical question answering, Information retrieval, Passage retrieval, Natural language processing, Machine learning, Biomedical informatics, BioASQ},
abstract = {Background and objective
Question answering (QA), the identification of short accurate answers to users questions written in natural language expressions, is a longstanding issue widely studied over the last decades in the open-domain. However, it still remains a real challenge in the biomedical domain as the most of the existing systems support a limited amount of question and answer types as well as still require further efforts in order to improve their performance in terms of precision for the supported questions. Here, we present a semantic biomedical QA system named SemBioNLQA which has the ability to handle the kinds of yes/no, factoid, list, and summary natural language questions.
Methods
This paper describes the system architecture and an evaluation of the developed end-to-end biomedical QA system named SemBioNLQA, which consists of question classification, document retrieval, passage retrieval and answer extraction modules. It takes natural language questions as input, and outputs both short precise answers and summaries as results. The SemBioNLQA system, dealing with four types of questions, is based on (1) handcrafted lexico-syntactic patterns and a machine learning algorithm for question classification, (2) PubMed search engine and UMLS similarity for document retrieval, (3) the BM25 model, stemmed words and UMLS concepts for passage retrieval, and (4) UMLS metathesaurus, BioPortal synonyms, sentiment analysis and term frequency metric for answer extraction.
Results and conclusion
Compared with the current state-of-the-art biomedical QA systems, SemBioNLQA, a fully automated system, has the potential to deal with a large amount of question and answer types. SemBioNLQA retrieves quickly users’ information needs by returning exact answers (e.g., “yes”, “no”, a biomedical entity name, etc.) and ideal answers (i.e., paragraph-sized summaries of relevant information) for yes/no, factoid and list questions, whereas it provides only the ideal answers for summary questions. Moreover, experimental evaluations performed on biomedical questions and answers provided by the BioASQ challenge especially in 2015, 2016 and 2017 (as part of our participation), show that SemBioNLQA achieves good performances compared with the most current state-of-the-art systems and allows a practical and competitive alternative to help information seekers find exact and ideal answers to their biomedical questions. The SemBioNLQA source code is publicly available at https://github.com/sarrouti/sembionlqa.}
}
@article{NAGOEV202371,
title = {The symbol grounding problem in the system of general artificial intelligence based on multi-agent neurocognitive architecture},
journal = {Cognitive Systems Research},
volume = {79},
pages = {71-84},
year = {2023},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723000025},
author = {Zalimkhan Nagoev and Olga Nagoeva and Murat Anchokov and Kantemir Bzhikhatlov and Sultan Kankulov and Ahmed Enes},
keywords = {Artificial general intelligence, Symbol grounding problem, Multi-agent neurocognitive architectures},
abstract = {The purpose of this study is to develop the basic principles and algorithms for solving the fundamental scientific problem of substantiating natural language symbols in the process of its development by a general artificial intelligence system when interacting with a social communicative environment. The signs of a minimally functional system of general artificial intelligence based on a rational agent of artificial life under the control of a multi-agent neurocognitive architecture are formulated. It is shown that such a software agent is capable of solving a spectrum of information processing tasks that is universal within the state space of the “intelligent agent - environment” system. Universalism is achieved through the implementation of the problem solving paradigm based on the synthesis of the behavior of an intelligent agent aimed at moving along the graph of problem situations, defined in the state space, marked by the values of the objective function. A simulation model of a multi-agent neurocognitive architecture of a minimal system of general artificial intelligence immersed in a communicative environment has been developed that implements the dynamic formation of functional systems based on the cooperation of neuron agents in its composition based on the principles of ontoneuromorphogenesis - situationally determined development of a multi-agent knowledge base. The main conclusion of the study is that the problem of substantiating language symbols can be solved with the help of a constructed and tested universal algorithm for acquiring language symbols that uses such functional systems to represent semantics, taking into account the communicative context, meaning and specific linguistic forms of these statements.}
}
@article{ZHANG2024103468,
title = {Transcriptome analysis reveals the genes involved in spermatogenesis in white feather broilers},
journal = {Poultry Science},
volume = {103},
number = {4},
pages = {103468},
year = {2024},
issn = {0032-5791},
doi = {https://doi.org/10.1016/j.psj.2024.103468},
url = {https://www.sciencedirect.com/science/article/pii/S0032579124000476},
author = {Gaomeng Zhang and Peihao Liu and Ruiping Liang and Fan Ying and Dawei Liu and Meng Su and Li Chen and Qi Zhang and Yuhong Liu and Sha Liu and Guiping Zhao and Qinghe Li},
keywords = {broiler, semen volume, transcriptome analysis},
abstract = {ABSTRACT
Semen volume is an important economic trait of broilers and one of the important indices for continuous breeding. The objective of this study was to identify genes related to semen volume through transcriptome analysis of the testis tissue of white feather broilers. The testis samples with the highest semen volume (H group, n = 5) and lowest semen volume (L group, n = 5) were selected from 400-day-old roosters for transcriptome analysis by RNA sequencing. During the screening of differentially expressed genes (DEGs) between the H and L groups, a total of 386 DEGs were identified, among which 348 were upregulated and 38 were downregulated. Gene ontology (GO) enrichment analysis and Kyoto Encyclopedia of Genes and Genomes (KEGG) analysis revealed that the immune response, leukocyte differentiation, cell adhesion molecules and collagen binding played vital roles in spermatogenesis. The results showed that 4 genes related to spermatogenesis, namely, COL1A1, CD74, ARPC1B and APOA1, were significantly expressed in Group H, which was consistent with the phenotype results. Our findings may provide a basis for further research on the genetic mechanism of semen volume in white feather broilers.}
}
@article{CHEN2024106424,
title = {A syntactic evidence network model for fact verification},
journal = {Neural Networks},
volume = {178},
pages = {106424},
year = {2024},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106424},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024003484},
author = {Zhendong Chen and Siu Cheung Hui and Fuzhen Zhuang and Lejian Liao and Meihuizi Jia and Jiaqi Li and Heyan Huang},
keywords = {Fact verification, Syntactic information, Sentence attention mechanism},
abstract = {In natural language processing, fact verification is a very challenging task, which requires retrieving multiple evidence sentences from a reliable corpus to verify the authenticity of a claim. Although most of the current deep learning methods use the attention mechanism for fact verification, they have not considered imposing attentional constraints on important related words in the claim and evidence sentences, resulting in inaccurate attention for some irrelevant words. In this paper, we propose a syntactic evidence network (SENet) model which incorporates entity keywords, syntactic information and sentence attention for fact verification. The SENet model extracts entity keywords from claim and evidence sentences, and uses a pre-trained syntactic dependency parser to extract the corresponding syntactic sentence structures and incorporates the extracted syntactic information into the attention mechanism for language-driven word representation. In addition, the sentence attention mechanism is applied to obtain a richer semantic representation. We have conducted experiments on the FEVER and UKP Snopes datasets for performance evaluation. Our SENet model has achieved 78.69% in Label Accuracy and 75.63% in FEVER Score on the FEVER dataset. In addition, our SENet model also has achieved 65.0% in precision and 61.2% in macro F1 on the UKP Snopes dataset. The experimental results have shown that our proposed SENet model has outperformed the baseline models and achieved the state-of-the-art performance for fact verification.}
}
@article{SAENZDESICILIA2018131,
title = {Production=signification: towards a semiotic materialism},
journal = {Language Sciences},
volume = {70},
pages = {131-142},
year = {2018},
note = {Karl Marx and the Language Sciences – Critical Encounters},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2018.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0388000117303765},
author = {Andrés {Saenz De Sicilia} and Sandro Brito Rojas},
keywords = {Karl Marx, Bolívar Echeverría, Materialism, Semiotics, Social reproduction, Ideology},
abstract = {This article sketches the outline of a semiotic materialism, drawing on Mexican-Ecuadoran philosopher Bolívar Echeverría's thesis that production=signification. For Echeverría, every process of social production and consumption is and must at the same time be a process of signification and interpretation. This thesis, initially developed in the mid-1970s, emerges most immediately from a novel synthesis of Marx with the work of Jakobson and Hjelmslev. It also establishes an expansive and highly original social ontology, at the core of which is a ‘trans-naturalised’ conception of the specificity of human social reproduction. This ontology both grounds the conceptual innovations of structural linguistics within a critical understanding of reproduction, as the general structure of material life, and demonstrates the necessarily semiotic character of all acts of production and consumption. We analyse this proposition and elaborate upon its critical implications, particularly a deepened theorisation of ideology and fetishism, which Echeverría describes as enacting a ‘subcodification’ of every explicit ‘message’ that can be transmitted, regardless of its content. By inscribing signification at the heart of critical social theory, Echeverría offers a unique account both of social reproduction and the effects of capitalist accumulation upon that process.}
}
@article{RABHI2021209,
title = {Design of an innovative IT platform for analytics knowledge management},
journal = {Future Generation Computer Systems},
volume = {116},
pages = {209-219},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.10.022},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2032999X},
author = {Fethi A. Rabhi and Madhushi Bandara and Kun Lu and Saif Dewan},
keywords = {Predictive analytics, Knowledge management, Knowledge base, Semantic modelling, Ontologies},
abstract = {An organisation wishing to conduct data analytics to support day-to-day decision making often needs a system to help analysts represent and maintain knowledge about research variables, datasets or analytical models, and effectively determine the best combination to use when solving the problem at hand. Often, such knowledge is not explicitly captured by the organisation. To address this problem, this paper presents the design of an innovative Information Technology (IT) platform which enables data sharing between different analytics models and provides the ability to extend or customise models or data sources without necessarily involving the analysts who created them. It can make analytics knowledge readily available and modifiable for future use and problem-solving by analysts and other stakeholders. In the context of our work, we organise analytics knowledge around the concept of a research variable, which analysts often use when defining and proving a hypothesis. By focusing on such a concept, this platform is particularly suited to develop empirical data analytics applications in any domain. This paper presents the architecture of this platform, including the knowledge base and the Application Programming Interface (API) layer. Capabilities of this platform are illustrated through a software prototype and a use case on property price prediction across Sydney, Australia.}
}
@article{GRAS2021232,
title = {The role of intonation in Construction Grammar: On prosodic constructions},
journal = {Journal of Pragmatics},
volume = {180},
pages = {232-247},
year = {2021},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2021.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0378216621001843},
author = {Pedro Gras and Wendy Elvira-García},
keywords = {Construction Grammar, Prosody, Prosodic constructions, Intonation, Insubordinate conditionals, Spanish},
abstract = {In constructionist approaches, constructions are defined as pairings of phonological, morphological and syntactic form, and semantic, pragmatic or discursive meaning. However, in practice very few constructional analyses deal specifically with the intonational properties of constructions. Moreover, constructional approaches are not clear about the ontological status of intonation in a constructional model of language. Combining insights from Construction Grammar and Intonational Studies, specifically the Autosegmental Metrical framework of intonational phonology and the Tones and Breaks Indices (ToBI) transcription model, this paper discusses the role of prosody in a construction-based approach to language, through the analysis of the insubordinate conditional construction (ICC) in Spanish. Building on the analysis of the prosody of 90 tokens of the ICC uttered by 14 native speakers of Peninsular Spanish, we argue that the best way to account for the relationship between intonational patterns and (lexicogrammatical) constructions is to treat intonational patterns as constructions that pair a phonological form with a semantic-pragmatic meaning, which are then inherited by sentence-level constructions as long as their meanings are compatible.}
}
@article{LI2025103996,
title = {Basis is also explanation: Interpretable Legal Judgment Reasoning prompted by multi-source knowledge},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {103996},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103996},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324003558},
author = {Shangyuan Li and Shiman Zhao and Zhuoran Zhang and Zihao Fang and Wei Chen and Tengjiao Wang},
keywords = {Legal judgment prediction, Prompt learning, Contrastive learning, Knowledge encoding, Interpretability},
abstract = {The task of Legal Judgment Prediction (LJP) aims to forecast case outcomes by analyzing fact descriptions, playing a pivotal role in enhancing judicial system efficiency and fairness. Existing LJP methods primarily focus on improving representations of fact descriptions to enhance judgment performance. However, these methods typically depend on the superficial case information and neglect the underlying legal basis, resulting in a lack of in-depth reasoning and interpretability in the judgment process of long-tail or confusing cases. Recognizing that the basis for judgments in real-world legal contexts encompasses both factual logic and related legal knowledge, we introduce the interpretable legal judgment reasoning framework with multi-source knowledge prompted. The essence of this framework is to transform the implicit factual logic of cases and external legal knowledge into explicit basis for judgment, aiming to enhance not only the accuracy of judgment predictions but also the interpretability of the reasoning process. Specifically, we design a chain prompt reasoning module that guides a large language model to elucidate factual logic basis through incremental reasoning, aligning the model prior knowledge with task-oriented knowledge in the process. To match the above fact-based information with legal knowledge basis, we propose a contrastive knowledge fusing module to inject external statutes knowledge into the fact description embedding. It pushes away the distance of similar knowledge in the semantic space during the encoding of external knowledge base without manual annotation, thus improving the judgment prediction performance of long-tail and confusing cases. Experimental results on two real datasets indicate that our framework significantly outperforms existing LJP baseline methods in accuracy and interpretability, achieving new state-of-the-art performance. In addition, tests on specially constructed long-tail and confusing case datasets demonstrate that the proposed framework possesses improved generalization abilities for predicting these complex cases.}
}
@article{REHM2024123355,
title = {A virtual driving instructor that assesses driving performance on par with human experts},
journal = {Expert Systems with Applications},
volume = {248},
pages = {123355},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.123355},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424002203},
author = {Johannes Rehm and Irina Reshodko and Odd Erik Gundersen},
keywords = {Virtual driving instructor, Multi-agent system, Knowledge graph, Ontology, Real-time assessment, Driver education, Traffic situation awareness, Driving simulation},
abstract = {The advent of virtual driving instructors has the potential to revolutionize driver education by providing real-time, unbiased feedback to learner drivers. This paradigm shift aims to mitigate the innate subjectivity associated with human evaluations. Our research focused on the creation of a virtual driving instructor capable of assessing a learner driver’s performance in real-time, with an emphasis on eliminating the inherent biases associated with human evaluations. Our approach involved the development of a rule-based assessment system, employing a multi-agent system based on the subsumption architecture. Each agent in the system was tasked with assessing a specific aspect of driving performance. Additionally, we utilized a knowledge graph to maintain a continuous understanding of the situational context, further enhancing the system’s assessment capabilities. We posited that our system, given its methodical structure and objective rule-based framework, would be able to accurately and objectively assess various driving scenarios. Further, we hypothesized that our system’s performance would be on par with expert human evaluations. The validation of our system was conducted using real driving sessions in simulators with actual students. The system was tested on various scenarios including intersections, roundabouts, and overtakes. The assessment results aligned closely with expert consensus, showcasing the system’s capacity to match the evaluative precision of human experts.}
}
@article{JIM2024100059,
title = {Recent advancements and challenges of NLP-based sentiment analysis: A state-of-the-art review},
journal = {Natural Language Processing Journal},
volume = {6},
pages = {100059},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100059},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000074},
author = {Jamin Rahman Jim and Md Apon Riaz Talukder and Partha Malakar and Md Mohsin Kabir and Kamruddin Nur and M.F. Mridha},
keywords = {Sentiment classification, Text classification, Natural language processing, Emotion detection, Sentiment analysis},
abstract = {Sentiment analysis is a method within natural language processing that evaluates and identifies the emotional tone or mood conveyed in textual data. Scrutinizing words and phrases categorizes them into positive, negative, or neutral sentiments. The significance of sentiment analysis lies in its capacity to derive valuable insights from extensive textual data, empowering businesses to grasp customer sentiments, make informed choices, and enhance their offerings. For the further advancement of sentiment analysis, gaining a deep understanding of its algorithms, applications, current performance, and challenges is imperative. Therefore, in this extensive survey, we began exploring the vast array of application domains for sentiment analysis, scrutinizing them within the context of existing research. We then delved into prevalent pre-processing techniques, datasets, and evaluation metrics to enhance comprehension. We also explored Machine Learning, Deep Learning, Large Language Models and Pre-trained models in sentiment analysis, providing insights into their advantages and drawbacks. Subsequently, we precisely reviewed the experimental results and limitations of recent state-of-the-art articles. Finally, we discussed the diverse challenges encountered in sentiment analysis and proposed future research directions to mitigate these concerns. This extensive review provides a complete understanding of sentiment analysis, covering its models, application domains, results analysis, challenges, and research directions.}
}
@article{PAREDES2025105552,
title = {The scientometric approach to Code Biology: What the title tells about the field},
journal = {BioSystems},
volume = {256},
pages = {105552},
year = {2025},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2025.105552},
url = {https://www.sciencedirect.com/science/article/pii/S0303264725001625},
author = {Omar Paredes and Robert Prinz},
abstract = {Code Biology has emerged as a conceptual framework for investigating how information is encoded, transmitted, and interpreted in living systems. Building on recent efforts to catalog biological codes across disciplines, we present the first comprehensive scientometric analysis of the field. Using a curated corpus of publications explicitly invoking the term code, we apply full-text natural language processing and unsupervised topic modeling to map the intellectual landscape of Code Biology. Our analysis reveals 24 distinct thematic clusters, ranging from molecular mechanisms and regulatory architectures to neural information processing and philosophical discourse on meaning and organization. This approach offers insights that conventional literature reviews often miss—uncovering latent patterns, inter-topic correlations, and conceptual blind spots. In doing so, we expose the field's current fragmentation into isolated knowledge niches and highlight the need for integrative models of how biological codes interact across scales. Temporal and geographical analyses reveal distinct phases in the development of Code Biology, shifting from gene-centric and mechanistic views to increasingly symbolic, cognitive, and systems-oriented paradigms. Collaboration network analysis further shows the emergence of modular scientific communities and identifies key interdisciplinary contributors shaping the field. Taken together, our results establish the foundation for a new branch of code biology, dedicated to the empirical and conceptual mapping of coding processes in biology based on literature. We propose key research directions, including the structural grammar of neural codes, the role of prebiotic and evolutionary codes in transitions of life, and the intersection between biological and artificial coding systems. This work provides not only a roadmap for future research but also a call to develop standardized frameworks capable of bridging molecular, neural, and symbolic dimensions of biological information processing.}
}
@article{EKTA2024100368,
title = {Transcriptome analyses revealed differential gene expression patterns in contrasting rice landraces as a response to acidic or proton toxicity stress},
journal = {Current Plant Biology},
volume = {39},
pages = {100368},
year = {2024},
issn = {2214-6628},
doi = {https://doi.org/10.1016/j.cpb.2024.100368},
url = {https://www.sciencedirect.com/science/article/pii/S2214662824000501},
author = { Ekta and Anil Kumar Singh and Sanjib Kumar Panda and Dev Mani Pandey},
keywords = {Acidic stress, DEGs. Gene ontology, Low pH, Rice, RNA-Seq},
abstract = {Acidic soil is harmful for plants and limits its growth. Four different rice landraces of Jharkhand, India, i.e., Jhilli Dhan, Gora Dhan, Desi Lalat Dhan and Khijur Jhopa Dhan were screened physiologically to identify low pH (acid) -sensitive and -tolerant landraces. The contrasting pairs were taken for RNA sequencing and further the data was analysed to elucidate the differential gene expression in rice leaves under different acidic stress conditions (pH 6.5 and pH 4.5). Based on the screening, Jhilli Dhan was identified as the acid-sensitive and Khijur Jhopa Dhan as acid-tolerant variety. Total 149,017,664 raw paired-end reads were generated from four rice samples (two from each variety) with percentage annotation of 99.89 % with the reference genome from RNA sequencing. Total 3617 differentially expressed genes were identified as significantly expressed genes in both the rice landraces. Gene Ontology analysis indicated the enrichment of genes associated to a variety of stimulus and responses to acidic stress. Kyoto Encyclopedia of Genes and Genomes based pathway analysis revealed that studied genes were related primarily to phenylpropanoid biosynthesis and flavonoid biosynthesis. Total 58 transcription factor families were differentially expressed that includes AP2-EREBP, bHLH, NAC, MYB, FAR1, and WRKY. Furthermore, 15 significantly expressed genes were validated through qRT-PCR to confirm the expression of these genes under acidic stress. Interestingly, OsCDT3 (LOC_Os01g08300), metal cation transporter (LOC_Os03g46454), cysteine rich receptor- like protein kinase 10 precursor (LOC_Os01g38910), calcium transporting ATPase (LOC_Os03g42020), UDP-glucoronosyl and UDP-glucosyl transferase (LOC_Os02g14540), OsASR5 (LOC_Os11g06720) and C2H2 zinc finger family (LOC_Os01g39110) genes were up-regulated by 2.5, 5.2, 3.5, 3.7, 4.5, 2.5 and 4.7 fold, respectively. These genes will prove to be beneficial in developing low-pH (acidic) stress tolerant rice varieties.}
}
@incollection{BURGHARDT2025,
title = {Digital humanities},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-443-26629-4.00016-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443266294000162},
author = {Manuel Burghardt},
keywords = {Digital humanities, Computational humanities, Distant reading, Distant viewing, Cultural analytics, Epistemology, Modeling, Statistics, Machine learning, Deep learning, Transformers, Large language models},
abstract = {This contribution explores the intersection of digital humanities and social measurement, with an emphasis on quantitative approaches such as statistical methods and machine learning techniques. Key challenges are discussed, including epistemological shifts, the operationalization of abstract concepts, and the development of digital models for cultural artifacts. A typical digital humanities research workflow is proposed, including the steps of data acquisition, enrichment, analysis, and interpretation, with the aim of deepening the understanding of how computational tools are applied in the humanities, especially in the context of large-scale cultural data.}
}
@article{PRADEEP201944,
title = {The MOM of context-aware systems: A survey},
journal = {Computer Communications},
volume = {137},
pages = {44-69},
year = {2019},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2019.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0140366418309472},
author = {Preeja Pradeep and Shivsubramani Krishnamoorthy},
keywords = {Context-awareness, Context-aware computing, Context-aware systems, Context modeling, Context organization, Ontology, Middleware, Pervasive and ubiquitous computing},
abstract = {Context-aware computing enriches the capabilities of intelligent devices complemented with smart applications and helps establish smart ecosystems in fields such as Ambient Intelligence, Internet of Things, Mobile Computing, and Pervasive and Ubiquitous Computing. Though the literature has many surveys that outline existing systems, it still remains critical to elucidate the basics of actually building an effective context-aware ecosystem. We outline the basic components required and essential for the same. We believe that a context-aware ecosystem becomes effective when these components are designed and implemented effectively. We call it the MOM of context-aware systems: generic and effective context Modeling, an efficient context Organization, and a robust context Middleware. Context modeling affords a syntax to the raw pieces of relevant information, the organization mechanism furnishes semantic import to the information and relationships, and the middleware compiles and integrates the information, enabling sharing of context. We discuss various context-aware ecosystems and middleware from the literature and highlight how the three building components function in each case. This paper will benefit newcomers to the field who are looking to learn about and build context-aware ecosystems.}
}
@article{OYELADE2018117,
title = {ST-ONCODIAG: A semantic rule-base approach to diagnosing breast cancer base on Wisconsin datasets},
journal = {Informatics in Medicine Unlocked},
volume = {10},
pages = {117-125},
year = {2018},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2017.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S2352914817301922},
author = {O.N. Oyelade and A.A. Obiniyi and S.B. Junaidu and S.A. Adewuyi},
keywords = {Reasoning algorithm, Select and test, Diagnoses, Breast cancer, Instances, Attributes},
abstract = {Breast cancer is a major terminal disease that occurs largely among females. This disease stems from abnormal mutations in the genes of normal cells, thereby resulting in development of cancerous cells. Though there have being several research breakthroughs in the field of medicine in taming this disease, however, computer aided diagnosis on the other hand has proven very supportive in the quest. Techniques such as Machine Learning (ML) and Medical Expert Systems (MES) algorithms have added impetus to the use of artificial intelligence in detecting and diagnosing breast cancer. While MES may seem promising in machine based diagnostic systems, their accuracy is often impaired by inefficient medical reasoning algorithms employed. This paper therefore seeks to address the limitation of one such reasoning algorithm known as Select and Test (ST). The approach in this paper is to first create an efficient input mechanism that enables the system to read, filter and clean input from datasets. Secondly, semantic web languages (ontologies and rule languages) were used to create a coordinated rule set and a knowledge representation framework was created to aid the reasoning algorithm. As a result, the reasoning structures of ST were modified to accommodate this enhancement. Thereafter, the input generating mechanism was used to transform instances of the databases of Breast Cancer Wisconsin Data set retrieved from UCI Learning Repository. The generated inputs were passed into the improved ST algorithm to diagnose breast cancer in patients captured in the datasets. Experiments were carried out, and result show that 26.60%, 56.17%, and 54.05% were diagnosed of breast cancer in Wisconsin Breast Cancer Database (WBCD), Wisconsin Diagnostic Breast Cancer (WDBC), and Wisconsin Prognostic Breast Cancer (WPBC) respectively.}
}
@article{DESSI2022109945,
title = {SCICERO: A deep learning and NLP approach for generating scientific knowledge graphs in the computer science domain},
journal = {Knowledge-Based Systems},
volume = {258},
pages = {109945},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109945},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122010383},
author = {Danilo Dessí and Francesco Osborne and Diego {Reforgiato Recupero} and Davide Buscaldi and Enrico Motta},
keywords = {Knowledge graph, Scholarly domain, Scientific facts, Artificial intelligence},
abstract = {Science communication has a number of bottlenecks that include the rising number of published research papers and its non-machine-accessible and document-based paradigm, which makes the exploration, reading, and reuse of research outcomes rather inefficient. Recently, Knowledge Graphs (KG), i.e., semantic interlinked networks of entities, have been proposed as a new core technology to describe and curate scholarly information with the goal to make it machine readable and understandable. However, the main drawback of the use of such a technology is that researchers are asked to manually annotate their research papers and add their contributions within the KGs. To address this problem, in this paper we propose SCICERO, a novel KG generation approach that takes in input text from research articles and generates a KG of research entities. SCICERO uses Natural Language Processing techniques to parse the content of scientific papers to discover entities and relationships, exploits state-of-the-art Deep Learning Transformer models to make sense and validate extracted information, and uses Semantic Web best practices to formally represent the extracted entities and relationships, making the written content of research papers machine-actionable. SCICERO has been tested on a dataset of 6.7M papers about Computer Science generating a KG of about 10M entities. It has been evaluated on a manually generated gold standard of 3,600 triples that cover three Computer Science subdomains (Information Retrieval, Natural Language Processing, and Machine Learning) obtaining remarkable results.}
}
@article{DELLACORTE2022207,
title = {Self-reporting data assets and their representation in the pharmaceutical industry},
journal = {Drug Discovery Today},
volume = {27},
number = {1},
pages = {207-214},
year = {2022},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2021.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S1359644621003287},
author = {Dennis {Della Corte} and Wolfgang Colsman and Heiko Fessenmayr and Alexandre {Sawczuk da Silva} and Dana E. Vanderwall},
keywords = {Data format, ADF, Scientific data, JSON, JCAMP-DX, Pharmaceutical data, AnIML, FAIR data, Ontology, Data model},
abstract = {Standardizing data is crucial for preserving and exchanging scientific information. In particular, recording the context in which data were created ensures that information remains findable, accessible, interoperable, and reusable. Here, we introduce the concept of self-reporting data assets (SRDAs), which preserve data and contextual information. SRDAs are an abstract concept, which requires a suitable data format for implementation. Four promising data formats or languages are popularly used to represent data in pharma: JCAMP-DX, JSON, AnIML, and, more recently, the Allotrope Data Format (ADF). Here, we evaluate these four options in common use cases within the pharmaceutical industry using multiple criteria. The evaluation shows that ADF is the most suitable format for the implementation of SRDAs.}
}
@article{BANTON2023100673,
title = {Making sense of cranial osteopathy: An interpretative phenomenological analysis},
journal = {International Journal of Osteopathic Medicine},
volume = {50},
pages = {100673},
year = {2023},
issn = {1746-0689},
doi = {https://doi.org/10.1016/j.ijosm.2023.100673},
url = {https://www.sciencedirect.com/science/article/pii/S1746068923000172},
author = {Amanda Banton and Steven Vogel and Geraldine Lee-Treweek},
keywords = {Cranial osteopathy, Interpretative phenomenological analysis, Embodiment, Lifeworld, Metaphor, Ontology of healthcare, Phenomenological experience, Perception of healthcare, Therapeutic relationship},
abstract = {Objective
This study arose from a praxial problem: how best to communicate with patients about the mechanism of cranial osteopathy. The research question was rooted in the phenomenological concept of ‘sense-making’, and was expressed as: ‘What sense do osteopaths and their patients make of the phenomenon of cranial osteopathy?’
Method
Interpretative Phenomenological Analysis (IPA) was used to explore the ‘lived experience’ and embedded sense-making of pairs of osteopaths and patients. Four Fellows of the Sutherland Cranial College of Osteopathy (SCCO) participated, as did one patient of each. The osteopath participants were experienced practitioners, and the patient participants had had positive experiences of cranial osteopathy. The participants were interviewed about their experience of the phenomenon of cranial osteopathy. The semi-structured interviews were audio-recorded, transcribed, and analysed. The analysis was audited alongside the use of a reflexive diary and an account of the theoretical ‘fore-structure’ of the principal investigator, in order to monitor influences on their hermeneutic analysis of the data.
Results
The IPA revealed that both patients and practitioners establish epistemological grounds for their sense-making about their embodied experience of cranial osteopathy (Theme 1: Making sense of sense-making), that they use embodied metaphor and linguistic meta-metaphor to understand their lived experience of cranial osteopathy (Theme 2: Metaphors for mechanisms), and that the mechanism of cranial osteopathy is considered by both patients and practitioners to arise in part from the therapeutic relationship (Theme 3: The meaningful osteopathic relationship).
Conclusions
The main outcome of the study is a hermeneutic model of cranial osteopathy, which posits that the shared, embodied therapeutic relationship facilitates a collaborative rapport which enables the osteopath and the patient to come to an understanding of the source of the patient's malady, and that this understanding is the causal context for the patient's lived experience of better health.}
}
@article{HASSAN2025108128,
title = {Enhanced dysarthria detection in cerebral palsy and ALS patients using WaveNet and CNN-BiLSTM models: A comparative study with model interpretability},
journal = {Biomedical Signal Processing and Control},
volume = {110},
pages = {108128},
year = {2025},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2025.108128},
url = {https://www.sciencedirect.com/science/article/pii/S1746809425006391},
author = {Esraa Hassan and Abeer Saber and Tarek {Abd El-Hafeez} and T. Medhat and Mahmoud Y. Shams},
keywords = {WaveNet, Dysarthria, Audio classification, Signals},
abstract = {Dysarthria, a motor speech disorder commonly associated with neurological conditions such as cerebral palsy and ALS, poses significant challenges for early diagnosis and intervention. Traditional diagnostic methods often rely on subjective assessments, leading to delays in effective treatment. Recent advances in machine learning and deep learning offer promising avenues for automated dysarthria detection, enabling faster and more reliable assessments. However, these approaches often face challenges in handling diverse languages and operating efficiently on resource-constrained devices. In this study, we leverage the TORGO database with cerebral palsy, ALS, or control subjects, along with a Russian-voice dataset, to explore automated dysarthria classification. We evaluate the performance of machine learning models, including Random Forest Classifier and XGBClassifier, in comparison with state-of-the-art deep learning architectures such as CNN-LSTM. Building upon these findings, we suggest an enhanced WaveNet-based model specifically optimized to capture the temporal characteristics of audio signals across large and diverse datasets. To address real-world challenges, we introduce a Real-Time Adaptation Framework, enabling efficient and accurate dysarthria detection even on resource-constrained devices. Additionally, explainability enhancements using SHAP values and Grad-CAM techniques are integrated, providing insights into the model’s decision-making process and improving its reliability for clinical applications. The suggested WaveNet-based model demonstrates exceptional performance, achieving precision, recall, and F-score values of 0.92040, 0.91979, and 0.91965, respectively, alongside a Matthews Correlation Coefficient (MCC) of 0.89320, outperforming other models. These results highlight the potential of our approach for rapid and reliable dysarthria detection, particularly in multilingual and resource-limited settings. By combining explainability and real-time capabilities, our solution is well-positioned to support healthcare professionals in timely and accurate diagnosis, ultimately enhancing patient care and outcomes.}
}
@article{ALMADHOUN2023101184,
title = {Students’ difficulties with inserting and deleting nodes in a singly linked list in the C programming language},
journal = {Journal of Computer Languages},
volume = {74},
pages = {101184},
year = {2023},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2022.101184},
url = {https://www.sciencedirect.com/science/article/pii/S2590118422000818},
author = {Eman Almadhoun and Jennifer Parham-Mocello},
keywords = {Data structures, Linked lists, C programming language, Reasoning, Misunderstandings},
abstract = {Since linked lists serve as a bridge to understanding more advanced data structures, we believe that it is critical to identify students’ misunderstandings early. We found that students had a good conceptual understanding of how to insert and delete nodes in a singly linked list in C. However, many students continued to struggle with C syntax, pointer manipulation, and memory management needed to correctly implement singly linked lists. Students reported that the abstract nature of pointers, relating linked lists to the real world, and prior knowledge about dynamic arrays contributed the most to their difficulties with linked lists in C.}
}
@article{JIN2025115469,
title = {Network toxicological analysis of sodium dehydroacetate in food safety},
journal = {Food and Chemical Toxicology},
volume = {201},
pages = {115469},
year = {2025},
issn = {0278-6915},
doi = {https://doi.org/10.1016/j.fct.2025.115469},
url = {https://www.sciencedirect.com/science/article/pii/S0278691525002376},
author = {Jing Jin and Yan Xue and Liang Tian},
keywords = {Sodium dehydroacetate, Network toxicological analysis, Food safety, Food additive},
abstract = {Sodium dehydroacetate (Na-DHA), a synthetic preservative under tightened regulations, was evaluated for multi-organ toxicity using network toxicology. ADMETlab3.0 predicted genotoxicity, hepatotoxicity, and carcinogenicity risks. Target mining identified 13 cancer-related, 11 liver injury-related, and 8 genotoxicity-related core genes, with shared hubs (ALOX5, PTGS2, SMAD3, TNF) across pathologies. Functional analyses revealed inflammation, oxidative stress, and immune dysregulation as central mechanisms. KEGG pathway analysis linked cancer/liver injury to AGE-RAGE signaling (TNF, NOX4) and genotoxicity to efferocytosis impairment (PTGS2, ALOX5), suggesting DNA repair disruption. The integrated network demonstrated Na-DHA's pleiotropic effects through convergent pathways, transcending organ-specific toxicity. This systemic profile challenges conventional single-endpoint assessments, advocating comprehensive multi-organ risk evaluation.}
}
@article{NOZZA2021102537,
title = {LearningToAdapt with word embeddings: Domain adaptation of Named Entity Recognition systems},
journal = {Information Processing & Management},
volume = {58},
number = {3},
pages = {102537},
year = {2021},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102537},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321000455},
author = {Debora Nozza and Pikakshi Manchanda and Elisabetta Fersini and Matteo Palmonari and Enza Messina},
keywords = {Named Entity Recognition, Domain adaptation, Word embeddings},
abstract = {The task of Named Entity Recognition (NER) is aimed at identifying named entities in a given text and classifying them into pre-defined domain entity types such as persons, organizations, locations. Most of the existing NER systems make use of generic entity type classification schemas, however, the comparison and integration of (more or less) different entity types among different NER systems is a complex problem even for human experts. In this paper, we propose a supervised approach called L2AWE (Learning To Adapt with Word Embeddings) which aims at adapting a NER system trained on a source classification schema to a given target one. In particular, we validate the hypothesis that the embedding representation of named entities can improve the semantic meaning of the feature space used to perform the adaptation from a source to a target domain. The results obtained on benchmark datasets of informal text show that L2AWE not only outperforms several state of the art models, but it is also able to tackle errors and uncertainties given by NER systems.}
}
@article{BUSELIC2025741385,
title = {Unravelling the intricate language of fish guts: Impact of plant-based vs. plant-insect-poultry-based diets on intestinal pathways in European seabass},
journal = {Aquaculture},
volume = {594},
pages = {741385},
year = {2025},
issn = {0044-8486},
doi = {https://doi.org/10.1016/j.aquaculture.2024.741385},
url = {https://www.sciencedirect.com/science/article/pii/S0044848624008469},
author = {Ivana Bušelić and Željka Trumbić and Jerko Hrabar and Ivana Lepen-Pleić and Tanja Šegvić-Bubić and Elisavet Kaitetzidou and Emilio Tibaldi and Ivana Bočina and Leon Grubišić and Elena Sarropoulou},
keywords = {RNA-Seq, Intestinal metabolism, Lipid metabolism, Alternative feed, Nutrigenomic},
abstract = {The long-term sustainability of aquaculture depends on finding economically viable and environmentally friendly feed ingredients to reduce the use of fishmeal and fish oil. An optimal strategy for the industry is not to identify substitutes or alternatives, but to find a combination of complementary raw materials that together meet the specific nutritional requirements for a given farmed fish species. The study aimed to examine the effects of different diets on the pyloric caeca and distal intestine of subadult European seabass (Dicentrarchus labrax) by applying transcriptomics and transmission electron microscopy. The study examined three dietary approaches: the classic fishmeal-based diet, a plant-based diet, and a plant-insect-poultry-based diet. The distal intestine was more sensitive to dietary changes than the pyloric caeca. The differentially expressed genes in both experimental diets were mainly involved in the digestion and absorption of proteins, fats, and vitamins. The overall transcriptomic changes were greater in the plant-based group than in the plant-insect-poultry-based group and included a greater number of overrepresented metabolic and signalling pathways. In contrast to the transcriptomic results, the ultrastructural findings showed decreased inflammation and/or evidence of tissue repair in the plant-based group, particularly in the pyloric caeca. Since the nutritional quality of all fish groups in this study was previously evaluated positively, the detected transcriptome-level changes can serve as evidence supporting the efficient nutrient utilisation and adaptability in European seabass. The study provides valuable insight into the potential benefits and implications of these dietary modifications on intestinal health and pathway regulation in European seabass. This can serve as a basis for further development of sustainable European seabass aquaculture practices and optimisation of diet formulations.}
}
@article{BOZYIGIT202171,
title = {Linking software requirements and conceptual models: A systematic literature review},
journal = {Engineering Science and Technology, an International Journal},
volume = {24},
number = {1},
pages = {71-82},
year = {2021},
issn = {2215-0986},
doi = {https://doi.org/10.1016/j.jestch.2020.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S2215098620342580},
author = {Fatma Bozyiğit and Özlem Aktaş and Deniz Kılınç},
keywords = {Software requirements, Concept identification, Conceptual model, Systematic literature review},
abstract = {Identification of stakeholder needs and documentation of software requirements are the critical steps to launch a software project. Natural language requirements serve as an agreement among the project stakeholders and they must be transformed into easy-to-understand conceptual models to avoid communication problems. Although conceptual models are mostly created manually with human involvement from the software team, it is seen in recent times that there is a significant increase in studies that automatically generate conceptual models from software requirements. In this study, a Systematic Literature Review (SLR) based on the search of forty-four primary studies (published between 1996 and 2020), which automatically transform software requirements into conceptual models, is reported. These studies are evaluated regarding their approaches, functionalities, dataset used, evaluation methods, generated model types, and languages supported. Finally, several improvable points in the current approaches are highlighted and suggestions are provided as further works.}
}
@article{LIU2024112041,
title = {Event extraction as machine reading comprehension with question-context bridging},
journal = {Knowledge-Based Systems},
volume = {299},
pages = {112041},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112041},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124006750},
author = {Liu Liu and Ming Liu and Shanshan Liu and Kun Ding},
keywords = {Event extraction, Natural language processing, Graph neural network, Machine reading comprehension},
abstract = {Most existing methods regard event extraction as the classification task. They not only heavily rely on named entity recognition, causing error propagation, but are also inefficient in low resource scenarios. To deal with above challenges, we propose an improved machine reading comprehension (MRC) approach, namely MRCBEE. Firstly, a new paradigm is applied to redefine event extraction as MRC task by designing question templates for event detection and argument extraction tasks. Specially, Question-Context Bridging is a new graph structure drawn to reconstruct the semantic relationship between the template and the text, in order to strengthen the prompt role of question templates. Then, a cross-domain attention module is designed to integrate both the semantic feature and global feature of words. A new GNN based on gated mechanism is proposed to capture the global feature and filter the information of invalid neighbors. Finally, the results of our experiments show that MRCBEE achieves better performance than the state-of-the-art methods on ACE2005 and ERE datasets. And it outperforms prior methods in low resource and complex text scenarios.}
}
@article{ZHOU2024103815,
title = {Chinese nouns are mass nouns: An information-theoretic computational proof},
journal = {Lingua},
volume = {311},
pages = {103815},
year = {2024},
issn = {0024-3841},
doi = {https://doi.org/10.1016/j.lingua.2024.103815},
url = {https://www.sciencedirect.com/science/article/pii/S0024384124001463},
author = {Wei Zhou and Guangyan Zhang and Yujie Chen},
keywords = {Mass noun, Classifiers, Atomicity, Homogeneity, Chinese nouns, Mutual information},
abstract = {The mass–count distinction in Mandarin Chinese has been heatedly debated in linguistics. Previous research investigated the mass–count distinction primarily through qualitative methods. Our study applied mutual information and created three variation conditions to explore the relationship between Chinese nouns and the individuation function of classifiers. Furthermore, we examined the mass–count distinction by quantitatively analyzing 1,000 instances of Num–CL–N (numeral–classifier–noun) structures. The computational results indicate that the individuation function of classifiers is influenced by noun homogeneity. Moreover, we argue that Chinese nouns exhibit an inclination for homogeneity and the deep semantic processing of nouns is similar to classes or sets, providing evidence for the broad mass noun hypothesis and the collective-name hypothesis in the philosophy of language.}
}
@article{KHATTAK202153,
title = {A survey on sentiment analysis in Urdu: A resource-poor language},
journal = {Egyptian Informatics Journal},
volume = {22},
number = {1},
pages = {53-74},
year = {2021},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2020.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1110866520301171},
author = {Asad Khattak and Muhammad Zubair Asghar and Anam Saeed and Ibrahim A. Hameed and Syed {Asif Hassan} and Shakeel Ahmad},
keywords = {Urdu sentiment analysis, Pre-processing, Sentiment lexicon, Datasets, Corpus, Urdu sentiment classification, Semantic orientation},
abstract = {Background/introduction
The dawn of the internet opened the doors to the easy and widespread sharing of information on subject matters such as products, services, events and political opinions. While the volume of studies conducted on sentiment analysis is rapidly expanding, these studies mostly address English language concerns. The primary goal of this study is to present state-of-art survey for identifying the progress and shortcomings saddling Urdu sentiment analysis and propose rectifications.
Methods
We described the advancements made thus far in this area by categorising the studies along three dimensions, namely: text pre-processing lexical resources and sentiment classification. These pre-processing operations include word segmentation, text cleaning, spell checking and part-of-speech tagging. An evaluation of sophisticated lexical resources including corpuses and lexicons was carried out, and investigations were conducted on sentiment analysis constructs such as opinion words, modifiers, negations.
Results and conclusions
Performance is reported for each of the reviewed study. Based on experimental results and proposals forwarded through this paper provides the groundwork for further studies on Urdu sentiment analysis.}
}
@article{ZHANG2023311,
title = {The Ethical Turn of Emerging Design Practices},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {9},
number = {3},
pages = {311-329},
year = {2023},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2023.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2405872623000618},
author = {Li Zhang},
keywords = {Ethical design, Emerging design practices, Ethical turn, Design ethics, Alternative},
abstract = {Ignited by the transformative impact of technoscience, diverse alternative design practices have emerged, each distinct from prevailing norms. Within this heterogeneous landscape, a common theme emerges: an inclination toward ethical considerations, closely tied to or stemming from these novel approaches and practices, warranting thorough investigation. This article adopts a dual strategy. First, it introduces technoscience as the catalyst behind this transition, framing the contextual backdrop. Second, based on Jürgen Habermas’s knowledge-interest theory and its tripartite structure, this article focuses on those emerging design practices which deal with ethical and social design, a category that has become especially significant since 2000. The subsequent analysis explores the significant ethical turn within diverse disciplines and these emerging design practices, illustrated across domains. This transformation unfolds in ontological, epistemological, and methodological dimensions, signifying a renewal of design ethics. The article establishes a fundamental framework to comprehend a field without rigid boundaries, facilitating future critique and ongoing design research progress.}
}
@article{WANG2025200300,
title = {Identifying translation biases of cognate terms in Chinese textbooks: A GloVe-based method with semantic similarity analysis},
journal = {Systems and Soft Computing},
volume = {7},
pages = {200300},
year = {2025},
issn = {2772-9419},
doi = {https://doi.org/10.1016/j.sasc.2025.200300},
url = {https://www.sciencedirect.com/science/article/pii/S2772941925001188},
author = {Youjing Wang},
keywords = {Translation of words, Bias, Binary statistics, GloVe model, Chinese as a foreign language, Textbooks, English},
abstract = {With the increasing internationalization of China and the expansion of Chinese language teaching programs, the issue of translation bias in Chinese language textbooks has become increasingly prominent. To improve the accuracy of identifying translation biases in cognate terms, this study proposes a diagnostic method based on Global Vectors for Word Representation (GloVe) word embeddings and semantic similarity. The experimental results indicate that the proposed method achieves a maximum diagnostic accuracy of 0.93, with the best diagnostic accuracy for pronouns reaching 0.95, an improvement of 0.8 compared to traditional methods. Additionally, the proposed method maintains an F1 score ranging from 0.85 to 0.94 across multiple datasets, with the lowest false positive rate. This method effectively improves the translation quality of cognate terms in Chinese language textbooks, enhances the scientific accuracy of semantic expression evaluation, and provides strong support for improving textbook proofreading efficiency and the quality of Chinese language teaching.}
}
@article{DUO2025111004,
title = {Prognostic and immune microenvironment analysis of cuproptosis-related lncRNAs in human pancreatic cancer},
journal = {Computers in Biology and Medicine},
volume = {197},
pages = {111004},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.111004},
url = {https://www.sciencedirect.com/science/article/pii/S0010482525013563},
author = {Hong Duo and Jun Luo and Dilinigeer Tayier and Jian You and Cuijiao Tian and Siqi Du and Qifa Ye and Wei Zhou and Zhiyong Yang and Zhiliang Wang},
keywords = {Pancreatic cancer, Cuproptosis, LncRNA, Tumor mutation burden, Prognostic marker},
abstract = {Pancreatic cancer remains one of the most lethal malignancies, with limited treatment options and poor prognosis. Emerging evidence implicates cuproptosis-related long non-coding RNAs (lncRNAs) in tumor progression, though their roles in pancreatic cancer remain unexplored. To identify cuproptosis-linked lncRNAs in pancreatic cancer, RNA-seq data from The Cancer Genome Atlas (TCGA) were evaluated. A prognostic model was developed utilizing multivariate Cox regression that focused on important lncRNAs. Additionally, analyses were executed including Kaplan-Meier survival analysis, receiver operating characteristic (ROC) curves, as well as Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway assessments. Evaluations of tumor mutation burden (TMB) and the immune microenvironment were also carried out. Our research corroborated the results using human pancreatic cancer tissue samples, pinpointing LINC00900 and AC078820.1 as lncRNAs significantly correlated with overall survival, thus underscoring their potential as prognostic indicators and therapeutic targets in pancreatic cancer. LINC00900 indicated a favorable prognosis, whereas AC078820.1 was associated with poorer outcomes. The findings from GO and KEGG analyses illustrated their roles in regulating immune responses and involvement in the PPAR signaling pathway. High-risk group patients showed an increased TMB, with notable mutations in genes such as KRAS and TP53. Furthermore, analysis of the immune microenvironment indicated heightened cytolytic activity, T-cell co-stimulation, and expression of immune checkpoints in the high-risk group, suggesting possibilities for immune evasion. LINC00900 and AC078820.1 emerge as promising prognostic indicators in pancreatic cancer, offering potential implications for personalized treatment strategies that focus on cuproptosis-related pathways.}
}
@article{VERLEYEN2024872,
title = {A systematic review and cross-database analysis of single nucleotide polymorphisms underlying hip morphology and osteoarthritis reveals shared mechanisms},
journal = {Osteoarthritis and Cartilage},
volume = {32},
number = {8},
pages = {872-885},
year = {2024},
issn = {1063-4584},
doi = {https://doi.org/10.1016/j.joca.2024.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S1063458424012111},
author = {Marlies Verleyen and Yukun He and Arne Burssens and Marta Santana Silva and Bert Callewaert and Emmanuel Audenaert},
keywords = {SNP, GWAS, Hip, Morphology, Gene Ontology},
abstract = {Summary
Objective
Understanding the mechanisms of hip disease, such as osteoarthritis (OA), is crucial to advance their treatment. Such hip diseases often involve specific morphological changes. Genetic variations, called single nucleotide polymorphisms (SNPs), influence various hip morphological parameters. This study investigated the biological relevance of SNPs correlated to hip morphology in genome-wide association studies (GWAS). The SNP-associated genes were compared to genes associated with OA in other joints, aiming to see if the same genes play a role in both hip development and the risk of OA in other lower limb joints.
Methodology
A systematic literature review was conducted to identify SNPs correlated with hip morphology, based on the Population, Intervention, Comparison, Outcome, and Study (PICOS) framework. Afterwards, Gene Ontology (GO) analysis was performed, using EnrichR, on the SNP-associated genes and compared with non-hip OA-associated genes, across different databases.
Results
Reviewing 49 GWAS identified 436 SNPs associated with hip joint morphology, encompassing variance in bone size, structure and shape. Among the SNP-associated genes, SOX9 plays a pivotal role in size, GDF5 impacts bone structure, and BMP7 affects shape. Overall, skeletal system development, regulation of cell differentiation, and chondrocyte differentiation emerged as crucial processes influencing hip morphology. Eighteen percent of GWAS-identified genes related to hip morphology were also associated with non-hip OA.
Conclusion
Our findings indicate the existence of multiple shared genetic mechanisms across hip morphology and OA, highlighting the necessity for more extensive research in this area, as in contrast to the hip, the genetic background on knee or foot morphology remains largely understudied.}
}
@article{OPPERMANN2021100070,
title = {Finding and analysing energy research funding data: The EnArgus system},
journal = {Energy and AI},
volume = {5},
pages = {100070},
year = {2021},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100070},
url = {https://www.sciencedirect.com/science/article/pii/S2666546821000240},
author = {Leif Oppermann and Simon Hirzel and Alexander Güldner and Karoline Heiwolt and Joachim Krassowski and Ulrich Schade and Christoph Lange and Wolfgang Prinz},
keywords = {Energiewende, Information System, Wiki, Ontology, Databases, Evaluation},
abstract = {This paper presents the concept, a system-overview, and the evaluation of EnArgus, the central information system for energy research funding in Germany. Initiated by the German Federal Ministry for Economic Affairs and Energy (BMWi), EnArgus establishes a one-stop information system about all recent and ongoing energy research funding projects in Germany. Participants ranging from laypersons to experts were surveyed in three workshops to evaluate both the public and expert interfaces of the EnArgus system in comparison to peer systems. The results showed that the EnArgus system was predominantly evaluated positively by the various participants. It contributes to making the energy sector more transparent and offers clear advantages for professional use compared to similar systems. The system’s semantic processing enables more precise hits and better coverage by including semantically related terms in search results; its intelligence makes it fail-safe, rendering it suitable for areas where poor results can have dire consequences. Reporting on an actual real-world system, the paper also provides a roadmap-view of how electronic filing of administrative project data can be semantically enhanced and opened-up to provide the basis for new ways into the data that are key for future breakthrough AI interfaces.}
}
@article{XU2024105579,
title = {Semantic model-based large-scale deployment of AI-driven building management applications},
journal = {Automation in Construction},
volume = {165},
pages = {105579},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105579},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524003157},
author = {Kan Xu and Zhe Chen and Fu Xiao and Jing Zhang and Hanbei Zhang and Tianyou Ma},
keywords = {Smart building energy management, Data-driven application, Brick schema, Large-scale deployment, Semantic model},
abstract = {Digitalization and Artificial Intelligent (AI) are revolutionizing building operation management. The abundance of data generated with the digitalization of buildings in the whole lifecycle can be harnessed to enhance building operational efficiency through data-driven control and optimization applications. However, the heterogeneity of data across building datasets hampers data interactivity and interoperability, presenting obstacles to the large-scale deployment of AI-enabled data-driven solutions. A semantic model-based framework is developed to integrate multi-sources data from buildings' air-conditioning system, supporting the large-scale deployment of AI-enabled data-driven building management applications. Both static and temporal data from multi sources are stored in the database guided by the semantic model. To demonstrate the framework's effectiveness, a building cooling load prediction application is implemented and evaluated across three typical buildings. The successful deployment of the proposed semantic model-based framework demonstrates its potential for facilitating large-scale deployment of AI-enabled data-driven applications in building sector.}
}
@article{BALAJI20181273,
title = {Brick : Metadata schema for portable smart building applications},
journal = {Applied Energy},
volume = {226},
pages = {1273-1292},
year = {2018},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2018.02.091},
url = {https://www.sciencedirect.com/science/article/pii/S0306261918302162},
author = {Bharathan Balaji and Arka Bhattacharya and Gabriel Fierro and Jingkun Gao and Joshua Gluck and Dezhi Hong and Aslak Johansen and Jason Koh and Joern Ploennigs and Yuvraj Agarwal and Mario Bergés and David Culler and Rajesh K. Gupta and Mikkel Baun Kjærgaard and Mani Srivastava and Kamin Whitehouse},
keywords = {Smart buildings, Building management, Metadata, Schema, Ontology},
abstract = {Buildings account for 32% of worldwide energy usage. A new regime of exciting new “applications” that span a distributed fabric of sensors, actuators and humans has emerged to improve building energy efficiency and operations management. These applications leverage the technological advances in embedded sensing, processing, networking and methods by which they can be coupled with supervisory control and data acquisition systems deployed in modern buildings and with users on mobile wireless platforms. There are, however, several technical challenges to confront before such a vision of smart building applications and cyber-physical systems can be realized. The sensory data produced by these systems need significant curation before it can be used meaningfully. This is largely a manual, cost-prohibitive task and hence such solutions rarely experience widespread adoption due to the lack of a common descriptive schema. Recent attempts have sought to address this through data standards and metadata schemata but fall short in capturing the richness of relationships required by applications. This paper describes Brick, a uniform metadata schema for representing buildings that builds upon recent advances in the area. Our schema defines a concrete ontology for sensors, subsystems and the relationships between them, which enables portable applications. We demonstrate the completeness and effectiveness of Brick by using it to represent the entire vendor-specific sensor metadata of six diverse buildings across different campuses, comprising 17,700 data points, and running eight unmodified energy efficiency applications on these buildings.}
}
@article{WOLFENSTETTER2018342,
title = {Introducing TRAILS: A tool supporting traceability, integration and visualisation of engineering knowledge for product service systems development},
journal = {Journal of Systems and Software},
volume = {144},
pages = {342-355},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.06.079},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218301365},
author = {Thomas Wolfenstetter and Mohammad R. Basirati and Markus Böhm and Helmut Krcmar},
keywords = {Model-based systems engineering, Traceability, Product service systems, Model integration},
abstract = {Developing state of the art product service systems (PSS) requires the intense collaboration of different engineering domains, such as mechanical, software and service engineering. This can be a challenging task, since each engineering domain uses their own specification artefacts, software tools and data formats. However, to be able to seamlessly integrate the various components that constitute a PSS and also being able to provide comprehensive traceability throughout the entire solution life cycle it is essential to have a common representation of engineering data. To address this issue, we present TRAILS, a novel software tool that joins the heterogeneous artefacts, such as process models, requirements specifications or diagrams of the systems structure. For this purpose, our tool uses a semantic model integration ontology onto which various source formats can be mapped. Overall, our tool provides a wide range of features that supports engineers in ensuring traceability, avoiding system inconsistencies and putting collaborative engineering into practice. Subsequently, we show the practical implementation of our approach using the case study of a bike sharing system and discuss limitations as well as possibilities for future enhancement of TRAILS.}
}
@article{SCHLACHTER2022104258,
title = {Using Linked Building Data for managing temporary construction items},
journal = {Automation in Construction},
volume = {139},
pages = {104258},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104258},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522001315},
author = {Alexander Schlachter and Mads Holten Rasmussen and Jan Karlshøj},
keywords = {Linked Building Data, Temporary construction items, BIM},
abstract = {For decades, the construction industry has experienced poor productivity due to challenges such as increasing project complexity and a fragmented project environment. Even though some technological innovations around Building Information Modeling (BIM) might have the potential to overcome these challenges, data integration across disciplines, companies and software solutions is yet to be solved entirely. Trending advancements try to enrich existing BIM data using Linked Data technologies to semantically describe the building information and facilitate data integration. By that, project data from different data sources is made available in an accessible format, so project participants can use it for their planning efforts. In this paper we explore the use of Linked Building Data (LBD) on a specific use case to answer the question of how the planning of Temporary Construction Items (TCIs) can be improved by integrating data and automating the demand calculation. A literature review concludes that TCIs only experience little attention in the current planning of construction projects but have a critical impact on the outcome of a project. Thus, the objective of this paper is to develop standard ontologies to provide a semantically rich terminology of the data and to propose a framework for TCI consideration within a BIM based project delivery system. A prototype solution is developed, taking formwork as a TCI representative. The result is a process for automatically creating a TCI utilization plan that quantifies the precise time- and location-based on-site TCI demand by integrating data from BIM, Location-Based Scheduling (LBS) and TCI information. Based on the results of prototyping and findings from expert interviews, this research integrates the solution into the process of construction and finally proposes two implementation scenarios for the solution – one being based on the current industry situation and one exploring the future vision of a more integrated and decentralized project delivery in the construction industry.}
}
@article{GARG2024101649,
title = {IKDSumm: Incorporating key-phrases into BERT for extractive disaster tweet summarization},
journal = {Computer Speech & Language},
volume = {87},
pages = {101649},
year = {2024},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101649},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824000329},
author = {Piyush Kumar Garg and Roshni Chakraborty and Srishti Gupta and Sourav Kumar Dandapat},
keywords = {Social media, Disaster events, Tweet summarization, Key-phrase extraction},
abstract = {Online social media platforms, such as Twitter, are one of the most valuable sources of information during disaster events. Humanitarian organizations, government agencies, and volunteers rely on a concise compilation of such information for effective disaster management. Existing methods to make such compilations are mostly generic summarization approaches that do not exploit domain knowledge. In this paper, we propose a disaster-specific tweet summarization framework, IKDSumm, which initially identifies the crucial and important information from each tweet related to a disaster through key-phrases of that tweet. We identify these key-phrases by utilizing the domain knowledge (using existing ontology) of disasters without any human intervention. Further, we utilize these key-phrases to automatically generate a summary of the tweets. Therefore, given tweets related to a disaster, IKDSumm ensures fulfillment of the summarization key objectives, such as information coverage, relevance, and diversity in summary without any human intervention. We evaluate the performance of IKDSumm with 8 state-of-the-art techniques on 12 disaster datasets. The evaluation results show that IKDSumm outperforms existing techniques by approximately 2−79% in terms of ROUGE-N F1-score.}
}
@article{KARATAS2023e19278,
title = {Determining the dominant understandings in security science literature in Turkey: A bibliometric analysis on the journal of security science},
journal = {Heliyon},
volume = {9},
number = {8},
pages = {e19278},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e19278},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023064861},
author = {Adnan Karataş},
keywords = {Security science, Scientific research, Bibliometric analysis, Content analysis, Ontology, Methodology},
abstract = {The development of a scientific discipline is closely related to the quantity and quality of research conducted within that discipline. Therefore, examining the studies published in a particular field is crucial for evaluating and understanding its development. While there are numerous national and international studies that assess the quality and quantity of research in various disciplines, no such study has been found specifically for the field of security science. This study aims to determine the subjects, approaches, and methods used in research within the field of security science. By analyzing 129 articles published in the Journal of Security Sciences between 2012 and 2021, the study seeks to provide insights and recommendations for the advancement of research in this field. The articles were subjected to bibliometric analysis, considering various criteria such as research objectives, methods, topics, and theoretical foundations. The contributions of these studies to the development of the discipline were evaluated. Based on the analysis of these articles, it is evident that methodological aspects of studies in this field are still in the early stages. The most significant deficiency identified is the lack of practical research in the analyzed articles. Conducting empirical studies is deemed essential for the advancement of security science. Consequently, several suggestions are proposed for future research. Moreover, an evaluation is provided as a foundation for future studies in the field of security science.}
}
@article{QIN2018394,
title = {Construction of personal knowledge maps for a peer-to-peer information-sharing environment},
journal = {The Electronic Library},
volume = {36},
number = {3},
pages = {394-413},
year = {2018},
issn = {0264-0473},
doi = {https://doi.org/10.1108/EL-03-2017-0071},
url = {https://www.sciencedirect.com/science/article/pii/S0264047318000140},
author = {Chunxiu Qin and Pengwei Zhao and Jian Mou and Jin Zhang},
keywords = {Classification, Ontology, Distributed document sharing, Knowledge map, Knowledge structure, P2P environment, Self-organizing map},
abstract = {Purpose
Browsing knowledge documents in a peer-to-peer (P2P) environment is difficult because knowledge documents in such an environment are large in quantity and distributed over different peers who organize the documents according to their own views. This paper aims to propose a method for constructing a personal knowledge map for a peer to facilitate knowledge browsing and alleviate information overload in P2P environments.
Design/methodology/approach
The research presents a method for constructing a personal knowledge map. The method adopts an ontology-concept-tree-based classification algorithm to recognize a peer’s personal knowledge structure and construct a personal knowledge map, and uses a self-organizing map algorithm to cluster and visualize the knowledge documents. The correctness of the created knowledge map is evaluated with a collection of abstracts of academic papers.
Findings
The method for constructing a personal knowledge map is the main finding of this research. The evaluation shows that the created knowledge map is good in quality.
Research limitations/implications
The proposed method provides a way for P2P platforms to understand their users’ knowledge background, as well as to improve the P2P platform environment. However, the proposed method will not help a peer when he has nothing in his individual knowledge document repository (i.e. the “cold start” problem). The method also requires a relatively good ontology base for a P2P document sharing system to use the method effectively.
Originality/value
It is novel that the proposed method organizes the knowledge documents related to a peer’s knowledge background into a personal knowledge map. Moreover, the created knowledge map combines the advantages of a hierarchical display and a map display. It has values for a distributed P2P environment to facilitate users’ knowledge browsing and to alleviate information overload.}
}
@article{ISAZA2022250,
title = {Classifying cybergrooming for child online protection using hybrid machine learning model},
journal = {Neurocomputing},
volume = {484},
pages = {250-259},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.08.148},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221016489},
author = {Gustavo Isaza and Fabián Muñoz and Luis Castillo and Felipe Buitrago},
keywords = {Natural language processing, Convolutional neural networks, Cybergrooming detection, Semantic analysis},
abstract = {This paper shows a computational model that classifies Cybergrooming attacks in the context of COP (child online protection) using Natural Language Processing (NLP) and Convolutional Neural Networks (CNN). The model predicts a high number of false positives, therefore low precision and F-score, but a high accuracy. In this issue, where the number of messages in the context of grooming are so low compared to the number of conversations and messages from other contexts, it can be concluded that is a very consistent and useful result as it captures a high number of true positives, considering that the classifier works for messages. Performing the training of machine learning algorithms with neural networks, semantic analysis and NLP, allows approximate representation of knowledge contributing to discovery of pseudo-intelligent information in these environments and reducing human intervention for characterization of underlying abnormal behavior and detecting messages that potentially represent these attacks.}
}
@article{TUCAR2018116,
title = {Semantic Web Service Composition based on Graph Search},
journal = {Procedia Computer Science},
volume = {126},
pages = {116-125},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.07.215},
url = {https://www.sciencedirect.com/science/article/pii/S187705091831189X},
author = {Liana Ţucăr and Paul Diac},
keywords = {Service Composition, Semantic Services, Ontologies, Polynomial Time, Graph, Heuristic, Dynamic Scoring},
abstract = {In Web Service Composition, services or APIs from multiple and independent providers are combined to create new functionality. Web Service Challenges have formalized the problem at a series of events where the community competed with various solutions. The composition problem has been incrementally extended adding more expressiveness and since 2006 the semantic aspect was addressed by the use of ontologies for service parameter description. In this paper, we propose a polynomial-time algorithm based on graph search, that solves the 2008 challenge. The algorithm uses a heuristic to address the NP-Hard problem of optimizing the number of services selected. Experimental results show that our solution is several times faster and generates shorter compositions on all evaluation tests compared with all winning solutions of the competition. Also, it is up to 50 times faster and very close to generate shortest compositions on the tests, compared with the solution published in 2011, that generates optimal compositions.}
}
@article{GUO2025103372,
title = {A knowledge graph-based standardized modeling and quantitative retrieval method for product defect analysis-related knowledge},
journal = {Advanced Engineering Informatics},
volume = {66},
pages = {103372},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103372},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625002654},
author = {Jingyu Guo and Yalin Wang and Shuxian Li and Guanlin Chen},
keywords = {Product defect analysis, Knowledge standardized expression model, Multi-dimensional schema layer, Virtual relational logic node, Knowledge credibility, Quantitative multi-stage knowledge retrieval},
abstract = {Product defect analysis (PDA) is vital to improving product quality and reducing production costs. However, currently PDA related knowledge is stored dispersedly due to the lack of standardized expression. This makes it time-consuming to retrieve, reuse, and validate knowledge, which limits the efficiency and accuracy of PDA. Therefore, this paper proposes a KG-based standardized modeling method including a schema layer to constrain the organization format of knowledge and a data layer to store knowledge. For the schema layer, a dual-layer standardized organizational model is designed to organize the actual concepts and their relationships in the PDA field. Relationship attributes are introduced to quantitatively describe the knowledge credibility. Virtual relational logic nodes and their triggering mechanism are defined to accurately describe complex relationships such as multiple factors synchronizing to cause a defect or multiple operations executing simultaneously to resolve a defect. Based on the above, a design method for the multi-dimensional schema layer is proposed where the horizontal dimension distinguishes the virtual relational logic nodes and actual entities in the PDA field, while the vertical dimension reflects the inheritance and derivation relationships between entities, achieving clear organization of knowledge; For the data layer, an industrial text analysis system is developed, incorporating a virtual node recognition method guided by the keyword dictionary and a relationship matching method constrained by the schema layer for knowledge extraction. To achieve efficient knowledge retrieval and knowledge-driven intelligent PDA, knowledge retrieval path and its credibility calculation method are defined under the above knowledge expression model. Afterward, a multi-stage quantitative retrieval mechanism including rule-guided retrieval path query and credibility-based optimal path selection is proposed. Injection molding, a mainstream plastic production method, covers multiple stages such as mold design, raw material selection, injection, pressure holding, cooling, and demolding, each stage involving different knowledge. As personalized product demands grow, knowledge updates become frequent and knowledge credibility varies. Therefore, taking it as a typical case, a KG containing 1608 triples is constructed to verify the effectiveness of our method in practical PDA tasks. Based on this KG and the proposed retrieval mechanism, high-quality knowledge retrieval results can be quantitatively generated, covering the possibility, causes, and solutions of defects. The ‘result relevance’ between its results and the actual production results reaches 93.5%, significantly better than traditional methods, demonstrating higher accuracy and interpretability. Overall, this paper lays a foundation for fully utilizing the value of knowledge to improve the efficiency of PDA.}
}
@article{CAESAR2019307,
title = {Context-sensitive reconfiguration of collaborative manufacturing systems},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {13},
pages = {307-312},
year = {2019},
note = {9th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.194},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319311449},
author = {Birte Caesar and Michael Nieke and Aljosha Köcher and Constantin Hildebrandt and Christoph Seidl and Alexander Fay and Ina Schaefer},
keywords = {Software Product Lines, Collaborative Systems, Open, Dynamic Context, Reconfigurable Manufacturing Systems, Ontology Creation},
abstract = {To stay competitive in a highly dynamic environment, manufacturing companies have to quickly react to changing customer demands. Manufacturing systems may only serve demands that are covered by their capabilities. The manufacturability of a product can be analyzed by comparing the provided capabilities against the product’s requirements. The manufacturing capabilities depend on the current system and subsystem configuration. If a product cannot be manufactured, first, it must be analyzed whether any valid configuration exists that provides the required capabilities, and second, the system has to be reconfigured according to the new configuration. To the best of our knowledge no existing method exists that enables these previously mentioned steps. In this paper, we introduce a method for context-sensitive reconfiguration of multiple collaborating manufacturing systems that might come from different vendors to create a customized product on demand. We utilize variability models to capture the possible configuration space and describe influences of product properties on features of the variability model by providing a context-sensitive variability model. We apply our method to a demonstrator and show that we enable modeling of reconfigurable manufacturing systems that are automatically reconfigured on context change.}
}
@article{ZHANG201878,
title = {Two-way negotiation for intelligent hotel reservation based on multiagent: The model and system},
journal = {Knowledge-Based Systems},
volume = {161},
pages = {78-89},
year = {2018},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2018.07.032},
url = {https://www.sciencedirect.com/science/article/pii/S0950705118303836},
author = {Jinyu Zhang and Xuechun Luo and Yao Zhou and Weiwei Ruan and Y. Jiang and Zhifeng Hao},
keywords = {Intelligent agents, Agent collaboration, Electronic commerce, Two-way negotiation},
abstract = {Multiagent-based systems have been widely used in hotel reservations. Previous studies, however, mainly use one-way negotiation for decisions, which means that agents represent travelers selecting from a list of hotels to decide which room to reserve, while hotel providers cannot communicate sales strategies with travelers according to real-time situations to maximize profits. To address such problems, this paper proposes a Multiagent-Based Two-Way Negotiation for Hotel Reservation (MAB-TNHR) with three kinds of agents: agents representing the behavior of people (Tenant Agent and Landlord Agent), agents used for collecting data (Data Integrating Agent and Results Agent) and agents for system purposes (Controller Agent and Selector Agent) that together continuously observe the order requests and forecast their potential tenders to a number of qualified landlord agents based on the ontologies of collaborating participants. The traditional tenant-dominant reservation is converted into a form of tendering to implement this two-way negotiation, which means that Landlord Agents can actively respond to the selection by sending their tenders to suitable tenants and bargain on the price with the Tenant Agent to pursue their interests. In this paper, rules used in the application and a case study are presented to illustrate the implementation of MAB-TNHR. Verification shows that the MAB-TNHR can meet intricate and dynamic reservation demands automatically without the participations of tenants and landlords.}
}
@article{CLEMENTS2020571,
title = {A conceptual framework for digital civics pedagogy informed by the philosophy of information},
journal = {Journal of Documentation},
volume = {76},
number = {2},
pages = {571-585},
year = {2020},
issn = {0022-0418},
doi = {https://doi.org/10.1108/JD-07-2019-0139},
url = {https://www.sciencedirect.com/science/article/pii/S0022041820000363},
author = {Estelle Clements},
keywords = {Philosophy of information, Educational philosophy, Civics education, Digital civics, Digital teaching and learning, Information ethics},
abstract = {Purpose
The purpose of this paper is to draw on the philosophy of information, specifically the work of Luciano Floridi, to argue that digital civics must fully comprehend the implications of the digital environment, and consequently an informational ontology, to deliver to students an education that will prepare them for full participation as citizens in the infosphere.
Design/methodology/approach
Introducing this philosophy for use in education, the research discusses the ethical implications of ontological change in the digital age; informational organisms and their interconnectivity; and concepts of agency, both organic and artificial in digitally mediated civic interactions and civic education.
Findings
With the provision of a structural framework rooted in the philosophy of information, robust mechanisms for civics initiatives can be enacted.
Originality/value
The paper allows policy makers and practitioners to formulate healthy responses to digital age challenges in civics and civics education.}
}
@article{SHAKHOVSKA2019229,
title = {Big Data analysis in development of personalized medical system},
journal = {Procedia Computer Science},
volume = {160},
pages = {229-234},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.461},
url = {https://www.sciencedirect.com/science/article/pii/S187705091931676X},
author = {Natalia Shakhovska and Solomia Fedushko and Michal {Greguš ml.} and Natalia Melnykova and Iryna Shvorob and Yuriy Syerov},
keywords = {Big Data, medical system, personalized data, decision-making method, service science, information medical profile},
abstract = {This paper considers the actual problem of Big Data analysis of medical information. Big Data is characterized by heterogeneity and constant growth requires the use of non-standard approaches to storage and processing of data. The analysis of personalized medical information for system development is proposed. The methods of construction of patient information track model based on systematic verification of patient’s personal and medical data is developed. The semantics of the data personalization in decision-making consists of the following stags: stage of forming the ontology of medical knowledge of the medical process and stage of formalizing the process of finding standard solutions. A personalized approach to personalization of standard schemes is proposed by modifying the decision-making method based on decision trees, taking into account the relationship between patient parameters. The result of the method is presented in personalized scheme.}
}
@article{JIN2023,
title = {Natural Language Processing in a Clinical Decision Support System for the Identification of Venous Thromboembolism: Algorithm Development and Validation},
journal = {Journal of Medical Internet Research},
volume = {25},
year = {2023},
issn = {1438-8871},
doi = {https://doi.org/10.2196/43153},
url = {https://www.sciencedirect.com/science/article/pii/S1438887123002613},
author = {Zhi-Geng Jin and Hui Zhang and Mei-Hui Tai and Ying Yang and Yuan Yao and Yu-Tao Guo},
keywords = {venous thromboembolism, deep vein thrombosis, pulmonary embolism, natural language processing, electronic health record},
abstract = {Background
It remains unknown whether capturing data from electronic health records (EHRs) using natural language processing (NLP) can improve venous thromboembolism (VTE) detection in different clinical settings.
Objective
The aim of this study was to validate the NLP algorithm in a clinical decision support system for VTE risk assessment and integrated care (DeVTEcare) to identify VTEs from EHRs.
Methods
All inpatients aged ≥18 years in the Sixth Medical Center of the Chinese People's Liberation Army General Hospital from January 1 to December 31, 2021, were included as the validation cohort. The sensitivity, specificity, positive and negative likelihood ratios (LR+ and LR–, respectively), area under the receiver operating characteristic curve (AUC), and F1-scores along with their 95% CIs were used to analyze the performance of the NLP tool, with manual review of medical records as the reference standard for detecting deep vein thrombosis (DVT) and pulmonary embolism (PE). The primary end point was the performance of the NLP approach embedded into the EHR for VTE identification. The secondary end points were the performances to identify VTE among different hospital departments with different VTE risks. Subgroup analyses were performed among age, sex, and the study season.
Results
Among 30,152 patients (median age 56 [IQR 41-67] years; 14,247/30,152, 47.3% females), the prevalence of VTE, PE, and DVT was 2.1% (626/30,152), 0.6% (177/30,152), and 1.8% (532/30,152), respectively. The sensitivity, specificity, LR+, LR–, AUC, and F1-score of NLP-facilitated VTE detection were 89.9% (95% CI 87.3%-92.2%), 99.8% (95% CI 99.8%-99.9%), 483 (95% CI 370-629), 0.10 (95% CI 0.08-0.13), 0.95 (95% CI 0.94-0.96), and 0.90 (95% CI 0.90-0.91), respectively. Among departments of surgery, internal medicine, and intensive care units, the highest specificity (100% vs 99.7% vs 98.8%, respectively), LR+ (3202 vs 321 vs 77, respectively), and F1-score (0.95 vs 0.89 vs 0.92, respectively) were in the surgery department (all P<.001). Among low, intermediate, and high VTE risks in hospital departments, the low-risk department had the highest AUC (1.00 vs 0.94 vs 0.96, respectively) and F1-score (0.97 vs 0.90 vs 0.90, respectively) as well as the lowest LR– (0.00 vs 0.13 vs 0.08, respectively) (DeLong test for AUC; all P<.001). Subgroup analysis of the age, sex, and season demonstrated consistently good performance of VTE detection with >87% sensitivity and specificity and >89% AUC and F1-score. The NLP algorithm performed better among patients aged ≤65 years than among those aged >65 years (F1-score 0.93 vs 0.89, respectively; P<.001).
Conclusions
The NLP algorithm in our DeVTEcare identified VTE well across different clinical settings, especially in patients in surgery units, departments with low-risk VTE, and patients aged ≤65 years. This algorithm can help to inform accurate in-hospital VTE rates and enhance risk-classified VTE integrated care in future research.}
}
@article{ZHANG2024105632,
title = {Knowledge management for off-site construction},
journal = {Automation in Construction},
volume = {166},
pages = {105632},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105632},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524003686},
author = {Zhen Zhang and Yang Zou and Brian H.W. Guo and Johannes Dimyadi and Roy Davies and Lixin Jiang},
keywords = {Off-site construction (OSC), Knowledge management (KM), Artificial intelligence (AI), Systematic literature review},
abstract = {Off-site construction (OSC) is expected to boost productivity, shorten construction time, and reduce labour and material wastage. Despite these benefits, most OSC projects have not fully achieved these advantages, where a primary obstacle lies in the limited management of OSC knowledge. However, there is still no holistic understanding of the integration of KM in the OSC context. Therefore, this paper explores the latest development in KM for OSC through a systematic literature review using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines and template analysis. The review is based on 66 screened and assessed journal articles from all years to 2024 with a particular focus on KM and OSC. Through the quantitative and qualitative analysis, this study groups four main research themes including KM for OSC design, KM for OSC project management, knowledge-based OSC decision-making, and the management of OSC knowledge. The results are discussed to gain a systematic understanding of key OSC knowledge domains, investigate the integration of KM for OSC, and explore future research needs including emerging artificial intelligence (AI) technologies.}
}
@article{QIN2025102994,
title = {A comprehensive taxonomy of machine consciousness},
journal = {Information Fusion},
volume = {119},
pages = {102994},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.102994},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525000673},
author = {Ruilin Qin and Changle Zhou and Mengjie He},
keywords = {Consciousness, Machine consciousness, Artificial consciousness, Conscious robot, Qualia, Large language model},
abstract = {Machine consciousness (MC) is the ultimate challenge to artificial intelligence. Although great progress has been made in artificial intelligence and robotics, consciousness is still an enigma and machines are far from having it. To clarify the concepts of consciousness and the research directions of machine consciousness, in this review, a comprehensive taxonomy for machine consciousness is proposed, categorizing it into seven types: MC-Perception, MC-Cognition, MC-Behavior, MC-Mechanism, MC-Self, MC-Qualia and MC-Test, where the first six types aim to achieve a certain kind of conscious ability, and the last type aims to provide evaluation methods and criteria for machine consciousness. For each type, the specific research contents and future developments are discussed in detail. Especially, the machine implementations of three influential consciousness theories, i.e. global workspace theory, integrated information theory and higher-order theory, are elaborated in depth. Moreover, the challenges and outlook of machine consciousness are analyzed in detail from both theoretical and technical perspectives, with emphasis on new methods and technologies that have the potential to realize machine consciousness, such as brain-inspired computing, quantum computing and hybrid intelligence. The ethical implications of machine consciousness are also discussed. Finally, a comprehensive implementation framework of machine consciousness is provided, integrating five suggested research perspectives: consciousness theories, computational methods, cognitive architectures, experimental systems, and test platforms, paving the way for the future developments of machine consciousness.}
}
@article{YANG20225054,
title = {Classification for psychiatric disorders including schizophrenia, bipolar disorder, and major depressive disorder using machine learning},
journal = {Computational and Structural Biotechnology Journal},
volume = {20},
pages = {5054-5064},
year = {2022},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2022.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S2001037022004172},
author = {Qingxia Yang and Qiaowen Xing and Qingfang Yang and Yaguo Gong},
keywords = {Classification, Psychiatric disorder, Schizophrenia, Bipolar disorder, Major depressive disorder},
abstract = {Schizophrenia (SCZ), bipolar disorder (BP), and major depressive disorder (MDD) are the most common psychiatric disorders. Because there were lots of overlaps among these disorders from genetic epidemiology and molecular genetics, it is hard to realize the diagnoses of these psychiatric disorders. Currently, plenty of studies have been conducted for contributing to the diagnoses of these diseases. However, constructing a classification model with superior performance for differentiating SCZ, BP, and MDD samples is still a great challenge. In this study, the transcriptomic data was applied for discovering key genes and constructing a classification model. In this dataset, there were 268 samples including four groups (67 SCZ patients, 40 BP patients, 57 MDD patients, and 104 healthy controls), which were applied for constructing a classification model. First, 269 probes of differentially expressed genes (DEGs) among four sample groups were identified by the feature selection method. Second, these DEGs were validated by the literature review including disease relevance with the psychiatric disorders of these DEGs, the hub genes in the PPI (protein–protein interaction) network, and GO (gene ontology) terms and pathways. Third, a classification model was constructed using the identified DEGs by machine learning method to classify different groups. The ROC (receiver operator characteristic) curve and AUC (area under the curve) value were used to assess the classification capacity of the model. In summary, this classification model might provide clues for the diagnoses of these psychiatric disorders.}
}
@article{ZHOU2023104951,
title = {Generating risk response measures for subway construction by fusion of knowledge and deep learning},
journal = {Automation in Construction},
volume = {152},
pages = {104951},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.104951},
url = {https://www.sciencedirect.com/science/article/pii/S092658052300211X},
author = {Hong Zhou and Shilong Tang and Wen Huang and Xianbo Zhao},
keywords = {Risk response measures, Intelligent generation, Natural language processing, Deep learning, Knowledge enhancement},
abstract = {This study aims to propose a method to generate response measures for subway construction risks. The HowNet semantic theoretical system was selected and incorporated the knowledge of subway construction risk management to develop the semantic knowledge base. Then, a risk response generation algorithm was designed based on Knowledge-enabled Bidirectional Encoder Representation from Transformers (K-BERT) to fuse knowledge and deep learning (DL). Meanwhile, the BERT in the original K-BERT model was replaced by the Masked Language Model (MLM) as correction BERT (MacBERT) pre-training model suitable for Chinese natural language processing (NLP) tasks. The data-driven and knowledge-driven approaches were combined for better performance of DL. The pre-training model based on MacBERT was selected to undertake the comparative experiment of fusing knowledge and DL. The results showed that the proposed algorithm with the K-BERT fusion of knowledge and DL had better performance than the MacBERT-based model without the knowledge base.}
}
@article{DOST2022101975,
title = {Aligning and linking entity mentions in image, text, and knowledge base},
journal = {Data & Knowledge Engineering},
volume = {138},
pages = {101975},
year = {2022},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101975},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X2100094X},
author = {Shahi Dost and Luciano Serafini and Marco Rospocher and Lamberto Ballan and Alessandro Sperduti},
keywords = {AI, NLP, Computer Vision, Machine Learning, Knowledge Representation, Semantic Web, Entity recognition and linking},
abstract = {A picture is worth a thousand words, the adage reads. However, pictures cannot replace words in terms of their ability to efficiently convey clear (mostly) unambiguous and concise knowledge. Images and text, indeed reveal different and complementary information that, if combined will result in more information than the sum of that contained in a single media. The combination of visual and textual information can be obtained by linking the entities mentioned in the text with those shown in the pictures. To further integrate this with the agent’s background knowledge, an additional step is necessary. That is, either finding the entities in the agent knowledge base that correspond to those mentioned in the text or shown in the picture or, extending the knowledge base with the newly discovered entities. We call this complex task Visual-Textual-Knowledge Entity Linking (VTKEL). In this article, after providing a precise definition of the VTKEL task, we present two datasets called VTKEL1k* and VTKEL30k. These datasets consisting of images and corresponding captions, in which the image and textual mentions are both annotated with the corresponding entities typed according to the YAGO ontology. The datasets can be used for training and evaluating algorithms of the VTKEL task. Successively, we introduce a baseline algorithm called VT-LinKEr (Visual-Textual-Knowledge Entity Linker) for the solution of the VTKEL task. We evaluate the performances of VT-LinKEr on both datasets. We then contribute a supervised algorithm called ViTKan (Visual-Textual-Knowledge Alignment Network). We trained the ViTKan algorithm using features data of the VTKEL1k* dataset. The experimental results on VTKEL1k* and VTKEL30k datasets show that ViTKan substantially outperforms the baseline algorithm.}
}
@article{PARK2022116044,
title = {Examining the impact of adaptive convolution on natural language understanding},
journal = {Expert Systems with Applications},
volume = {189},
pages = {116044},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116044},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421013877},
author = {Jun-Hyung Park and Byung-Ju Choi and SangKeun Lee},
keywords = {Natural language understanding, Adaptive convolution, Network adaptation},
abstract = {Convolutional neural networks (CNNs) have shown promising results on many natural language understanding (NLU) tasks owing to their ability to capture informative features in local patches. However, they extract informative patterns in a static manner; they use the same set of filters regardless of different inputs. In this article, we propose an adaptive convolution to provide greater flexibility to traditional CNNs. Unlike the traditional convolution, the adaptive convolution utilizes adaptively generated convolution filters which are conditioned on inputs. We achieve this by attaching filter-generating networks, which are carefully designed to generate input-specific filters, to each convolution block in existing CNNs. We show the efficacy of our approach for existing CNNs through extensive performance evaluations. Our results indicate that adaptive convolutions improve all the baselines, without any exception, by as much as 2.6 percentage points (%p) on sentiment analysis, 1.6 %p on text classification, and 3.6 %p on textual entailment.}
}
@article{BARDHAN2024,
title = {Question Answering for Electronic Health Records: Scoping Review of Datasets and Models},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/53636},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124007222},
author = {Jayetri Bardhan and Kirk Roberts and Daisy Zhe Wang},
keywords = {medical question answering, electronic health record, EHR, electronic medical records, EMR, relational database, knowledge graph},
abstract = {Background
Question answering (QA) systems for patient-related data can assist both clinicians and patients. They can, for example, assist clinicians in decision-making and enable patients to have a better understanding of their medical history. Substantial amounts of patient data are stored in electronic health records (EHRs), making EHR QA an important research area. Because of the differences in data format and modality, this differs greatly from other medical QA tasks that use medical websites or scientific papers to retrieve answers, making it critical to research EHR QA.
Objective
This study aims to provide a methodological review of existing works on QA for EHRs. The objectives of this study were to identify the existing EHR QA datasets and analyze them, study the state-of-the-art methodologies used in this task, compare the different evaluation metrics used by these state-of-the-art models, and finally elicit the various challenges and the ongoing issues in EHR QA.
Methods
We searched for articles from January 1, 2005, to September 30, 2023, in 4 digital sources, including Google Scholar, ACL Anthology, ACM Digital Library, and PubMed, to collect relevant publications on EHR QA. Our systematic screening process followed PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines. A total of 4111 papers were identified for our study, and after screening based on our inclusion criteria, we obtained 47 papers for further study. The selected studies were then classified into 2 non–mutually exclusive categories depending on their scope: “EHR QA datasets” and “EHR QA models.”
Results
A systematic screening process obtained 47 papers on EHR QA for final review. Out of the 47 papers, 53% (n=25) were about EHR QA datasets, and 79% (n=37) papers were about EHR QA models. It was observed that QA on EHRs is relatively new and unexplored. Most of the works are fairly recent. In addition, it was observed that emrQA is by far the most popular EHR QA dataset, both in terms of citations and usage in other papers. We have classified the EHR QA datasets based on their modality, and we have inferred that Medical Information Mart for Intensive Care (MIMIC-III) and the National Natural Language Processing Clinical Challenges datasets (ie, n2c2 datasets) are the most popular EHR databases and corpuses used in EHR QA. Furthermore, we identified the different models used in EHR QA along with the evaluation metrics used for these models.
Conclusions
EHR QA research faces multiple challenges, such as the limited availability of clinical annotations, concept normalization in EHR QA, and challenges faced in generating realistic EHR QA datasets. There are still many gaps in research that motivate further work. This study will assist future researchers in focusing on areas of EHR QA that have possible future research directions.}
}
@article{TCHOUANGUEMDJUEDJA2021102930,
title = {An integrated Linked Building Data system: AEC industry case},
journal = {Advances in Engineering Software},
volume = {152},
pages = {102930},
year = {2021},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2020.102930},
url = {https://www.sciencedirect.com/science/article/pii/S0965997820309765},
author = {Justine Flore {Tchouanguem Djuedja} and Fonbeyin Henry Abanda and Bernard Kamsu-Foguem and Pieter Pauwels and Camille Magniont and Mohamed Hedi Karray},
keywords = {Construction product databases, Linked Building Data (LBD), Environmental data, Building Information Modelling (BIM), Semantic web, Linked data},
abstract = {Environmental assessment is a critical activity for ensuring buildings are performing according to specified requirements, and efficient, seamless exchange of building information is crucial for environmental assessment. Therefore, all those involved in built environment issues should be able to access and share not only building information but also data about products, especially environmental assessment results for the products used in building projects. Of the several approaches that have been proposed to achieve efficient information exchange, semantic web technologies are amongst the most promising due to their capability to share data and enhance interoperability between the most heterogeneous systems. This study proposes an approach that can be used to make environmental data available in the early phases of the building lifecycle. It relies on Semantic Web techniques, especially Linked Data principles, while building on emerging Building Information Modelling (BIM) technology to propose an approach that facilitates information exchange to enhance the sustainability assessment of buildings. The paper ends with an illustration of how lifecycle inventory databases can be integrated, linked to BIM software and used in exchanging environmental building data.}
}
@incollection{GYRARD2022171,
title = {Chapter 9 - SAREF4EHAW-compliant knowledge discovery and reasoning for IoT-based preventive health and well-being: IoT-based preventive health and well-being knowledge discovery and reasoning},
editor = {Sanju Tiwari and Fernando {Ortiz Rodriguez} and M.A. Jabbar},
booktitle = {Semantic Models in IoT and eHealth Applications},
publisher = {Academic Press},
pages = {171-198},
year = {2022},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-323-91773-5},
doi = {https://doi.org/10.1016/B978-0-32-391773-5.00015-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323917735000157},
author = {Amelie Gyrard and Antonio Kung},
keywords = {Preventive health, Smart health, Internet of things, Artificial intelligence, Health ontology, Well-being, ETSI SmartM2M SAREF, Standard, Semantic reasoning, Semantic web technologies},
abstract = {Assisting people to stay independent at home and to decrease hospital costs but also social isolation for the (elderly) people is getting more attention. The integration of heterogeneous technologies and devices require an interoperable solution to describe devices and data exchanged for preventive health and well-being. Several devices are designed to monitor patients' vital signs: Apple HealthKit, smartwatches, etc. If the companies do not develop the full-stack compatible with (1) sensors, (2) the final application for consumers, (3) and other products; the designed product can fail to succeed. It illustrates the need for interoperability between devices, applications, and sensor data. We focus on semantic interoperability on data to infer meaningful information applied to preventive health and well-being. ETSI SmartM2M SAREF is an ontology that can be extended to any IoT vertical domains such as eHealth and Ageing Well (SAREF4EHAW) and wearables (SAREF4WEAR). SAREF can be used to achieve interoperability among IoT projects, architectures, etc., and describe data sent through communication protocols or once that data must be processed (e.g., on the cloud, gateways, devices). ETSI does not provide tools yet that support the SAREF ontology to avoid engineers developing from scratch. There is a need to design a SAREF-compliant sensor dictionary for health and well-being scenarios. The health dictionary is employed with a reasoner, to infer meaningful knowledge from health sensor data. The scenarios are accessible via online demonstrators.}
}
@article{VERDIER2023102380,
title = {The myth of workforce reduction efficiency: The performativity of accounting language},
journal = {Critical Perspectives on Accounting},
volume = {90},
pages = {102380},
year = {2023},
issn = {1045-2354},
doi = {https://doi.org/10.1016/j.cpa.2021.102380},
url = {https://www.sciencedirect.com/science/article/pii/S104523542100099X},
author = {Marie-Anne Verdier and Jennifer {Boutant Lapeyre}},
keywords = {Workforce reductions, Critical accounting, Earnings management, Performativity, Accounting language},
abstract = {This paper draws on Austin’s conceptualization of performativity to show how the accounting strategies implemented before a workforce reduction can contribute to performing the myth of workforce reduction efficiency. To address this issue, we use both quantitative and qualitative methods. First, through quantitative analyses on a sample of 117 workforce reductions announced by 101 French listed firms from 2007 to 2012, we shed light how, by not taking accounting strategies into account in their models, prior mainstream studies may have conveyed false knowledge about improved performances after the operation and performed the workforce reduction efficiency supported by economic theories. Second, through a case study, we provide an in-depth illustration of the performativity effect of these accounting strategies and more precisely of downward earnings management, which can be considered a calculation act that contributes to performing the myth of the efficiency of these operations. Overall, this paper contributes to the workforce reduction literature by providing a critical illustration of how accounting numbers construct efficiency through the performative role given to earnings management. It also contributes to the critical accounting project by notably participating in the debate on the use of quantitative and mixed research methodologies in the critical accounting project.}
}
@article{HERITAGE2025158,
title = {Social identity and implicature: Exploring the pragmatics of transgender coming out narratives in videogames},
journal = {Journal of Pragmatics},
volume = {246},
pages = {158-169},
year = {2025},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2025.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0378216625001729},
author = {Frazer Heritage},
keywords = {Implicature, Particuarlized conversational implicature, Transgender characters, Videogames, Social identity, Inference},
abstract = {Despite being one of the most popular forms of media, videogames are an under-researched text type within linguistics. This paper examines the language used within videogames, specifically in relation to how videogame writers index the identities of transgender characters. Utilising data from the LGBTQ Video Game Archive (Shaw, 2017), which documents the history of lesbian, gay, bisexual, transgender, and queer content in games, I identify 22 videogames which contain references to transgender characters. A close qualitative reading of instances where transgender characters occur reveals two broad trends: i) there are multiple instances where such representation is unclear and ii) transgender identities are rarely overtly indexed. Within this second trend, I specifically focus on the role of implicature and inferencing in how transgender identities are constructed. That is, how these characters implicitly “come out” to the player and the shared knowledge needed to understand these implicatures. I argue that such shared knowledge is influenced by the social identities and lived experiences of the interpreter, which should be considered in the degree to which meaningNN is considered relevant. The research presented has implications for analysing more coming-out narratives and creating more connections between pragmatics, language, gender, and sexuality studies, as well as videogame/media studies.}
}
@article{COZMAN201896,
title = {The complexity of Bayesian networks specified by propositional and relational languages},
journal = {Artificial Intelligence},
volume = {262},
pages = {96-141},
year = {2018},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370218303047},
author = {Fabio G. Cozman and Denis D. Mauá},
keywords = {Bayesian networks, Complexity theory, Relational logic, Plate models, Probabilistic relational models},
abstract = {We examine the complexity of inference in Bayesian networks specified by logical languages. We consider representations that range from fragments of propositional logic to function-free first-order logic with equality; in doing so we cover a variety of plate models and of probabilistic relational models. We study the complexity of inferences when network, query and domain are the input (the inferential and the combined complexity), when the network is fixed and query and domain are the input (the query/data complexity), and when the network and query are fixed and the domain is the input (the domain complexity). We draw connections with probabilistic databases and liftability results, and obtain complexity classes that range from polynomial to exponential levels; we identify new languages with tractable inference, and we relate our results to languages based on plates and probabilistic relational models.}
}
@article{MOORE2025100212,
title = {Generating better understandings of linguistic dissociation using multiscalar temporal accounts},
journal = {Research Methods in Applied Linguistics},
volume = {4},
number = {2},
pages = {100212},
year = {2025},
issn = {2772-7661},
doi = {https://doi.org/10.1016/j.rmal.2025.100212},
url = {https://www.sciencedirect.com/science/article/pii/S2772766125000333},
author = {Ashley R. Moore},
keywords = {Linguistic dissociation, First language dissociation, Critical realism, Grounded theory method, Temporality},
abstract = {Linguistic dissociation is “a relatively enduring psychosocial process in which an individual or group distances themselves from a set of linguistic practices already within their repertoire because those practices have come to connote a state of significant intersubjective disharmony, or contrasubjectivity” (Moore, 2023, p. 1152). Generating data to theorise the nature and causes of linguistic dissociation among particular kinds of language users represents a methodological challenge; as intersubjective phenomena, linguistic dissociation and contrasubjectivity emerge across multiple converging, connected temporal scales. With reference to data generated with two participants from a larger critical realist grounded theory method investigation into the nature and causes of L1 dissociation among some Japanese-English late plurilinguals, I show how I combined two data generation activities with distinct temporal scales—a multimodal timeline through which participants constructed an account of their affective relationship to Japanese over their lifespan and a two-day language use journal—with follow-up interviews to produce different types of data about linguistic dissociation, which in turn made it possible for the participants and me to construct different forms of knowledge about the phenomenon. Further, I contend that we used those knowledges to arrive at better (i.e., more verisimilitudinous) understandings of the nature and causes of the L1 dissociation they had experienced. I finish by sharing methodological implications for applied linguists investing various intersubjective phenomena that shape the linguistic repertoire and reflecting on the limits of my own knowledge of linguistic dissociation.}
}
@article{GAMBO2024e36729,
title = {Identifying and resolving conflict in mobile application features through contradictory feedback analysis},
journal = {Heliyon},
volume = {10},
number = {17},
pages = {e36729},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e36729},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024127600},
author = {Ishaya Gambo and Rhodes Massenon and Roseline Oluwaseun Ogundokun and Saurabh Agarwal and Wooguil Pak},
keywords = {Natural language processing, BERT, RoBERTa, Mobile app, Sentiment analysis, iOS app store, Google play store},
abstract = {As mobile applications proliferate and user feedback becomes abundant, the task of identifying and resolving conflicts among application features is crucial for delivering satisfactory user experiences. This research, motivated to align application development with user preferences, introduces a novel methodology that leverages advanced Natural Language Processing techniques. The paper showcases the use of sentiment analysis using RoBERTa, topic modeling with Non-negative matrix factorization (NMF), and semantic similarity measures from Sentence-BERT. These techniques enable the identification of contradictory sentiments, the discovery of latent topics representing application features, and the clustering of related feedback instances. The approach detects conflicts by analyzing sentiment distributions within semantically similar clusters, further enhanced by incorporating antonym detection and negation handling. It employs majority voting, weighted ranking based on rating scores, and frequency analysis of feature mentions to resolve conflicts, providing actionable insights for prioritizing requirements. Comprehensive evaluations on large-scale iOS App Store and Google Play Store datasets demonstrate the approach's effectiveness, outperforming baseline methods and existing techniques. The research improves mobile application development and user experiences by aligning features with user preferences and providing interpretable conflict resolution strategies, thereby introducing a novel approach to the field of mobile application development.}
}
@article{LI2025103611,
title = {Multi-modal causal hypergraph reasoning for enhancing collaborative diagnosis of equipment composite failures},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103611},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103611},
url = {https://www.sciencedirect.com/science/article/pii/S147403462500504X},
author = {Fei Li and Xinyu Li and Sijie Wen and Jinsong Bao},
keywords = {Composite fault diagnosis, CoCT-prompted LLMs, Multi-modal causal knowledge hypergraph, Causal relation cascade graph attention network, Human-machine collaborative},
abstract = {The causal relationships between equipment fault tickets and component production numerical data reflect the equipment’s fault state. However, given the complexity and diversity of equipment failures, existing graph construction and graph embedding methods overlook the long causal chain semantics between failures, the multivariate relationships of composite faults formed by connections of more than two entities, and the rich multimodal information in their triplets. To address these issues, this paper proposes a method based on multi-modal causal knowledge hypergraph (MCKH) inference to enhance the collaborative diagnosis of equipment composite faults. First, a few-shot chain of causal thought (CoCT) strategy is used to extract multivariate hyperedge relationships and long causal chain knowledge from fault investigation forms, combining fault factors, fault images, and numerical data to create multimodal hyper-nodes and form the MCKH. Second, a low-rank multimodal fusion strategy is adopted to achieve dynamic semantic modeling between hyper-node modalities. The MCKH encoder-decoder architecture is designed, where the encoder is a causal relation cascade graph attention network (CRCGAT) that includes a hyperedge-dominated node-level attention and layer-level attention mechanism. The joint decoder, ConvE, is used for the triplet link prediction of composite faults. Third, a collaborative fault diagnosis strategy is introduced through LLM-based human–machine interaction, leveraging CRCGAT + ConvE for MCKH inference and long causal chain backtracking to improve composite fault diagnosis accuracy. Finally, the method is applied to bridge crane composite fault diagnosis, with ablation experiments validating its effectiveness and providing a new perspective for multimodal fault diagnosis and root cause analysis in equipment spot inspection.}
}
@article{M2018765,
title = {Consumer insight mining: Aspect based Twitter opinion mining of mobile phone reviews},
journal = {Applied Soft Computing},
volume = {68},
pages = {765-773},
year = {2018},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2017.07.056},
url = {https://www.sciencedirect.com/science/article/pii/S1568494617304763},
author = {Rathan M. and Vishwanath R. Hulipalled and K.R. Venugopal and L.M. Patnaik},
keywords = {Emoji, Ontology, Sentiment analysis, Social media, Twitter, SVM},
abstract = {Micro-blogging sites such as Twitter are often considered as rich source of opinions of the masses towards products. The character length limit in tweets encourages people to use emojis, emoticons and out of vocabulary words. Due to the huge volume of tweets being generated, it is difficult to manually label tweets and create a supervised learning model for sentiment analysis. Looking into these challenges, the research paper aims to create a feature level sentiment analysis model for Twitter data mining including features such as emoji detection, spelling correction and emoticon detection. The proposed model consists of automated training data labeling by using lexicon based approach. It is an ontology based system with the domain of “Smartphone”. In addition to the general lexicon used, a set of lexicons specific for each attribute of the domain “Smartphone” are used to improve classification accuracy for training data generation. This is used to classify tweets obtained about a particular mobile phone using SVM classifier. Experimental results show that the classifier based on automated training data provides good accuracy. It also demonstrates the importance of emoji detection and the attribute specific lexicons which help improve the classification accuracy.}
}
@article{GALLEGOSRIOFRIO2025100253,
title = {Frogs, coalitions, and mining: Transformative insights for planetary health and earth system law from Ecuador's struggle to enforce Nature's rights},
journal = {Earth System Governance},
volume = {24},
pages = {100253},
year = {2025},
issn = {2589-8116},
doi = {https://doi.org/10.1016/j.esg.2025.100253},
url = {https://www.sciencedirect.com/science/article/pii/S2589811625000199},
author = {Carlos Andres Gallegos-Riofrío and Mario A. Moncayo-Altamirano and Andrea Terán-Valdez and Gustavo Redin-Guerrero and Carlos Varela and Stephen Posner and Amaya Carrasco-Torrontegui},
keywords = {Earth system law, Planetary health, Anthropocene gap, Pluriverse, Just transition, Rights of nature},
abstract = {Pachamama, Mother Earth, faces a mass extinction threat. A radical transformation in human systems is essential, guided by equity and justice at local and global scales. This transformation must reconfigure the World-System's power structures, impacting the ecosphere (ecological functions, biodiversity, and resource regimes) and the ethnosphere (ontological, epistemological, and legal pluralism). Together, these shape the Pluriverse—a planet of many worlds. The status quo is unsustainable. Effective solutions must prioritize a just transition that integrates the pluriverse. Alternatives from the so-called Global South offer valuable tools for this shift, such as the Rights of Nature, which views nature as a rights-bearing entity, not merely an object of regulation. The Llurimagua case—a dispute over a mining concession in Ecuador's cloud forest—illustrates this approach, providing a unique opportunity to rethink Earth System Governance and address the Anthropocene Gap (i.e., disconnect with Earth System Law), crucial for tackling planetary health challenges.}
}
@article{LIU2020110527,
title = {SemanticGO: a tool for gene functional similarity analysis in Arabidopsis thaliana and rice},
journal = {Plant Science},
volume = {297},
pages = {110527},
year = {2020},
issn = {0168-9452},
doi = {https://doi.org/10.1016/j.plantsci.2020.110527},
url = {https://www.sciencedirect.com/science/article/pii/S0168945220301321},
author = {Wei Liu},
keywords = {Functional similarity, Gene ontology (GO), Gene vector, Pathway vector, Vector arithmetic},
abstract = {Gene or pathway functional similarities are important information for researchers. However, these similarities are often described sparsely and qualitatively. The latent semantic analysis of Arabidopsis thaliana (Arabidopsis) Gene Ontology (GO) data produced a set of 200-dimension feature vectors for each gene. Pathways were represented by summing the vectors of the pathway member genes. Thus, the similarities between genes and pathways were assessed. Additionally, the gene feature vectors were correlated with external gene data, including gene expression and gene network connectivity, to elucidate the associated functions. The gene feature vectors were decoded, and their applications were demonstrated. A simple online tool, SemanticGO (http://bioinformatics.fafu.edu.cn/semanticGO/), is herein provided to enable researchers to explore the similarities between genes and pathways in both Arabidopsis and rice.}
}
@article{JENSEN2021100846,
title = {Pedestrians as floating life - On the reinvention of the pedestrian city},
journal = {Emotion, Space and Society},
volume = {41},
pages = {100846},
year = {2021},
issn = {1755-4586},
doi = {https://doi.org/10.1016/j.emospa.2021.100846},
url = {https://www.sciencedirect.com/science/article/pii/S1755458621000840},
author = {Ole B. Jensen and Michael Martin and Markus Löchtefeld},
keywords = {Mobilities, Pedestrianism, Cities, Embodiment, Design, Multisensorialism},
abstract = {Walking with its average speed of 5 km/h was for a very long period the primary mode of moving and engaging with the immediate material environment for humans. However, over the past half-century, the socio-technical systems of automobility as well as other forms of non-human powered mobility have changed the ways in which cities are experienced. Most recently, however, the pedestrian mode has been reprioritised resulting in a shift of emphasis, particularly in European cities, toward recognising the destructive forces of automobility. This shift has been accompanied by a variety of pedestrian reprioritisation strategies including the pedestrianisation of city streets as well as restricted vehicular access to particular inner city zones at prescribed times. The challenge for many cities is how to legitimately change mindsets, from automobility to walking. This paper explores the reprioritisation of urban walking not as ‘infrastructure’ or an ‘intervention’ but as transitory, ‘floating life’ across space and time. We conceptualise walking as a multi-sensorial, effective, and mobile engagement with the material environment. In doing so, we ask how the ‘floating life’ of pedestrianism may be reflected upon as part of the so-called ‘mobilities turn’ and in particular how theories of materiality, embodiment, design and experience interlink with walking. In this paper walking as a pedestrian is therefore a particular quality of mobility. The way in which we ‘inhabit’ the city is significant when we walk, and turning to walking as ‘floating life’ pays attention to this underemphasised ontological dimension.}
}
@article{MARLOR2020101339,
title = {Explaining knowledge pluralisms; the intertwining of culture and materiality},
journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
volume = {84},
pages = {101339},
year = {2020},
issn = {1369-8486},
doi = {https://doi.org/10.1016/j.shpsc.2020.101339},
url = {https://www.sciencedirect.com/science/article/pii/S1369848620301503},
author = {Chantelle Marlor},
keywords = {Knowledge pluralisms, Culture, Materialism, Process ontology, Indigenous knowledge, Incommensurability},
abstract = {A wide variety of theories explain how social factors influence and shape knowledges. Other theories describe how materialism and social elements coalesce. Largely still missing, however, is an argument that substantially addresses both culture and materiality. Using examples from four ethnographic case studies of culturally-distinct practitioners (two groups of Indigenous harvesters, a group of contaminant ecologists and a group of fisheries biologists) creating knowledge about the same topic (clams), I develop an explanation of how and why (useful) knowledge pluralisms exist. Using a process-based ontology for theorizing about materialism, I explore how conceptual frameworks and knowledge-making practices become intertwined with materiality. I argue that this intertwining allows for the creation of knowledge while simultaneously resulting in potentially differing knowledges about the same subject.}
}
@article{RUAN2025781,
title = {Exploring the Constituents and Mechanisms of Polygonum multiflorum Thunb. in Mitigating Ischemic Stroke: A Network Pharmacology and Molecular Docking Study},
journal = {Combinatorial Chemistry & High Throughput Screening},
volume = {28},
number = {5},
pages = {781-797},
year = {2025},
issn = {1386-2073},
doi = {https://doi.org/10.2174/0113862073285988240229081558},
url = {https://www.sciencedirect.com/science/article/pii/S1386207325000813},
author = {Lingyu Ruan and Mengyun Zheng and Xinru Xia and Chaofan Pang and Yating Wang and Zhiwei Fan and Jingtian Yang and Qing Qing and Hongyan Lin and Yuheng Tao and Junsong Wang and Liqun Wang},
keywords = { Thunb., ischemic stroke, network pharmacology, molecular docking, oxygen-glucose deprivation and reperfusion (OGD/R), protein-protein interaction (PPI)},
abstract = {Backgound
Polygonum multiflorum Thunb. (PMT) has shown promise in exerting cerebrovascular protective effects, and its potential for treating ischemic stroke (IS) has garnered attention. However, the lack of clarity regarding its chemical constituents and mechanisms has significantly hindered its clinical application.
Methods
In this study, we employed network pharmacology and molecular docking techniques for the first time to elucidate the potential compounds and targets of PMT in treating IS. The databases CTD, DrugBank, DisGeNET, GeneCards, OMIM, TTD, PGKB, NCBI, TCMIP, CNKI, PubMed, ZINC, STITCH, BATMAN, ETCM and Swiss provided information on targets related to IS and components of PMT, along with their associated targets. We constructed “compound-target” and protein-protein interaction (PPI) networks sourced from the STRING database using the Cytoscape software. Gene Ontology (GO) enrichment analysis and KEGG pathway analysis were conducted using the DAVID database. Molecular docking between core targets and active compounds was conducted using Autodock4 software. Experiments were performed in an oxygen-glucose deprivation and reperfusion (OGD/R) model to validate the anti-IS activity of compounds isolated from PMT preliminarily. Network pharmacological analysis revealed 16 core compounds, including resveratrol, polydatin, TSG, ω-hydroxyemodin, emodin anthrone, tricin, moupinamide, and others, along with 11 high-degree targets, such as PTGS1, PTGS2, ADORA1, ADORA2, CA1, EGFR, ESR1, ESR2, SRC, MMP3 and MMP9.
Results
GO and KEGG enrichment analyses revealed the involvement of HIF-1, Akt signaling pathway and energy metabolism-related signaling pathways. Molecular docking results emphasized eight key compounds and confirmed their interactions with corresponding targets. In vitro OGD/R model experiments identified TSG and tricin as the primary active substances within PMT for its anti-stroke activity.
Conclusion
This study contributes new insights into the potential development of PMT for stroke prevention and treatment.}
}
@article{WANG2023121136,
title = {Data-driven and Knowledge-based predictive maintenance method for industrial robots for the production stability of intelligent manufacturing},
journal = {Expert Systems with Applications},
volume = {234},
pages = {121136},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121136},
url = {https://www.sciencedirect.com/science/article/pii/S095741742301638X},
author = {Xiaoqiao Wang and Mingzhou Liu and Conghu Liu and Lin Ling and Xi Zhang},
keywords = {Industrial robots, Predictive maintenance, Intelligent manufacturing, Deep learning, Knowledge graph},
abstract = {The service stability of industrial robots (IRs) is considered the basis for ensuring intelligent manufacturing operations. Knowledge-based work plays a central role in the practical application of intelligent manufacturing because the staff have professional knowledge of production and manufacturing after learning, undergoing training, and thinking. They can use their knowledge to analyze complex states, assess them accurately, and make innovative decisions. Therefore, expressing and constructing IR data and knowledge is a key issue in the application of knowledge to the predictive maintenance (PdM) of IRs. Considering the intelligent management of IRs as the research objective of this study, a PdM method based on data and knowledge was developed. A running-state feature-recognition model based on a long short-term memory network was first established to recognize future running states using the history and real-time running data of IRs. Furthermore, the k-nearest neighbor algorithm was used to analyze the correlation between the running-state feature data and faults to predict possible faults. The prediction results were then input into the knowledge graphs (KGs) of IRs for reasoning. PdM strategies were automatically formulated based on the KGs. Finally, a PdM system for IRs was designed and developed. The effectiveness of the proposed method and system were verified by applying them to the welding robots in a new energy automotive welding workshop. The findings of this study provide new concepts and tools for the PdM of IRs, as well as theoretical and methodological support for the production stability of intelligent manufacturing.11Industrial robot (IR), predictive maintenance (PdM), knowledge graph (KG), industrial Internet of Things (IIoT), intelligent manufacturing (IM), intelligent manufacturing systems (IMS), k-nearest neighbor (KNN), long short-term memory (LSTM), process control (OPC), programmable logic controller (PLC), factory automation system (FAS), manufacturing execution system (MES), recipe cluster management system (RCMS), recurrent neural network (RNN), support vector machine (SVM), knowledge ontology (KO), physical production system (PPS), physical modeling modules (PMM), application management modules (AMM), system management module (SMM), fault information management (FIM), knowledge graph management (KGM), maintenance plan management (MPM), information push management (IPM), statistical analysis management (SAM), user interface (UI).}
}
@article{MARTINEZPELLITERO2018285,
title = {Knowledge base model for automatic probe orientation and configuration planning with CMMs},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {49},
pages = {285-300},
year = {2018},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2017.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S0736584517300376},
author = {S. Martínez-Pellitero and J. Barreiro and E. Cuesta and A.I. Fernández-Abia},
keywords = {Knowledge, KBE, Inspection planning, Coordinate measuring, Contact inspection},
abstract = {Different support applications are available for generating programs to run inspection with Coordinate Measuring Machines (CMM). However, the operator must decide the criteria and strategies to apply, so that quality of inspection depends on his knowledge and skills. Nevertheless, many of the activities involved in inspection planning are repetitive, therefore an adequate formalization of the involved knowledge makes it possible to develop applications to automate inspection and, thereby, to avoid operator errors during the decision making in planning. To this end, a knowledge base model for inspecting prismatic parts with complex surfaces has been developed. First, the required knowledge is represented in an informal model using an ontology called ONTO-Process. Later on, a formal model is generated that constitutes the basis for developing a computer application to support the automatic inspection planning. This article provides the definition and modeling of the necessary rules for one of the most complex activities to automate: determination of probe orientations in the inspection by contact of mechanical parts.}
}
@article{WALEK2025125816,
title = {A text-based recommender system for recommending relevant news articles},
journal = {Expert Systems with Applications},
volume = {266},
pages = {125816},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125816},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424026836},
author = {Bogdan Walek and Patrik Müller},
keywords = {Recommender system, Text-based recommender system, News recommender system, Word2Vec, Doc2Vec, TF-IDF},
abstract = {Despite recent advances in the related fields and the growing popularity of AI-based tools, small businesses, and public institutions still face challenges when implementing recommendation systems to increase profits and provide personalised services. This paper provides a comprehensive overview of state-of-the-art systems and a case study of a recommender system for a small, low-budget project, battling with several constraints: the use of uncommon natural language, the use of raw, unlabelled textual data, thus using mainly techniques coming from the field of data mining and text mining rather than relying on supervised learning methods. Another constraint was the reliance on the smaller dataset and the limited resources available for the development. This led us to explore a range of content-based methods, including the utilisation of the similarity measures applied to word embeddings derived from the Word2Vec and Doc2Vec shallow neural network models, as well as the TF-–IDF method. Additionally, a topic modelling approach utilising the Latent Dirichlet Allocation was used, as well as collaborative filtering methods, such as the algorithm using the Singular Value Decomposition method, and hybrid methods integrating the fuzzy inference and fuzzy expert systems. Some of the obstacles identified were subsequently demonstrated to be too challenging for the development of accurate recommendations. However, the item-to-item similarity solutions, which were primarily content-based, yielded satisfactory results when the threshold of 75% of the average precision of recommendations assessed by users was exceeded. One such solution was the one that used the Word2Vec-based model, which had been trained on the parameters obtained from the word similarities and analogies tests. Furthermore, an overview of alternative techniques and methodologies is provided.}
}
@article{KOVALYOV2022460,
title = {Distributed Energy Resources Management: From Digital Twin to Digital Platform},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {9},
pages = {460-465},
year = {2022},
note = {11th IFAC Symposium on Control of Power and Energy Systems CPES 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.07.080},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322004657},
author = {Serge P. Kovalyov},
keywords = {Smart Grid Technologies, Future Challenges To Electrical Networks, their Solutions, Operation of Control of Renewable Energy Systems, Digital Twin, Digital Platform},
abstract = {The paper presents an approach to the development of a distributed energy resources management platform based on the digital twin (DT). The platform employs advanced Industry 4.0 technologies to let users improve the power supply quality, reduce costs, and gain emerging market opportunities. The DT and the platform are designed with the viewpoint-based approach established by the systems engineering standard ISO/IEC/IEEE 42010. The typical power system DT architecture is described. The distributed energy domain ontology provides the unified semantic basis for interaction between heterogeneous models that constitute the DT. The DT-based platform architecture is represented from three key viewpoints: functional, information, and software. To formalize and ultimately automate the DT composition process, we propose novel mathematical methods based on category theory.}
}
@article{QI20191,
title = {A semantic-based inference control algorithm for OWL repository privacy protection},
journal = {Computer Networks},
volume = {156},
pages = {1-8},
year = {2019},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2019.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S1389128619303731},
author = {Yuying Qi and Xuanxia Yao and Tao Zhu and Huansheng Ning},
keywords = {OWL Repository, Inference rules, Sensitive triples, Semantic value},
abstract = {Web Ontology Language (OWL) designed to represent the class and relationships between classes is bringing a technological revolution in the Semantic Web. More and more knowledge is stored in the form of OWL. OWL repository refers to the systems that can store, query and infer OWL data. In OWL repository, information that users are forbidden to access (sensitive triples) can be inferred from information that the user have access to (non-sensitive triples) through inference rules, which brings about a security problem that some users’ sensitive information may be leaked. To protect sensitive triples, some researches have been done on the security problem, which mainly focus on removing some non-sensitive triples with low access frequency to achieve inference control. However, access frequency of a triple is unstable and it can not fully represent the importance of semantic information. In this paper, an inference control algorithm based on Semantic Value and access frequency is proposed. We introduced the concept of Degree of triples and Semantic Value, which can fully represent the importance of the semantic information represented by the triple. Moreover, we did experiments from the perspective of protecting sensitive triples of every user. Experimental results show that this algorithm can effectively prevent illegal inference and minimize the loss of semantic information.}
}
@article{STOJANOV2021,
title = {A Fine-Tuned Bidirectional Encoder Representations From Transformers Model for Food Named-Entity Recognition: Algorithm Development and Validation},
journal = {Journal of Medical Internet Research},
volume = {23},
number = {8},
year = {2021},
issn = {1438-8871},
doi = {https://doi.org/10.2196/28229},
url = {https://www.sciencedirect.com/science/article/pii/S1438887121008049},
author = {Riste Stojanov and Gorjan Popovski and Gjorgjina Cenikj and Barbara {Koroušić Seljak} and Tome Eftimov},
keywords = {food information extraction, named-entity recognition, fine-tuning BERT, semantic annotation, information extraction, BERT, bidirectional encoder representations from transformers, natural language processing, machine learning},
abstract = {Background
Recently, food science has been garnering a lot of attention. There are many open research questions on food interactions, as one of the main environmental factors, with other health-related entities such as diseases, treatments, and drugs. In the last 2 decades, a large amount of work has been done in natural language processing and machine learning to enable biomedical information extraction. However, machine learning in food science domains remains inadequately resourced, which brings to attention the problem of developing methods for food information extraction. There are only few food semantic resources and few rule-based methods for food information extraction, which often depend on some external resources. However, an annotated corpus with food entities along with their normalization was published in 2019 by using several food semantic resources.
Objective
In this study, we investigated how the recently published bidirectional encoder representations from transformers (BERT) model, which provides state-of-the-art results in information extraction, can be fine-tuned for food information extraction.
Methods
We introduce FoodNER, which is a collection of corpus-based food named-entity recognition methods. It consists of 15 different models obtained by fine-tuning 3 pretrained BERT models on 5 groups of semantic resources: food versus nonfood entity, 2 subsets of Hansard food semantic tags, FoodOn semantic tags, and Systematized Nomenclature of Medicine Clinical Terms food semantic tags.
Results
All BERT models provided very promising results with 93.30% to 94.31% macro F1 scores in the task of distinguishing food versus nonfood entity, which represents the new state-of-the-art technology in food information extraction. Considering the tasks where semantic tags are predicted, all BERT models obtained very promising results once again, with their macro F1 scores ranging from 73.39% to 78.96%.
Conclusions
FoodNER can be used to extract and annotate food entities in 5 different tasks: food versus nonfood entities and distinguishing food entities on the level of food groups by using the closest Hansard semantic tags, the parent Hansard semantic tags, the FoodOn semantic tags, or the Systematized Nomenclature of Medicine Clinical Terms semantic tags.}
}
@article{AMENT2020106421,
title = {An ecological monetary theory},
journal = {Ecological Economics},
volume = {171},
pages = {106421},
year = {2020},
issn = {0921-8009},
doi = {https://doi.org/10.1016/j.ecolecon.2019.106421},
url = {https://www.sciencedirect.com/science/article/pii/S0921800919306962},
author = {Joe Ament},
keywords = {Monetary theory, Ecological economics, Ecofeminism, State theory, Credit theory, Nature of money},
abstract = {While money is critical to the modern world, ecological economics does not have a theory of money that is applicable to its theoretical framework and policy prescriptions. Accordingly, the field often defers to an orthodox conception of money that is historically inaccurate and ontologically inconsistent. The dualized nature of Western philosophy informs orthodoxy by defining money according its function as a medium of exchange. This conceptualization creates logical and historical problems that can be addressed by exploring an interdisciplinary literature that defines money according to its nature as a social relation expressed in a unit of account. This paper develops an ecological monetary theory that is simultaneously rooted in a socio-historical understanding of money's nature, and in an ontology of social and ecological embeddedness. Such a theory provides ecological economists, and others concerned with social and ecological equity, with a framework from which to address monetary systems and policy.}
}
@article{RISSOLA2022102890,
title = {Mental disorders on online social media through the lens of language and behaviour: Analysis and visualisation},
journal = {Information Processing & Management},
volume = {59},
number = {3},
pages = {102890},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.102890},
url = {https://www.sciencedirect.com/science/article/pii/S030645732200019X},
author = {Esteban A. Ríssola and Mohammad Aliannejadi and Fabio Crestani},
keywords = {Social media mining, Mental health, Language analysis, Behaviour analysis, Information extraction, Visualisation},
abstract = {Due to the worldwide accessibility to the Internet along with the continuous advances in mobile technologies, physical and digital worlds have become completely blended, and the proliferation of social media platforms has taken a leading role over this evolution. In this paper, we undertake a thorough analysis towards better visualising and understanding the factors that characterise and differentiate social media users affected by mental disorders. We perform different experiments studying multiple dimensions of language, including vocabulary uniqueness, word usage, linguistic style, psychometric attributes, emotions’ co-occurrence patterns, and online behavioural traits, including social engagement and posting trends. Our findings reveal significant differences on the use of function words, such as adverbs and verb tense, and topic-specific vocabulary, such as biological processes. As for emotional expression, we observe that affected users tend to share emotions more regularly than control individuals on average. Overall, the monthly posting variance of the affected groups is higher than the control groups. Moreover, we found evidence suggesting that language use on micro-blogging platforms is less distinguishable for users who have a mental disorder than other less restrictive platforms. In particular, we observe on Twitter less quantifiable differences between affected and control groups compared to Reddit.}
}
@article{CUI2025126128,
title = {Research on the mechanism of organizing and managing mainstream integrated media information resources in the era of big data},
journal = {Expert Systems with Applications},
volume = {266},
pages = {126128},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126128},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424029956},
author = {Jindong Cui and Chenyu Li and Chenrui Bao and Guoli Qu},
keywords = {Mainstream integrated media, Information organization, Management Mechanism, Big data},
abstract = {As mainstream integrated media assumes increasing importance in guiding public opinion, achieving efficient organization and management of its information resources within the context of big data and the information economy has become a fundamental cornerstone for its development and value creation. This paper focuses on the content and dissemination characteristics of mainstream integrated media information, constructs a model for its organization and management, and examines various aspects such as decentralized information collection, multimodal resource processing, semantic feature extraction, information unit construction and association, information chain traceability, and simulation results. Additionally, it proposes management strategies and countermeasures from a comprehensive perspective of the entire information organization chain. The developed organization and management mechanism aligns with the evolving requirements of mainstream integrated media in the big data era, providing a foundation for the deep utilization and enhancement of information value.}
}
@article{LIU2023838,
title = {Overhead transmission line condition assessment based on intention classification and slot filling using optimized BERT model},
journal = {Energy Reports},
volume = {9},
pages = {838-846},
year = {2023},
note = {2022 The 3rd International Conference on Power Engineering},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2023.04.357},
url = {https://www.sciencedirect.com/science/article/pii/S2352484723006522},
author = {Jiaxin Liu and Shuai Wang and Qianqian Zhu and Chenchen Zhao and Guogang Zhang and Zijian Zhao and Xuchen Lu},
keywords = {Overhead transmission line, Condition assessment, Intention classification, Slot filling, BERT, Hierarchical weighted scoring method},
abstract = {As the key equipment in power system, the running state of overhead transmission lines is affected by various complex and random factors, and the maintenance workload of the line is too heavy to overhaul regularly. Therefore, it is very necessary to build a refined condition assessment system to improve the diagnosis accuracy and maintenance efficiency of overhead transmission lines. Recent years, State Grid Corporation of China (SGCC) has recorded mass of monitoring reports of transmission lines. The natural language processing (NLP) with deep learning model provides an effective way to extract the key defect information from the monitoring reports. In this paper, the joint model of intention classification and slot filling (ICSF) based on bidirectional encoder representation from transformers (BERT) is introduced. To improve the precision of defect information extraction, two optimization models of BERT are presented. The results show that Robustly Optimized BERT Pre-Training Approach (RoBERTa) has achieved better effects on ICSF with the extraction accuracy of 92.22%. Then, the hierarchical weighted scoring method is introduced to score the status of overhead transmission line based on the results of ICSF-RoBERTa. And the assessment results of the overhead transmission line state and the corresponding maintenance strategies are provided according to the scores. Finally, the feasibility of the proposed method is validated by practical cases of line inspection reports.}
}
@article{LYONS201816,
title = {Listening to the voice of children with developmental speech and language disorders using narrative inquiry: Methodological considerations},
journal = {Journal of Communication Disorders},
volume = {72},
pages = {16-25},
year = {2018},
issn = {0021-9924},
doi = {https://doi.org/10.1016/j.jcomdis.2018.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S002199241730045X},
author = {Rena Lyons and Sue Roulstone},
abstract = {There are policy and theoretical drivers for listening directly to children’s perspectives. These perspectives can provide insights to children’s experiences of their daily lives and ways in which they construct their multiple identities. Qualitative methodology is a useful research paradigm with regard to exploring children’s experiences. However, listening to the perspectives of children with speech and language disorders is a relatively new field of research. Therefore, it is important that researchers share their experiences of using methods and reflect on the strengths and limitations of these methods. The authors have used narrative inquiry with children with speech and language disorders to explore ways in which these children make sense of their experiences and construct their identities. In this paper, the authors reflect on methodological considerations when using narrative inquiry with children with speech and language disorders. They critically discuss three methodological considerations: narrative inquiry as a methodological choice, methods for data generation, limitations, and rigour.}
}
@article{ZHANG2022417,
title = {A multi-scale modeling method for digital twin shop-floor},
journal = {Journal of Manufacturing Systems},
volume = {62},
pages = {417-428},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521002600},
author = {He Zhang and Qinglin Qi and Fei Tao},
keywords = {Digital twin, Modeling, Model assembly, Model fusion, Model update, Digital twin shop-floor},
abstract = {Digital twin has attracted more and more attentions in the past few years. To put digital twin into practice, modeling is one of the most important foundations. Under this background, some modeling research on digital twin shop-floor which is regarded as the basic unit for realizing smart manufacturing has been carried out. However, current research pays scant attention to the multi-scale features of shop-floor, which hinders the effective application of digital twin shop-floor. As for the problem about how to realize the model construction from the perspective of time-scale and space-scale, this paper firstly proposed a multi-layer modeling framework for supporting model construction from unit layer to system layer to system of system layer. Moreover, the mechanism of model changes over time is also considered. Then, the specific procedures and methods of model assembly, model fusion and model update towards machines and shop-floor are discussed respectively. Finally, a satellite AIT(Assembly, Integration and Test) shop-floor is chosen as the case to validate the correctness and feasibility of proposed framework, procedures and methods.}
}
@article{SYCHEV2020264,
title = {Automatic grading and hinting in open-ended text questions},
journal = {Cognitive Systems Research},
volume = {59},
pages = {264-272},
year = {2020},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2019.09.025},
url = {https://www.sciencedirect.com/science/article/pii/S1389041719304978},
author = {Oleg Sychev and Anton Anikin and Artem Prokudin},
keywords = {e-learning, Automatic error recognition, Regular expressions, Editing distances, Computational linguistics, Ontology, Formative feedback},
abstract = {Open-ended text questions provide better assessment of learner’s knowledge, but analysing answers for this kind of questions, checking their correctness, and generating of detailed formative feedback about errors for the learner are more difficult and complex tasks than for closed-ended questions like multiple-choice. The analysis of answers for open-ended questions can be performed on different levels. Analysis on character level allows to find errors in characters’ placement inside a word or a token; it is typically used to detect and correct typos, allowing to differ typos from actual errors in the learner’s answer. The word-level or token-level analysis allows finding misplaced, extraneous, or missing words in the sentence. The semantic-level analysis is used to capture formally the meaning of the learner’s answer and compare it with the meaning of the correct answer that can be provided in a natural or formal language. Some systems and approaches use analysis on several levels. The variability of the answers for open-ended questions significantly increases the complexity of the error search and formative feedback generation tasks. Different types of patterns including regular expressions and their use in questions with patterned answers are discussed. The types of formative feedback and modern approaches and their capabilities to generate feedback on different levels are discussed too. Statistical approaches or loosely defined template rules are inclined to false-positive grading. They are generally lowering the workload of creating questions, but provide low feedback. Approaches based on strictly-defined sets of correct answers perform better in providing hinting and answer-until-correct feedback. They are characterised by a higher workload of creating questions because of the need to account for every possible correct answer by the teacher and fewer types of detected errors. The optimal choice for creating automatised e-learning courses are template-based open-ended question systems like OntoPeFeGe, Preg, METEOR, and CorrectWriting which allows answer-until-correct feedback and are able to find and report various types of errors. This approach requires more time to create questions, but less time to manage the learning process in the courses once they are run.}
}
@article{LI2025224,
title = {Meta-knowledge triple driven multi-modal knowledge graph construction method and application in production line control with Gantt charts},
journal = {Journal of Manufacturing Systems},
volume = {80},
pages = {224-242},
year = {2025},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2025.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0278612525000639},
author = {Laiyi Li and Maolin Yang and Inno Lorren Désir Makanda and Pingyu Jiang},
keywords = {Manufacturing knowledge graph, Entity-relationships model, The meta-knowledge triples, Gantt chart, 3D printing, Anomaly detection},
abstract = {Digital manufacturing involves complex and multidimensional interactions among production line resources, resulting in massive multi-modal knowledge. The knowledge often lacks correlation and contextual readability, leading to data silos. The rapid development of knowledge graphs (KGs) has rekindled interest in manufacturing knowledge engineering. Investigating the framework of multi-modal manufacturing data assets in enterprises and transforming them into a general-purpose KG database to support manufacturing processes is of significant importance. Guided by the principle of using KG as a manufacturing database, this study developed a multi-modal production line manufacturing knowledge graph (PLMKG) to support dynamic manufacturing on production lines. Firstly, the schema layer of the PLMKG is constructed using the Entity-Relationship model and a manufacturing knowledge pattern framework, with meta-knowledge triples proposed for schema data expression. Secondly, an event-state trigger dynamic instantiation method based on triples binding is proposed to enable self-growth. Third, a method integrating dynamic Gantt charts is introduced to synchronize the control of PLMKG and the manufacturing process. The anomaly detection model is employed to detect production, with the results stored in the PLMKG and Gantt charts for process control. Finally, a PLMKG prototype system for data management and process visualization is developed, with a 3D printing production line case study validating the construction and application of PLMKG. The results indicate that the proposed PLMKG integrates multi-modal manufacturing knowledge structurally and provides AI readiness for manufacturing, finally supporting the production line operation as a database.}
}
@article{HU2019102929,
title = {Automated structural defects diagnosis in underground transportation tunnels using semantic technologies},
journal = {Automation in Construction},
volume = {107},
pages = {102929},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.102929},
url = {https://www.sciencedirect.com/science/article/pii/S0926580518306538},
author = {Min Hu and Yunru Liu and Vijayan Sugumaran and Biwen Liu and Juan Du},
keywords = {Defects diagnosis, Transportation tunnel, Semantic Web, BIM, IFC},
abstract = {Detecting structural defects and finding the underlying causes accurately and timely is crucial for developing effective maintenance strategies to keep up tunnel safety and availability. Data-driven methods are gradually becoming the norm for tunnel health diagnosis but data involved are usually limited to a single system or a particular type of defect. These methods are not effective for cross-system or multi-stage detection because of the following difficulties: (1) high data heterogeneity, (2) complex spatiotemporal relationships, and (3) high expert knowledge involvement required. In order to overcome these challenges, this paper follows the constructive research approach to develop a system called Tunnel Defects Diagnosis System (TDDS) based on Industry Foundation Classes (IFC) and Semantic Web technologies. In TDDS, a meta-standard is introduced to set up mapping rules for the integration of heterogeneous data and a Tunnel Diagnosis Ontology (TDO) is established to formally define the complex spatiotemporal relationships among data. Predefined rules based on expert knowledge enable automatic reasoning to provide support for decision making with respect to cause detection. This system has been applied to the Dalian Road Tunnel in Shanghai to demonstrate its feasibility and effectiveness.}
}
@article{VANZO2020103181,
title = {Grounded language interpretation of robotic commands through structured learning},
journal = {Artificial Intelligence},
volume = {278},
pages = {103181},
year = {2020},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2019.103181},
url = {https://www.sciencedirect.com/science/article/pii/S0004370218302935},
author = {Andrea Vanzo and Danilo Croce and Emanuele Bastianelli and Roberto Basili and Daniele Nardi},
keywords = {Spoken language understanding, Automatic interpretation of robotic commands, Grounded language learning, Human-Robot interaction},
abstract = {The presence of robots in everyday life is increasing day by day at a growing pace. Industrial and working environments, health-care assistance in public or domestic areas can benefit from robots' services to accomplish manifold tasks that are difficult and annoying for humans. In such scenarios, Natural Language interactions, enabling collaboration and robot control, are meant to be situated, in the sense that both the user and the robot access and make reference to the environment. Contextual knowledge may thus play a key role in solving inherent ambiguities of grounded language as, for example, the prepositional phrase attachment. In this work, we present a linguistic pipeline for semantic processing of robotic commands, that combines discriminative structured learning, distributional semantics and contextual evidence extracted from the working environment. The final goal is to make the interpretation process of linguistic exchanges depending on physical, cognitive and language-dependent aspects. We present, formalize and discuss an adaptive Spoken Language Understanding chain for robotic commands, that explicitly depends on the operational context during both the learning and processing stages. The resulting framework allows to model heterogeneous information concerning the environment (e.g., positional information about the objects and their properties) and to inject it in the learning process. Empirical results demonstrate a significant contribution of such additional dimensions, achieving up to a 25% of relative error reduction with respect to a pipeline that only exploits linguistic evidence.}
}
@article{BAKER2021100021,
title = {Four paradigms in learning analytics: Why paradigm convergence matters},
journal = {Computers and Education: Artificial Intelligence},
volume = {2},
pages = {100021},
year = {2021},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2021.100021},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X21000151},
author = {Ryan S. Baker and Dragan Gašević and Shamya Karumbaiah},
keywords = {Learning analytics, Artificial intelligence in education, Quantitative ethnography, Learning at scale, Machine learning, Research paradigms},
abstract = {Learning analytics has matured significantly since its early days. The field has rapidly grown in terms of the reputation of its publication venues, established a vibrant community, and has demonstrated an increasing impact on policy and practice. However, the boundaries of the field are still being explored by many researchers in a bid to determine what differentiates a contribution in learning analytics from contributions in related fields, which also center around data in education. In this paper, we propose that instead of emphasizing the examination of differences, a healthy development of the field should focus on collaboration and be informed by the developments in related fields. Specifically, the paper presents a framework for analysis how contemporary fields focused on the study of data in education influence trends in learning analytics. The framework is focused on the methodological paradigms that each of the fields is primarily based on – i.e., essentialist, entatitive/reductionst, ontological/dialectical, and existentialist. The paper uses the proposed framework to analyze how learning analytics (ontological) is being methodologically influenced by recent trends in the fields of educational data mining (entatitive), quantitative ethnography (existentialist), and learning at scale (essentialist). Based on the results of the analysis, this paper identifies gaps in the literature that warrant future research.}
}
@article{ENRIQUEZ20246,
title = {Generative AI and composing: an intergenerational conversation among literacy scholars},
journal = {English Teaching: Practice & Critique},
volume = {23},
number = {1},
pages = {6-22},
year = {2024},
issn = {1175-8708},
doi = {https://doi.org/10.1108/ETPC-08-2023-0104},
url = {https://www.sciencedirect.com/science/article/pii/S1175870824000190},
author = {Grace Enriquez and Victoria Gill and Gerald Campano and Tracey T. Flores and Stephanie Jones and Kevin M. Leander and Lucinda McKnight and Detra Price-Dennis},
keywords = {Writing, Literacy, Critical literacy, Artificial intelligence, Composing, ChatGPT},
abstract = {Purpose
The purpose of this paper is to provide a transcript of a dialogue among literacy educators and researchers on the impact of generative aritficial intelligence (AI) in the field. In the spring of 2023, a lively conversation emerged on the National Council of Research on Language and Literacy (NCRLL)’s listserv. Stephanie initiated the conversation by sharing an op-ed she wrote for Atlanta Journal-Constitution about the rise of ChatGPT and similar generative AI platforms, moving beyond the general public’s concerns about student cheating and robot takeovers. NCRLL then convened a webinar of eight leading scholars in writing and literacies development, inspired by that listerv conversation and an organizational interest in promoting intergenerational collaboration among literacy scholars.
Design/methodology/approach
As former doctoral students of two of the panel participants, webinar facilitators Grace and Victoria positioned themselves primarily as learners about this topic and gathered questions from colleagues, P-16 practitioners and those outside the field of education to assess the concerns and wonderings that ChatGPT and generative AI have raised. The following webinar conversation was recorded on two different days due to scheduling conflicts. It has been merged and edited into one dialogue for coherence and convergence.
Findings
Panel participants raise a host of questions and issues that go beyond topics of ethics, morality and basic writing instruction. Furthermore, in dialogue with one another, they describe possibilities for meaningful pedagogy and critical literacy to ensure that generative AI is used for a socially just future for students. While the discussion addressed matters of pedagogy, definitions of literacy and the purpose of (literacy) education, other themes included a critique of capitalism; an interrogation of the systems of power and oppression involved in using generative AI; and the philosophical, ontological, ethical and practical life questions about being human.
Originality/value
This paper provides a glimpse into one of the first panel conversations about ChatGPT and generative AI in the field of literacy. Not only are the panel members respected scholars in the field, they are also former doctoral students and advisors of one another, thus positioning all involved as both learners and teachers of this new technology.}
}
@article{ANDRES2021103398,
title = {A data model for collaborative manufacturing environments},
journal = {Computers in Industry},
volume = {126},
pages = {103398},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103398},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521000051},
author = {Beatriz Andres and Raul Poler and Raquel Sanchis},
keywords = {Data model, Cloud environment, Collaborative manufacturing},
abstract = {This paper addresses the problem of information exchange required for collaborative manufacturing planning processes at supply network levels. For a few years now, the traditional manufacturing planning paradigm has shifted toward optimizing collaborative plans at the inter-enterprise level driven by cloud-enabled tools (manufacturing platforms), from which the need to share data between networked partners arises to solve intra-enterprise plans in real-time scenarios. Collaboration among enterprises has to increasingly deal with the sharing the information encoded in ontologies. Cloud repositories are seen as collaboration mechanisms whose main aim is to exchange data regardless of their origins and nature. In line with this, industrial planning research is encouraged by solving problems regarding information exchange, sharing and storage in cloud collaborative environments. The contribution of this research paper lies in identifying information used by manufacturing enterprises when they follow their planning processes in a cloud collaborative manufacturing context to propose common formalized data terminologies. This paper introduces a novel data model for collaborative manufacturing environments (CMData) by building a taxonomy of data concepts to represent information about the replenishment, production and delivery planning domains. Although there are standards aimed at exchanging product data that covers different parts of the history of a product from conceptual design to disposal, this proposal is defined as a detailed version of such current standard initiatives covering the specific area in the domain of production planning and supply chain collaboration. Moreover, this proposal complements and gives an answer to the new cloud collaborative business needs from industry. The CMData has the following objectives: (i) to solve problems related with data interpretation; (ii) to solve the sharing between enterprise legacy systems and cloud environments through the mapping procedure and (iii) to identify the required data coming from different enterprises to collaboratively compute the joint planning activities. The proposed CMData are validated through their application in the collaborative production scheduling plan of a second-tier supplier and a first-tier supplier in the automotive industry. Future work and open issues are also discussed.}
}
@article{KONYS20203626,
title = {Knowledge based approach to sustainability assessment},
journal = {Procedia Computer Science},
volume = {176},
pages = {3626-3635},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.023},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920319153},
author = {Agnieszka Konys},
keywords = {Sustainability assessment, knowledge model, sustainability, ontology, indicators},
abstract = {The world is currently trying to implement activities that will contribute to achieving sustainable development. Therefore, questions arise about how to comprehensively assess the impact of undertaken actions on sustainable development, their effective measurement and extraction of relevant information that will help in making even better decisions on sustainable development. Due to the fact that many efforts have been made to promote, implement and maintain an appropriate level of sustainable development, the very process of assessing sustainable development is now becoming increasingly important. However, the problem arises which type of assessment tool may be more beneficial to measure selected aspects of sustainable development and where to find the necessary knowledge about it. This paper proposes an extendable knowledge model to assess sustainability more comprehensively and to capture interrelationships across the multiple measures, frameworks, indicators and other approaches to measure sustainability. The need for extension of SA is resulted for two reasons, the obvious need to update the methodological approaches contained in the knowledge base and the need to increase the practical possibilities of the model by including relevant datasets.}
}
@article{ZHANG2023100697,
title = {Exploring the development of student feedback literacy in the second language writing classroom},
journal = {Assessing Writing},
volume = {55},
pages = {100697},
year = {2023},
issn = {1075-2935},
doi = {https://doi.org/10.1016/j.asw.2023.100697},
url = {https://www.sciencedirect.com/science/article/pii/S1075293523000053},
author = {Tiefu Zhang and Zhicheng Mao},
keywords = {Feedback literacy, Written feedback, Assessment literacy, Classroom writing assessment, Second language writing},
abstract = {Despite the growing recognition of the important role of student feedback literacy in exploiting feedback opportunities, little is known about the trajectory of student feedback literacy in L2 writing classrooms. To fill this gap, the current study investigated the development of student feedback literacy in an authentic L2 writing classroom where pedagogical approaches and feedback practices were structured towards enabling feedback opportunities. Data were gathered from multiple sources over a 16-week semester, including pre- and post-study student questionnaires, interviews with students and their writing teacher, students’ original and revised drafts, and classroom documents. The results revealed that through the teacher’s systematic approach that integrated preparatory activities (peer feedback training and assessment criteria elaborating), multi-source feedback practices (peer and teacher feedback), and post-feedback reinforcement (reflective activities), students experienced sustained opportunities and perceived an improvement in their feedback literacy over the semester. Specifically, the student participants reported enhanced capacities to elicit feedback, make judgments, and take actions, as well as strengthened dispositions to appreciate feedback and manage affect. The present study advances our knowledge of the nascent concept of feedback literacy and informs L2 pedagogy by presenting a developmental picture of L2 student writers’ feedback literacy.}
}
@article{HARRISON2024e00484,
title = {Breaking bad? Playing the fool and constructing the ‘bad researcher’ in entrepreneurship},
journal = {Journal of Business Venturing Insights},
volume = {22},
pages = {e00484},
year = {2024},
issn = {2352-6734},
doi = {https://doi.org/10.1016/j.jbvi.2024.e00484},
url = {https://www.sciencedirect.com/science/article/pii/S2352673424000362},
author = {Richard T. Harrison},
keywords = {Entrepreneurship, Posthumanism, Post-qualitative inquiry, Crises, Knowledge production, ‘Bad’ research, The fool/trickster, Administration and heuristic of fear},
abstract = {How to deal with grand challenges and the crisis of knowledge production and their implications for entrepreneurial research and practice is a topic of growing interest. In this paper we argue that we need to rethink who is involved in entrepreneurship research and how that research is conducted and communicated. This begins by moving beyond the traditional ostensible objective separation of the ‘researcher’ from the ‘research subject’ to adopt a posthuman and post-qualitative inquiry perspective that questions the dominant position of the human subject and challenges the humanistic belief in the essential, conscious and intentional human as the primary source of agency. As such, it adopts a process ontology, stresses hybridity and difference and encourages experimentation. This requires us to become ‘bad researchers’, undertaking subversive research that goes beyond the oppositions of quantitative/qualitative and foundationalist/non-foundationalist. In this we take the ‘fool’ (jester, trickster) as our guide. Historically associated with inversion, usurping authority and putting down the mighty the fool is a liminal character who has the duty to ask all those questions that no one else dares to ask. The paper concludes with suggestions as to how this may inform a re-newed entrepreneurship for the crisis-laden twenty first century.}
}
@article{BERGOMI2024102924,
title = {Reshaping free-text radiology notes into structured reports with generative question answering transformers},
journal = {Artificial Intelligence in Medicine},
volume = {154},
pages = {102924},
year = {2024},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.102924},
url = {https://www.sciencedirect.com/science/article/pii/S0933365724001660},
author = {Laura Bergomi and Tommaso M. Buonocore and Paolo Antonazzo and Lorenzo Alberghi and Riccardo Bellazzi and Lorenzo Preda and Chandra Bortolotto and Enea Parimbelli},
keywords = {Natural language processing, Clinical text, Generative artificial intelligence, Radiology, Lymphoma, Biomedical information extraction},
abstract = {Background
Radiology reports are typically written in a free-text format, making clinical information difficult to extract and use. Recently, the adoption of structured reporting (SR) has been recommended by various medical societies thanks to the advantages it offers, e.g. standardization, completeness, and information retrieval. We propose a pipeline to extract information from Italian free-text radiology reports that fits with the items of the reference SR registry proposed by a national society of interventional and medical radiology, focusing on CT staging of patients with lymphoma.
Methods
Our work aims to leverage the potential of Natural Language Processing and Transformer-based models to deal with automatic SR registry filling. With the availability of 174 Italian radiology reports, we investigate a rule-free generative Question Answering approach based on the Italian-specific version of T5: IT5. To address information content discrepancies, we focus on the six most frequently filled items in the annotations made on the reports: three categorical (multichoice), one free-text (free-text), and two continuous numerical (factual). In the preprocessing phase, we encode also information that is not supposed to be entered. Two strategies (batch-truncation and ex-post combination) are implemented to comply with the IT5 context length limitations. Performance is evaluated in terms of strict accuracy, f1, and format accuracy, and compared with the widely used GPT-3.5 Large Language Model. Unlike multichoice and factual, free-text answers do not have 1-to-1 correspondence with their reference annotations. For this reason, we collect human-expert feedback on the similarity between medical annotations and generated free-text answers, using a 5-point Likert scale questionnaire (evaluating the criteria of correctness and completeness).
Results
The combination of fine-tuning and batch splitting allows IT5 ex-post combination to achieve notable results in terms of information extraction of different types of structured data, performing on par with GPT-3.5. Human-based assessment scores of free-text answers show a high correlation with the AI performance metrics f1 (Spearman's correlation coefficients>0.5, p-values<0.001) for both IT5 ex-post combination and GPT-3.5. The latter is better at generating plausible human-like statements, even if it systematically provides answers even when they are not supposed to be given.
Conclusions
In our experimental setting, a fine-tuned Transformer-based model with a modest number of parameters (i.e., IT5, 220 M) performs well as a clinical information extraction system for automatic SR registry filling task. It can extract information from more than one place in the report, elaborating it in a manner that complies with the response specifications provided by the SR registry (for multichoice and factual items), or that closely approximates the work of a human-expert (free-text items); with the ability to discern when an answer is supposed to be given or not to a user query.}
}
@article{GROSS202125,
title = {The Business Process Design Space for exploring process redesign alternatives},
journal = {Business Process Management Journal},
volume = {27},
number = {8},
pages = {25-56},
year = {2021},
issn = {1463-7154},
doi = {https://doi.org/10.1108/BPMJ-03-2020-0116},
url = {https://www.sciencedirect.com/science/article/pii/S1463715421000157},
author = {Steven Gross and Katharina Stelzl and Thomas Grisold and Jan Mendling and Maximilian Röglinger and Jan {vom Brocke}},
keywords = {Business process redesign, Process improvement, Process innovation, Design space exploration},
abstract = {Purpose
Process redesign refers to the intentional change of business processes. While process redesign methods provide structure to redesign projects, they provide limited support during the actual creation of to-be processes. More specifically, existing approaches hardly develop an ontological perspective on what can be changed from a process design point of view, and they provide limited procedural guidance on how to derive possible process design alternatives. This paper aims to provide structured guidance during the to-be process creation.
Design/methodology/approach
Using design space exploration as a theoretical lens, the authors develop a conceptual model of the design space for business processes, which facilitates the systematic exploration of design alternatives along different dimensions. The authors utilized an established method for taxonomy development for constructing the conceptual model. First, the authors derived design dimensions for business processes and underlying characteristics through a literature review. Second, the authors conducted semi-structured interviews with professional process experts. Third, the authors evaluated their artifact through three real-world applications.
Findings
The authors identified 19 business process design dimensions that are grouped into different layers and specified by underlying characteristics. Guiding questions and illustrative real-world examples help to deploy these design dimensions in practice. Taken together, the design dimensions form the “Business Process Design Space” (BPD-Space).
Research limitations/implications
Practitioners can use the BPD-Space to explore, question and rethink business processes in various respects.
Originality/value
The BPD-Space complements existing approaches by explicating process design dimensions. It abstracts from specific process flows and representations of processes and supports an unconstrained exploration of various alternative process designs.}
}
@article{KIM2025110290,
title = {Modulating vascular smooth muscle cell phenotype via Wnt-Independent FRZB pathways},
journal = {Archives of Biochemistry and Biophysics},
volume = {764},
pages = {110290},
year = {2025},
issn = {0003-9861},
doi = {https://doi.org/10.1016/j.abb.2025.110290},
url = {https://www.sciencedirect.com/science/article/pii/S0003986125000037},
author = {Hyomin Kim and Eun Kyoung Kim and Yeuni Yu and Hye Jin Heo and Dokyoung Kim and Su-Yeon Cho and Yujin Kwon and Won Kyu Kim and Kihun Kim and Dai Sik Ko and Yun Hak Kim},
keywords = {Frizzled-related protein, Vascular smooth muscle cell, Atherosclerosis, Phenotype modulation, Focal adhesion},
abstract = {Background and aims
Vascular smooth muscle cells are pivotal in atherosclerosis, transitioning from a contractile to a synthetic phenotype, which is associated with increased proliferation and inflammation. FRZB, a Wnt signaling modulator, has been implicated in vascular pathology, but its specific role in vascular smooth muscle cell phenotype modulation is not well understood. This study investigates the role of FRZB in regulating vascular smooth muscle cell phenotypes.
Methods
Vascular smooth muscle cell regions were categorized based on FRZB expression levels, and various analyses, including differential gene expression, KEGG pathway analysis, and Disease Ontology analysis, were conducted. FRZB knockdown in human aortic vascular smooth muscle cell was performed using siRNA, followed by assessments of cell migration, proliferation, and phenotype marker expression.
Results
FRZB expression was significantly reduced in synthetic type compared to contractile type in both mouse models and human samples. FRZB knockdown in human vascular smooth muscle cells led to increased cell migration and proliferation, alongside decreased expression of contractile markers and increased synthetic markers. Unexpectedly, FRZB knockdown suppressed Wnt signaling. Pathway analysis revealed associations with the PI3K-Akt signaling pathway, focal adhesion, and ECM interactions.
Conclusions
Our study highlights FRZB's role in Vascular smooth muscle cell phenotype modulation, showing that reduced FRZB expression correlates with a synthetic phenotype and increased disease markers. FRZB does not enhance Wnt signaling but may regulate vascular smooth muscle cell behavior through alternative pathways. These findings suggest FRZB as a potential therapeutic target for stabilizing vascular smooth muscle cells and managing atherosclerosis.}
}
@article{ANGEL201890,
title = {Automated modelling assistance by integrating heterogeneous information sources},
journal = {Computer Languages, Systems & Structures},
volume = {53},
pages = {90-120},
year = {2018},
issn = {1477-8424},
doi = {https://doi.org/10.1016/j.cl.2018.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1477842417301690},
author = {Mora Segura Ángel and Juan {de Lara} and Patrick Neubauer and Manuel Wimmer},
keywords = {Modelling, (Meta-)modelling, Modelling assistance, Domain-specific languages, Language engineering},
abstract = {Model-Driven Engineering (MDE) uses models as its main assets in the software development process. The structure of a model is described through a meta-model. Even though modelling and meta-modelling are recurrent activities in MDE and a vast amount of MDE tools exist nowadays, they are tasks typically performed in an unassisted way. Usually, these tools cannot extract useful knowledge available in heterogeneous information sources like XML, RDF, CSV or other models and meta-models. We propose an approach to provide modelling and meta-modelling assistance. The approach gathers heterogeneous information sources in various technological spaces, and represents them uniformly in a common data model. This enables their uniform querying, by means of an extensible mechanism, which can make use of services, e.g., for synonym search and word sense analysis. The query results can then be easily incorporated into the (meta-)model being built. The approach has been realized in the Extremo tool, developed as an Eclipse plugin. Extremo has been validated in the context of two domains – production systems and process modelling – taking into account a large and complex industrial standard for classification and product description. Further validation results indicate that the integration of Extremo in various modelling environments can be achieved with low effort, and that the tool is able to handle information from most existing technological spaces.}
}
@article{ZHOU20253673,
title = {AOP-DRL: A deep representation learning framework for the computational prediction of antioxidant peptides},
journal = {Computational and Structural Biotechnology Journal},
volume = {27},
pages = {3673-3684},
year = {2025},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2025.08.014},
url = {https://www.sciencedirect.com/science/article/pii/S2001037025003368},
author = {Yongzhu Zhou and Wanlin Liu and Qiao Liu and Jie Liu and Xing Yu and Jie Ma and Yunping Zhu},
keywords = {Antioxidant peptides, Deep representation learning, Protein language models, Text-CNN, Peptide sequence prediction},
abstract = {Molecular oxygen is vital for life but can generate reactive oxygen species (ROS), leading to oxidative stress and biomolecular damage. Antioxidant peptides mitigate ROS but face scalability challenges in traditional identification methods. To address this, we developed antioxidant peptides deep representation learning (AOP-DRL), a deep learning framework that combines protein language models with hierarchical convolutional networks for high-throughput prediction. The training dataset from the publicly available original study includes experimentally validated antioxidant sequences and negative controls orthogonally verified via public repositories under standard redox proteomics data partitioning protocols. Our architecture overcomes limitations in handling variable peptide lengths and capturing nonlinear residue interactions. Benchmarking experiments reveal that AOP-DRL outperforms state-of-the-art models in terms of accuracy while generalizing well across diverse datasets. Our model achieved accuracy improvements of 7.26 %, 2.57 %, 2.59 %, and 4.20 % on the prepartitioned subsets P60, P70, P80, and P90, respectively, compared with the mean accuracy of previously specialized models for antioxidant peptides. This approach provides a cost-effective, scalable alternative to wet-lab methods, accelerating the discovery of therapeutic and nutraceutical peptides. The adaptability of the framework also suggests broader applications in bioactive peptide prediction for personalized medicine and functional foods.}
}
@article{JIN20232595,
title = {HydRA: Deep-learning models for predicting RNA-binding capacity from protein interaction association context and protein sequence},
journal = {Molecular Cell},
volume = {83},
number = {14},
pages = {2595-2611.e11},
year = {2023},
issn = {1097-2765},
doi = {https://doi.org/10.1016/j.molcel.2023.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S1097276523004665},
author = {Wenhao Jin and Kristopher W. Brannan and Katannya Kapeli and Samuel S. Park and Hui Qing Tan and Maya L. Gosztyla and Mayuresh Mujumdar and Joshua Ahdout and Bryce Henroid and Katherine Rothamel and Joy S. Xiang and Limsoon Wong and Gene W. Yeo},
keywords = {RNA-binding proteins, machine learning, RNA-binding domains, protein-protein interaction network},
abstract = {Summary
RNA-binding proteins (RBPs) control RNA metabolism to orchestrate gene expression and, when dysfunctional, underlie human diseases. Proteome-wide discovery efforts predict thousands of RBP candidates, many of which lack canonical RNA-binding domains (RBDs). Here, we present a hybrid ensemble RBP classifier (HydRA), which leverages information from both intermolecular protein interactions and internal protein sequence patterns to predict RNA-binding capacity with unparalleled specificity and sensitivity using support vector machines (SVMs), convolutional neural networks (CNNs), and Transformer-based protein language models. Occlusion mapping by HydRA robustly detects known RBDs and predicts hundreds of uncharacterized RNA-binding associated domains. Enhanced CLIP (eCLIP) for HydRA-predicted RBP candidates reveals transcriptome-wide RNA targets and confirms RNA-binding activity for HydRA-predicted RNA-binding associated domains. HydRA accelerates construction of a comprehensive RBP catalog and expands the diversity of RNA-binding associated domains.}
}
@article{YU2023120114,
title = {Discovering topics and trends in the field of Artificial Intelligence: Using LDA topic modeling},
journal = {Expert Systems with Applications},
volume = {225},
pages = {120114},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120114},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423006164},
author = {Dejian Yu and Bo Xiang},
keywords = {Artificial Intelligence, Topic model, Latent Dirichlet allocation, Topic distribution},
abstract = {Artificial Intelligence (AI) has affected all aspects of social life in recent years. This study reviews 177,204 documents published in 25 journals and 16 conferences in the AI research from 1990 to 2021, and applies the Latent Dirichlet allocation (LDA) model to extract the 40 topics from the abstracts. According to the topics obtained, 7 subfields of the AI field can be discovered: Approximate Reasoning, Computational Theory, Intelligent Automation, Artificial Neural Network, Machine Learning, Natural Language Processing, and Computer Vision. This study aggregates the results of the LDA model from the perspectives of year, publication source, and country/region. The aggregated result is the topic distribution from different perspectives. Analysis of the aggregated results reveals the research characteristics of different publication sources (countries/regions) in the AI research, and which publication sources (countries/regions) have similar research content. These results provide help for scholars and research institutions to choose research directions, and new entrants to understand the dynamics of the field.}
}
@article{LOSSIOVENTURA201831,
title = {A novel framework for biomedical entity sense induction},
journal = {Journal of Biomedical Informatics},
volume = {84},
pages = {31-41},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2018.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S1532046418301138},
author = {J.A. Lossio-Ventura and J. Bian and C. Jonquet and M. Roche and M. Teisseire},
keywords = {Word sense induction, Polysemy detection, Biomedicine, BioNLP, Clustering, Classification, Number of cluster prediction},
abstract = {Background
Rapid advancements in biomedical research have accelerated the number of relevant electronic documents published online, ranging from scholarly articles to news, blogs, and user-generated social media content. Nevertheless, the vast amount of this information is poorly organized, making it difficult to navigate. Emerging technologies such as ontologies and knowledge bases (KBs) could help organize and track the information associated with biomedical research developments. A major challenge in the automatic construction of ontologies and KBs is the identification of words with its respective sense(s) from a free-text corpus. Word-sense induction (WSI) is a task to automatically induce the different senses of a target word in the different contexts. In the last two decades, there have been several efforts on WSI. However, few methods are effective in biomedicine and life sciences.
Methods
We developed a framework for biomedical entity sense induction using a mixture of natural language processing, supervised, and unsupervised learning methods with promising results. It is composed of three main steps: (1) a polysemy detection method to determine if a biomedical entity has many possible meanings; (2) a clustering quality index-based approach to predict the number of senses for the biomedical entity; and (3) a method to induce the concept(s) (i.e., senses) of the biomedical entity in a given context.
Results
To evaluate our framework, we used the well-known MSH WSD polysemic dataset that contains 203 annotated ambiguous biomedical entities, where each entity is linked to 2–5 concepts. Our polysemy detection method obtained an F-measure of 98%. Second, our approach for predicting the number of senses achieved an F-measure of 93%. Finally, we induced the concepts of the biomedical entities based on a clustering algorithm and then extracted the keywords of reach cluster to represent the concept.
Conclusions
We have developed a framework for biomedical entity sense induction with promising results. Our study results can benefit a number of downstream applications, for example, help to resolve concept ambiguities when building Semantic Web KBs from biomedical text.}
}
@article{MISKOVZIVANOV20211713,
title = {Natural language processing to aggregate knowledge of biological networks},
journal = {Brain Stimulation},
volume = {14},
number = {6},
pages = {1713},
year = {2021},
issn = {1935-861X},
doi = {https://doi.org/10.1016/j.brs.2021.10.411},
url = {https://www.sciencedirect.com/science/article/pii/S1935861X21006574},
author = {Natasa Miskov-Zivanov and Stefan Andjelkovic}
}
@article{SARKER2024101110,
title = {Multi-aspect rule-based AI: Methods, taxonomy, challenges and directions towards automation, intelligence and transparent cybersecurity modeling for critical infrastructures},
journal = {Internet of Things},
volume = {25},
pages = {101110},
year = {2024},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2024.101110},
url = {https://www.sciencedirect.com/science/article/pii/S2542660524000520},
author = {Iqbal H. Sarker and Helge Janicke and Mohamed Amine Ferrag and Alsharif Abuadbba},
keywords = {Cybersecurity, Rule-based Modeling, Explainable AI, Responsible AI, Machine learning, Security data analytics, Knowledge discovery, Data-driven decision-making, Automation, Intelligence, Transparency, Trustworthiness, Critical infrastructures},
abstract = {Critical infrastructure (CI) typically refers to the essential physical and virtual systems, assets, and services that are vital for the functioning and well-being of a society, economy, or nation. However, the rapid proliferation and dynamism of today’s cyber threats in digital environments may disrupt CI functionalities, which would have a debilitating impact on public safety, economic stability, and national security. This has led to much interest in effective cybersecurity solutions regarding automation and intelligent decision-making, where AI-based modeling is potentially significant. In this paper, we take into account “Rule-based AI” rather than other black-box solutions since model transparency, i.e., human interpretation, explainability, and trustworthiness in decision-making, is an essential factor, particularly in cybersecurity application areas. This article provides an in-depth study on multi-aspect rule based AI modeling considering human interpretable decisions as well as security automation and intelligence for CI. We also provide a taxonomy of rule generation methods by taking into account not only knowledge-driven approaches based on human expertise but also data-driven approaches, i.e., extracting insights or useful knowledge from data, and their hybridization. This understanding can help security analysts and professionals comprehend how systems work, identify potential threats and anomalies, and make better decisions in various real-world application areas. We also cover how these techniques can address diverse cybersecurity concerns such as threat detection, mitigation, prediction, diagnosis for root cause findings, and so on in different CI sectors, such as energy, defence, transport, health, water, agriculture, etc. We conclude this paper with a list of identified issues and opportunities for future research, as well as their potential solution directions for how researchers and professionals might tackle future generation cybersecurity modeling in this emerging area of study.}
}
@article{YE2022100680,
title = {Automated conversion of engineering rules: Towards flexible manufacturing collaboration},
journal = {Results in Engineering},
volume = {16},
pages = {100680},
year = {2022},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2022.100680},
url = {https://www.sciencedirect.com/science/article/pii/S2590123022003504},
author = {Xinfeng Ye and Yuqian Lu and Sathiamoorthy Manoharan},
keywords = {Natural language processing, Machine learning, Smart manufacturing, Cloud manufacturing, Semantic web, Engineering knowledge},
abstract = {Rapid on-demand manufacturing resource sharing within and between factories are critical to achieving responsive autonomous manufacturing collaborations towards mass personalization. To this end, cloud manufacturing technologies allow resource owners/service providers to virtualize and encapsulate their resources as services accessible over the Internet. Decision-making in cloud manufacturing needs to utilize real-world engineering knowledge from different parties. Many existing systems have adopted the semantic web-based decision-making framework, in which engineering knowledge is modeled using structured syntax. However, manually converting engineering rules to semantic rules is time-consuming and error prone. This research proposes a machine learning model, based on the Transformer model, that uses neural machine translation techniques to convert engineering knowledge expressed in natural language to structured semantic rules directly. The model is implemented using neural network. The model is first trained using typical sentences that are used for describing engineering knowledge. From these sample sentences, the model learns the patterns and the meaning of the sentences. This allows the model to identify the service providers, resource users, and the resources described in the sentences. As a result, the corresponding semantic rules can be constructed. Compared with previous approaches, the proposed scheme not only improves the conversion accuracy but also reduces the amount of required human interaction, simplifying the system and its use.}
}
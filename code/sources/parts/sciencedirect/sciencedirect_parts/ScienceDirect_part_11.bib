@article{KUMARI2025103997,
title = {Can LLMs revolutionize text mining in chemistry? A comparative study with domain-specific tools},
journal = {Computer Standards & Interfaces},
volume = {94},
pages = {103997},
year = {2025},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2025.103997},
url = {https://www.sciencedirect.com/science/article/pii/S0920548925000261},
author = {Madhavi Kumari and Rohit Chauhan and Prabha Garg},
keywords = {Chemical Named Entity Recognition (CNER), Large Language Models (LLMs), Text mining, Retrieval-Augmented Generation (RAG), LLaMA-2},
abstract = {The exponential growth of chemical literature necessitates advanced tools for efficient data extraction and utilization. This study investigates the performance of Large Language Models (LLMs) in Chemical Named Entity Recognition (CNER), comparing them against traditional domain-specific tools. We fine-tuned the LLaMA-2 model using the NLM-Chem corpus and integrated a Retrieval-Augmented Generation (RAG) pipeline to enhance performance. The results revealed that fine-tuned LLaMA-2 models, particularly those incorporating RAG, achieved an F1 score of 0.82, surpassing the score of traditional CNER tools. Furthermore, LLMs demonstrated superior generalizability across different datasets. The study also explores the dependency of LLMs size for CNER tasks. A practical case study highlighting the application of these models in chemical entity extraction from pharmaceutical literature, achieving high accuracy in identifying drug and their interactions. These findings establish LLMs as a robust and adaptable alternative to traditional CNER tools, paving the way for transformative applications in chemoinformatics.}
}
@article{FANG2020103310,
title = {Knowledge graph for identifying hazards on construction sites: Integrating computer vision with ontology},
journal = {Automation in Construction},
volume = {119},
pages = {103310},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103310},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519309082},
author = {Weili Fang and Ling Ma and Peter E.D. Love and Hanbin Luo and Lieyun Ding and Ao Zhou},
keywords = {Hazards, Ontology, Computer vision, Safety, Knowledge graph database},
abstract = {Hazards potentially affect the safety of people on construction sites include falls from heights (FFH), trench and scaffold collapse, electric shock and arc flash/arc blast, and failure to use proper personal protective equipment. Such hazards are significant contributors to accidents and fatalities. Computer vision has been used to automatically detect safety hazards to assist with the mitigation of accidents and fatalities. However, as safety regulations are subject to change and become more stringent prevailing computer vision approaches will become obsolete as they are unable to accommodate the adjustments that are made to practice. This paper integrates computer vision algorithms with ontology models to develop a knowledge graph that can automatically and accurately recognise hazards while adhering to safety regulations, even when they are subjected to change. Our developed knowledge graph consists of: (1) an ontological model for hazards: (2) knowledge extraction; and (3) knowledge inference for hazard identification. We focus on the detection of hazards associated with FFH as an example to illustrate our proposed approach. We also demonstrate that our approach can successfully detect FFH hazards in varying contexts from images.}
}
@article{GEORGE2019103642,
title = {Review of ontology-based recommender systems in e-learning},
journal = {Computers & Education},
volume = {142},
pages = {103642},
year = {2019},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2019.103642},
url = {https://www.sciencedirect.com/science/article/pii/S0360131519301952},
author = {Gina George and Anisha M. Lal},
keywords = {Human-computer interface, Intelligent tutoring systems, Computer-mediated communication, Cooperative/collaborative learning},
abstract = {In recent years there has been an enormous increase in learning resources available online through massive open online courses and learning management systems. In this context, personalized resource recommendation has become an even more significant challenge, thereby increasing research in that direction. Recommender systems use ontology, artificial intelligence, among other techniques to provide personalized recommendations. Ontology is a way to model learners and learning resources, among others, which helps to retrieve details. This, in turn, generates more relevant materials to learners. Ontologies have benefits of reusability, reasoning ability, and supports inference mechanisms, which helps to provide enhanced recommendations. The comprehensive survey in this paper gives an overview of the research in progress using ontology to achieve personalization in recommender systems in the e-learning domain.}
}
@article{FAHLAND2025102416,
title = {How well can a large language model explain business processes as perceived by users?},
journal = {Data & Knowledge Engineering},
volume = {157},
pages = {102416},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102416},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000114},
author = {Dirk Fahland and Fabiana Fournier and Lior Limonad and Inna Skarbovsky and Ava J.E. Swevels},
keywords = {Business process, Methodologies and tools, AI, Explainability, Large Language Models},
abstract = {Large Language Models (LLMs) are trained on a vast amount of text to interpret and generate human-like textual content. They are becoming a vital vehicle in realizing the vision of the autonomous enterprise, with organizations today actively adopting LLMs to automate many aspects of their operations. LLMs are likely to play a prominent role in future AI-augmented business process management systems (ABPMSs) catering functionalities across all system lifecycle stages. One such system’s functionality is Situation-Aware eXplainability (SAX), which relates to generating causally sound and yet human-interpretable explanations that take into account the process context in which the explained condition occurred. In this paper, we present the SAX4BPM framework developed to generate SAX explanations. The SAX4BPM suite consists of a set of services and a central knowledge repository. The functionality of these services is to elicit the various knowledge ingredients that underlie SAX explanations. A key innovative component among these ingredients is the causal process execution view. In this work, we integrate the framework with an LLM to leverage its power to synthesize the various input ingredients for the sake of improved SAX explanations. Since the use of LLMs for SAX is also accompanied by a certain degree of doubt related to its capacity to adequately fulfill SAX along with its tendency for hallucination and lack of inherent capacity to reason, we pursued a methodological evaluation of the perceived quality of the generated explanations. To this aim, we developed a designated scale and conducted a rigorous user study. Our findings show that the input presented to the LLMs aided with the guard-railing of its performance, yielding SAX explanations having better-perceived fidelity. This improvement is moderated by the perception of trust and curiosity. More so, this improvement comes at the cost of the perceived interpretability of the explanation.}
}
@article{KNOLL2019427,
title = {Developing an internal logistics ontology for process mining},
journal = {Procedia CIRP},
volume = {79},
pages = {427-432},
year = {2019},
note = {12th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 18-20 July 2018, Gulf of Naples, Italy},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.02.116},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119302331},
author = {Dino Knoll and Julian Waldmann and Gunther Reinhart},
keywords = {Process mining, Preprocessing, Internal logistics, Ontology},
abstract = {Process mining offers the potential for internal logistics process improvement using data. While there exists a massive amount of data, identifying and understanding relevant data across different information systems and complex data models is difficult. To address this issue, ontologies can be used to formalize a shared understanding. This paper aims to provide an extended internal logistics ontology focusing the process perspective. Existing internal ontologies are reviewed, compared and merged. Additionally, related resources such as products and packaging is integrated. In conclusion, this paper proposes a domain ontology to support process mining within internal logistics.}
}
@article{WU2021103428,
title = {Ontological knowledge base for concrete bridge rehabilitation project management},
journal = {Automation in Construction},
volume = {121},
pages = {103428},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103428},
url = {https://www.sciencedirect.com/science/article/pii/S0926580520310086},
author = {Chengke Wu and Peng Wu and Jun Wang and Rui Jiang and Mengcheng Chen and Xiangyu Wang},
keywords = {Ontology, Bridge rehabilitation, Semantic reasoning, OWL API, Constraint management},
abstract = {Concrete bridges are critical infrastructures, which require effective rehabilitation to maintain a good condition. Bridge rehabilitation projects have complex constraints and multiple participants. Constraint management is critical for such projects. Integrating and searching for relevant information is a key step for constraint management to timely remove constraints. However, accessing project information still relies on manual searching, which can delay information flow in constraint management and hinder constraint removal. Thus, this study introduces the concrete bridge rehabilitation project management ontology (CBRPMO) to improve information integration and constraint management. The CBRPMO was created by comprehensively collecting domain knowledge of bridge rehabilitation and following standard procedures. Reasoning rules were combined with an application programming interface (i.e. OWL API) to enable functions not supported in traditional ontologies (e.g. temporal computation and dynamic updating). As such, the CBRPMO can effectively handle dynamic information in ongoing projects. The CBRPMO was validated in a case study. The results show that the CBRPMO can: 1) integrate project information of constraints, tasks and procedures, participants, and relations between these project entities; 2) support various management functions based on dynamic project information, including evaluating project progress, constraint removal, and participants' performance. The CBRPMO contributes to the industry by: 1) extending the application of ontologies in the bridge sector to cover the rehabilitation stage; 2) enhancing functions of conventional ontologies; and 3) reducing information searching time compared to manual searching, which improves constraint management approaches by automating the information searching step.}
}
@article{MARTIN20211227,
title = {Graph pattern mining on top of a domain ontology - preliminary results from a dairy production application},
journal = {Procedia Computer Science},
volume = {192},
pages = {1227-1236},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.126},
url = {https://www.sciencedirect.com/science/article/pii/S187705092101615X},
author = {Tomas Martin and Victor Fuentes and Petko Valtchev and Abdoulaye Baniré Diallo and René Lacroix and Mounir Boukadoum and Maxime Leduc},
keywords = {Knowledge discovery from data, Pattern mining, Ontologies, Graph data, Generalized patterns, Dairy production, Precision agriculture},
abstract = {A domain ontology (DO) is a machine-readable knowledge repository which, whenever properly exploited, can help to discover meaningful and intelligible patterns from compatible datasets. Yet since such data is naturally graph-shaped, the corresponding task amounts to mining what we call ontologically-generalized graph patterns. We study the underlying problem within a dairy production context where a dedicated DO has been designed beforehand. Two alternative mining approaches have been designed, both representing adaptations of methods from the literature. We evaluated them on an excerpt from our dairy production dataset and report here their respective limitations. We also sketch a way to approach the design of ontology-powered graph miner.}
}
@article{MORADI2018137,
title = {Toward improving the quality compliance checking of urban private constructions in Iran: An ontological approach},
journal = {Sustainable Cities and Society},
volume = {38},
pages = {137-144},
year = {2018},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2017.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S2210670717305231},
author = {Hossein Moradi and Mohamad Hassan Sebt and Eghbal Shakeri},
keywords = {Urban private constructions, Quality compliance checking, Urban processes, Ontology, Integration},
abstract = {City, as a complex system, needs to be managed as an integrated entity so that every encumbrance that slows its processes could be eliminated. However, developing a single mature model to integrate the whole processes of the various systems in the city is not practical. Yet, it is possible to develop purposeful and consistent models, for each specific system, that all together eventuate in the integrated management. In this regard, the mechanism of constructions quality compliance checking is one out of many systems in Iran that needs to be addressed for integration due to three main reasons; first, the multiplicity of problems in this section; secondly, the multidimensional significance of this section; and finally the ability to improve the status quo through using appropriate approach. Accordingly, the present study, by developing an ontology, which is named QA-ontology, as a missing chain, and using existent ontologies in previous studies, presents a conceptual model. This model takes advantages of the ability of ontologies in the information storing as a graph. This type of information storing provides the homogeneous information that is needed for both integration in the quality compliance checking of urban private constructions and assimilation into further extensions of integration in other mechanisms of a city.}
}
@article{OKONTA2025106328,
title = {Semantic interoperability on IoT: Aligning IFC and Smart Application Reference (SAREF) sensor data models},
journal = {Automation in Construction},
volume = {177},
pages = {106328},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106328},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525003681},
author = {Ebere Donatus Okonta and Farzad Rahimian and Vladimir Vukovic and Sergio Rodriguez},
keywords = {Semantic web, IFC, BIM, SAREF, Ontology alignment, IoT, Data models},
abstract = {This paper proposes extending the Smart Application Reference (SAREF) ontology to enable sensor modelling based on the Industry Foundation Classes (IFC) standard, enhancing semantic interoperability between IoT (Internet of Things) and Building Information Modelling (BIM). The paper introduces the Information Assigned to Device Based Ontology Matching approach (IADOM) to align saref:Sensor and IfcSensor data models. Leveraging RDF (Resource Definition Framework) Semantic Web technology, the research modelled and visualised sensor data models in the Protégé software environment, exploring basic information that defined the sensor, including class, properties, relations, attributes, geometry, and interaction. Ontology results indicate property and interaction similarities and differences in saref:Sensor and IfcSensor. The extended ontology provides a standardised and interoperable representation of sensor data and their relationships within BIM and proves that SAREF ontology extension can enhance semantic interoperability between IoT devices and BIM systems, facilitating efficient data exchange, enabling advanced analytics and decision-making processes in smart buildings.}
}
@incollection{PREISIG20191099,
title = {More power to the individual modeller using ontologies},
editor = {Anton A. Kiss and Edwin Zondervan and Richard Lakerveld and Leyla Özkan},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {46},
pages = {1099-1104},
year = {2019},
booktitle = {29th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-12-818634-3.50184-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186343501843},
author = {Heinz A. Preisig and Arne Tobias Elve and Sigve Karolius},
keywords = {Process Modelling, Automatic Code Generation, Ontology, Graph-based Modelling},
abstract = {Ontology driven process simulation yields a significant improvement not only in terms of accelerating the modelling process itself, but also the quality. Time saving allows for exploring many more alternatives in a shorter time, thus the domain of design for a given process, is readily expanded and quality reduced debugging. To make the use of ontologies successful, one needs to step away from the common data-mining utilisation of ontology, but rather use them to control and guide the modelling process. The construction of the ontology itself is a stage-wise process, where the previously added knowledge is used to control the next stage. From the root definition to the structure of the multi-disciplinary domain, to adding the mathematical description, to designing the controls of the modelling-user graphical interface, the control of constructing the model interactively, to the instantiation an the numerical solution to the visualisation, the ontology is involved at every state improving the overall task equally on every stage.}
}
@article{BHUSHAN2021114167,
title = {Classifying and resolving software product line redundancies using an ontological first-order logic rule based method},
journal = {Expert Systems with Applications},
volume = {168},
pages = {114167},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.114167},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420309052},
author = {Megha Bhushan and José {Ángel Galindo Duarte} and Piyush Samant and Ashok Kumar and Arun Negi},
keywords = {Feature model, First-order logic, Ontologies, Quality, Redundancy, Software product line},
abstract = {Software product line engineering improves software quality and diminishes development cost and time by efficiently developing software products. Its success lies in identifying the commonalities and variabilities of a set of software products which are generally modeled using feature models. The success of software product lines heavily relies upon the quality of feature models to derive high quality products. However, there are various defects that reduce profits of software product line. One of such defect is redundancy. While the majority of research work focuses on the identification of redundancies, their causes and corrections have been poorly explored. Causes and corrections must be as accurate and comprehensible as possible in order to support the developer in resolving the cause of a redundancy. This research work classified redundancies in the form of a typology. An ontological first-order logic rule based method is proposed to deal with redundancies. A two-step process is presented for mapping model to ontology based on predicate logic. First-order logic based rules are developed and applied to the generated ontology for identifying redundancies, their causes and corrections to resolve redundancies. The proposed method is illustrated using a case study from software product lines online tools repository. The results of experiments performed on 35 models with varied sizes of real world models as well as automatically-generated models from the Software Product Line Online Tools repository and models created via FeatureIDE tool conclude that the method is accurate, efficient and scalable with FM up to 30,000 features. Thus, enables deriving redundancy free end products from the product line and ultimately, improves its quality.}
}
@article{CARDOSO20181,
title = {Supporting biomedical ontology evolution by identifying outdated concepts and the required type of change},
journal = {Journal of Biomedical Informatics},
volume = {87},
pages = {1-11},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2018.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S1532046418301680},
author = {Silvio Domingos Cardoso and Cédric Pruski and Marcos {Da Silveira}},
keywords = {Ontology evolution, Biomedical ontology, Semantic Web},
abstract = {The consistent evolution of ontologies is a major challenge for systems using semantically enriched data, for example, for annotating, indexing, or reasoning. The biomedical domain is a typical example where ontologies, expressed with different formalisms, have been used for a long time and whose dynamic nature requires the regular revision of underlying systems. However, the automatic identification of outdated concepts and proposition of revision actions to update them are still open research questions. Solutions to these problems are of great interest to organizations that manage huge and dynamic ontologies. In this paper, we present an approach for (i) identifying the concepts of an ontology that require revision and (ii) suggesting the type of revision. Our analysis is based on three aspects: structural information encoded in the ontology, relational information gained from external source of knowledge (i.e., PubMed and UMLS) and temporal information derived from the history of the ontology. Our approach aims to evaluate different methods and parameters used by supervised learning classifiers to identify both the set of concepts that need revision, and the type of revision. We applied our approach to four well-known biomedical ontologies/terminologies (ICD-9-CM, MeSH, NCIt and SNOMED CT) and compared our results to similar approaches. Our model shows accuracy ranging from 68% (for SNOMED CT) to 91% (for MeSH), and an average of 71% when considering all datasets together.}
}
@article{MERLO2018,
title = {Development and Validation of a Functional Behavioural Assessment Ontology to Support Behavioural Health Interventions},
journal = {JMIR Medical Informatics},
volume = {6},
number = {2},
year = {2018},
issn = {2291-9694},
doi = {https://doi.org/10.2196/medinform.7799},
url = {https://www.sciencedirect.com/science/article/pii/S2291969418000297},
author = {Gianluca Merlo and Giuseppe Chiazzese and Davide Taibi and Antonella Chifari},
keywords = {ontology, behavioral interventions, functional behavioral assessment, eHealth care, evidence-based practice},
abstract = {Background
In the cognitive-behavioral approach, Functional Behavioural Assessment is one of the most effective methods to identify the variables that determine a problem behavior. In this context, the use of modern technologies can encourage the collection and sharing of behavioral patterns, effective intervention strategies, and statistical evidence about antecedents and consequences of clusters of problem behaviors, encouraging the designing of function-based interventions.
Objective
The paper describes the development and validation process used to design a specific Functional Behavioural Assessment Ontology (FBA-Ontology). The FBA-Ontology is a semantic representation of the variables that intervene in a behavioral observation process, facilitating the systematic collection of behavioral data, the consequential planning of treatment strategies and, indirectly, the scientific advancement in this field of study.
Methods
The ontology has been developed deducing concepts and relationships of the ontology from a gold standard and then performing a machine-based validation and a human-based assessment to validate the Functional Behavioural Assessment Ontology. These validation and verification processes were aimed to verify how much the ontology is conceptually well founded and semantically and syntactically correct.
Results
The Pellet reasoner checked the logical consistency and the integrity of classes and properties defined in the ontology, not detecting any violation of constraints in the ontology definition. To assess whether the ontology definition is coherent with the knowledge domain, human evaluation of the ontology was performed asking 84 people to fill in a questionnaire composed by 13 questions assessing concepts, relations between concepts, and concepts’ attributes. The response rate for the survey was 29/84 (34.52%). The domain experts confirmed that the concepts, the attributes, and the relationships between concepts defined in the FBA-Ontology are valid and well represent the Functional Behavioural Assessment process.
Conclusions
The new ontology developed could be a useful tool to design new evidence-based systems in the Behavioral Interventions practices, encouraging the link with other Linked Open Data datasets and repositories to provide users with new models of eHealth focused on the management of problem behaviors. Therefore, new research is needed to develop and implement innovative strategies to improve the poor reproducibility and translatability of basic research findings in the field of behavioral assessment.}
}
@article{SANTOSJUNIOR2021106570,
title = {From a Scrum Reference Ontology to the Integration of Applications for Data-Driven Software Development},
journal = {Information and Software Technology},
volume = {136},
pages = {106570},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106570},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921000537},
author = {Paulo Sérgio {Santos Júnior} and Monalessa Perini Barcellos and Ricardo de Almeida Falbo and João Paulo A. Almeida},
keywords = {Ontology, Scrum, Semantic Interoperability, Application Integration},
abstract = {Context
Organizations often use different applications to support the Scrum process, including project management tools, source repository and quality assessment tools. These applications store useful data for decision-making. However, data items often remain spread in different applications, each of which adopt different data and behavioral models, posing a barrier for integrated data usage. As a consequence, data-driven decisions in agile development are uncommon, missing valuable opportunities for informed decision making.
Objective
Considering the need to address semantic issues to properly integrate applications that support the agile development process, we aim to provide a common and comprehensive conceptualization about Scrum in the software development context and apply this conceptualization to support application integration.
Method
We have developed the Scrum Reference Ontology (SRO) and used it to semantically integrate Azure DevOps and Clockify.
Results
SRO served as a reference model to build software artifacts in a semantic integration architecture that enables applications to automatically share, exchange and combine data and services. The integrated solution was used in the software development unit of a Brazilian government agency. Results demonstrate that the integrated solution contributed to improving estimates, provided data that helped allocate teams, manage team productivity and project performance, and enabled to identify and fix problems in the Scrum process execution.
Conclusions
SRO can serve as an interlingua for application integration in the context of Scrum-process support. By capturing the conceptualization underlying Scrum, the reference ontology can address semantic conflicts and thereby support the development of integrated data-driven solutions for decision making.}
}
@article{RAGO2025104291,
title = {Argumentative review aggregation and dialogical explanations},
journal = {Artificial Intelligence},
volume = {340},
pages = {104291},
year = {2025},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2025.104291},
url = {https://www.sciencedirect.com/science/article/pii/S0004370225000104},
author = {Antonio Rago and Oana Cocarascu and Joel Oksanen and Francesca Toni},
keywords = {Argumentation, Argument mining, Review aggregation, Dialogical interaction, Conversational explanation},
abstract = {The aggregation of online reviews is one of the dominant methods of quality control for users in various domains, from retail to entertainment. Consequently, explainable aggregation of reviews is increasingly sought-after. We introduce quantitative argumentation technology to this setting, towards automatically generating reasoned review aggregations equipped with dialogical explanations. To this end, we define a novel form of argumentative dialogical agent (ADA), using ontologies to harbour information from reviews into argumentation frameworks. These agents may then be evaluated with a quantitative argumentation semantics and used to mediate the generation of dialogical explanations for item recommendations based on the reviews. We show how to deploy ADAs in three different contexts in which argumentation frameworks are mined from text, guided by ontologies. First, for hotel recommendations, we use a human-authored ontology and exemplify the potential range of dialogical explanations afforded by ADAs. Second, for movie recommendations, we empirically evaluate an ADA based on a bespoke ontology (extracted semi-automatically, by natural language processing), by demonstrating that its quantitative evaluations, which are shown to satisfy desirable theoretical properties, are comparable with those on a well-known movie review aggregation website. Finally, for product recommendation in e-commerce, we use another bespoke ontology (extracted fully automatically, by natural language processing, from a website's reviews) to construct an ADA which is then empirically evaluated favourably against review aggregations from the website.}
}
@article{BINDER2020307,
title = {Big Data Management Using Ontologies for CPQ Solutions},
journal = {Procedia Manufacturing},
volume = {52},
pages = {307-312},
year = {2020},
note = {System-Integrated Intelligence – Intelligent, Flexible and Connected Systems in Products and ProductionProceedings of the 5th International Conference on System-Integrated Intelligence (SysInt 2020), Bremen, Germany},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.11.051},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920321958},
author = {Alexander Binder and Eva-Maria Iwer and Werner Quint},
keywords = {CPQ, Semantic Technologies, Ontologies, Ontology Matching, Data Quality},
abstract = {In recent years, due to a progressive complexity of handling and processing business data, proper big data management has become a challenge, especially for SMEs that have limited resources for investing in the requested business transformation process. As a solution, we suggest an ontology-based CPQ software approach, where we show how the implementation of semantic technologies and ontologies affects data integration processes. We also propose a method called "ontology-based data matching", which allows the semiautomatic generation of alignments used to formalize the coherence between ontologies. The proposed method will ensure consistency during integration, significantly improving the productivity of enterprises.}
}
@article{IM20211578,
title = {Development of an Ontological Cost Estimating Knowledge Framework for EPC Projects},
journal = {KSCE Journal of Civil Engineering},
volume = {25},
number = {5},
pages = {1578-1591},
year = {2021},
issn = {1226-7988},
doi = {https://doi.org/10.1007/s12205-021-1582-8},
url = {https://www.sciencedirect.com/science/article/pii/S1226798824017094},
author = {Haekyung Im and Minhui Ha and Donghee Kim and Jaehyun Choi},
keywords = {Interoperability, Ontology, Estimation, Cost knowledge structure, Standardization, Cost classification},
abstract = {This research standardizes a knowledge structure for estimation in the construction field by creating a method to enhance cost management efficiency of construction projects while meeting the need for reusability of accumulated construction information. The construction knowledge structure was developed to execute the project cost estimation with an ontological concept. The knowledge framework was defined and relevant examples were addressed to explain the structure. The detailed estimation process and methodology for using standard unit price information was also developed to strengthen cost information interoperability by utilizing standard classification systems, such as MasterFormat, UniFormat, UniClass, and ISO 12006. This concept may be proposed as a method of connecting construction information based on a standard cost classification system in order to improve estimation efficiency by increasing the connectivity of cost information. Ontology can be a powerful tool when used in conjunction with interoperability to manage the massive volume of construction information for cost management. This methodology can expand to integrated management in cost and time to derive additional classes for work breakdown structure (WBS). Thus, ontology may improve the efficiency of cost estimation and systematization by reusing construction information. As a further step, the interoperability method will enhance overall construction project management.}
}
@article{DREGER20251221,
title = {Large language models for knowledge graph extraction from tables in materials science},
journal = {Digital Discovery},
volume = {4},
number = {5},
pages = {1221-1231},
year = {2025},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d4dd00362d},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X25000609},
author = {Max Dreger and Kourosh Malek and Michael Eikerling},
abstract = {Research in materials science increasingly harnesses machine learning (ML) models. These models are trained with experimental or theoretical data, the quality of their output hinges on the data's quantity and quality. Improving data quality and accessibility necessitates advanced data management solutions. Today, data are often stored in non-standardized table formats that lack interoperability, accessibility and reusability. To address this issue, we present a semi-automated data ingestion pipeline that transforms R&D tables into knowledge graphs. Utilizing large language models and rule-based feedback loops, our pipeline transforms tabular data into graph structures. The proposed process consists of entity recognition and relationship extraction. It facilitates better data interoperability and accessibility, by streamlining data integration from various sources. The pipeline is integrated into a platform harboring a graph database as well as semantic search capabilities.}
}
@article{AHMED2025103668,
title = {LLM-infused multi-module transformer for emotion-aware sentiment analysis in few-shot scenarios},
journal = {Information Fusion},
pages = {103668},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103668},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525007407},
author = {Kanwal Ahmed and Muhammad Imran Nadeem and Guanghui Wang and Fang Zuo and Zhijie Han},
keywords = {Natural language processing, Sentiment analysis, Large language models, Emotion analysis, Intent prediction, Crisis tweets},
abstract = {Sentiment analysis, particularly in few-shot scenarios and under constraints of limited data availability, presents significant challenges in accurately capturing the nuanced emotions conveyed in online reviews and public opinions. To address these limitations, this study introduces the Cognemotive Transformer (CogTrans), an advanced model that integrates emotion-cognitive reasoning with transformer-based generative approaches to enhance sentiment analysis. The proposed CogTrans framework consists of four key modules. The Quantity Augmentation Module utilizes large language models (LLMs) to generate synthetic data, thereby improving learning efficiency in few-shot settings. The Emotional Cognitive Analysis (ECA) Module constructs a sentence-emotion tree to facilitate a deeper understanding of sentiment contexts. The Transformer-based Semantic Representation (T-SR) Module employs a mask-transformer architecture to extract high-quality semantic features. Lastly, the Crisis Entity and Intent Prediction (CEIP) Module leverages natural language processing (NLP) techniques to identify critical entities in crisis-related texts and infer their underlying intentions using COMET-ATOMIC 2020. The integration of these components significantly enhances sentiment prediction, particularly in noisy and data-scarce environments. Experimental evaluations demonstrate that CogTrans outperforms existing models in both sentiment classification and interpretability, achieving state-of-the-art results across multiple benchmark datasets. Its ability to provide well-contextualized sentiment predictions while incorporating emotional context, cognitive reasoning, and crisis-relevant insights makes it a highly promising tool for practical applications in crisis management and review analysis.}
}
@article{HOLLAND2024100,
title = {Large language model based agent for process planning of fiber composite structures},
journal = {Manufacturing Letters},
volume = {40},
pages = {100-103},
year = {2024},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2024.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S2213846324000221},
author = {Maximilian Holland and Kunal Chaudhari},
keywords = {Large language model, Generative AI, Planning agent, Process planning, Fiber composites, LangChain, OpenAI},
abstract = {Process planning is a crucial activity, connecting product development and manufacturing of fiber composite structures. Recently published Large Language Models (LLM) promise more flexible and autonomous workflows compared to state of the art automation methods. An autonomous agent for process planning of fiber composite structures is implemented with the LangChain framework, based on OpenAI’s GPT-4 language model. The agent is equipped with deterministic tools which encode a-priori process planning knowledge. It can handle different process planning problems, such as cycle time estimation and resource allocation. Combinations thereof are solved through executing a multi-step solution path.}
}
@article{ALSAYED201982,
title = {Towards evaluation of cloud ontologies},
journal = {Journal of Parallel and Distributed Computing},
volume = {126},
pages = {82-106},
year = {2019},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2018.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0743731518306105},
author = {Mustafa M. Al-Sayed and Hesham A. Hassan and Fatma A. Omara},
keywords = {Cloud ontology, Cloud computing, Heterogeneity, Evaluation},
abstract = {Many enterprises consider that cloud computing will continue to play a key role due to its ability to deliver various types of on-demand IT services, and according to customer needs. Although cloud technology has characteristics that distinguish it from other technologies, unfortunately, services in this technology suffer from the heterogeneity issue. As a result, cloud services have many challenges, such as service description, interoperability, and service discovery. Many studies have contributed dealing with such challenges using ontology, as it can participate as a mapping layer to present such services in a unified description. Cloud ontologies included in these studies can be classified into three types of ontologies; ontologies that focus on representing functional features of cloud services, ontologies that focus on non-functional features, and ontologies that focus on both function and non-function features. These ontologies are not unified in representing such features, where they agree in some features and differ in others. By using techniques developed to evaluate ontologies related to the other technologies, it is difficult to decide which cloud ontology captures cloud services in the right way. We have noticed that there is not a framework or a benchmark to evaluate the cloud ontologies. In this survey, the current cloud ontologies are analyzed to identify a set of observations that represent their strengths and weaknesses. According to these observations, a set of suggested rules are introduced and verified to be considered during the construction or the evaluation of cloud ontologies. The work in this paper can be considered as the fundamental step for developing a formal framework to automatically evaluate cloud ontologies.}
}
@article{KURFALI2025106243,
title = {Representations of smells: The next frontier for language models?},
journal = {Cognition},
volume = {264},
pages = {106243},
year = {2025},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2025.106243},
url = {https://www.sciencedirect.com/science/article/pii/S0010027725001830},
author = {Murathan Kurfalı and Pawel Herman and Stephen Pierzchajlo and Jonas Olofsson and Thomas Hörberg},
keywords = {Large language models, Olfaction, Human perception, Chemical senses, Human perception modeling},
abstract = {Whereas human cognition develops through perceptually driven interactions with the environment, language models (LMs) are “disembodied learners” which might limit their usefulness as model systems. We evaluate the ability of LMs to recover sensory information from natural language, addressing a significant gap in cognitive science research literature. Our investigation is carried out through the sense of smell — olfaction — because it is severely underrepresented in natural language and thus poses a unique challenge for linguistic and cognitive modeling. By systematically evaluating the ability of three generations of LMs, including static word embedding models (Word2Vec, FastText), encoder-based models (BERT), and the decoder-based large LMs (LLMs; GPT-4o, Llama 3.1 among others), under nearly 200 training configurations, we investigate their proficiency in acquiring information to approximate human odor perception from textual data. As benchmarks for the performance of the LMs, we use three diverse experimental odor datasets including odor similarity ratings, imagined similarities of odor pairings from word labels, and odor-to-label ratings. The results reveal the possibility for LMs to accurately represent olfactory information, and describe the conditions under which this possibility is realized. Static, simpler models perform best in capturing odor-perceptual similarities under certain training configurations, while GPT-4o excels in simulating olfactory-semantic relationships, as suggested by its superior performance on datasets where the collected odor similarities are derived from word-based assessments. Our findings show that natural language encodes latent information regarding human olfactory information that is retrievable through text-based LMs to varying degrees. Our research shows promise for LMs to be useful tools in investigating the long debated relation between symbolic representations and perceptual experience in cognitive science.}
}
@article{WANG2025103246,
title = {An integrated approach for automatic safety inspection in construction: Domain knowledge with multimodal large language model},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103246},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103246},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625001399},
author = {Yiheng Wang and Hanbin Luo and Weili Fang},
keywords = {Safety, Safety Inspection, Large Language Model, Multi-Modality},
abstract = {This research addresses the challenge of dynamically integrating visual and textual data in construction safety inspections while enhancing adaptability to new safety hazards and ensuring faithful interpretation of safety rules. We propose a novel approach that seamlessly combines multi-modal techniques with domain knowledge, advancing beyond current methods that often struggle with multi-modal understanding and adaptation to new safety hazards. Our approach consists of three key components: (1) a fine-tuned multi-modal LLM for visual and textual processing, (2) a domain knowledge base for evolving safety standards adaptability and output faithfulness, and (3) a multi-step reasoning engine to tackle complex safety inspection tasks. We validate our approach using on-site data from Wuhan subway construction sites, demonstrating its capability to perform moderately accurate (0.57 hazard identification correctness), contextually relevant (0.96 on task relevancy), and faithful safety assessments (0.95 and 0.99 on reasoning faithfulness). The results suggest promising performance in construction scene perception, as well as textual analysis and reasoning. This approach represents an advancement in automatic construction safety inspection and contributes to the broader discourse on formalizing multi-modal processing of construction data, offering insights into creating more flexible and comprehensive safety management systems.}
}
@article{SHOJAEEMEND2020100353,
title = {Developing a mobile-based disease ontology for traditional Persian medicine},
journal = {Informatics in Medicine Unlocked},
volume = {20},
pages = {100353},
year = {2020},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2020.100353},
url = {https://www.sciencedirect.com/science/article/pii/S2352914820300563},
author = {Hassan Shojaee-Mend and Haleh Ayatollahi and Azam Abdolahadi},
keywords = {Traditional medicine, Traditional Persian medicine, Ontology development, Ontology evaluation, Mobile applications},
abstract = {Introduction
The use of traditional medicine has increased in many countries over the past decades. However, computer applications have been rarely developed and used in this field. This study aimed to develop a mobile-based disease ontology for traditional Persian medicine.
Methods
This research was carried out in two phases. In the first phase, the Methontology method was used for ontology development, and the ontology was evaluated in terms of accuracy and domain coverage. In the second phase, a mobile-based application was developed, and its usability was evaluated by traditional Persian medicine specialists.
Results
Initially, diseases were divided into 24 groups. The results showed that the developed ontology covered the concepts of traditional Persian medicine (90%) and their synonyms in English (83%) successfully. In terms of accuracy, the values for the concepts and their synonyms in English were (14%) and (46%), respectively. Eventually, users implemented the mobile-based application and evaluated its usability at a satisfactory level.
Conclusion
The disease ontology developed in the current study can provide traditional Persian medicine specialists and researchers with consistent, reusable, and sustainable descriptions of disease terms. In order to complete the ontology and use it in clinical decision support systems, expanding the ontology and adding more details to each disease are recommended.}
}
@article{TEYMOURLOUIE2018312,
title = {Detecting hidden errors in an ontology using contextual knowledge},
journal = {Expert Systems with Applications},
volume = {95},
pages = {312-323},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.11.034},
url = {https://www.sciencedirect.com/science/article/pii/S095741741730790X},
author = {Mehdi Teymourlouie and Ahmad Zaeri and Mohammadali Nematbakhsh and Matthias Thimm and Steffen Staab},
keywords = {Ontology debugging, Hidden modeling errors, Contextual knowledge, Incoherency, Inconsistency},
abstract = {Due to modeling errors in designing ontologies, an ontology may carry incorrect information. Ontology debugging can be helpful in detecting errors in ontologies that are increasing in size and expressiveness day by day. While current ontology debugging methods can detect logical errors (incoherences and inconsistencies), they are incapable of detecting hidden modeling errors in coherent and consistent ontologies. From the logical perspective, there are no errors in such ontologies, but this study shows some modeling errors may not break the coherency of the ontology by not participating in any contradiction. In this paper, contextual knowledge is exploited to detect such hidden errors. Our experiments show that adding general ontologies like DBpedia as contextual knowledge in the ontology debugging process results in detecting contradictions in ontologies that are coherent.}
}
@article{EFTIMOV2019382,
title = {ISO-FOOD ontology: A formal representation of the knowledge within the domain of isotopes for food science},
journal = {Food Chemistry},
volume = {277},
pages = {382-390},
year = {2019},
issn = {0308-8146},
doi = {https://doi.org/10.1016/j.foodchem.2018.10.118},
url = {https://www.sciencedirect.com/science/article/pii/S0308814618319009},
author = {Tome Eftimov and Gordana Ispirova and Doris Potočnik and Nives Ogrinc and Barbara {Koroušić Seljak}},
keywords = {Isotopic data, Semantic web, Ontology development, ISO-FOOD ontology},
abstract = {To link and harmonize different knowledge repositories with respect to isotopic data, we propose an ISO-FOOD ontology as a domain ontology for describing isotopic data within Food Science. The ISO-FOOD ontology consists of metadata and provenance data that needs to be stored together with data elements in order to describe isotopic measurements with all necessary information required for future analysis. The new domain has been linked with existing ontologies, such as Units of Measurements Ontology, Food, Nutrient and the Bibliographic Ontology. To show how such an ontology can be used in practise, it was populated with 20 isotopic measurements of Slovenian food samples. Describing data in this way offers a powerful technique for organizing and sharing stable isotope data across Food Science.}
}
@article{KAUSHIK201860,
title = {Automatic relationship extraction from agricultural text for ontology construction},
journal = {Information Processing in Agriculture},
volume = {5},
number = {1},
pages = {60-73},
year = {2018},
issn = {2214-3173},
doi = {https://doi.org/10.1016/j.inpa.2017.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S2214317317300227},
author = {Neha Kaushik and Niladri Chatterjee},
keywords = {Relation extraction, Term EXtraction, NLP, Ontology, Knowledge-based relation extraction, Self-supervised relation extraction},
abstract = {In the present era of Big Data the demand for developing efficient information processing techniques for different applications is expanding steadily. One such possible application is automatic creation of ontology. Such an ontology is often found to be helpful for answering queries for the underlying domain. The present work proposes a scheme for designing an ontology for agriculture domain. The proposed scheme works in two steps. In the first step it uses domain-dependent regular expressions and natural language processing techniques for automatic extraction of vocabulary pertaining to agriculture domain. In the second step semantic relationships between the extracted terms and phrases are identified. A rule-based reasoning algorithm RelExOnt has been proposed for the said task. Human evaluation of the term extraction output yields precision and recall of 75.7% and 60%, respectively. The relation extraction algorithm, RelExOnt performs well with an average precision of 86.89%.}
}
@article{DEBELLIS2018242,
title = {A Universal Moral Grammar (UMG) Ontology},
journal = {Procedia Computer Science},
volume = {137},
pages = {242-248},
year = {2018},
note = {Proceedings of the 14th International Conference on Semantic Systems 10th – 13th of September 2018 Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.09.023},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316284},
author = {Michael DeBellis},
keywords = {Ethics, Ontology, Universal Moral Grammar (UMG), Universal Grammar (UG), Evolutionary Psychology, Ethical Philosophy},
abstract = {This paper describes an OWL ontology that is a Universal Moral Grammar (UMG). UMG has been hypothesized by students of Chomsky to play the same role in human ethics as Universal Grammar (UG) does in Linguistics. I.e., the UMG describes a moral faculty hypothesized to have a genetic basis. This approach utilizes the modular view of the mind developed by Chomsky and currently utilized by many evolutionary psychology researchers. In this paper I describe the ontology and how it represents ethical choices, rules, scenarios, and systems. This includes representation of choices governed by the golden rule, utilitarianism, and Moral Foundations Theory. The foundation for the model is an Artificial Intelligence model of events, plans, and decisions. This plan model represents what is known as Theory of Mind (TOM). The TOM model is extended to the ethical domain by an analysis of recent research in evolutionary psychology and the representation of 40 different scenarios such as the trolley problem from the philosophical, psychological, and biological literature. The ontology is an example of how semantic technology can be used to provide mathematical rigor to the study of human ethics. This version demonstrates the breadth of the UMG, that it is capable of representing many diverse examples from the literature. However, there are already meaningful results from this version such as a resolution to Hume’s Is-Ought problem.}
}
@incollection{SERNA2022847,
title = {Application of an ontology-based decision support system for the design of emulsion-based cosmetic products},
editor = {Ludovic Montastruc and Stephane Negny},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {51},
pages = {847-852},
year = {2022},
booktitle = {32nd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-95879-0.50142-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323958790501429},
author = {Juliana Serna and Jose L. Rivera-Gil and Alex Gabriel and Javier A. Arrieta-Escobar and Vincent Boly and Véronique Falk and Paulo C. Narváez-Rincón},
keywords = {Product design, Cosmetic emulsions, Ontology},
abstract = {The decision-making process for the design of formulated products faces different challenges because of its intrinsic complexity. On the one hand, it is not sequential, but iterative due to the fragmented and heterogeneous nature of available information. On the other hand, there is not a unique design workflow because it changes from company to company according to its context and specific requirements. The lack of structure of knowledge for product formulation requires developing a robust knowledge representation to show coherently and explicitly concepts, models, and data. Furthermore, this representation must allow design teams to use it flexibly and to adapt it to specific design contexts. In view of the above, this work proposes an ontology for formulated products with emphasis on cosmetic emulsions. This ontology integrates concepts from emulsion science, cosmetic formulation, expert knowledge, and design heuristics in a systematic and accessible way. It was done based on the recent work of our research group in Chemical Product Design. This document shows an overview of the ontology and one of its possible applications: verification of the formulation of a skin care cream. As a conclusion, it was found that the ontology enables the access to precise information according to design requirements. It is a versatile and useful information tool for the design of emulsion-based products.}
}
@article{MUZANENHAMO2024102735,
title = {ChatGPT and accounting in African contexts: Amplifying epistemic injustice},
journal = {Critical Perspectives on Accounting},
volume = {99},
pages = {102735},
year = {2024},
issn = {1045-2354},
doi = {https://doi.org/10.1016/j.cpa.2024.102735},
url = {https://www.sciencedirect.com/science/article/pii/S1045235424000340},
author = {Penelope Muzanenhamo and Sean Bradley Power},
keywords = {ChatGPT, Epistemic injustice, Large language models, Pluriversality, Accounting, Africa},
abstract = {Large Language Models (LLMs) such as ChatGPT are likely to amplify epistemic injustice through the lack of transparency and traceability of data sources. The unethical alienation of original knowledge producers from their intellectual products, which are repackaged by LLMs as artificial intelligence, conceals power asymmetries in the global knowledge production and dissemination system. As elaborated by Miranda Fricker (2010), Western White male actors traditionally dominate knowledge production; therefore, ChatGPT and other LLMs are inclined to reproduce patriarchal perspectives as universal understandings of the World. Our commentary applies this logic to accounting practice and research in Africa, and asserts that epistemic injustice, resulting from colonization and racism, means that ontological and epistemological approaches situated in the accounting needs and experiences of African communities are missing from or poorly articulated by ChatGPT and other LLMs. If LLMs are to attain legitimacy as (ethical) sources of knowledge, regulation must be enforced to ensure transparency—as a foundation for promoting pluriversality and eliminating epistemic injustice.}
}
@article{SABA2019189,
title = {Development of new ontological solution for an energy intelligent management in Adrar city},
journal = {Sustainable Computing: Informatics and Systems},
volume = {21},
pages = {189-203},
year = {2019},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2019.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S221053791830341X},
author = {Djamel Saba and Youcef Sahli and Fonbeyin Henry Abanda and Rachid Maouedj and Boudjemaa Tidjar},
keywords = {Energy efficient, Ontology, Semantic web rule language, Smart home, Web ontology language},
abstract = {Currently, the growth in building energy consumption presents raises concerns with greenhouse gases and environmental pollution. The building occupants face significant difficulties to control the consumed power due to several instabilities such as climate change and human behavior. Therefore, it is necessary to have a decision tool that optimizes the consumed energy and ensures an acceptable comfort for the occupants. This article presents an intelligent solution based on an ontology that presents knowledge about the internal and external environment of a residence, as well as the occupants’ behavior and activities. The system openness with the external environment, the knowledge presentation flexibility built on Web Ontology Language (OWL) and the possibility of intelligent reasoning using Semantic Web Rule Language (SWRL) are the main reasons for choosing the ontological approach. The proposed ontology is applied to a real home located in the Adrar city, in the Algerian Sahara. A comparison between two scenarios (with and without the proposed system, Ontological Solution for Energy Intelligent Management (OSEIM) revealed the effectiveness of the proposed solution. The obtained results present a significant energy saving of 4.58%.}
}
@article{LETERTRE2021129,
title = {The operational framework for quantum theories is both epistemologically and ontologically neutral},
journal = {Studies in History and Philosophy of Science Part A},
volume = {89},
pages = {129-137},
year = {2021},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2021.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0039368121001205},
author = {Laurie Letertre},
keywords = {Quantum mechanics, Operational frameworks, Realism, Antirealism, Indefinite causal order, Process matrix formalism},
abstract = {Operational frameworks are very useful to study the foundations of quantum mechanics, and are sometimes used to promote antirealist attitudes towards the theory. The aim of this paper is to review three arguments aiming at defending an antirealist reading of quantum physics based on various developments of standard quantum mechanics appealing to notions such as quantum information, non-causal correlations and indefinite causal orders. Those arguments will be discussed in order to show that they are not convincing. Instead, it is argued that there is conceptually no argument that could favour realist or antirealist attitudes towards quantum mechanics based solely on some features of some formalism. In particular, both realist and antirealist views are well accomodable within operational formulations of the theory. The reason for this is that the realist/antirealist debate is located at a purely epistemic level, which is not engaged by formal aspects of theories. As such, operational formulations of quantum mechanics are epistmologically and ontologically neutral. This discussion aims at clarifying the limits of the historical and methodological affinities between scientific antirealism and operational physics while engaging with recent discoveries in quantum foundations. It also aims at presenting various realist strategies to account for those developments.}
}
@article{MATENTZOGLU20181,
title = {Inference Inspector: Improving the verification of ontology authoring actions},
journal = {Journal of Web Semantics},
volume = {49},
pages = {1-15},
year = {2018},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2017.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1570826817300367},
author = {Nicolas Matentzoglu and Markel Vigo and Caroline Jay and Robert Stevens},
keywords = {OWL, Ontologies, Human computer interaction, Ontology engineering, Ontology authoring, Reasoning},
abstract = {Ontologies are complex systems of axioms in which unanticipated consequences of changes are both frequent, and difficult for ontology authors to apprehend. The effects of modelling actions range from unintended inferences to outright defects such as incoherency or even inconsistency. One of the central ontology authoring activities is verifying that a particular modelling step has had the intended consequences, often with the help of reasoners. For users of Protégé, this involves, for example, exploring the inferred class hierarchy. This paper provides evidence that making entailment set changes explicit to authors significantly improves the understanding of authoring actions regarding both correctness and speed. This is tested by means of the Inference Inspector, a Protégé plugin we created that provides authors with specific details about the effects of an authoring action. We empirically validate the effectiveness of the Inference Inspector in two studies. In a first, exploratory study we determine the feasibility of the Inference Inspector for supporting verification and isolating authoring actions. In a second, controlled study we formally evaluate the Inference Inspector and determine that making changes to key entailment sets explicit significantly improves author verification compared to the standard static hierarchy/frame-based approach. We discuss the advantages of the Inference Inspector for different types of verification questions and find that our approach is best suited for verifying added restrictions where no new signature, such as class names, is introduced, with a 42% improvement in verification correctness.}
}
@article{MISHRA2025104045,
title = {PageLLM: Incremental approach for updating a Security Knowledge Graph by using Page ranking and Large language model},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104045},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.104045},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324004047},
author = {Chinmaya Mishra and Himangshu Sarma and Saravanan M.},
keywords = {Security knowledge graph, Knowledge graph, Knowledge representation learning, Page ranking, Embedding, Generative AI, Large language models (LLMs), Static knowledge graph (SKG), Incremental knowledge graph (IKG), Full knowledge graph (FKG)},
abstract = {Due to increase in cyber crime and evolution of sophisticated tools and techniques, Threat Intelligence plays a critical role. It helps defenders to stay ahead of attackers by developing the right defense mechanism to invade those attacks. In this regards security knowledge graph plays a critical role which can be used to signify complex entities and their relationship in a graphical structure. Further projecting those entities and relationships in to the lower dimension using several embedding techniques such as TransE help in many down streaming task. The learned embedding can be used to predict new cyber threat which is very helpful for defenders to stay alert and develop necessary weapons to stay ahead of an attack. One of the major challenge security knowledge graph has its dynamic nature of changing intelligence. Active learning can be used to only update the substantial portion of embedding rather than retraining the knowledge graph from scratch which has higher time and space complexity. Also given the rise in generative AI and large language models which are super rich in context, there is a scope of utilizing those for building a robust and good quality security knowledge graph. We will discuss a novel methodology called PageLLM which utilizes page ranking and LLMs to enable active learning in an incremental way and will improve the quality of knowledge graph through enriched context.}
}
@article{NEUMAYR2021101937,
title = {Providing packages of relevant ATM information: An ontology-based approach},
journal = {Journal of Air Transport Management},
volume = {90},
pages = {101937},
year = {2021},
issn = {0969-6997},
doi = {https://doi.org/10.1016/j.jairtraman.2020.101937},
url = {https://www.sciencedirect.com/science/article/pii/S0969699720305202},
author = {Bernd Neumayr and Christoph G. Schuetz and Eduard Gringinger and Christoph Fabianek and Audun Vennesland and Michael Schrefl and Scott Wilson},
keywords = {Data mediation, Information tailoring, System wide information management, Air traffic management},
abstract = {ATM information providers publish reports and notifications of different types using standardized information exchange models. For a typical information user, e.g., an aircraft pilot, only a fraction of the published information is relevant for a particular task. Filtering out irrelevant information from different information sources is in itself a challenging task, yet it is only a first step in providing relevant information, the challenges concerning maintenance, auditability, availability, integration, comprehensibility, and traceability. This paper presents the Semantic Container approach, which employs ontology-based faceted information filtering and allows for the packaging of filtered information and associated metadata in semantic containers, thus facilitating reuse of filtered information at different levels. The paper formally defines an abstract model of ontology-based information filtering and the structure of semantic containers, their composition, versioning, discovery, and replicated physical allocation. The paper further discusses different usage scenarios, the role of semantic containers in SWIM, an architecture for a semantic container management system, as well as a proof-of-concept prototype. Finally the paper discusses a blockchain-based notary service to realize tamper-proof version histories for semantic containers.}
}
@article{PHILLIPS2020103,
title = {Ontologies in radiation oncology},
journal = {Physica Medica},
volume = {72},
pages = {103-113},
year = {2020},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2020.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S1120179720300727},
author = {Mark H. Phillips and Lucas M. Serra and Andre Dekker and Preetam Ghosh and Samuel M.H. Luk and Alan Kalet and Charles Mayo},
abstract = {Ontologies are a formal, computer-compatible method for representing scientific knowledge about a given domain. They provide a standardized vocabulary, taxonomy and set of relations between concepts. When formatted in a standard way, they can be read and reasoned upon by computers as well as by humans. At the 2019 International Conference on the Use of Computers in Radiation Therapy, there was a session devoted to ontologies in radiation therapy. This paper is a compilation of the material presented, and is meant as an introduction to the subject. This is done by means of a didactic introduction to the topic followed by a series of applications in radiation therapy. The goal of this article is to provide the medical physicist and related professionals with sufficient background that they can understand their construction as well as their practical uses.}
}
@article{TANG2018847,
title = {Exchanging knowledge for test-based diagnosis using OWL Ontologies and SWRL Rules},
journal = {Procedia Computer Science},
volume = {131},
pages = {847-854},
year = {2018},
note = {Recent Advancement in Information and Communication Technology:},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.04.279},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918306598},
author = {Xilang Tang and Mingqing Xiao and Bin Hu and Dongqing Pan},
keywords = {diagnosis, test, knowledge, OWL ontology, SWRL rules},
abstract = {To solve the difficulty of exchanging knowledge of complex engineering systems for diagnosis, which resulting in huge work and long cycle for developing test-based diagnosis equipment, this paper proposes a novel method of exchanging knowledge of systems under diagnosis using OWL ontologies and SWRL rules. The knowledge model of systems for diagnosis is decomposed into structure model and function model, and this paper proposes a general procedure to represent the two models with OWL and SWRL. For that OWL and SWRL support reasoning, so that a generic reasoning engine for test-based diagnosis can be developed and the knowledge can be used directly by the reasoning engine. Therefore, the software of ADE can be generic and portable, so that the development process of diagnosis equipment can be accelerated.}
}
@article{HAO2025105669,
title = {Uncovering compound urban crises with large language model-assisted knowledge graph construction},
journal = {International Journal of Disaster Risk Reduction},
volume = {127},
pages = {105669},
year = {2025},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2025.105669},
url = {https://www.sciencedirect.com/science/article/pii/S2212420925004935},
author = {Haiyan Hao and Xiaorui Chen and Yudi Chen and Nan Li},
abstract = {Urban areas frequently face an array of crises, including natural hazards, public health emergencies, and technological disruptions, which are becoming more interconnected due to the projected growing trend of climate change and rapid urbanization. While prior studies have investigated compound events, much of the focus has been on weather- and climate-related events, leaving a gap in understanding compound urban crises that integrate diverse types of crises at the city level. To address this, we propose a novel method for mining location-specific knowledge of compound urban crises using large language models (LLMs), enhanced with techniques such as Retrieval-Augmented Generation (RAG), iterative self-refinement, and prompt engineering. By applying the method to 1941 news articles collected from Shenzhen, China, we constructed knowledge graphs (KGs) that reveal the interconnections and compounding patterns among 13 distinct types of urban crises. The results demonstrate the effectiveness of the proposed method in identifying and mapping compound urban crises at the city level. The findings offer practical implications for urban crisis management, equipping cities with tailored knowledge to better anticipate and respond to complex, interrelated crisis scenarios.}
}
@article{KANG2018210,
title = {Disease Specific Ontology of Adverse Events: Ontology extension and adaptation for Chronic Kidney Disease},
journal = {Computers in Biology and Medicine},
volume = {101},
pages = {210-217},
year = {2018},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2018.08.024},
url = {https://www.sciencedirect.com/science/article/pii/S0010482518302506},
author = {Yin Kang and Jeffrey C. Fink and Rebecca Doerfler and Lina Zhou},
keywords = {Adverse event ontology, Disease specific adverse event, Ontology extension and adaptation, Chronic kidney disease},
abstract = {Background
Adverse Event (AE) ontology can be used to support interoperability and computer-assisted reasoning of AEs. Despite significant progress in developing biomedical ontologies, they are facing the obstacle of adoption partly because those ontologies are too general to meet the requirements of a specific domain. Understanding and representing of AEs for a specific domain such as Chronic Kidney Disease (CKD) has both theoretical and clinical significance. CKD patients are at a high risk for an array of disease-intervention specific AEs, and these in turn can contribute to disease progression unlike other diseases. This study proposes Disease Specific Ontology of Adverse Events (DSOAE) to address specific requirements of CKD, and applies it to different usage scenarios with real data.
Methods
We introduce a method for developing DSOAE through the extension and adaption of general ontologies by incorporating domain-specific information and usage requirements. It starts with specifying the goal and scope of a target domain (i.e. selecting seed ontologies), followed by identifying main AE classes and relations, extracting and creating classes and relations, aligning and identifying upper-level classes and lower-level classes, and finally populating the ontology with instances. Any of these steps may be repeated to refine the ontology.
Results
DSOAE contains 22 CKD-specific AE classes, which are grouped into two general categories: patient-reported AEs and biochemical/laboratory-related AEs. In addition, disease history and comorbidity classes as introduced in this study help model patient-related risk factors for AEs. With the support of DSOAE, we build a knowledge base of CKD-specific AEs using data from different sources (e.g. patient cohort data and social media), and apply the knowledge base to data analysis and data integration.
Conclusions
DSOAE enables the interoperability of AEs across different sources and supports the development of a knowledge base of domain-specific AEs. DSOAE can also meet the needs of different usage scenarios. The approach to constructing DSOAE is generalizable and can be used to develop AE ontology in other domains.}
}
@article{IATRELLIS2018,
title = {A Review on Software Project Management Ontologies},
journal = {International Journal of Information Technology Project Management},
volume = {9},
number = {4},
year = {2018},
issn = {1938-0232},
doi = {https://doi.org/10.4018/IJITPM.2018100104},
url = {https://www.sciencedirect.com/science/article/pii/S1938023218000044},
author = {Omiros Iatrellis and Panos Fitsilis},
keywords = {Ontologies, Project Management, Software Project Management},
abstract = {This article aims to provide the reader with a comprehensive background for understanding current knowledge and research works on ontologies for software project management (SPM). It constitutes a systematic literature review behind key objectives of the potential adoption of ontologies in PM. Ontology development and engineering could facilitate substantially the software development process and improve knowledge management, software and artifacts reusability, internal consistency within project management processes of various phases of software life cycle. The authors examined the literature focusing on software project management ontologies and analyzed the findings of these published papers and categorized them accordingly. They used qualitative methods to evaluate and interpret findings of the collected studies. The literature review, among others, has highlighted lack of standardization in terminology and concepts, lack of systematic domain modeling and use of ontologies mainly in prototype ontology systems that address rather limited aspects of software project management processes.}
}
@article{SANTHANAVIJAYAN2019192,
title = {Multi swarm optimization based automatic ontology for e-assessment},
journal = {Computer Networks},
volume = {160},
pages = {192-199},
year = {2019},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2019.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S138912861830896X},
author = {A. Santhanavijayan and S.R. Balasundaram},
keywords = {Automated ontology, E-assessment, Unsupervised quick reduct algorithm, Multi-swarm optimization, Statistical pattern},
abstract = {The utilization of ontology in the e-assessment area has grown tremendously. The context of e-learning is significant to the students for educational purposes. This makes the testing process easy for the students and also for the teachers. The majority of the approaches that deals with the ontology issue have suggested that the individual ontology models have merely a fraction of the assessment domain. To trounce such drawbacks, here, an automated ontology creation is proposed for the e-assessment systems. Initially, the text is extracted from the web utilizing the Unsupervised Quick Reduct (UQR) algorithm. This is trailed by the summarization of the texts using the multi-swarm optimization (MSO) based on preference learning. Finally, the sentence of the summary is then transmuted to multiple choice questions (MCQ). The keys are created using statistical pattern (SP). The efficiency of the system is examined using the experimental outcomes like error rate, precision, recall and accuracy. In accuracy, the proposed UQR algorithm achieves 97.7%, MSO achieve 96.2% accuracy and key generation achieves 94.7% accuracy. The proposed automatic ontology system indicates better when weighed against the top-notch methods.}
}
@article{DIMASSI2021103374,
title = {An ontology-based framework to formalize and represent 4D printing knowledge in design},
journal = {Computers in Industry},
volume = {126},
pages = {103374},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103374},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520306084},
author = {Saoussen Dimassi and Frédéric Demoly and Christophe Cruz and H. Jerry Qi and Kyoung-Yun Kim and Jean-Claude André and Samuel Gomes},
keywords = {4D Printing, Smart materials, Additive manufacturing, Design for 4D printing, Ontology, Description logics},
abstract = {Over the last decade, 4D printing paradigm has received intensive research efforts, whether from researchers in additive manufacturing (AM) or in smart materials (SMs) development. Related research works have thereby generated a large number of ad-hoc solutions with relevant disparate and scattered knowledge. This lack of common core knowledge is mainly due to the multiple involved expertise for fabricating stimulus-reactive structures. The scientific issue of federating and reconciling knowledge is also reinforced especially if such technology must be integrated into the product design process, falling under the field of design for 4D printing. To tackle this challenge, it becomes crucial to formalize and represent knowledge relating AM processes/techniques, SMs behaviours, stimuli and transformation functions with the variety of design objects. In such a context, the paper aims at developing an ontology-based framework for the semantic and logical description of transformable objects in the era of 4D printing for product-process design related purposes. This framework – which is built upon a foundational ontology associated with mereotopology for describing dynamical phenomena called basic formal ontology – consists in introducing a domain ontology equipped with reasoning capabilities supported by description logics for SMs selection and distribution, transformation sequence planning and AM process planning purposes.}
}
@article{CELIK2025113495,
title = {Efficient Inference, Training, and Fine-tuning of Protein Language Models},
journal = {iScience},
pages = {113495},
year = {2025},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2025.113495},
url = {https://www.sciencedirect.com/science/article/pii/S2589004225017560},
author = {Muhammed Hasan Çelik and Xiaohui Xie},
keywords = {protein language models, efficient deep learning, FlashAttention, sequence packing, quantization, activation checkpointing, zero-offload, LoRA fine-tuning, missense variant effect prediction, protein property prediction},
abstract = {SUMMARY
Protein language models (PLMs) have shown great promise in protein structure and function predictions, but their adoption is limited by computational cost. We address this challenge by enhancing the efficiency of Evolutionary Scale Modeling (ESM). Using FlashAttention and sequence packing, we achieve 4–9x faster inference and 3–14x lower memory usage. Four-bit quantization of billion-parameter models further reduces memory by 2–3x while preserving accuracy for missense variant effect prediction. Training is also optimized, cutting runtime sixfold with methods such as activation checkpointing and DeepSpeed Zero-Offload. Parameter-efficient fine-tuning of a few adapter weights yields state-of-the-art performance at protein property and function predictions, resulting in 70% Spearman’s correlation for melting point and 87% AU-PRC for transcription factor identification. Our efficient ESM (ESME) implementation significantly lowers the barrier to using these powerful models, making them accessible to academic laboratories with limited computational resources. The code is available on GitHub (github.com/uci-cbcl/esm-efficient).}
}
@article{HARROW20192068,
title = {Ontology mapping for semantically enabled applications},
journal = {Drug Discovery Today},
volume = {24},
number = {10},
pages = {2068-2075},
year = {2019},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2019.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S1359644618304215},
author = {Ian Harrow and Rama Balakrishnan and Ernesto Jimenez-Ruiz and Simon Jupp and Jane Lomax and Jane Reed and Martin Romacker and Christian Senger and Andrea Splendiani and Jabe Wilson and Peter Woollard},
abstract = {In this review, we provide a summary of recent progress in ontology mapping (OM) at a crucial time when biomedical research is under a deluge of an increasing amount and variety of data. This is particularly important for realising the full potential of semantically enabled or enriched applications and for meaningful insights, such as drug discovery, using machine-learning technologies. We discuss challenges and solutions for better ontology mappings, as well as how to select ontologies before their application. In addition, we describe tools and algorithms for ontology mapping, including evaluation of tool capability and quality of mappings. Finally, we outline the requirements for an ontology mapping service (OMS) and the progress being made towards implementation of such sustainable services.}
}
@article{DASILVASERAPIAOLEAL2019100100,
title = {An ontology for interoperability assessment: A systemic approach},
journal = {Journal of Industrial Information Integration},
volume = {16},
pages = {100100},
year = {2019},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2019.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X19300433},
author = {Gabriel {da Silva Serapião Leal} and Wided Guédria and Hervé Panetto},
keywords = {Enterprise interoperability, Interoperability assessment, Ontology, System engineering},
abstract = {Enterprise Interoperability is a requirement for ensuring an effective collaboration within a network of enterprises. Therefore, interoperability should be continuously assessed and improved for avoiding collaboration issues. To do so, an interoperability assessment can be performed by the concerned enterprises. Such an assessment provides an overview of the enterprise systems’ strengths and weaknesses regarding interoperability. A plethora of assessment approaches are proposed in the literature. The majority of them focus on one single aspect of interoperability. In general, to have a holistic view of the assessed systems, i.e. consider different aspects, enterprises have to apply different approaches. However, the application of multiple approaches may cause redundancy and confusion when assessing the same system using different metrics and viewpoints. Therefore, this article is to propose an ontology for interoperability assessment. The main objective of such an ontology is to provide a sound description of all relevant concepts and relationships regarding an interoperability assessment. Inference rules are also provided for reasoning on interoperability problems. A case study based on a real enterprise in presented to evaluate the proposed ontology.}
}
@article{FERDOUSH20241548,
title = {A biomarker identification model from protein protein interaction network using natural language processing and graph convolutional network},
journal = {Procedia Computer Science},
volume = {246},
pages = {1548-1557},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.616},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924026693},
author = {Zannatul Ferdoush and Ziad Kobti},
keywords = {Biomarker, Natural Language Processing, Graph Convolutional Networks, Glioblastoma Multiforme},
abstract = {A biomarker identification model, integrating natural language processing (NLP) and graph convolutional neural network (GCN), offers a novel approach to enhance a simple neural network’s ability to capture the contextual semantics of genes and extract spatial feature information by utilizing gene ontology (GO) annotations. First, we explore gene expression datasets to identify differentially expressed genes (DEGs) and construct a protein-protein interaction (PPI) network. By employing Word2Vec, an NLP algorithm, for vectorizing GO annotations, our model reveals complex biological relationships among genes. GO annotations are crucial as they provide comprehensive information about gene functions, biological processes, and cellular components, thus augmenting our understanding of how genes interact within the network. Integrating multi-layered GCN facilitates effective learning of complex semantic relations and spatial feature information within the PPI network. Experiments on publicly available datasets of Glioblastoma Multiforme (GBM), the most aggressive form of brain tumour, demonstrate that our model significantly enhances biomarker identification compared to existing state-of-the-art methods, showcasing its potential for advancing GBM research and clinical decision-making.}
}
@article{LEE2024,
title = {Optimizing Clinical Trial Eligibility Design Using Natural Language Processing Models and Real-World Data: Algorithm Development and Validation},
journal = {JMIR AI},
volume = {3},
year = {2024},
issn = {2817-1705},
doi = {https://doi.org/10.2196/50800},
url = {https://www.sciencedirect.com/science/article/pii/S2817170524000383},
author = {Kyeryoung Lee and Zongzhi Liu and Yun Mai and Tomi Jun and Meng Ma and Tongyu Wang and Lei Ai and Ediz Calay and William Oh and Gustavo Stolovitzky and Eric Schadt and Xiaoyan Wang},
keywords = {natural language processing, real-world data, clinical trial eligibility criteria, eligibility criteria–specific ontology, clinical trial protocol optimization, data-driven approach},
abstract = {Background
Clinical trials are vital for developing new therapies but can also delay drug development. Efficient trial data management, optimized trial protocol, and accurate patient identification are critical for reducing trial timelines. Natural language processing (NLP) has the potential to achieve these objectives.
Objective
This study aims to assess the feasibility of using data-driven approaches to optimize clinical trial protocol design and identify eligible patients. This involves creating a comprehensive eligibility criteria knowledge base integrated within electronic health records using deep learning–based NLP techniques.
Methods
We obtained data of 3281 industry-sponsored phase 2 or 3 interventional clinical trials recruiting patients with non–small cell lung cancer, prostate cancer, breast cancer, multiple myeloma, ulcerative colitis, and Crohn disease from ClinicalTrials.gov, spanning the period between 2013 and 2020. A customized bidirectional long short-term memory– and conditional random field–based NLP pipeline was used to extract all eligibility criteria attributes and convert hypernym concepts into computable hyponyms along with their corresponding values. To illustrate the simulation of clinical trial design for optimization purposes, we selected a subset of patients with non–small cell lung cancer (n=2775), curated from the Mount Sinai Health System, as a pilot study.
Results
We manually annotated the clinical trial eligibility corpus (485/3281, 14.78% trials) and constructed an eligibility criteria–specific ontology. Our customized NLP pipeline, developed based on the eligibility criteria–specific ontology that we created through manual annotation, achieved high precision (0.91, range 0.67-1.00) and recall (0.79, range 0.50-1) scores, as well as a high F1-score (0.83, range 0.67-1), enabling the efficient extraction of granular criteria entities and relevant attributes from 3281 clinical trials. A standardized eligibility criteria knowledge base, compatible with electronic health records, was developed by transforming hypernym concepts into machine-interpretable hyponyms along with their corresponding values. In addition, an interface prototype demonstrated the practicality of leveraging real-world data for optimizing clinical trial protocols and identifying eligible patients.
Conclusions
Our customized NLP pipeline successfully generated a standardized eligibility criteria knowledge base by transforming hypernym criteria into machine-readable hyponyms along with their corresponding values. A prototype interface integrating real-world patient information allows us to assess the impact of each eligibility criterion on the number of patients eligible for the trial. Leveraging NLP and real-world data in a data-driven approach holds promise for streamlining the overall clinical trial process, optimizing processes, and improving efficiency in patient identification.}
}
@article{BREIMANN2024168717,
title = {AAontology: An Ontology of Amino Acid Scales for Interpretable Machine Learning},
journal = {Journal of Molecular Biology},
volume = {436},
number = {19},
pages = {168717},
year = {2024},
issn = {0022-2836},
doi = {https://doi.org/10.1016/j.jmb.2024.168717},
url = {https://www.sciencedirect.com/science/article/pii/S0022283624003267},
author = {Stephan Breimann and Frits Kamp and Harald Steiner and Dmitrij Frishman},
keywords = {physicochemical properties, protein evolution, clustering, interpretability, protein prediction},
abstract = {Amino acid scales are crucial for protein prediction tasks, many of them being curated in the AAindex database. Despite various clustering attempts to organize them and to better understand their relationships, these approaches lack the fine-grained classification necessary for satisfactory interpretability in many protein prediction problems. To address this issue, we developed AAontology—a two-level classification for 586 amino acid scales (mainly from AAindex) together with an in-depth analysis of their relations—using bag-of-word-based classification, clustering, and manual refinement over multiple iterations. AAontology organizes physicochemical scales into 8 categories and 67 subcategories, enhancing the interpretability of scale-based machine learning methods in protein bioinformatics. Thereby it enables researchers to gain a deeper biological insight. We anticipate that AAontology will be a building block to link amino acid properties with protein function and dysfunctions as well as aid informed decision-making in mutation analysis or protein drug design.}
}
@article{PASTUSZUK20211011,
title = {Cybersecurity Ontology for Dynamic Analysis of IT Systems},
journal = {Procedia Computer Science},
volume = {192},
pages = {1011-1020},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.104},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921015921},
author = {Jakub Pastuszuk and Patryk Burek and Bogdan Ksiȩżopolski},
keywords = {cybersecurity ontology, cybersecurity, ontology, cybersecurity knowledge acquisition},
abstract = {Today’s IT systems are characterized by a high complexity and the increasing number of sub-systems, as well as the physical equipment needed. That raises the problem of keeping the whole architecture secure, as it has been revealed in recent years by numerous scandals related to data leaks from inadequately secured systems. Several ontology-based approaches tied to the cybersecurity domain have been developed aiming at solving the problem of identifying vulnerabilities in real world systems. Unfortunately, these approaches do not address the essential problem: since real IT system architectures are dynamic and highly variable, it is necessary to enable real-time inventory and observation of all system components, as well as to retrieve up-to-date data about those systems and potential risks. The current paper has a threefold purpose. First, we examine the existing cybersecurity ontologies and identify the deficiencies that prevent us from using them in real, dynamic IT systems. Next, we introduce and propose a framework based on the Dynamic CyberSecurity Ontology, which fills the existing gaps in current solutions. Finally, we outline a monitoring system based on the developed ontology, which implements automatic data mining mechanisms that aggregate results from dynamic knowledge sources, such as Shodan or Censys. As a result, the ontological examination of systems over time becomes possible.}
}
@article{KOONCE20191678,
title = {Metrics to gauge the success of a manufacturing ontology},
journal = {Procedia Manufacturing},
volume = {38},
pages = {1678-1682},
year = {2019},
note = {29th International Conference on Flexible Automation and Intelligent Manufacturing ( FAIM 2019), June 24-28, 2019, Limerick, Ireland, Beyond Industry 4.0: Industrial Advances, Engineering Education and Intelligent Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.116},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920301177},
author = {David Koonce and Dusan Sormaz},
keywords = {Ontology Evaluation, Ontology Usage, Manufacturing Ontology},
abstract = {Ontologies are structures of concepts that define a high-level representation of a system or area. They can serve as a foundational understanding for developing or integrating software representations of said system. And while the construction of can be a complex, multi-party exercise, the assessment of ontologies is often defined less on usage and more on completeness and coverage. In manufacturing, ontology development ranges from supply chain to production to design. Owing to the computer science foundations of ontology design and representation, the value or quality of an ontology can be assessed on notions of completeness and coverage. Recently, researchers have posited that usage should factor into the Ontology Lifecycle. Similar to how the market, and not technology, defines the success of a product or technology, this paper will examine how utilization of an ontology can define the value or quality of the ontology}
}
@article{FERNANDEZLOPEZ2019100492,
title = {Why are ontologies not reused across the same domain?},
journal = {Journal of Web Semantics},
volume = {57},
pages = {100492},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300726},
author = {Mariano Fernández-López and María Poveda-Villalón and Mari Carmen Suárez-Figueroa and Asunción Gómez-Pérez},
keywords = {Ontology reuse, Ontology reliability, Ontology heterogeneity, Percentage of reuse, Reuse patterns, Drawbacks for reuse},
abstract = {Even though one of the main characteristics of ontologies has always been claimed to be their reusability, throughout this paper it will be shown that ontology reuse across a given domain is not a consolidated practice. We have carried out a statistical study on ontology reuse in the ontologies collected in Linked Open Vocabularies (LOV), in addition to a particular analysis of a use case. The results of the present work show that, when building an ontology, the heterogeneity between the needed conceptualization and that of available ontologies, as well as the deficiencies in some of such ontologies (concerning documentation, licensing, etc.) are important obstacles for reusing ontologies of the same domain of the ontology under development. A possible approach to lessen these problems could be the creation of communities similar to open software ones in charge of developing and maintaining ontologies.}
}
@article{REN201924,
title = {Building an ontological knowledgebase for bridge maintenance},
journal = {Advances in Engineering Software},
volume = {130},
pages = {24-40},
year = {2019},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2019.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0965997818307634},
author = {Guoqian Ren and Rui Ding and Haijiang Li},
keywords = {Bridge maintenance, Semantic web, Ontology, Multi-criteria decision support, Suppliers selection, Event management},
abstract = {The operation stage has the biggest potential value in the bridge life cycle management, and it often critically influences the overall cost of the bridge. As such, changes in the efficiency of the project's operation stage could be of significant benefit to the overall project. However, current approaches in the operation stage often lack the effective support of computer-aided tools. This research presents a holistic method based on an ontology to achieve automatic rule checking and improve the management and communication of knowledge related to bridge maintenance. The developed ontology can also facilitate a smarter decision-making process for bridge management by informing engineers of choices with different considerations. Three approaches; semantic validation, syntactical validation, and case study validation, have been adopted to evaluate this ontology and demonstrate how the developed ontology can be used by engineers when dealing with different issues. The results showed that this approach can create a holistic knowledge base that can integrate various domain knowledge to enable bridge engineers to make more comprehensive decisions rather than a single objective-targeted delivery.}
}
@article{ZHANG2021107472,
title = {Zero-shot fine-grained entity typing in information security based on ontology},
journal = {Knowledge-Based Systems},
volume = {232},
pages = {107472},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107472},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121007346},
author = {Han Zhang and Jiaxian Zhu and Jicheng Chen and Junxiu Liu and Lixia Ji},
keywords = {Fine-grained entity typing, Clustering algorithm, Representation method for categories, Information security, Unified cybersecurity ontology},
abstract = {The field of information security suffers from the lack of labelled entities. This study proposes a zero-shot hybrid approach, combining a clustering algorithm with a method for representing category labels, to classify fine-grained entity typing based on unified cybersecurity ontology (UCO) to address this issue. However, certain category labels in UCO do not have distinct domain features, while certain abbreviations cannot be obtained directly from word embedding using Word2vec. Thus, we propose a new method, referred to as mixed entities and hierarchy of UCO (MEHC), to represent the category labels. Moreover, to further improve the performance of fine-grained entity typing we propose the triClustering algorithm to re-cluster coarse-grained classification results or determine corresponding types for new entities, based on the theorem that the sum of two sides of a triangle is greater than the third. The experimental results prove that our triClustering algorithm can effectively shorten the computation time and that the proposed hybrid method is superior to other baselines for information security applications.}
}
@article{KONYS20203324,
title = {Ontology Learning Approaches to Provide Domain-Specific Knowledge Base},
journal = {Procedia Computer Science},
volume = {176},
pages = {3324-3334},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.065},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920319608},
author = {Agnieszka Konys and Zygmunt Drążek},
keywords = {Ontology learning, Knowledge base, Methods, tools supporting ontology learning},
abstract = {Ontology learning techniques are based on extensive basic knowledge, from unstructured data to structured data. Creating an ontology manually is undoubtedly a time consuming process. In addition, manually-curated background knowledge is a scarce resource for many domains. Ontology learning is a relatively new research area that draws from related fields. However, a number of approaches are still rising up and, what is more, they vary and evolve across different aspects. Therefore, the purpose of this article is to provide comprehensive knowledge about selected groups of methods, tools and approaches supporting ontology. Presenting different approaches to learning ontology and including them in one publicly available summary, aims to provide an overview of the most appropriate methods, tools and approaches to building ontologies from various sources. Besides, the proposed solution in this article will expand the already offered ontology of learning methods with additional, more detailed functions. In addition to the domain ontology offered, this is another goal of this article. The correctness of the proposed ontology was verified by the constructed competence questions and implemented using Description Logic query mechanism.}
}
@article{RODLER201992,
title = {Are query-based ontology debuggers really helping knowledge engineers?},
journal = {Knowledge-Based Systems},
volume = {179},
pages = {92-107},
year = {2019},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119302126},
author = {Patrick Rodler and Dietmar Jannach and Konstantin Schekotihin and Philipp Fleiss},
keywords = {Knowledge base debugging, Interactive debugging, User study, Ontologies, Model-based diagnosis, Protégé, Ontology debugging tool},
abstract = {Real-world semantic or knowledge-based systems can become large and complex, e.g., in the biomedical domain. Tool support for the localization and repair of faults within knowledge bases of such systems can therefore be essential for their practical success. Correspondingly, a number of knowledge base debugging approaches, in particular for ontology-based systems, were proposed in recent years. Query-based debugging is a comparably recent interactive approach that localizes the true cause of an observed problem by asking knowledge engineers a series of questions. Concrete implementations of this approach exist, such as the OntoDebug plug-in for the ontology editor Protégé. To validate that a newly proposed method is favorable over an existing one, researchers often rely on simulation-based comparisons. Such an evaluation approach however has certain limitations and often cannot fully inform us about a method’s true usefulness. We therefore conducted a range of user studies to assess the practical value of query-based ontology debugging. One main insight from the studies is that the considered interactive approach is indeed more efficient than an alternative algorithmic debugging based on test cases. We also observed that users frequently made errors in the process, which highlights the importance of a careful design of the queries that users need to answer.}
}
@article{DAI2025106956,
title = {Large Language Model Enhanced Logic Tensor Network for Stance Detection},
journal = {Neural Networks},
volume = {183},
pages = {106956},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106956},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024008852},
author = {Genan Dai and Jiayu Liao and Sicheng Zhao and Xianghua Fu and Xiaojiang Peng and Hu Huang and Bowen Zhang},
keywords = {Logic tensor network, Stance detection, Chain-of-thought},
abstract = {Social media platforms, rich in user-generated content, offer a unique perspective on public opinion, making stance detection an essential task in opinion mining. However, traditional deep neural networks for stance detection often suffer from limitations, including the requirement for large amounts of labeled data, uninterpretability of prediction results, and difficulty in incorporating human intentions and domain knowledge. This paper introduces the First-Order Logic Aggregated Reasoning framework (FOLAR), an innovative approach that integrates first-order logic (FOL) with large language models (LLMs) to enhance the interpretability and efficacy of stance detection. FOLAR comprises three key components: a Knowledge Elicitation module that generates FOL rules using a chain-of-thought prompting method, a Logic Tensor Network (LTN) that encodes these rules for stance detection, and a Multi-Decision Fusion mechanism that aggregates LTNs’ outputs to minimize biases and improve robustness. Our experiments on standard benchmarks demonstrate the effectiveness of FOLAR, showing it as a promising solution for explainable and accurate stance detection. The source code will be made publicly available to foster further research.}
}
@article{JAIN2021100009,
title = {A fuzzy ontology framework in information retrieval using semantic query expansion},
journal = {International Journal of Information Management Data Insights},
volume = {1},
number = {1},
pages = {100009},
year = {2021},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2021.100009},
url = {https://www.sciencedirect.com/science/article/pii/S2667096821000021},
author = {Shivani Jain and K.R. Seeja and Rajni Jindal},
keywords = {Semantic Web, Information retrieval, Fuzzy-ontology, Query-expansion, Search engines, Information systems development},
abstract = {World Wide Web (WWW) constitutes fuzzy information and requires soft computing techniques to deal context of the query. It works on the principle of keyword matching yielding low precision and recall. Semantic web, an extension WWW improves the information retrieval process. Query expansion is utmost importance in information retrieval to retrieve relevant results. To overcome the weaknesses of current web system and to utilize the strengths query expansion a novel framework based on fuzzy ontology is proposed for information retrieval. In the proposed framework, domain specific knowledge is utilized for ontology construction. In framework pre-defined domain ontologies and Global ontology, ConceptNet is used to construct a fuzzy ontology. Based on constructed fuzzy ontology most semantically related words for a query are identified and query is expanded. A fuzzy membership function is defined for different semantic relationships present among the Global ontology ConceptNet. Based on the proposed framework queries are expanded (Semantic query expansion) and evaluated on four popular search engines namely Google, Yahoo, Bing and Exalead. The performance metrics used are Precision, Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), R-precision and Number of documents retrieved. The Web search engines are precision oriented. Based on the proposed framework all the metrics are improved approx. by 10%. Precision before the query expansion lies between 0.75-0.81 whereas after the query expansion lies between 0.85-0.89 on various search engines. The number of documents retrieved is almost improved 1/1000 after the query expansion.}
}
@article{WONG2024104082,
title = {Construction contract risk identification based on knowledge-augmented language models},
journal = {Computers in Industry},
volume = {157-158},
pages = {104082},
year = {2024},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2024.104082},
url = {https://www.sciencedirect.com/science/article/pii/S0166361524000101},
author = {Saika Wong and Chunmo Zheng and Xing Su and Yinqiu Tang},
keywords = {Large language models, Construction contract risk, Knowledge augmentation, Knowledge database},
abstract = {Contract review is an essential step in construction projects to prevent potential losses. However, the current methods for reviewing construction contracts lack effectiveness and reliability, leading to time-consuming and error-prone processes. Although large language models (LLMs) have shown promise in revolutionizing natural language processing (NLP) tasks, they struggle with domain-specific knowledge and addressing specialized issues. This paper presents a novel approach that leverages LLMs with construction contract knowledge to emulate the process of contract review by human experts. Our tuning-free approach incorporates construction contract domain knowledge to enhance language models for identifying construction contract risks. The use of natural language when building the domain knowledge base facilitates practical implementation. We evaluated our method on real construction contracts and achieved solid performance. Additionally, we investigated how LLMs employ logical thinking during the task and provided insights and recommendations for future research.}
}
@article{LEGUILLON2023104325,
title = {Integrating a new knowledge organisation system for monoclonal antibodies for therapeutic use authorised in Europe into HeTOP terminology-ontology server},
journal = {Journal of Biomedical Informatics},
volume = {140},
pages = {104325},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104325},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423000461},
author = {Romain Léguillon and Laura Gosselin and Christophe Carnoy and Thibaut Pressat-Laffouilhere and Catherine Letord and Badisse Dahamna and Stéfan J. Darmoni and Julien Grosjean},
keywords = {Thesaurus, Data warehousing, Natural language processing, Monoclonal antibodies},
abstract = {Monoclonal antibodies (MAs) are increasingly used in the therapeutic arsenal. Clinical Data Warehouses (CDWs) offer unprecedented opportunities for research on real-word data. The objective of this work is to develop a knowledge organization system on MAs for therapeutic use (MATUs) applicable in Europe to query CDWs from a multi-terminology server (HeTOP). After expert consensus, three main health thesauri were selected: the MeSH thesaurus, the National Cancer Institute thesaurus (NCIt) and the SNOMED CT. These thesauri contain 1,723 MAs concepts, but only 99 (5.7 %) are identified as MATUs. The knowledge organisation system proposed in this article is a six-level hierarchical system according to their main therapeutic target. It includes 193 different concepts organised in a cross lingual terminology server, which will allow the inclusion of semantic extensions. Ninety nine (51.3 %) MATUs concepts and 94 (48.7 %) hierarchical concepts composed the knowledge organisation system. Two separates groups (an expert group and a validation group) carried out the selection, creation and validation processes. Queries identify, for unstructured data, 83 out of 99 (83.8 %) MATUs corresponding to 45,262 patients, 347,035 hospital stays and 427,544 health documents, and for structured data, 61 out of 99 (61.6 %) MATUs corresponding to 9,218 patients, 59,643 hospital stays and 104,737 hospital prescriptions. The volume of data in the CDW demonstrated the potential for using these data in clinical research, although not all MATUs are present in the CDW (16 missing for unstructured data and 38 for structured data). The knowledge organisation system proposed here improves the understanding of MATUs, the quality of queries and helps clinical researchers retrieve relevant medical information. The use of this model in CDW allows for the rapid identification of a large number of patients and health documents, either directly by a MATU of interest (e.g. Rituximab) but also by searching for parent concepts (e.g. Anti-CD20 Monoclonal Antibody).}
}
@incollection{ADEL2019353,
title = {Chapter 14 - A unified fuzzy ontology for distributed electronic health record semantic interoperability},
editor = {Nilanjan Dey and Amira S. Ashour and Simon James Fong and Surekha Borra},
booktitle = {U-Healthcare Monitoring Systems},
publisher = {Academic Press},
pages = {353-395},
year = {2019},
series = {Advances in Ubiquitous Sensing Applications for Healthcare},
issn = {25891014},
doi = {https://doi.org/10.1016/B978-0-12-815370-3.00014-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128153703000141},
author = {Ebtsam Adel and Shaker El-Sappagh and Sherif Barakat and Mohammed Elmogy},
keywords = {Electronic health records (EHR), Semantic interoperability, Process interoperability, Healthcare system, Ontology, Terminology},
abstract = {Electronic health records (EHR) provide efficient management of clinical information in any healthcare organization. It is a complete and longitudinal electronic registration of all occasions and data identified with the person's health status, from birth to death. Medical data are growing rapidly. These data are heterogeneous, distributed, and nonstructured. Each data element can have its schema, structure, standard, format, coding system, level of abstraction, and semantic. Medical personnel need to query the distributed EHR systems anonymously by using a single language. Combination and integration of the data are vital to recover the history of patients, to share information, and to elicit queries. Semantic interoperability provides a meaningful exchange and the use of clinical data between many healthcare systems. Physicians often send fuzzy questions to EHR systems and need answers from distributed systems. In this chapter, a unified semantic interoperability framework for distributed EHR based on fuzzy ontology is proposed. The framework architecture consists of three main layers. The lowest layer (local ontologies construction) stores the EHRs heterogeneous data with different database schemas, standards, terminologies, purposes, locations, and formats. The sources of this information may be different databases (e.g., MySQL, SqlServer, DB2, Access, and Oracle) in heterogeneous schemas, EHR standards, XML files, spreadsheet files, or archetype definition language (ADL) files. These different inputs are transformed into crisp ontology using a mediator (e.g., DB2OWL, X2OWL or ADL2OntoModule) suitable for each type. In the middle layer (global ontology construction), the local ontologies are mapped (using mapping algorithms or human experts with the help of common terminology vocabularies) to a crisp global one. The global reference ontology combines and integrates all local ontologies and therefore describes all data. Then this crisp ontology is converted to a unified fuzzy ontology. Finally, the third layer is the user interface in which a doctor or any specialist can ask any linguistic or semantic queries by dealing with only the global reference fuzzy ontology. That ontology is more dynamic and helps in understanding natural language deep medical queries. The result is a global and robust semantic interoperability technique. The proposed solution is based on a fuzzy ontology semantic to integrate different healthcare systems. That framework has many benefits and advantages over frameworks that rely on crisp ontology only, including: (1) it moves toward achieving full semantic interoperability of heterogeneous EHRs, (2) it supports the idea of plug and play where any system with any structure can be integrated anonymously with existing systems without affecting the current working environment, and (3) it is an expandable and designed in a modular way as it based on using ontologies and terminologies; the functionality of the proposed framework can be extended uniformly. We expect that our framework will handle the current EHR semantic interoperability challenges, reduce the cost of the integration process, and get a higher acceptance and accuracy rate than previous studies.}
}
@article{WATROBSKI20203356,
title = {Ontology learning methods from text - an extensive knowledge-based approach},
journal = {Procedia Computer Science},
volume = {176},
pages = {3356-3368},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.061},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920319566},
author = {Jarosław Wątróbski},
keywords = {Ontology learning from text, Methods for ontology leaninf from text, Domain ontology learning, Ontology integration},
abstract = {Ontologies are a key element of the Semantic Web. They aim to capture basic knowledge by providing appropriate terms and formal relationships between them, so that they can be used in a machine-processable manner. Accordingly they enable automatic aggregation and practical use as well as unexpected reuse of distributed data sources. Ontologies may come from many different sources, pursuing different goals and quality criteria. However, performed manually ontology construction is a very complex and tedious task, thus many methods proposed offer automatic or semi-automatic way for ontology construction. Many of the methods have their own, specific features. Therefore, this paper proposes an extensive knowledge-based approach covering the domain of ontology learning methods from text. This work aims to collect the knowledge of available approaches for ontology learning and the prominent differences between them, drawing on best practices in ontology engineering. The proposed approach refers to methods and aims to enrich knowledge in the field of ontology learning (OL). In this paper, the author’s ontology contains a set of various types of methods with main techniques used, and the necessary features in the miscellaneous approaches. The proposed an extensive knowledge-based approach uses a reasoning mechanism based on competency questions for individual approaches to determine their ontology learning method profiles. The validation stage has also been carried out. At the same time, it is an extension of the previous study in the form of a repository of knowledge about OL tools. In addition, the combination of both ontologies: tools and methods aim to provide a more efficient OL solution from text.}
}
@article{YANG2019148,
title = {Ontology-based systems engineering: A state-of-the-art review},
journal = {Computers in Industry},
volume = {111},
pages = {148-171},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0166361518307887},
author = {Lan Yang and Kathryn Cormican and Ming Yu},
keywords = {systems engineering, ontology, state of the art, systematic literature review},
abstract = {In recent years ontology-based systems engineering has grown significantly. Its raison d’etre is to use ontologies to improve the systems engineering body of knowledge. Specifically, ontologies act as an enabler of good knowledge management as they focus on establishing well-defined domain concepts in terms of terminologies, definitions, and relationships. In addition, the use of formal semantics is essential for explicit, sharable, and reusable knowledge representation. However, little research exists that evaluates the impact and real benefits of ontologies for systems engineering. A thorough review of the state of the art of ontology-based systems engineering will contribute to the future development of the discipline. Therefore, the primary objective of this paper is to draw a clear roadmap of how ontologies support systems engineering and to determine what extent they have been applied in this domain. This review contributes to a holistic examination of the primary studies relevant to the topic of ontology-based systems engineering, spanning nearly two decades. The findings provide an integrated and comprehensive understanding of and shed new light on (1) the systems engineering knowledge areas supported by ontologies; (2) the contribution that ontologies make to systems engineering problems; (3) the existing ontologies that are created to support systems engineering; and (4) the techniques adopted from an ontology engineering perspective. It assesses the influence of ontologies in systems engineering knowledge areas, expounding and highlighting the effects of ontologies. All in all, it presents a comprehensive summary of the state of the art of ontology-based systems engineering, as well as illuminating a roadmap for future directions.}
}
@article{LI2019152,
title = {Enhancing energy management at district and building levels via an EM-KPI ontology},
journal = {Automation in Construction},
volume = {99},
pages = {152-167},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2018.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517309494},
author = {Yehong Li and Raúl García-Castro and Nandana Mihindukulasooriya and James O'Donnell and Sergio Vega-Sánchez},
keywords = {District, Building, Energy management, Stakeholders, Ontology, Linked data},
abstract = {The use of information and communication technologies facilitates energy management (EM) at both district and building levels, but also generates a considerable amount of data. To gain insights into such data, it is essential to resolve the cross-domain data interoperability problem and determine an approach to exchange performance information and insightful data among various stakeholders. This paper developed an EM-KPI (key performance indicator) ontology to exchange key performance information and data for districts and buildings. The ontology contains two components: namely KPIs and EM master data; these, respectively, represent the multi-level performance information for energy performance tracking and the key data for data exploitation. Through a demonstration, a sample linked dataset generated using the data correlation predefined in the ontology is presented. The linked data analysis proves the feasibility of the ontology for exchanging data among different stakeholders and for exploring insights in relation to performance improvement.}
}
@article{HAI2021190,
title = {Ontology knowledge base combined with Bayesian networks for integrated corridor risk warning},
journal = {Computer Communications},
volume = {174},
pages = {190-204},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421001687},
author = {Nan Hai and Daqing Gong and Shifeng Liu},
keywords = {Integrated corridor, Risk warning, Ontology knowledge, Bayesian networks},
abstract = {With the accelerated urbanization process, the emergence of urban underground integrated pipeline corridors is the trend for cities, especially large and medium-sized cities. However, due to the complexity of the internal system of the integrated corridor, there are various risks in the process of its construction and operation and maintenance, and the risk factors are complex and diverse. In this paper, we introduce ontology technology and knowledge base construction into the risk management of integrated pipeline corridor, build an ontology-based knowledge base of integrated pipeline corridor risk, and construct a Bayesian network based on the established risk knowledge base for risk evaluation of identified risk factors. The combination of ontology knowledge base construction and Bayesian network method of integrated pipeline corridor risk makes the risk identification system completer and more effective, and the method can effectively evaluate the disaster risk level of integrated pipeline corridor operation and maintenance, which can meet the practical needs of integrated pipeline corridor operation and maintenance risk management and disaster prevention and mitigation work.}
}
@article{KIM2018338,
title = {Extracting and applying evaluation criteria for ontology quality assessment},
journal = {Library Hi Tech},
volume = {37},
number = {3},
pages = {338-354},
year = {2018},
issn = {0737-8831},
doi = {https://doi.org/10.1108/LHT-01-2019-0012},
url = {https://www.sciencedirect.com/science/article/pii/S0737883118000787},
author = {Seonghun Kim and Sam G. Oh},
keywords = {Ontology, Knowledge management, Delphi method},
abstract = {Purpose
The purpose of this paper is to formulate apposite criteria for ontology evaluation and test them through assessments of existing ontologies.
Design/methodology/approach
A literature review provided the basis from which to extract the categories relevant to an evaluation of internal ontology components. According to the ontology evaluation categories, a panel of experts provided the evaluation criteria for each category via Delphi survey. Reliability was gauged by applying the criteria to assessments of existing smartphone ontologies.
Findings
Existing research tends to approach ontology evaluation through comparison with well-engineered ontologies, implementation in target applications and appropriateness/interconnection appraisals in relation to raw data, but such methodologies fall short of shedding light on the internal workings of ontologies, such as structure, semantic representation and interoperability. This study adopts its evaluation categories from previous research while also collecting concrete evaluation criteria from an expert panel and verifying the reliability of the resulting 53 criteria.
Originality/value
This is the first published study to extract ontology evaluation criteria in terms of syntax, semantics and pragmatics. The results can be used as an evaluation index following ontology construction.}
}
@article{WHEELER20188,
title = {Feasibility and usability of an ontology-based mobile intervention for patients with hypertension},
journal = {International Journal of Medical Informatics},
volume = {119},
pages = {8-16},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618301710},
author = {Tyler S. Wheeler and T. {Michael Vallis} and Nicholas B. Giacomantonio and Samina R. Abidi},
keywords = {Mobile health, Ontology, Hypertension, Chronic disease self-management, Behaviour change},
abstract = {Background
Lifestyle changes and the adoption of healthy behaviours are well established recommendations for the management of hypertension—a risk factor for cardiovascular and kidney disease. Mobile health interventions offer unique advantages and novel approaches to helping individuals make and maintain such behaviour changes; however, current interventions often lack theoretical and scientific grounding.
Objective
The goal of this study is to effectively model the knowledge, concepts and relationships relevant to the management of a chronic illness like hypertension, and to implement this knowledge model within a mobile self-management application that can be used by patients.
Methods
A behaviour modification approach based on COM-B (capability, opportunity, motivation, behaviour) Model and the associated Behaviour Change Wheel was developed. An ontology-based knowledge model was implemented to formally conceptualise relevant knowledge in hypertension clinical practice guidelines, behaviour change models and associated behaviour change strategies. A hypertension management decision support framework was designed and implemented as a proof-of-concept mobile phone application (EmpowerBP) using the aforementioned model. The usability of this pilot application was tested using think-aloud protocol by eight individuals with hypertension while performing predefined tasks. Thematic analysis with inductive thematic coding was performed to identify specific feedback and areas for improvement.
Results
The most common positive feedback included participants finding application resources interesting or helpful and liking the user interface. The most common negative feedback was finding the included salt calculator confusing or laborious to use and finding the profile creation questionnaire too long. The derived themes were: features, profile creation, resources, scenario, usability, user interface.
Conclusions
The ontology knowledge model formalises variables, properties, and relationships such that they can be used for problem solving. By integrating and computerising complex knowledge from clinical practice guidelines, behaviour change theories, and associated behaviour change strategies, it is possible to model existing information about the management of hypertension as an ontology. This proof-of-concept application creates clinical and behavioural profiles of a user to provide them with personalised management strategies, rooted in established behaviour change theory, that will engage and empower them to manage their condition. Given the nature of ontological models, this approach can be easily modified to address a variety of chronic illnesses.}
}
@article{CHOU2018538,
title = {Generation and visualization of earthquake drill scripts for first responders using ontology and serious game platforms},
journal = {Advanced Engineering Informatics},
volume = {38},
pages = {538-554},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1474034617304317},
author = {Chien-Cheng Chou and An-Ping Jeng and Chun-Ping Chu and Chih-Hsiung Chang and Ru-Guan Wang},
keywords = {Disaster response, HAZUS-MH, Earthquake drill script, Serious game, Ontology},
abstract = {As there is an increasing number of disasters happening worldwide, numerous mitigation approaches have been proposed to alleviate the impact of disasters. In Taiwan, public and private organizations often work together to prepare various disaster scenarios to train emergency response units. Thus, the design of appropriate drill scripts plays an important role in enhancing the capabilities of first responders in a real disaster. However, developing a reasonable drill script is a time-consuming, error-prone, and costly task. Drill scripts designed may need to accommodate time-dependent, region-specific requirements so that first responders can see varied disaster scenarios for improvement. Therefore, an ontology model with Semantic Web Rule Language (SWRL) constructs is proposed to help the drill script generation process for earthquakes in Taiwan. Drill script designers need to prepare an input data set describing a simulated earthquake using the Taiwan Earthquake Loss Estimation System, a simplified version of the Hazards U.S. – Multi Hazard (HAZUS-MH) program. Then, a drill script following pre-defined rules can be generated and combined with Unity, a serious game platform, in order to display all earthquake-related events in a virtual environment. Additional rules to accommodate varied requirements of an earthquake can be represented by customized SWRL constructs, which can be seamlessly added into the proposed drill script generation process. The developed system is demonstrated using data sets for buildings in Taiwan. During a disaster exercise, first responders can gain better situational awareness regarding an earthquake’s spatiotemporal progress. Finally, it is suggested that first responders review the scene using the proposed approach immediately after a real earthquake, so that improved search and rescue plans can be defined and implemented.}
}
@article{MING2020103145,
title = {An Ontology for Representing Knowledge of Decision Interactions in Decision-Based Design},
journal = {Computers in Industry},
volume = {114},
pages = {103145},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.103145},
url = {https://www.sciencedirect.com/science/article/pii/S0166361519304191},
author = {Zhenjun Ming and Gehendra Sharma and Janet K. Allen and Farrokh Mistree},
keywords = {Decision Interaction, Decision-Based Design, Decision Workflow, Ontology, Knowledge Representation},
abstract = {Design processes for complex engineered systems are inherently complex due to the dependencies among subsystems at the same level and between levels of a partitioned hierarchy. In decision-based design, this results in interactions among sets of design decisions. Representing and capturing the knowledge related to the decision interactions is critical for designing decision-based design processes. Two key challenges in modeling the interactions are: •there are different types of decisions, and•these decisions are made at different levels. To address these challenges, in this paper we identify nine basic interaction patterns among decisions and propose an ontology to define the knowledge associated with these interaction patterns. Key advantages of the ontology include that we can capture both the vertical and horizontal interactions between decisions in a decision-based design process, and we can design flexible, reusable, and executable decision workflows for designing complex systems using the ontology. The utility of the ontology is illustrated via a one[HYPHEN]stage reduction gearbox design example, a hot rod rolling process design example and a composite structure design example.}
}
@article{LULA20202425,
title = {An Advanced Analysis of Cloud Computing Concepts Based on the Computer Science Ontology},
journal = {Computers, Materials and Continua},
volume = {66},
number = {3},
pages = {2425-2443},
year = {2020},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2021.013771},
url = {https://www.sciencedirect.com/science/article/pii/S1546221820000223},
author = {Paweł Lula and Octavian Dospinescu and Daniel Homocianu and Napoleon-Alexandru Sireteanu},
keywords = {Cloud computing scientific literature, cloud related concepts, CSO ontology},
abstract = {Our primary research hypothesis stands on a simple idea: The evolution of top-rated publications on a particular theme depends heavily on the progress and maturity of related topics. And this even when there are no clear relations or some concepts appear to cease to exist and leave place for newer ones starting many years ago. We implemented our model based on Computer Science Ontology (CSO) and analyzed 44 years of publications. Then we derived the most important concepts related to Cloud Computing (CC) from the scientific collection offered by Clarivate Analytics. Our methodology includes data extraction using advanced web crawling techniques, data preparation, statistical data analysis, and graphical representations. We obtained related concepts after aggregating the scores using the Jaccard coefficient and CSO Ontology. Our article reveals the contribution of Cloud Computing topics in research papers in leading scientific journals and the relationships between the field of Cloud Computing and the interdependent subdivisions identified in the broader framework of Computer Science.}
}
@article{MCDANIEL201832,
title = {Assessing the quality of domain ontologies: Metrics and an automated ranking system},
journal = {Data & Knowledge Engineering},
volume = {115},
pages = {32-47},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X16304062},
author = {Melinda McDaniel and Veda C. Storey and Vijayan Sugumaran},
keywords = {Domain ontology, Interoperability, Metrics, Ontology assessment, Ontology evaluation, Ranking, Ontology, Semiotics, Semiotic layers, Domain Ontology Ranking System},
abstract = {The ability of a user to select an appropriate, high-quality domain ontology from a set of available options would be most useful in knowledge engineering and other intelligent applications. This capability, however, requires good quality assessment metrics as well as automated support when there is a large number of ontologies from which to make a selection. This research analyzes existing metrics for domain ontology evaluation and extends them to derive a Layered Ontology Metrics Suite based on semiotic theory. The metrics are implemented in a Domain Ontology Ranking System (DoORS) prototype, the purpose of which is to search an ontology library for specific terms to retrieve candidate domain ontologies and then assess their quality and suitability based upon the suite of metrics. The prototype system is compared to existing approaches to automated ontology quality ranking to illustrate the usefulness of the research.}
}
@article{MARIANI2022270,
title = {Non-accessible mass and the ontology of GRW},
journal = {Studies in History and Philosophy of Science},
volume = {91},
pages = {270-279},
year = {2022},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2021.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S0039368121001928},
author = {Cristian Mariani},
keywords = {Collapse theories, Mass density GRW, Ontic indeterminacy, Quantum mechanics, Tails problem},
abstract = {The Mass Density approach to GRW (GRWM for short) has been widely discussed in the quantum foundations literature. A crucial feature of GRWM is the introduction of a Criterion of Accessibility for mass, which allows to explain the determinacy of experimental outcomes thus also addressing the tails problem of GRW. However, the Criterion of Accessibility leaves the ontological meaning of the non-accessible portion of mass utterly unexplained. In this paper I discuss two viable approaches to non-accessible mass, which I call anti-realist and realist, and will defend the latter. First, I show that the anti-realist approach suffers from various objections. Second, I develop an account of non-accessible mass density states as objectively indeterminate states of affairs.}
}
@article{HAN2021633,
title = {APTMalInsight: Identify and cognize APT malware based on system call information and ontology knowledge framework},
journal = {Information Sciences},
volume = {546},
pages = {633-664},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.08.095},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520308628},
author = {Weijie Han and Jingfeng Xue and Yong Wang and Fuquan Zhang and Xianwei Gao},
keywords = {APT attack, APT malware, System call information, Ontology},
abstract = {APT attacks have posed serious threats to the security of cyberspace nowadays which are usually tailored for specific targets. Identification and understanding of APT attacks remains a key issue for society. Attackers often utilize malware as the weapons to launch cyber-attacks. For this reason, detecting APT malware and gaining an insight of its malicious behaviors can strengthen the power to understand and counteract APT attacks. Based on the above motivation, this paper proposes a novel APT malware detection and cognition framework named APTMalInsight aiming at identifying and cognizing APT malware by leveraging system call information and ontology knowledge. We systematically study APT malware and extracts dynamic system call information to describe its behavioral characteristics. With respect to the established feature vectors, the APT malware can be detected and clustered into their belonging families accurately. Furthermore, a horizontal comparison between APT malware and the traditional malware is conducted from the perspective of behavior types, to understand the behavioral characteristics of APT malware in depth. On the above basis, the ontology model is introduced to construct the APT malware knowledge framework to represent its typical malicious behaviors, thereby implementing the systematic cognition of APT malware and providing contextual understanding of APT attacks. The evaluation results based on real APT malware samples demonstrate that the detection and clustering accuracy can reach up to 99.28% and 98.85% respectively. In addition, APTMalInsight supplies an effective cognition framework for APT malware and enhances the capability to understand APT attacks.}
}
@article{MOHDALI2019191,
title = {A product life cycle ontology for additive manufacturing},
journal = {Computers in Industry},
volume = {105},
pages = {191-203},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0166361518301647},
author = {Munira {Mohd Ali} and Rahul Rai and J. Neil Otte and Barry Smith},
keywords = {, Manufacturing process ontology, Ontology engineering, Product life cycle, Dentistry product manufacturing},
abstract = {The manufacturing industry is evolving rapidly, becoming more complex, more interconnected, and more geographically distributed. Competitive pressure and diversity of consumer demand are driving manufacturing companies to rely more and more on improved knowledge management practices. As a result, multiple software systems are being created to support the integration of data across the product life cycle. Unfortunately, these systems manifest a low degree of interoperability, and this creates problems, for instance when different enterprises or different branches of an enterprise interact. Common ontologies (consensus-based controlled vocabularies) have proved themselves in various domains as a valuable tool for solving such problems. In this paper, we present a consensus-based Additive Manufacturing Ontology (AMO) and illustrate its application in promoting re-usability in the field of dentistry product manufacturing.}
}
@article{LV2020106050,
title = {A novel meta-matching approach for ontology alignment using grasshopper optimization},
journal = {Knowledge-Based Systems},
volume = {201-202},
pages = {106050},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.106050},
url = {https://www.sciencedirect.com/science/article/pii/S0950705120303403},
author = {Zhaoming Lv and Rong Peng},
keywords = {Ontology matching, Grasshopper optimization algorithm, Ontology alignment evaluation initiative},
abstract = {Ontology alignment is a fundamental task to support information sharing and reuse in heterogeneous information systems. Optimizing the combination of matchers by evolutionary algorithms to align ontology is an effective method. However, such methods have two significant shortcomings: weights need to be set manually to combine matchers, and a reference alignment is required during the optimization process. In this paper, a meta-matching approach GSOOM for automatically configuring weights and threshold using grasshopper optimization algorithm (GOA) has been proposed. In this approach, the ontology alignment problem is modeled as optimizing individual fitness of GOA. A fitness function is proposed, which includes two goals: maximizing the number of matching and the similarity score. Since it does not require an expert to provide a reference alignment, it is more suitable for real-world scenarios. To demonstrate the advantages of the approach, we conduct exhaustive experiments tasks on several standard datasets and compare its performance to other state-of-the-art methods. The experimental results illustrate that our approach is more efficiently and is significantly superior to other metaheuristic-based methods.}
}
@article{MARTINLAMMERDING2023119027,
title = {An ontology-based system to avoid UAS flight conflicts and collisions in dense traffic scenarios},
journal = {Expert Systems with Applications},
volume = {215},
pages = {119027},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.119027},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422020450},
author = {David Martín-Lammerding and José Javier Astrain and Alberto Córdoba and Jesús Villadangos},
keywords = {UAS, Ontology, Autonomous, Collision avoidance systems, Knowledge, Situational-awareness},
abstract = {New Unmanned Aerial Systems (UAS) applications will increase air traffic densities in metropolitan regions. Collision avoidance systems (CAS) are a key component in integrating a high number of UAS into the airspace in a safe way. This paper presents a distributed, autonomous, and knowledge-based CAS, called Dronetology System (DroS), for UASs. The CAS proposed here is managed using a novel ontology, called Dronetology-cas, which allows to make autonomous decisions according to the knowledge inferred from the data gathered by the UAS. DroS is deployed as part of the payload of the UAS. So, it is designed to run in an embedded platform with limited processing capacity and low battery consumption. DroS collects data from sensors and collaborative elements to make smart decisions using knowledge obtained from collaborative UASs, adapting the maneuvers of the aerial vehicles to their original flight plans, their kind of vehicle, and the collision scenario. DroS accountability involves recording its internal operation to assist with reconstructing the circumstances surrounding an autonomous maneuver or the details previous to a collision. DroS has been verified using the hardware in the loop (HIL) technique with a UAS traffic environment simulator. Results obtained show a significant improvement in terms of safety by avoiding collisions.}
}
@article{ROLDANGARCIA2018176,
title = {Towards an ontology-driven clinical experience sharing ecosystem: Demonstration with liver cases},
journal = {Expert Systems with Applications},
volume = {101},
pages = {176-195},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2018.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S095741741830071X},
author = {María del Mar Roldán-García and Suzan Uskudarli and Neda B. Marvasti and Burak Acar and José F. Aldana-Montes},
keywords = {Ontology driven application, Medical case representation, Knowledge representation, Biomedical resources and systems},
abstract = {Past medical cases, hence clinical experience, are invaluable resources in supporting clinical practice, research, and education. Medical professionals need to be able to exchange information about patient cases and explore them from subjective perspectives. This requires a systematic and flexible methodology to case representation for supporting the exchange of processable patient information. We present an ontology based approach to modeling patient cases and use patients with liver disease conditions as an example. To this end a novel ontology, lico, that utilizes well known medical standards is proposed to represent liver patient cases. The utility of the proposed approach is demonstrated with semantic queries and reasoning using data collected from real patients. The preliminary results are promising in regards to the potentials of ontology based medical case representation for building case-based search and retrieval systems, paving the way towards a Clinical Experience Sharing platform for comparative diagnosis, research, and education.}
}
@article{POTONIEC2020105098,
title = {Dataset of ontology competency questions to SPARQL-OWL queries translations},
journal = {Data in Brief},
volume = {29},
pages = {105098},
year = {2020},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2019.105098},
url = {https://www.sciencedirect.com/science/article/pii/S2352340919314544},
author = {Jedrzej Potoniec and Dawid Wiśniewski and Agnieszka Ławrynowicz and C. Maria Keet},
keywords = {Ontology authoring, Competency questions, SPARQL-OWL, Semantic web},
abstract = {This data article reports on a new set of 234 competency questions for ontology development and their formalisation into a set of 131 SPARQL-OWL queries. This is the largest set of competency questions with their linked queries to date, covering several ontologies of different type in different subject domains developed by different groups of question authors and ontology developers. The dataset is focused specifically on the ontology TBox (terminological part). The dataset may serve as a manually created gold standard for testing and benchmarking, research into competency questions and querying ontologies, and tool development. The data is available in Mendeley Data. Its analysis is presented in “Analysis of Ontology Competency Questions and their formalizations in SPARQL-OWL” [15].}
}
@article{JANOWICZ20191,
title = {SOSA: A lightweight ontology for sensors, observations, samples, and actuators},
journal = {Journal of Web Semantics},
volume = {56},
pages = {1-10},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300295},
author = {Krzysztof Janowicz and Armin Haller and Simon J.D. Cox and Danh {Le Phuoc} and Maxime Lefrançois},
keywords = {Ontology, Sensor, Observation, Actuator, Linked data, Web of Things, Internet of Things, Schema.org},
abstract = {The Sensor, Observation, Sample, and Actuator (SOSA) ontology provides a formal but lightweight general-purpose specification for modellingthe interaction between the entities involved in the acts of observation, actuation, and sampling. SOSA is the result of rethinking the W3C-XG Semantic Sensor Network (SSN) ontology based on changes in scope and target audience, technical developments, and lessons learned over the past years. SOSA also acts as a replacement of SSN’s Stimulus Sensor Observation (SSO) core. It has been developed by the first joint working group of the Open Geospatial Consortium (OGC) and the World Wide Web Consortium (W3C) on Spatial Data on the Web. In this work, we motivate the need for SOSA, provide an overview of the main classes and properties, and briefly discuss its integration with the new release of the SSN ontology as well as various other alignments to specifications such as OGC’s Observations and Measurements (O&M), Dolce-Ultralite (DUL), and other prominent ontologies. We will also touch upon common modelling problems and application areas related to publishing and searching observation, sampling, and actuation data on the Web. The SOSA ontology and standard can be accessed at https://www.w3.org/TR/vocab-ssn/.}
}
@article{BAUDRIT2022,
title = {Decision Support Tool for the Agri-Food Sector Using Data Annotated by Ontology and Bayesian Network:},
journal = {International Journal of Agricultural and Environmental Information Systems},
volume = {13},
number = {1},
year = {2022},
issn = {1947-3192},
doi = {https://doi.org/10.4018/IJAEIS.309136},
url = {https://www.sciencedirect.com/science/article/pii/S1947319222000041},
author = {Cédric Baudrit and Patrice Buche and Nadine Leconte and Christophe Fernandez and Maëllis Belna and Geneviève Gésan-Guiziou},
keywords = {Bayesian Network, Data Integration, INRAE, Knowledge Base, Knowledge Integration, Milk Microfiltration, Ontology, Reliability, Uncertainty},
abstract = {ABSTRACT
The scientific literature is a valuable source of information for developing predictive models to design decision support systems. However, scientific data are heterogeneously structured expressed using different vocabularies. This study developed a generic workflow that combines ontology, databases, and computer calculation tools based on the theory of belief functions and Bayesian networks. The ontology paradigm is used to help integrate data from heterogeneous sources. Bayesian network is estimated using the integrated data taking into account their reliability. The proposed method is unique in the sense that it proposes an annotation and reasoning tool dedicated to systematic analysis of the literature, which takes into account expert knowledge of the domain at several levels: ontology definition, reliability criteria, and dependence relations between variables in the BN. The workflow is assessed successfully by applying it to a complex food engineering process: skimmed milk microfiltration. It represents an original contribution to the state of the art in this application domain.}
}
@article{DUNLAP202339,
title = {The green economy as counterinsurgency, or the ontological power affirming permanent ecological catastrophe},
journal = {Environmental Science & Policy},
volume = {139},
pages = {39-50},
year = {2023},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2022.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S1462901122003197},
author = {Alexander Dunlap},
keywords = {Green economy, Counterinsurgency, Renewable energy, Smart technologies, Climate change},
abstract = {As old as industrialism or civilization itself, socio-ecological problems are nothing new. Despite all efforts to resolve environmental dilemmas, socio-ecological catastrophe has only intensified. Governments, in response, have unveiled the green economy to confront ecological and climate catastrophe. The green economy, however, has worsened socio-ecological conditions, invigorating the present trajectory of (techno)capitalist development. This article argues that the green economy serves as a tool of global counterinsurgency, managing, preempting and redirecting the inevitable ecological anxiety that could mobilize for radical social change. While fragmenting ecological opposition, the green economy meanwhile serves as a “force multiplier” for market expansion and capitalist development, as opposed to actually working towards real socio-ecological mitigation and remediation. The article proceeds by defining counterinsurgency, and indicating its relevance to the green economy. Dissecting the technics of the green economy, the next section reviews its origins and epistemological foundations by investigating the concepts and operationalization of ‘energy’, ‘biodiversity’ and ‘carbon’. Then, briefly, the article reviews the extractive reality of low-carbon infrastructures, revealing the socio-ecological harm implied and justified by the green economic and decarbonization schemes. The green economy, it concludes, is a governmental technology, preventing collective self-reflection and action to (adequately) rehabilitate ecosystems and address the structural socio-ecological problems threatening the planet, thus preforming a counter-insurrectionary function in the service of state and capital.}
}
@article{KOORAPATI20226077,
title = {Towards a unified ontology for IoT fabric with SDDC},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {8, Part B},
pages = {6077-6091},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821001014},
author = {Koundinya Koorapati and Rubini Pandu and Prem Kumar Ramesh and Sairam Veeraswamy and Usha Narasappa},
keywords = {Internet of Things (IoT), Cloud computing, Software-Defined Data Center (SDDC), Ontology, Semantic web},
abstract = {The ever growing Internet of Things (IoT) paradigm aims to put people’s incorporation into the new phase of connectivity and sensor technology. While, IoT has the potential to deliver new value-added services in order to make life easier and healthier for people, there are still several issues to be addressed in order to harness the widespread dissemination and adoption of the IoT paradigm, considering its potential benefits. In this context, considering an IoT ecosystem holistically i.e. end-to-end which includes the proprietary Operational Technology (OT) comprising of software and hardware to monitor, detect and control the equipment through sensors and actuators and the general Information Technology (IT) which comprises of the data center infrastructure catering to the backend needs of IoT such as compute, storage and network, one major challenge is converging OT with IT. Such an OT/IT convergence has the potential to explore many problems that exist in the end-to-end IoT ecosystem. One such problem is related to drawing actionable insights through operational analytics. The data center paradigm which concerns this research is the Software-Defined Data Center (SDDC) which is touted as the preferred data center paradigm for IoT. While most of the research happening on IoT today deals with the data from the IoT applications and sensors, this research focuses on the plethora of the metadata contained in the end-to-end IoT ecosystem. This research demonstrates the modelling of the end-to-end IoT ecosystem using semantic-based approach such as ontology. We arrive at a cohesive unified ontology unifying the OT and the IT constituents of the IoT fabric with SDDC together with rich context aware attributes embedded in the unified ontology, thereby addressing the problem of OT/IT convergence as well as laying the foundation to explore many solutions to problems pertinent in such a converged OT/IT ecosystem.}
}
@article{KUHNE2025102481,
title = {Supporting Sound Multi-Level Modeling—Specification and Implementation of a Multi-Dimensional Modeling Approach},
journal = {Data & Knowledge Engineering},
volume = {160},
pages = {102481},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102481},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X2500076X},
author = {Thomas Kühne and Manfred A. Jeusfeld},
keywords = {Conceptual modeling, Multi-level modeling, Well-formedness, Integrity constraints, Modeling anti-patterns},
abstract = {Multiple levels of classification naturally occur in many domains. Several multi-level modeling approaches account for this, and a subset of them attempt to provide their users with sanity-checking mechanisms in order to guard them against conceptually ill-formed models. Historically, the respective multi-level well-formedness schemes have either been overly restrictive or too lax. Orthogonal Ontological Classification has been proposed as a foundation for sound multi-level modeling that combines the selectivity of strict schemes with the flexibility afforded by laxer schemes. In this article, we present the second iteration of a formalization of Orthogonal Ontological Classification, which we empirically validated to demonstrate some of its hitherto only postulated claims using an implementation in ConceptBase. We discuss the expressiveness of the formal language used, ConceptBase’s evaluation efficiency, and the usability of our realization based on a digital twin example model.}
}
@article{FORTH2024110312,
title = {Semantic enrichment for BIM-based building energy performance simulations using semantic textual similarity and fine-tuning multilingual LLM},
journal = {Journal of Building Engineering},
volume = {95},
pages = {110312},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.110312},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224018801},
author = {Kasimir Forth and André Borrmann},
keywords = {BIM to BEM, BEPS, Semantic enrichment, Semantic textual similarity, Fine tuning LLM},
abstract = {To achieve the global targets of the Paris Agreement of limiting global warming, it is necessary to reduce the operational energy of buildings, which are responsible for around 30% of the global greenhouse gas emissions. Building Energy Performance Simulation (BEPS) is an established method to estimate the building’s energy demand in early design stages. Building Information Models (BIM) provides geometric and semantic information to create precise Building Energy Models (BEM) in early design stages. However, manual enrichment of missing semantic information is still a time-consuming and laborious process. Therefore, we propose a novel methodology to automatically enrich missing information to BIM using Semantic Textual Similarity (STS) and fine-tuned Large Language Models (LLM). For every IfcSpace, we match room-specific space types and constructions with missing thermal properties using the semantic most similar pairs of the BIM model and the according databases. We use three real-world case studies to fine-tune LLMs, and two case studies evaluate the whole methodology. Different fine-tuning strategies, such as using different loss functions, adding opposing word pairs or domain-specific abbreviations, significantly improve the accuracy of the matching. At the same time, however, findings show that semantic matching based on multilingual fine-tuned LLM performs worse than translated, monolingually fine-tuned LLM. Finally, BEPS results from automatically enriched BEM only slightly deviate from manually enriched BEM.}
}
@article{PEREIRA2020101760,
title = {A knowledge representation of the beginning of the innovation process: The Front End of Innovation Integrative Ontology (FEI2O)},
journal = {Data & Knowledge Engineering},
volume = {125},
pages = {101760},
year = {2020},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2019.101760},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X18301162},
author = {Ariane Rodrigues Pereira and João José Pinto Ferreira and Alexandra Lopes},
keywords = {Front end of innovation, Ontology, Entrepreneurship, Concept development, Design science},
abstract = {The initial phase of the innovation process is widely accepted as an important driver of positive results for new products and for the success of businesses. The Front End of Innovation (FEI) is a multidisciplinary area that includes a variety of activities, such as ideation, opportunity identification and analysis, feasibility analysis, global trends analysis, concept definition, customer and competitor analysis, and even business model development. Due to the number and variety of FEI responsibilities, this phase entails a considerable level of complexity and decision making. This fact is reflected in the literature, where one finds a variety of FEI approaches and proposals, seldom overlapping and offering no clear consensual guidance. This work aimed at overcoming this gap by proposing an Ontology for the Front End of Innovation as a comprehensive knowledge representation of the FEI, the so-called Front End of Innovation Integrative Ontology (FEI2O). The ontology balanced the differences and addressed the shortcomings of the main FEI Reference Models and included contributions from the field. This research builds on a combination of qualitative and quantitative methodologies. It combines the qualitative methods of interviewing and focus group discussion to collect the views of domain experts, used to refine the artefact and later to evaluate the final ontology. Quantitative analysis of data was carried out using the Attribute Agreement approach. The FEI2O explicitly provides a description of a domain regarding concepts, properties and relations of concepts. The main benefit of the FEI2O is to provide a comprehensive formal reference model and a common vocabulary.}
}
@article{KATSUMI201853,
title = {Ontologies for transportation research: A survey},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {89},
pages = {53-82},
year = {2018},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2018.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X18300858},
author = {Megan Katsumi and Mark Fox},
keywords = {Transportation ontology, Knowledge representation, Reasoning, Interoperability, Formal logic, Semantic Web},
abstract = {Transportation research relies heavily on a variety of data. From sensors to surveys, data supports day-to-day operations as well as long-term planning and decision-making. The challenges that arise due to the volume and variety of data that are found in transportation research can be effectively addressed by ontologies. This opportunity has already been recognized – there are a number of existing transportation ontologies, however the relationship between them is unclear. The goal of this work is to provide an overview of the opportunities for ontologies in transportation research and operation, and to present a survey of existing transportation ontologies to serve two purposes: (1) to provide a resource for the transportation research community to aid in understanding (and potentially selecting between) existing transportation ontologies; and (2) to identify future work for the development of transportation ontologies, by identifying areas that may be lacking.}
}
@article{TAHER20212382,
title = {Formalizing knowledge representation in earthwork operations through development of domain ontology},
journal = {Engineering, Construction and Architectural Management},
volume = {29},
number = {6},
pages = {2382-2414},
year = {2021},
issn = {0969-9988},
doi = {https://doi.org/10.1108/ECAM-10-2020-0810},
url = {https://www.sciencedirect.com/science/article/pii/S0969998821000217},
author = {Alhusain Taher and Faridaddin Vahdatikhaki and Amin Hammad},
keywords = {Earthwork operations, Taxonomies, Earthwork ontology},
abstract = {Purpose
This study proposes a framework for Earthwork Ontology (EW-Onto) to support and enhance data exchange in the project and the efficient decision-making in the planning and execution phases.
Design/methodology/approach
The development of EW-Onto started from defining the concepts and building taxonomies for earthwork operations and equipment following the METHONTOLOGY approach. In addition, several rules have been extracted from safety codes and implemented as SWRL rules. The ontology has been implemented using Protégé. The consistency of EW-Onto has been checked and it has been evaluated using a survey.
Findings
The assessment of EW-Onto by experts indicates an adequate level of consensus with the framework, as an initial step for explicit knowledge exchanges within the earthwork domain.
Practical implications
The use of an ontology within the earthwork domain can help: (1) link and identify the relationships between concepts, define earthwork semantics, and classify knowledge in a hierarchical way accepted by experts and end-users; (2) facilitate the management of earthwork operations and simplify information exchange and interoperability between currently fragmented systems; and (3) increase the stakeholders' knowledge of earthwork operations through the provision of the information, which is structured in the context of robust knowledge.
Originality/value
This paper proposes a framework for Earthwork Ontology (EW-Onto) to support and enhance data exchange in the project and the efficient decision-making in the planning and execution phases. EW-Onto represents the semantic values of the entities and the relationships, which are identified and formalized based on the basic definitions available in the literature and outlined by experts.}
}
@article{CLAEYS2018176,
title = {Ontological Model for Managing Context-aware Assembly Instructions},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {176-181},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.254},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318313788},
author = {A. Claeys and S. Hoedt and M. Schamp and H. Van Landeghem and J. Cottyn},
keywords = {human operator support, intelligent interfaces, manual assembly, context-modeling, context sensitive information, content filtering},
abstract = {High model variance and complexity in mixed model assembly systems causes a vast amount of information. Current information systems neglect the user’s situation. Consequently, the user has to process loads of information that may be unnecessary. Information filtering and personalization are for this reason indispensable. By marking assembly instructions with context information, content filtering can be applied. Based on current research in several manual assembly environments in Flanders, a generic ontological model for managing all assembly related context is designed. The model consists of eight categories that define the four main domain context categories (people, places, things and work tasks) and their interrelationships (experience, condition, use and requirement). In order to validate the context model, an experimental approach on a lab set-up is proposed.}
}
@article{MENDONCA2020101045,
title = {Ontological emergence scheme in self-organized and emerging systems},
journal = {Advanced Engineering Informatics},
volume = {44},
pages = {101045},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101045},
url = {https://www.sciencedirect.com/science/article/pii/S1474034620300148},
author = {Maribel Mendonça and Niriaska Perozo and Jose Aguilar},
keywords = {Emerging systems, Self-organized systems, Ontology mining, Web semantic, Ontological emergence, Knowledge discovery},
abstract = {This paper presents the concept of “Ontological Emergence”, a process that seeks to adapt an ontology to the changes and new components in a self-organized and emergent system, through the application of a set of rules that allows the emergence of a new conceptualization (emerging concepts). The Ontological Emergence provides the structuration of the information and knowledge that could be generated in the system, creating conceptual models that can adequately represent the new behavior that is emerging. It arises from the need to represent ontologically a conceptualization of a reality that is dynamic, which cannot be pre-defined or pre-determined, in order to generate emerging knowledge models that follows the scalability and the evolution of it. In that sense, in this paper is proposed an “Ontological Emergence Scheme” based on a set of processes of registration, monitoring, analysis and adaptation of the various conceptual models that interact in the system, as well as on some processing rules in regard to requirements and information of the context, in order to allow the ontological emergence. In this proposal scheme, the Meta-ontologies guide the ontological emergence process through the definition of general categories, to facilitate the integration of concepts from different ontologies or data sources. Finally, the paper presents some case studies, showing its utility in self-organized and emergent systems.}
}
@article{JASKO2020103300,
title = {Development of manufacturing execution systems in accordance with Industry 4.0 requirements: A review of standard- and ontology-based methodologies and tools},
journal = {Computers in Industry},
volume = {123},
pages = {103300},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103300},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520305340},
author = {Szilárd Jaskó and Adrienn Skrop and Tibor Holczinger and Tibor Chován and János Abonyi},
keywords = {Manufacturing execution systems (MES), Industry 4.0, Ontologies, Semantic models of Industry 4.0, Reference architectural model for industry 4.0 (RAMI 4.0)},
abstract = {This work presents how recent trends in Industry 4.0 (I4.0) solutions are influencing the development of manufacturing execution systems (MESs) and analyzes what kinds of trends will determine the development of the next generation of these technologies. This systematic and thematic review provides a detailed analysis of I4.0-related requirements in terms of MES functionalities and an overview of MES development methods and standards because these three aspects are essential in developing MESs. The analysis highlights that MESs should interconnect all components of cyber-physical systems in a seamless, secure, and trustworthy manner to enable high-level automated smart solutions and that semantic metadata can provide contextual information to support interoperability and modular development. The observed trends show that formal models and ontologies will play an even more essential role in I4.0 systems as interoperability becomes more of a focus and that the new generation of linkable data sources should be based on semantically enriched information. The presented overview can serve as a guide for engineers interested in the development of MESs as well as for researchers interested in finding worthwhile areas of research.}
}
@article{KONYS20182194,
title = {Knowledge systematization for ontology learning methods},
journal = {Procedia Computer Science},
volume = {126},
pages = {2194-2207},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.07.229},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918312043},
author = {Agnieszka Konys},
keywords = {ontology learning, knowledge systematization, ontology learning approaches, ontology},
abstract = {The need for interoperable semantics in modern information systems forces to develop more and more intelligent solutions. The increasing demand for these solutions, the explosion of various types of information and the technological development pose new challenges and requirements. Ontologies are often viewed as the answer to this need. The connections between ontologies and Semantic Web become a very promising area. The Semantic Web’s success is dependent on the quality of its underline ontologies, whereas ontologies provide a shared and a common understanding of a domain enabling communication between people and heterogeneous and distributed systems. However, key issue helps ontologies to power the Semantic Web have made ontology learning from various data sources a very auspicious field of research. It aims at semi-automatically or automatically building ontologies from given data sources with a limited human exert. A huge number of available approaches for ontology learning and the prominent differences between them cause the necessity of knowledge systematization for this domain. The paper yields the author’s proposal of ontological elaboration for methods for ontology learning and their features, providing formal, practical and technological guidance to knowledge management based approach to methods supporting ontology learning.}
}
@article{LAMY2020103407,
title = {Explainable decision support through the learning and visualization of preferences from a formal ontology of antibiotic treatments},
journal = {Journal of Biomedical Informatics},
volume = {104},
pages = {103407},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103407},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420300356},
author = {Jean-Baptiste Lamy and Karima Sedki and Rosy Tsopra},
keywords = {Clinical decision support system, Explainable artificial intelligence, Preference learning, Preference visualization, Ontologies, Antibiotics},
abstract = {The aim of eXplainable Artificial Intelligence (XAI) is to design intelligent systems that can explain their predictions or recommendations to humans. Such systems are particularly desirable for therapeutic decision support, because physicians need to understand rcommendations to have confidence in their application and to adapt them if required, e.g. in case of patient contraindication. We propose here an explainable and visual approach for decision support in antibiotic treatment, based on an ontology. There were three steps to our method. We first generated a tabular dataset from the ontology, containing features defined on various domains and n-ary features. A preference model was then learned from patient profiles, antibiotic features and expert recommendations found in clinical practice guidelines. This model made the implicit rationale of the expert explicit, including the way in which missing data was treated. We then visualized the preference model and its application to all antibiotics available on the market for a given clinical situation, using rainbow boxes, a recently developed technique for set visualization. The resulting preference model had an error rate of 3.5% on the learning data, and 5.2% on test data (10-fold validation). These findings suggest that our system can help physicians to prescribe antibiotics correctly, even for clinical situations not present in the guidelines (e.g. due to allergies or contraindications for the recommended treatment).}
}
@article{CIMINO202452,
title = {An ontology-based, general-purpose and Industry 4.0-ready architecture for supporting the smart operator (Part II – Virtual Reality case)},
journal = {Journal of Manufacturing Systems},
volume = {73},
pages = {52-64},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524000013},
author = {Antonio Cimino and Francesco Longo and Giovanni Mirabelli and Vittorio Solina and Saverino Verteramo},
keywords = {Virtual reality, Digital twin, Smart operator, Smart factory, Industry 4.0},
abstract = {The ongoing transformation of the manufacturing sector towards Industry 4.0 is driven by the increasing importance of new technologies, such as Virtual Reality (VR) and Digital Twins (DTs). VR enables immersive and interactive experiences, while DTs facilitate real-time monitoring and control of manufacturing systems. However, their widespread adoption faces barriers including the lack of ready-to-use applications, interoperability, and the absence of comprehensive platforms. To overcome these challenges, the main purpose of this paper, which is PART II of Longo et al. [1], is to show how the existing “Knowledge on Industry 4.0 for Innovation” (KNOW4I) platform can be exploited in the case of VR. Specifically, by designing and implementing a framework which enables the creation of immersive virtual environments acting as DTs, the authors aim at supporting Smart Operators in monitoring and controlling in real-time complex industrial systems. The case study conducted in the steel industry confirms the practical application of the KNOW4I VR capabilities and demonstrates the immersive and realistic experience provided by the VR environment, enabling operators to easily monitor and control in real-time industrial processes. Finally, the analysis of end user experience within the presented case study highlights the effectiveness and usability of the proposed solution.}
}
@article{MORENTEMOLINERA2023109868,
title = {Introducing disruption on stagnated Group Decision Making processes using Fuzzy Ontologies},
journal = {Applied Soft Computing},
volume = {132},
pages = {109868},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.109868},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622009176},
author = {J.A. Morente-Molinera and A. Morfeq and R. Al-Hmouz and E.B. Ashary and J.F. Su and E. Herrera-Viedma},
keywords = {Group Decision Making, Fuzzy Ontologies, Consensus measures},
abstract = {In Group Decision Making processes, experts debate about how to rank a set of alternatives. It is usual that, at a certain point of the discussion, the debate gets stuck. In this paper, a novel Group Decision Making method for environments with a high number of alternatives is presented. Fuzzy Ontologies are used in order to represent the alternatives and their characteristics. Moreover, a novel stagnation analysis is used in order to determine if the debate gets stuck. If it does, the method modifies the alternatives set in order to introduce new options and remove the least popular ones. This way, the debate can revive since that the new alternatives provide different points of view. The presented method helps experts to conduct long and thorough debates in order for them to be able to make effective and reliable decisions.}
}
@article{NASIRI2019181,
title = {Knowledge representation and management based on an ontological CBR system for dementia caregiving},
journal = {Neurocomputing},
volume = {350},
pages = {181-194},
year = {2019},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.04.027},
url = {https://www.sciencedirect.com/science/article/pii/S092523121930551X},
author = {Sara Nasiri and Golnaz Zahedi and Simone Kuntz and Madjid Fathi},
keywords = {Case-based reasoning, Ontology, WHO ICF codes, Semantic retrieval, Caregiving, DePicT Dementia CLASS},
abstract = {Case-based reasoning (CBR) is a problem-solving methodology that uses past knowledge and experiences to interpret or solve new problems. It is appropriate for experience-based repeatable problems. This article describes a method for managing the semantic consistency of an ontology of dementia caregiving as an ontological CBR system. Ontologies are used for our knowledge model and case representation for creating our case base. Using international classification of functioning, disability, and health (ICF) from world health organization (WHO) report, a dementia ontology such as ADO and a systematic review of quantitative and qualitative studies about the needs of informal caregivers in this related diseases enhances the development and integration of our recommender system. It is a caregiver support tool to Detect and Predict dementia using information of carings Tasks by Ontological Case-based Learning Assistant System, called DePicT Dementia Onto-CLASS. This paper proposes an ontological CBR system by utilizing protégé and myCBR open source tools. Representing CBR knowledge using this ontology and building a case retrieval algorithm improves the accuracy of resulting systems. Moreover, the representation of dependencies among concepts and relationships can assist caregivers in certifying the semantic consistency of this ontology. The ontology evaluated with Hermit and myCBR, and caregivers validated the obtained results.}
}
@article{BHARTI2021100325,
title = {Ontology development for measurement process and uncertainty of results},
journal = {Measurement: Sensors},
volume = {18},
pages = {100325},
year = {2021},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2021.100325},
url = {https://www.sciencedirect.com/science/article/pii/S2665917421002889},
author = {Priyanka Bharti and QingPing Yang and Alistair Forbes and Marina Romanchikova and Jean-Laurent Hippolyte},
abstract = {In future manufacturing and metrology, there is increasing demand to organize relevant metadata and knowledge to present information in semantically meaningful, reusable, easily accessible, and interoperable form. Up-to-date information on measurement uncertainty is key to interpretation of measurement results and to assessment of the quality of the measurement process. Although various technologies from knowledge engineering have been proposed to fulfil this requirement, previous work has not fully addressed the uncertainty during the measurement process. This paper presents the method to develop an ontology of the measurement process and the uncertainty of results on the example of coordinate measurements. The resulting ontology model based on a set of competency questions, including key concepts and relationships between them, is presented and discussed. The consistency of the ontology model is verified by inferencing rules and answering competency questions in Protégé software. The presented ontology will find wide applications in metrology and Industry 4.0.}
}
@article{ERGEN2024105907,
title = {Development of ontological algorithms for exact QTO of reinforced concrete construction items},
journal = {Structures},
volume = {60},
pages = {105907},
year = {2024},
issn = {2352-0124},
doi = {https://doi.org/10.1016/j.istruc.2024.105907},
url = {https://www.sciencedirect.com/science/article/pii/S2352012424000596},
author = {Faruk Ergen and Önder Halis Bettemir},
keywords = {Building Information Modeling, Computer aided design, Python, Semantic modeling, Quantity take-off},
abstract = {Building Information Modeling (BIM) provides significant benefits to the construction industry throughout the project management process. However, state-of-the-art BIM software provides erroneous quantity take-off (QTO) results above the negligible margin. In this study, QTO calculation algorithms have been developed for rough construction and implemented on BIM software to solve the stated problem. The developed QTO algorithms establish semantic relations and search the neighborhood of the structural elements to detect the intersecting structural elements. Amount of intersection is calculated by the dimensions and the locations of the intersecting structural elements. Exact length and spacing of the rebar and stirrups are calculated by considering the created semantic relationships. QTO of formwork is accurately calculated by considering the voids of the QTO computations which are obtained from the constructed semantic relationships by investigating the column-beam, slab-beam, and slab-column contact areas. Furthermore, the construction type of the scaffolding for the formwork is determined by considering both the dimensions of the structure and the established semantic relations. The developed algorithms are executed on BIM capable software and tested in four case studies with special conditions. The comparison with manual condition revealed that the proposed algorithms provided exact results. The semantic QTO algorithms developed in this study have the potential to be useful for BIM software developers.}
}
@article{GARCIA2019100487,
title = {Grounding knowledge acquisition with ontology explanation:A case study},
journal = {Journal of Web Semantics},
volume = {57},
pages = {100487},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300672},
author = {Ana Cristina B. Garcia and Adriana S. Vivacqua},
keywords = {Explanation, Knowledge acquisition, Ontology validity, Ontology engineering},
abstract = {Knowledge validation is still a challenge when constructing knowledge-based systems. It is one of the major reasons for user rejection and disagreement between project participants. Systematic and periodic reviews of the domain ontology, with a formal agreement of the whole development team (including the experts) are a recommended good practice. Nevertheless, these reviews do not guarantee system success. This paper presents a case study of the construction process of a knowledge-based system. The process involved a group of experts with varied work experience. A great deal of negotiation happened during knowledge acquisition meetings, which took place during a 6-month period. After each meeting, changes in the ontology were verified through a web-based questionnaire, from which either consensual agreement was reached (and changes implemented) or the need for a new meeting was ascertained. An explanatory review at the beginning of each meeting further solidified the understanding of all participants. This cyclic process led to a final version of the ontology, ratified by all participants. This model supports diagnosis and prediction of failures in mechanical drilling rigs in oil exploration sites. Unexpectedly, during system trials, experts disagreed with results, which raised questions about the validity of the domain ontology. The system’s explanation module provided a cornerstone for a reflective process that helped identify inconsistencies and corrections needed. These reflections led to adjustments to the ontology, and a reflection about previous decisions and element definitions. Explanations, derived from the ontology and instantiated using real scenarios, shed light on knowledge gaps and semantic inconsistencies of the domain model. In this paper we have three main goals: (1) to present our ontology construction process; (2) to highlight a particular situation where results were inadequate; and (3) to show how the explanation system helped experts and knowledge engineers identify gaps. We also present lessons learned from the whole process, that may apply in other situations.}
}
@article{HONG2023551,
title = {Operational Ontology for Oncology: A Framework for Improved Communication and Understanding in Cancer Care},
journal = {International Journal of Radiation Oncology*Biology*Physics},
volume = {117},
number = {3},
pages = {551-553},
year = {2023},
issn = {0360-3016},
doi = {https://doi.org/10.1016/j.ijrobp.2023.02.058},
url = {https://www.sciencedirect.com/science/article/pii/S0360301623002122},
author = {David S. Hong and Amanda Caissie and Coen W. Hurkmans and Andra V. Krauze and Randi Kudner and Thomas G. Purdie and Ying Xiao}
}
@article{BRAGATTO2020104204,
title = {Ageing management and monitoring of critical equipment at Seveso sites: An ontological approach},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {66},
pages = {104204},
year = {2020},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2020.104204},
url = {https://www.sciencedirect.com/science/article/pii/S0950423020304915},
author = {Paolo A. Bragatto and Silvia M. Ansaldi and Patrizia Agnello and Tiziano {Di Condina} and Fabio M. Zanzotto and Maria Francesca Milazzo},
keywords = {Equipment ageing, Major accident hazard, Industrial safety, Seveso directive, Ontology, Decision-making, Non-destructive testing, Integrity monitoring},
abstract = {A safe “ageing” of Seveso establishments is a challenge for both operators and regulators. To this scope, Seveso III Directive required to integrate the equipment integrity issue into the safety management system for the major accident prevention; at the same time, the Italian Authority adopted a short-cut method for a quick ageing evaluation, which awards the application of the best techniques to control integrity and prevent deterioration-related failures. In this paper, the use of the ontology has been proposed to support decision-making about the implementation of technical solutions to control equipment ageing and comply the requirements of the Seveso legislation. To contrast deterioration mechanisms, the rapid development of data intensive smart sensors should be exploited and, in this frame, the automated on-line direct monitoring of equipment conditions, based on innovative low-cost sensors, is a novelty and promising solution. The developed ontology-based system points towards the adoption, when possible, of on-line monitoring. This solution provides much more data than traditional measurements and it is essential for the operators to understand how to merge concurrent information and data and to adequately control equipment deterioration. The ontology-based approach appears a viable solution even for this purpose. To demonstrate its potentiality, a real use-case has been used, where the model has been tested in finding the best technical solutions to improve the ageing management of an atmospheric distillation unit of a refinery in order to comply with safety requirements. A further use-case is given to show how the model can be used to react, after real-time damage signals, to restore safety conditions by means of an adequate decision-making.}
}
@article{PREVENTIS2021275,
title = {CLONE: Collaborative Ontology Editor as a Service in the Cloud},
journal = {Procedia Computer Science},
volume = {184},
pages = {275-282},
year = {2021},
note = {The 12th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 4th International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921007791},
author = {Alexandros Preventis and Euripides G.M. Petrakis},
keywords = {Ontology, Collaborative editor, Service Oriented Architecture, Cloud Computing},
abstract = {The evolution of Web and cloud services technology has facilitated collaboration on the Web, providing the means for concurrent editing, change tracking and storing files in the cloud (e.g. Google Docs, Office 365). Ontology development teams could greatly benefit from this technology, that until now have been applied mainly to document processing. We introduce CLONE, a Web-based ontology editor that runs in the cloud and provides a real-time collaborative environment for creating and editing ontologies. CLONE is designed as a service-oriented architecture taking advantages of the easy extensibility and scalability features of this approach. CLONE provides all the essential features of stand-alone ontology editors, as well as significant collaboration features, including concurrent editing, editing history, team conversations and role-based access-authorization mechanisms.}
}
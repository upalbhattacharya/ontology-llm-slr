@article{ZHONG2025,
title = {Enhancing the Accuracy of Human Phenotype Ontology Identification: Comparative Evaluation of Multimodal Large Language Models},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/73233},
url = {https://www.sciencedirect.com/science/article/pii/S1438887125007757},
author = {Wei Zhong and Mingyue Sun and Shun Yao and YiFan Liu and Dingchuan Peng and Yan Liu and Kai Yang and HuiMin Gao and HuiHui Yan and WenJing Hao and YouSheng Yan and ChengHong Yin},
keywords = {multimodal large language models, ChatGPT, rare diseases, human phenotype ontology, open-source LLMs, large language model},
abstract = {Background
Identifying Human Phenotype Ontology (HPO) terms is crucial for diagnosing and managing rare diseases. However, clinicians, especially junior physicians, often face challenges due to the complexity of describing patient phenotypes accurately. Traditional manual search methods using HPO databases are time-consuming and prone to errors.
Objective
The aim of the study is to investigate whether the use of multimodal large language models (MLLMs) can improve the accuracy of junior physicians in identifying HPO terms from patient images related to rare diseases.
Methods
In total, 20 junior physicians from 10 specialties participated. Each physician evaluated 27 patient images sourced from publicly available literature, with phenotypes relevant to rare diseases listed in the Chinese Rare Disease Catalogue. The study was divided into 2 groups: the manual search group relied on the Chinese Human Phenotype Ontology website, while the MLLM-assisted group used an electronic questionnaire that included HPO terms preidentified by ChatGPT-4o as prompts, followed by a search using the Chinese Human Phenotype Ontology. The primary outcome was the accuracy of HPO identification, defined as the proportion of correctly identified HPO terms compared to a standard set determined by an expert panel. Additionally, the accuracy of outputs from ChatGPT-4o and 2 open-source MLLMs (Llama3.2:11b and Llama3.2:90b) was evaluated using the same criteria, with hallucinations for each model documented separately. Furthermore, participating physicians completed an additional electronic questionnaire regarding their rare disease background to identify factors affecting their ability to accurately describe patient images using standardized HPO terms.
Results
A total of 270 descriptions were evaluated per group. The MLLM-assisted group achieved a significantly higher accuracy rate of 67.4% (182/270) compared to 20.4% (55/270) in the manual group (relative risk 3.31, 95% CI 2.58‐4.25; P<.001). The MLLM-assisted group demonstrated consistent performance across departments, whereas the manual group exhibited greater variability. Among standalone MLLMs, ChatGPT-4o achieved an accuracy of 48% (13/27), while the open-source models Llama3.2:11b and Llama3.2:90b achieved 15% (4/27) and 18% (5/27), respectively. However, MLLMs exhibited a high hallucination rate, frequently generating HPO terms with incorrect IDs or entirely fabricated content. Specifically, ChatGPT-4o, Llama3.2:11b, and Llama3.2:90b generated incorrect IDs in 57.3% (67/117), 98% (62/63), and 82% (46/56) of cases, respectively, and fabricated terms in 34.2% (40/117), 41% (26/63), and 32% (18/56) of cases, respectively. Additionally, a survey on the rare disease knowledge of junior physicians suggests that participation in rare disease and genetic disease training may enhance the performance of some physicians.
Conclusions
The integration of MLLMs into clinical workflows significantly enhances the accuracy of HPO identification by junior physicians, offering promising potential to improve the diagnosis of rare diseases and standardize phenotype descriptions in medical research. However, the notable hallucination rate observed in MLLMs underscores the necessity for further refinement and rigorous validation before widespread adoption in clinical practice.}
}
@article{ROMANENKO2024102342,
title = {Evaluating quality of ontology-driven conceptual models abstractions},
journal = {Data & Knowledge Engineering},
volume = {153},
pages = {102342},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102342},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000661},
author = {Elena Romanenko and Diego Calvanese and Giancarlo Guizzardi},
keywords = {Conceptual model abstraction, Ontology-driven conceptual models, Quality evaluation of abstractions, Unified foundational ontology (UFO), FAIR model catalog, User studies in conceptual modeling},
abstract = {The complexity of an (ontology-driven) conceptual model highly correlates with the complexity of the domain and software for which it is designed. With that in mind, an algorithm for producing ontology-driven conceptual model abstractions was previously proposed. In this paper, we empirically evaluate the quality of the abstractions produced by it. First, we have implemented and tested the last version of the algorithm over a FAIR catalog of models represented in the ontology-driven conceptual modeling language OntoUML. Second, we performed three user studies to evaluate the usefulness of the resulting abstractions as perceived by modelers. This paper reports on the findings of these experiments and reflects on how they can be exploited to improve the existing algorithm.}
}
@article{OBIEDKOV2025109523,
title = {PAC learning of concept inclusions for ontology-mediated query answering},
journal = {International Journal of Approximate Reasoning},
volume = {186},
pages = {109523},
year = {2025},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2025.109523},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X25001641},
author = {Sergei Obiedkov and Barış Sertkaya},
keywords = {Ontologies, Description logics, Active learning, Knowledge acquisition, PAC learning},
abstract = {We present a probably approximately correct algorithm for learning the terminological part of a description-logic knowledge base via subsumption queries. The axioms we learn are concept inclusions between conjunctions of concepts from a specified set of concept descriptions. By varying the distribution of queries posed to the oracle, we adapt the algorithm to improve the recall when using the resulting TBox for ontology-mediated query answering. Experimental evaluation on OWL 2 EL ontologies suggests that our approach helps significantly improve recall while maintaining a high precision of query answering.}
}
@article{VALCALVO2025104042,
title = {OntoGenix: Leveraging Large Language Models for enhanced ontology engineering from datasets},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104042},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.104042},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324004011},
author = {Mikel Val-Calvo and Mikel {Egaña Aranguren} and Juan Mulero-Hernández and Ginés Almagro-Hernández and Prashant Deshmukh and José Antonio Bernabé-Díaz and Paola Espinoza-Arias and José Luis Sánchez-Fernández and Juergen Mueller and Jesualdo Tomás Fernández-Breis},
keywords = {Knowledge graphs, Large Language Models, Ontology engineering},
abstract = {Knowledge Graphs integrate data from multiple, heterogeneous sources, using ontologies to facilitate data interoperability. Ontology development is a resource-consuming task that requires the collaborative work of domain experts and ontology engineers. Therefore, companies invest considerable resources in order to generate and maintain Enterprise Knowledge Graphs and ontologies from large and complex datasets, most of which can be unfamiliar for ontology engineers. In this work, we study the use of Large Language Models to aid in the development of ontologies from datasets, ultimately increasing the automation of the generation of ontology-based Knowledge Graphs. As a result we have developed a structured workflow that leverages Large Language Models to enhance ontology engineering through data pre-processing, ontology planning, building, and entity improvement. Our method is also able to generate mappings and RDF data, but in this work we focus on the ontologies. The pipeline has been implemented in the OntoGenix tool. In this work we show the results of the application of OntoGenix to six datasets related to commercial activities. The findings indicate that the ontologies produced exhibit patterns of coherent modeling, and features that closely resemble those created by humans, although the most complex situations are better reflected by the ontologies developed by humans.}
}
@article{SHIM2025103001,
title = {OmEGa(Ω): Ontology-based information extraction framework for constructing task-centric knowledge graph from manufacturing documents with large language model},
journal = {Advanced Engineering Informatics},
volume = {64},
pages = {103001},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.103001},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624006529},
author = {Midan Shim and Hyojun Choi and Heeyeon Koo and Kaehyun Um and Kyong-Ho Lee and Sanghyun Lee},
keywords = {Ontology modeling, Manufacturing and maintenance process, Information extraction, Knowledge graph, Document understanding, Large language model},
abstract = {Manufacturing industry relies heavily on technical documents that encapsulate specialized knowledge essential for optimizing production and maintenance processes. However, extracting meaningful insights from these documents is challenging due to their complex structure, domain-specific terminology, and multimodal content, which includes text, images, and tables. Furthermore, there is a contextual gap between the generic training data of pre-trained language models (PLMs) and the specialized knowledge required for manufacturing documents. To address these issues, a Task-Centric Ontology (TCO) is designed to describe fundamental manufacturing tasks, and develop OmEGa, an Ontology-based Information Extraction Framework for Task-Centric Knowledge Graphs. OmEGa leverages large language models (LLMs) to perform instance recognition and relation classification on multimodal documents. By utilizing spatial embedding and modality linking, OmEGa addresses structural challenges, while TCO-driven reasoning mitigates contextual challenges. Experimental results demonstrate the effectiveness of OmEGa, achieving strong performance on both proprietary and open-source datasets. Additionally, a Knowledge Graph Question Answering (KGQA) system built on the extracted task-centric knowledge shows promise in enhancing communication among domain experts in the manufacturing sector.}
}
@incollection{CASCIANELLI2025380,
title = {Biological and Medical Ontologies: Introduction},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {380-391},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00061-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027000610},
author = {Silvia Cascianelli and Marco Masseroli},
keywords = {Controlled vocabulary, Interoperability, Knowledge discovery, Ontology, Open biological and biomedical ontologies, Semantic network, Terminology, Unified medical language system},
abstract = {The increasing availability of biomedical-molecular data and information pushed the need for their standard descriptions, which allow their automatic processing for knowledge extraction. Towards this goal, many biological and medical terminologies and ontologies were developed, and actively used to describe the features of numerous biomedical-molecular entities. However, bio-terminology and bio-ontology development grew dispersedly and not uniformly, producing not matching ontologies with their annotations being non-comparable, hampering their interoperability role. These issues have been mostly overcome with the spread of Open Biological and Biomedical Ontologies, a set of well-structured and orthogonal bio-ontologies, and of the Unified Medical Language System, which provides mapping among biomedical concepts and relations from multiple sources.}
}
@article{YOON2025112802,
title = {Ontology-enabled AI agent-driven intelligent digital twins for building operations and maintenance},
journal = {Journal of Building Engineering},
volume = {108},
pages = {112802},
year = {2025},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2025.112802},
url = {https://www.sciencedirect.com/science/article/pii/S2352710225010393},
author = {Sungmin Yoon and Jihwan Song and Jiteng Li},
keywords = {Digital twin (DT), Intelligent digital twin (IDT), Operation and maintenance (O&M), Built environments, AI agent, Large language model (LLM), Ontology, Building informatics},
abstract = {Building digital twins (DTs) are essential for enhancing operational efficiency, optimizing energy consumption, and reducing costs in buildings. However, the inherent complexity of buildings, their long operational lifespans, and the specific nature of the construction industry pose significant challenges in creating digital twins for buildings. Intelligent digital twins (IDTs) address these challenges by integrating existing digital twin models with AI, enabling a comprehensive representation of the building lifecycle while incorporating expert input. This study proposes an AI agent-based IDT framework using an ontological approach, where AI agents are engineered by DT administrators with building operations and maintenance (O&M) data, information, and applications within an ontological DT environment. Data and information generated within this environment are expressed in the DT ontology, enabling AI agents to gain a holistic understanding of the target system. Applications are integrated as a tool, thereby enabling AI agents to expand their actions and gain additional information from results. To validate this framework, virtual in-situ modeling (VIM) and fault detection and diagnosis (FDD) algorithms were implemented as DT applications to demonstrate the operation of the IDT system. Four case studies were conducted to demonstrate IDT-enabled O&M services, and LangSmith was used to visualize the AI agents' reasoning process as part of the result validation. It shows that AI agents have capabilities of performing building O&M tasks with high-level reasoning. The significance of this study lies in demonstrating the feasibility of implementing IDT models in building O&M by enabling AI agents to provide comprehensive, domain-specific knowledge and perform operational tasks, thereby serving as an assistant for both users and operators. Finally, this study underscores the critical role of engineers in managing and maintaining ontology and applications within the DT environment.}
}
@article{SAHBI20243083,
title = {Automatic Ontology Population from Textual Advertisements: LLM vs. Semantic Approach},
journal = {Procedia Computer Science},
volume = {246},
pages = {3083-3092},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.364},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924023925},
author = {Aya Sahbi and Céline Alec and Pierre Beust},
keywords = {Ontology Population, LLM, Textual Advertisement},
abstract = {Automatic ontology population involves identifying, extracting and integrating information from various sources to instantiate the classes and properties of an ontology, thereby building a domain Knowledge Graph (KG). In this paper, we compare two text-based ontology population techniques: KOnPoTe, a semantic approach based on textual and domain knowledge analysis, and a generative AI approach utilizing Claude, a Large Language Model (LLM). We present experiments conducted on two French sales advertisement domains: real estate and boats, and discuss the strengths and limitations of both approaches.}
}
@article{ZHANG2025104362,
title = {A semi-automated compliance checking framework for shield tunnel design integrating ontology and natural language processing},
journal = {Computers in Industry},
volume = {173},
pages = {104362},
year = {2025},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2025.104362},
url = {https://www.sciencedirect.com/science/article/pii/S0166361525001277},
author = {Yuxian Zhang and Xuhua Ren and Jixun Zhang},
keywords = {Ontology, Natural language processing, Shield tunnel, Compliance check},
abstract = {Conventional compliance checking for shield tunnel design models relies on two-dimensional drawings and the designer's subjective interpretation of specifications, which limits the efficiency and introduces potential errors. This study developed a semi-automated framework for shield tunnel design compliance checking using ontology and natural language processing. The adopted methodology establishes a shield tunnel design ontology (STDO) model, which includes six classes of information and relationships that need to be considered in the design phase. A novel method for converting text into a computer-readable format was proposed for design specification content. The design specification text is converted into a word sequence format, including STDO semantics, through word segmentation and semantic alignment. The pattern-matching method converts semantically enriched specification text into a Prolog rule format by extracting grammatical structure elements and transforming logical checking elements. The established design compliance checking framework generates facts through interaction with the building information model and performs compliance reasoning tasks using Prolog rules derived from the specification text. To demonstrate the effectiveness of the conversion method proposed in this study and the designed compliance checking framework, a shield tunnel project was selected for experimental verification. The results showed the following: (1) The proposed method of converting specification text into predicate logic achieved an F1 of 86.25 %, providing a convenient approach for transforming it into a computer-readable format. (2) The established semi-automated framework could provide a convenient solution to assist in conducting model compliance checking tasks according to both quantitative and non-quantitative clauses. The results of this study provide significant guidance for the intelligent design of shield tunnels.}
}
@incollection{TORZA2024,
title = {Language in the Ontology Room},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00237-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041002374},
author = {Alessandro Torza},
keywords = {Ontology, Metaontology, Existence, Quantification, First-order logic, Higher-order logic, Metasemantics, Internalism, Externalism, Ontological pluralism, Ontological realism, Ontological deflationism, Naturalness},
abstract = {The way we answer questions about what there is crucially depends on the language and the logic in which they are framed. This entry introduces the orthodox view on how to carry out such debates, as was formulated by W. V. O. Quine, as well as a number of influential alternatives. A further issue that is explored is whether disagreement about what there is turns on mind-independent features of reality, or it is an artifact of language.}
}
@article{SHU2025107824,
title = {Liability division for ship collision accidents based on ontology model and bayesian network},
journal = {Ocean & Coastal Management},
volume = {269},
pages = {107824},
year = {2025},
issn = {0964-5691},
doi = {https://doi.org/10.1016/j.ocecoaman.2025.107824},
url = {https://www.sciencedirect.com/science/article/pii/S0964569125002868},
author = {Yaqing Shu and Ao Dong and Beiyan Ye and Chengyong Liu and Langxiong Gan and Lan Song},
keywords = {Liability division, Ship collision accidents, Ontology model, Bayesian network, Improved apriori algorithm},
abstract = {With the increase in maritime transport, ship collision accidents have occurred frequently and caused serious impacts on maritime traffic safety and the environment. In this research, a new method combining the ontology model and Bayesian network is proposed to address liability division for ship collision accidents. Firstly, 241 maritime traffic accident reports were collected from the China Maritime Safety Administration (CHINA MSA) between 2018 and 2021. Then, the improved Apriori algorithm is proposed to extract strong association rules and to construct the ship collision negligence ontology based on accident reports. After that, the liability division model is obtained by the ontology mapping Bayesian network and the maximum likelihood estimation method is used for parameter learning for this model. Finally, the proposed method is verified using sample data from the accident reports. The results showed a good capability of liability division for ship collision accidents of the proposed model. This method could serve as a powerful tool for liability division for ship collision accidents in maritime traffic.}
}
@article{LOPES2023110385,
title = {Using terms and informal definitions to classify domain entities into top-level ontology concepts: An approach based on language models},
journal = {Knowledge-Based Systems},
volume = {265},
pages = {110385},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110385},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123001351},
author = {Alcides Lopes and Joel Carbonera and Daniela Schmidt and Luan Garcia and Fabricio Rodrigues and Mara Abel},
keywords = {Ontology learning, Top-level ontology, Language model},
abstract = {The classification of domain entities into top-level ontology concepts remains an activity performed manually by an ontology engineer. Although some works focus on automating this task by applying machine-learning approaches using textual sentences as input, they require the existence of the domain entities in external knowledge resources, such as pre-trained embedding models. In this context, this work proposes an approach that combines the term representing the domain entity and its informal definition into a single text sentence without requiring external knowledge resources. Thus, we use this sentence as the input of a deep neural network that contains a language model as a layer. Also, we present a methodology used to extract two novel datasets from the OntoWordNet ontology based on Dolce-Lite and Dolce-Lite-Plus top-level ontologies. Our experiments show that by using the transformer-based language models, we achieve promising results in classifying domain entities into 82 top-level ontology concepts, with 94% regarding micro F1-score.}
}
@article{CHEN2024123320,
title = {Optimizing automated compliance checking with ontology-enhanced natural language processing: Case in the fire safety domain},
journal = {Journal of Environmental Management},
volume = {371},
pages = {123320},
year = {2024},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2024.123320},
url = {https://www.sciencedirect.com/science/article/pii/S0301479724033061},
author = {Yian Chen and Huixian Jiang},
keywords = {Ontology automatic construction, Automated compliance checking (ACC), Named entity recognition, Relationship extraction, Pre-trained language model, Rule interpretation},
abstract = {The fire safety compliance checking (FSCC) plays a crucial role in ensuring the quality of fire engineering design and eliminating inherent fire hazards. It requires an objective and rational interpretation of fire regulations. However, the texts of fire regulations are filled with numerous rules related to spatial limitations, which pose a significant challenge in interpreting them. The current method of interpreting these rules mostly relies on manual translation, which is not efficient. To address this issue, this study proposes an innovative automated framework for interpreting rules by combining ontology technology with natural language processing (NLP). Through the utilization of pre-trained language models (PLMs), concepts and relationships are extracted from sentences, a domain-specific ontology is established, spatial knowledge is transformed into language-agnostic tree structures based on the ontology, and the semantic components of spatial relationships are extracted. The tree structure is then mapped to logical clauses based on semantic consistency, thereby improving the efficiency of interpretation. Experimental results demonstrate that the architecture achieves an F1 score of 86.27 for entity extraction and 81.81 for spatial relationship joint extraction tasks, with an accuracy of 96.26% in the formalization of logical rules, highlighting its proficiency in automatically interpreting fire spatial rules. This study offers technical support to enhance public understanding of fire safety management and fire prevention predictions, thereby promoting the intelligent management of the building safety environment.}
}
@article{QU2024105478,
title = {GeoFault: A well-founded fault ontology for interoperability in geological modeling},
journal = {Computers & Geosciences},
volume = {182},
pages = {105478},
year = {2024},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2023.105478},
url = {https://www.sciencedirect.com/science/article/pii/S0098300423001826},
author = {Yuanwei Qu and Michel Perrin and Anita Torabi and Mara Abel and Martin Giese},
keywords = {Domain ontology, Fault geological modeling, Artificial intelligence, Semantic technologies},
abstract = {Geological modeling currently uses various computer-based applications. Data harmonization at the semantic level using ontologies is essential to make these applications interoperable. Since geo-modeling is part of several multidisciplinary projects, interoperability requires semantic harmonization to exchange information between geological applications and integrate other domain knowledge at a general level. Therefore, domain ontologies that describe geological knowledge must be based on a sound ontological background to ensure this knowledge is integrable. Faults are essential for understanding and solving structural problems but are complex to model because the concept of fault includes a group of geological entities with a distinct ontological nature. A fault can correspond to thin, deformed rock volumes or spatial arrangements resulting from the displacement of geological blocks, but at a broader scale, geologists describe faults as surfaces or components of complex fault arrays. Our work intends to harmonize these views by presenting a domain ontology, GeoFault, resting on the Basic Formal Ontology (BFO) and the GeoCore ontology. GeoCore and GeoFault support the parametric description of geological sites as a preliminary step for quantitative and qualitative analysis. We have proposed GeoFault after systematically revising the literature and several knowledge-acquisition sessions with expert structural geologists. The ontology formalizes a vocabulary for fault “sensu stricto,” excluding ductile shear deformations. It covers the regional to outcrop scales, excluding structures at the microscopic, orogenic, and tectonics scales, and it avoids interpretive language associated with geological processes as far as possible. Extending the BFO and GeoCore ontologies allows the fault concept to be related to formal ontological classes in a consistent semantic-rich framework. The ontology artifact is implemented in OWL 2, validated by competency questions with two use cases, and tested using an in-house ontology-driven data entry application. The GeoFault ontology is publicly available and provides a solid framework for clarifying fault knowledge and a foundation for many applications.}
}
@article{HONG2024105800,
title = {Graph-based intelligent accident hazard ontology using natural language processing for tracking, prediction, and learning},
journal = {Automation in Construction},
volume = {168},
pages = {105800},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105800},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524005363},
author = {Eunbin Hong and SeungYeon Lee and Hayoung Kim and JeongEun Park and Myoung Bae Seo and June-Seong Yi},
keywords = {Natural language processing (NLP), Relation extraction, Network analysis, Weighted graph database, Knowledge modeling, Ontology of intelligence, Hazard analysis, Safety risk, Construction safety management},
abstract = {This paper addresses the challenge of dispersed accident-related information on construction sites, which hinders consensus among employers, workers, supervisors, and society. A robust NLP-based framework is presented to analyze and structure accident-related textual data into a comprehensive knowledge base that reveals accident patterns and risk information. Accident scenarios, including frequency and severity scores, are structured into a graph database through knowledge modeling, establishing an ontology to elucidate keyword relationships. Network analysis identifies accident patterns, quantifies scenario likelihood and severity, and predicts criticality, forming an accident hazard ontology. This vectorized ontology supports accident tracking, prediction, and learning with potential applications. The framework ensures reliable data integration, real-time hazard assessment, and proactive safety measures.}
}
@article{DEMURO20241,
title = {Artificial intelligence and the ethnographic encounter: Transhuman language ontologies, or what it means “to write like a human, think like a machine”},
journal = {Language & Communication},
volume = {96},
pages = {1-12},
year = {2024},
issn = {0271-5309},
doi = {https://doi.org/10.1016/j.langcom.2024.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0271530924000119},
author = {Eugenia Demuro and Laura Gurney},
keywords = {Artificial intelligence, Language ontologies, Ethnographic encounter, Transhumanism, Posthumanism},
abstract = {In this paper, we employ the language ontologies framework to artificial intelligence (specifically, OpenAI's ChatGPT) to investigate the ‘ethnographic encounter’ between human and non-human language users. Our focus is on the exchange and interplay between human language users and non-human artificial language generators in the production of written text. We analyse how such programs transform our understanding of what language is or might be; their practices to create language are unfamiliar, and yet they make sense to human interlocutors. Drawing from, and building on, the language ontologies framework, we discuss the practices involved in such encounters and suggest the need for an updated ‘toolkit’ in our understanding of language to account for transhuman interactions.}
}
@article{CIATTO2025112940,
title = {Large language models as oracles for instantiating ontologies with domain-specific knowledge},
journal = {Knowledge-Based Systems},
volume = {310},
pages = {112940},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112940},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124015740},
author = {Giovanni Ciatto and Andrea Agiollo and Matteo Magnini and Andrea Omicini},
keywords = {Ontology population, Large language models, Nutrition, Automation, Domain-specific knowledge},
abstract = {Background:
Endowing intelligent systems with semantic data commonly requires designing and instantiating ontologies with domain-specific knowledge. Especially in the early phases, those activities are typically performed manually by human experts possibly leveraging on their own experience. The resulting process is therefore time-consuming, error-prone, and often biased by the personal background of the ontology designer.
Objective:
To mitigate that issue, we propose a novel domain-independent approach to automatically instantiate ontologies with domain-specific knowledge, by leveraging on large language models (LLMs) as oracles.
Methods:
Starting from (i) an initial schema composed by inter-related classes and properties and (ii) a set of query templates, our method queries the LLM multiple times, and generates instances for both classes and properties from its replies. Thus, the ontology is automatically filled with domain-specific knowledge, compliant to the initial schema. As a result, the ontology is quickly and automatically enriched with manifold instances, which experts may consider to keep, adjust, discard, or complement according to their own needs and expertise.
Contribution:
We formalise our method in general way and instantiate it over various LLMs, as well as on a concrete case study. We report experiments rooted in the nutritional domain where an ontology of food meals and their ingredients is automatically instantiated from scratch, starting from a categorisation of meals and their relationships. There, we analyse the quality of the generated ontologies and compare ontologies attained by exploiting different LLMs. Experimentally, our approach achieves a quality metric that is up to five times higher than the state-of-the-art, while reducing erroneous entities and relations by up to ten times. Finally, we provide a SWOT analysis of the proposed method.}
}
@article{ATENCIO2024312,
title = {Enterprise architecture approach for project-based organizations modeling, design, and analysis: An ontology-driven tool proposal},
journal = {Alexandria Engineering Journal},
volume = {98},
pages = {312-327},
year = {2024},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2024.04.052},
url = {https://www.sciencedirect.com/science/article/pii/S1110016824004356},
author = {Edison Atencio and Mauro Mancini and Guillermo Bustos},
keywords = {Enterprise architecture, Project management, Ontologies, Semantic web, Systems thinking, Ontology mapping},
abstract = {Project-based organizations (PBOs) rely on project management as their core business process. These organizations are characterized by their disaggregated structure and diverse authority distribution, leading to complications in synchronizing projects, governance, and functional divisions. In this regard, there is a need of a tool to manage this complexity. This paper introduces a novel approach – the IModel- for modeling, designing and analyzing PBOs, following the Design Science Research Method, that seeks to is creating an artifact that solves a real-world problem in a specific context. This artifact is developed through a conceptual integration between project management (PM) and enterprise architecture (EA) sources based on ontologies. Ontologies are powerful, machine-readable graph-structured models which enables sophisticated models’ reasoning and querying. IModel fuses the organizational domain from the ArchiMate EA language with PM insights from PMBOK 7th edition. The resulting IModel has been submitted to an expert’s judgement to assess its intended purposes as well, its potential to be applied in the real-world to support the PBO.}
}
@article{SONG2024114983,
title = {Ontology-assisted GPT-based building performance simulation and assessment: Implementation of multizone airflow simulation},
journal = {Energy and Buildings},
volume = {325},
pages = {114983},
year = {2024},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.114983},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824010995},
author = {Jihwan Song and Sungmin Yoon},
keywords = {GPT, ChatGPT, Large language model (LLM), Building performance simulation (BPS), Building performance, Digital twins, Artificial intelligence, CONTAM},
abstract = {Building performance simulation (BPS) is crucial for building performance assessments across its lifecycle. However, the complexity of buildings and the iterative nature of simulation poses challenges, leading to high costs and low values. Previous studies focused on simplification, but did not fully utilize advanced simulation engines. Despite recent advancements, there is a lack of research on leveraging artificial intelligence (AI), specifically generative pre-trained transformer (GPT), for BPS. Therefore, this study proposes a GPT-based BPS system, enhancing simulation efficiency and value by integrating simulation engines and advanced data analytics in the GPT environment. The ontology for GPT-based BPS is also developed to enable comprehensive, reliable, informative BPS environments. Based on this framework, case studies were conducted for GPT-based multizone airflow network simulation in a high-rise residential building using CONTAM software. They demonstrate GPT’s capabilities in retrieving simulation data, visualizing results with data mining, answering questions based on building knowledge, checking compliance with design guidelines, and proposing design alternatives. Finally, this study emphasizes expert interventions with ontological engineering informatics to utilize strictly structured BPS engines.}
}
@article{ZHANG2025103468,
title = {A knowledge graph-enhanced large language model for question answering of hydraulic structure safety management},
journal = {Advanced Engineering Informatics},
volume = {66},
pages = {103468},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103468},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625003611},
author = {Dongliang Zhang and Gang Ma and Tongming Qu and Xudong Wang and Wei Zhou and Xiaomao Wang},
keywords = {Hydraulic structure safety management, Domain knowledge QA, Intent parsing, Knowledge graph, Large language model},
abstract = {Early detection and mitigation of hazards in hydraulic structures are crucial for effectively reducing economic and life losses. However, traditional hydraulic structure safety management methods rely on error-prone individual experience and emergency manuals, which are insufficient for making timely, scientifically informed decisions during crises. To address this challenge, this study presents an AI-driven framework for hydraulic structure safety management based on knowledge-based question answering. First, an ontology model was developed through a detailed analysis of safety management texts. Next, a partition fusion Kolmogorov-arnold network (PFKAN) enhanced with attention mechanisms was designed to jointly extract entities and relational knowledge. A safety management knowledge graph (KG) was then constructed from this knowledge. Subsequently, a large language model (LLM) was employed with a voting strategy to interpret query intent and extract relevant domain-specific knowledge from the KG. Finally, domain knowledge was integrated into the LLM to generate professional responses. Experimental results show that the F1 scores for entity and relation extraction with PFKAN reached 0.91 and 0.90, respectively, and the F1 score for query intent parsing with the voting strategy was 0.95, demonstrating competitive performance. The KG-enhanced LLM significantly improves decision-making in hydraulic structure safety management, providing an accurate and scalable tool for engineering safety managers.}
}
@article{RYS2024100720,
title = {Model management to support systems engineering workflows using ontology-based knowledge graphs},
journal = {Journal of Industrial Information Integration},
volume = {42},
pages = {100720},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100720},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24001638},
author = {Arkadiusz Ryś and Lucas Lima and Joeri Exelmans and Dennis Janssens and Hans Vangheluwe},
keywords = {Model management, Ontology, Process modelling, Knowledge graph},
abstract = {System engineering has been shifting from document-centric to model-based approaches, where assets are becoming more and more digital. Although digitisation conveys several benefits, it also brings several concerns (e.g., storage and access) and opportunities. In the context of Cyber-Physical Systems (CPS), we have experts from various domains executing complex workflows and manipulating models in a plethora of different formalisms, each with their own methods, techniques and tools. Storing knowledge on these workflows can reduce considerable effort during system development not only to allow their repeatability and replicability but also to access and reason on data generated by their execution. In this work, we propose a framework to manage modelling artefacts generated from workflow executions. The basic workflow concepts, related formalisms and artefacts are formally defined in an ontology specified in OML (Ontology Modelling Language). This ontology enables the construction of a knowledge graph that contains system engineering data to which we can apply reasoning. We also developed several tools to support system engineering during the design of workflows, their enactment, and artefact storage, considering versioning, querying and reasoning on the stored data. These tools also hide the complexity of manipulating the knowledge graph directly. Finally, we have applied our proposed framework in a real-world system development scenario of a drivetrain smart sensor system. Results show that our proposal not only helped the system engineer with fundamental difficulties like storage and versioning but also reduced the time needed to access relevant information and new knowledge that can be inferred from the knowledge graph.}
}
@article{SALES2023102210,
title = {A FAIR catalog of ontology-driven conceptual models},
journal = {Data & Knowledge Engineering},
volume = {147},
pages = {102210},
year = {2023},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2023.102210},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X23000708},
author = {Tiago Prince Sales and Pedro Paulo F. Barcelos and Claudenir M. Fonseca and Isadora Valle Souza and Elena Romanenko and César Henrique Bernabé and Luiz Olavo {Bonino da Silva Santos} and Mattia Fumagalli and Joshua Kritz and João Paulo A. Almeida and Giancarlo Guizzardi},
keywords = {Ontology-driven conceptual modeling, OntoUML, Unified Foundational Ontology, Model catalog, FAIR, Linked data},
abstract = {Multi-domain model catalogs serve as empirical sources of knowledge and insights about specific domains, about the use of a modeling language’s constructs, as well as about the patterns and anti-patterns recurrent in the models of that language crosscutting different domains. They may support domain and language learning, model reuse, knowledge discovery for humans, and reliable automated processing and analysis if built following generally accepted quality requirements for scientific data management. More specifically, not unlike scientific (meta)data, models should be shared according to the FAIR principles (Findability, Accessibility, Interoperability, and Reusability). In this paper, we report on the construction of a FAIR model catalog for Ontology-Driven Conceptual Modeling research, a trending paradigm lying at the intersection of conceptual modeling and ontology engineering in which the Unified Foundational Ontology (UFO) and OntoUML emerged among the most adopted technologies. The catalog, publicly available at https://w3id.org/ontouml-models, currently includes over one hundred and forty models, developed in a variety of contexts and domains.}
}
@article{ALDANAMARTIN2025103356,
title = {Advancing soil biology research: Empowering European databases with ontological frameworks for enhanced data integration of soil biodiversity data},
journal = {Ecological Informatics},
volume = {91},
pages = {103356},
year = {2025},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2025.103356},
url = {https://www.sciencedirect.com/science/article/pii/S1574954125003656},
author = {José F. Aldana-Martín and David J. Russell and Carlos A. Martínez-Muñoz and Christine Driller and Stephan Lesch and Ismael Navas-Delgado},
keywords = {Ontology, Semantic Web, Biodiversity, Taxonomy maintenance},
abstract = {Recognizing soil biodiversity’s critical role in soil quality and health has gained prominence in environmental policy and research. There is a pressing need to integrate taxonomic data with functional traits to understand the functional significance of soil biodiversity and its distribution across various environmental contexts. This long-term goal can only be achieved after comprehensive taxonomy ontologies are in place. Ontologies are a powerful tool to facilitate database interoperability, ensuring a seamless connection between diverse datasets. Adopting ontologies aligns with the FAIR principles, enhancing data discoverability, accessibility, and machine-readability. In biology, ontologies offer a robust framework for formalizing complex relationships between taxa, traits, and environments. Repositories like the OBO Foundry and NCBO BioPortal further promote the integration of controlled bioscientific vocabularies. However, careful selection of vocabulary is essential to ensure effective interoperability among ontologies, especially when dealing with closely related taxa. While databases like Edaphobase provide comprehensive taxonomic information for soil invertebrate animals, they lack specific ontologies for the underlying taxonomic structure. This research addresses this gap by proposing the EUdaphobase Taxonomy Ontology (EUTaxO) tailored to soil biology taxonomy. As Edaphobase is continuously updated to accommodate changes in taxonomic classifications, the related EUTaxO will require maintenance. This work presents an automated pipeline to synchronize the proposed ontology with Edaphobase’s classification. The integration of observational databases, such as Edaphobase, with domain-specific trait databases will enable the aggregation of species into functional or ecological groups based on traits. This integration, primarily reliant on taxonomic characteristics, will be critical in evaluating the spatio-temporal distribution of functional soil biodiversity across diverse habitats, soil types, climate zones, and land-use patterns.}
}
@article{GUIZZARDI2024102325,
title = {Explanation, semantics, and ontology},
journal = {Data & Knowledge Engineering},
volume = {153},
pages = {102325},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102325},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000491},
author = {Giancarlo Guizzardi and Nicola Guarino},
keywords = {Real-world semantics, Ontology, Explanation, Ontological unpacking, Semantic interoperability},
abstract = {The terms ‘semantics’ and ‘ontology’ are increasingly appearing together with ‘explanation’, not only in the scientific literature, but also in everyday social interactions, in particular, within organizations. Ontologies have been shown to play a key role in supporting the semantic interoperability of data and knowledge representation structures used by information systems. With the proliferation of applications of Artificial Intelligence (AI) in different settings and the increasing need to guarantee their explainability (but also their interoperability) in critical contexts, the term ‘explanation’ has also become part of the scientific and technical jargon of modern information systems engineering. However, all of these terms are also significantly overloaded. In this paper, we address several interpretations of these notions, with an emphasis on their strong connection. Specifically, we discuss a notion of explanation termed ontological unpacking, which aims at explaining symbolic domain descriptions (e.g., conceptual models, knowledge graphs, logical specifications) by revealing their ontological commitment in terms of their so-called truthmakers, i.e., the entities in one’s ontology that are responsible for the truth of a description. To illustrate this methodology, we employ an ontological theory of relations to explain a symbolic model encoded in the de facto standard modeling language UML. We also discuss the essential role played by ontology-driven conceptual models (resulting from this form of explanation processes) in supporting semantic interoperability tasks. Furthermore, we revisit a proposal for quality criteria for explanations from philosophy of science to assess our approach. Finally, we discuss the relation between ontological unpacking and other forms of explanation in philosophy and science, as well as in the subarea of Artificial Intelligence known as Explainable AI (XAI).}
}
@article{AFFINITO2025101726,
title = {Towards a unified ontology for monitoring ecosystem services},
journal = {Ecosystem Services},
volume = {73},
pages = {101726},
year = {2025},
issn = {2212-0416},
doi = {https://doi.org/10.1016/j.ecoser.2025.101726},
url = {https://www.sciencedirect.com/science/article/pii/S2212041625000300},
author = {F. Affinito and J.M. Holzer and M.-J. Fortin and A. Gonzalez},
keywords = {Ecosystem services monitoring, Interoperability, Ontology, Semantics, Ecosystem service conceptualisation},
abstract = {Ecosystem services (ES) are an important part of global and national environmental policies. In this context, there is a call for the monitoring of ES to support their management. However, the proliferation of terms used within ES science is a barrier to standardised monitoring. Monitoring ES requires knowing exactly what variables to measure and how they relate to change in the states of ES. It further requires interoperability between methodologies used by information systems to operationalise data flows. Here, we aim to systematise the language used to define ES and the terminology used in their monitoring by developing an ontology for ES monitoring. Ontologies are tools that operationalise concepts and the relationships among terms used to define them. An ontology allows people and machines to use terms consistently. Building on advances in other disciplines, the ES monitoring ontology systematises the language of ES across major conceptual frameworks advancing conceptual clarity and operationalisation of ES. We test the ES monitoring ontology with data from three ES in British Columbia, Canada, to highlight how it can enable information sharing and monitoring. We show that the ontology can organise and retrieve information and data for ES monitoring in a systematic way. Our work contributes to advancing interoperability of ES, taking a step towards systematically understanding ES change with automated tools. We invite members of the ES community to join the effort of developing this ontology for ES so that can it contribute to the challenge of systematically monitoring change in social-ecological systems.}
}
@article{RIQUELMEGARCIA20252155,
title = {Annotation of biological samples data to standard ontologies with support from large language models},
journal = {Computational and Structural Biotechnology Journal},
volume = {27},
pages = {2155-2167},
year = {2025},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2025.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S2001037025001837},
author = {Andrea Riquelme-García and Juan Mulero-Hernández and Jesualdo Tomás Fernández-Breis},
keywords = {Bioinformatics, Generative AI, Large language models, Data interoperability, Biological samples},
abstract = {The semantic integration of biological data is hindered by the vast heterogeneity of data sources and their limited semantic formalization. A crucial step in this process is mapping data elements to ontological concepts, which typically involves substantial manual effort. Large Language Models (LLMs) have demonstrated potential in automating complex language-related tasks and may offer a solution to streamline biological data annotation. This study investigates the utility of LLMs—specifically various base and fine-tuned GPT models—for the automatic assignment of ontological identifiers to biological sample labels. We evaluated model performance in annotating labels to four widely used ontologies: the Cell Line Ontology (CLO), Cell Ontology (CL), Uber-anatomy Ontology (UBERON), and BRENDA Tissue Ontology (BTO). Our dataset was compiled from publicly available, high-quality databases containing biologically relevant sequence information, which suffers from inconsistent annotation practices, complicating integrative analyses. Model outputs were compared against annotations generated by text2term, a state-of-the-art annotation tool. The fine-tuned GPT model outperformed both the base models and text2term in annotating cell lines and cell types, particularly for the CL and UBERON ontologies, achieving a precision of 47–64% and a recall of 88–97%. In contrast, base models exhibited significantly lower performance. These results suggest that fine-tuned LLMs can accelerate and improve the accuracy of biological data annotation. Nonetheless, our evaluation highlights persistent challenges, including variable precision across ontology categories and the continued need for expert curation to ensure annotation validity.}
}
@article{VIJAYA2025100262,
title = {ESGOnt: An ontology-based framework for Enhancing Environmental, Social, and Governance (ESG) assessments and aligning with Sustainable Development Goals (SDG)},
journal = {Resources, Environment and Sustainability},
volume = {22},
pages = {100262},
year = {2025},
issn = {2666-9161},
doi = {https://doi.org/10.1016/j.resenv.2025.100262},
url = {https://www.sciencedirect.com/science/article/pii/S266691612500074X},
author = {Annas Vijaya and Faris Dzaudan Qadri and Linda Salma Angreani and Hendro Wicaksono},
keywords = {ESG performance, Sustainable development goals, Ontology-based framework, Interoperability, ESG maturity evaluation model},
abstract = {This study proposes ESGOnt, an ontology-based framework that aligns Environmental, Social, and Governance (ESG) management with Sustainable Development Goals (SDGs). ESGOnt addresses key challenges in sustainable resource governance systems and cross-sector interoperability by providing a unified structure for ESG and SDG integration. The framework was developed through a systematic methodology that combines a literature review, standardization of ESG and SDG relationships, development of an adaptable maturity model, and ontology implementation using established methods such as Methontology and NeOn. ESGOnt enables the integration of diverse ESG taxonomies and ESG reporting standards, including GRI and ESRS, and assists companies in their ESG performance evaluation. Empirical validation through real-world use cases demonstrates its capability to (1) direct assessment of ESG assessments with specific SDG targets, such as SDG13 (Climate Action) and SDG 12 (Responsible Consumption and Production), (2) assess organizational ESG progress through different metrics, (3) facilitation of standardized and interoperable reporting for small and large enterprises, and (4) automatically validate organization compliance with EU Non-Financial Reporting Directive regulations. The findings show that ESGOnt resolves data inconsistency and transparency issues by enabling integrated and auditable sustainability reporting. The ontology-driven approach of the framework enables scalable and policy-relevant tools for tracking environmental and social impacts, while its maturity model focuses on strategic improvements in resource efficiency. Future studies will analyze and extend ESGOnt’s functionality for sector-specific capabilities, such as bioeconomy control systems, and explore advanced AI-driven inspection methods for real-time ESG-SDG assessment.}
}
@article{ANTONIOU2024102323,
title = {Semantic requirements construction using ontologies and boilerplates},
journal = {Data & Knowledge Engineering},
volume = {152},
pages = {102323},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102323},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000478},
author = {Christina Antoniou and Kalliopi Kravari and Nick Bassiliades},
keywords = {Requirement boilerplates, Requirements, Ontology, Requirement specification},
abstract = {This paper presents a combination of an ontology and boilerplates, which are requirements templates for the syntactic structure of individual requirements that try to alleviate the problem of ambiguity caused using natural language and make it easier for inexperienced engineers to create requirements. However, still the use of boilerplates restricts the use of natural language only syntactically and not semantically. Boilerplates consists of fixed and attributes elements. Using ontologies, restricts the vocabulary of the words used in the requirements boilerplates to entities, their properties and entity relationships that are semantically meaningful to the application domain, leading thus to fewer errors. In this work we combine the advantages of boilerplates and ontologies. Usually, the attributes of boilerplates are completed with the help of the ontology. The contribution of this paper is that the whole boilerplates are stored in the ontology, based on the fact that RDF triples have similar syntax to the boilerplate syntax, so that attributes and fixed elements are part of the ontology. This combination helps to construct semantically and syntactically correct requirements. The contribution and novelty of our method is that we exploit the natural language syntax of boilerplates mapping them to Resource Description Framework triples which have also a linguistic nature. In this paper we created and present the development of a domain-specific ontology as well as a minimal set of boilerplates for a specific application domain, namely that of engineering software for an ATM, while maintaining flexibility on the one hand and generality on the other.}
}
@article{WHALEY2025111921,
title = {GRADE concept paper 9: rationale and process for creating a GRADE Ontology},
journal = {Journal of Clinical Epidemiology},
volume = {187},
pages = {111921},
year = {2025},
issn = {0895-4356},
doi = {https://doi.org/10.1016/j.jclinepi.2025.111921},
url = {https://www.sciencedirect.com/science/article/pii/S0895435625002549},
author = {Paul Whaley and Brian Alper and Joanne Dehnbostel and Carlos Alva-Diaz and Stavros Antoniou and Antonio Bognanni and Javier Bracchiglione and Therese Kristine Dalsbø and Sean Grant and Jennifer Hunter and Alfonso Iorio and Malgorzata Lagisz and Harold Lehmann and Sheyu Li and Joerg Meerpohl and Saphia Mokrane and Cauê Monaco and Ignacio Neumann and Kevin Pottie and Shahab Sayfi and Nigar Sekercioglu and Jasvinder Singh and Bernardo Sousa-Pinto and Janice Tufte and Lenny Thinagaran Vasanthan and Li Wang and Jun Xia and Xiaomei Yao and Holger Schünemann},
keywords = {GRADE approach, Certainty of evidence, Evidence-to-decisions, Ontology, Data standards},
abstract = {Context
As the rate of research production accelerates, the ability to efficiently and unambiguously communicate judgments relating to the synthesis, evaluation, and use of scientific information becomes paramount.
Perspective
Scientific information can be viewed as a “layered infrastructure” of data, evidence, knowledge, and use. The GRADE approach serves as a de facto data standard for this infrastructure, supporting movement between layers by reducing ambiguity in claims to knowledge (in the form of judgements of certainty in the evidence when answering research questions) and level of commitment to possible solutions to problems (in the form of strength of recommendations for interventions).
Purpose
This GRADE concept paper outlines the structure, purpose, and potential benefits of the GRADE Ontology for (a) the creators of, educators in, and users of systematic reviews, health guidelines, and health technology assessments, and (b) the development of tools that help with conducting, finding, and summarising the same. This paper also presents the processes for the development and maintenance of the GRADE Ontology, a formalised terminology standard within GRADE that will support the efficiency, rigour, consistency, and interoperability of GRADE's use.
Plain Language Summary
The rate of research production is increasing exponentially. It is therefore becoming increasingly important to quickly, efficiently, and unambiguously communicate the judgments made and processes used when doing research and using evidence to inform policy decisions. GRADE is a widely used approach to assessing certainty of evidence when answering research questions and making recommendations for health interventions, designed to help with the efficient and transparent evaluation and use of evidence. However, the absence of a formalized terminology standard within GRADE limits the efficiency with which the results of its use can be communicated. In response, the GRADE Ontology is being created. This concept paper outlines what an ontology is, how it helps with communicating scientific information, the specific benefits of the GRADE Ontology, and the processes for developing and maintaining a useful, valid ontology that supports the use of the GRADE approach.}
}
@article{TABOADA2025103254,
title = {Ontology matching with Large Language Models and prioritized depth-first search},
journal = {Information Fusion},
volume = {123},
pages = {103254},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103254},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525003276},
author = {Maria Taboada and Diego Martinez and Mohammed Arideh and Rosa Mosquera},
keywords = {Ontology matching, Retrieval augmented generation, Greedy search, Large Language Models, Zero-shot setting},
abstract = {Ontology matching (OM) plays a key role in enabling data interoperability and knowledge sharing. Recently, methods based on Large Language Model (LLMs) have shown great promise in OM, particularly through the use of a retrieve-then-prompt pipeline. In this approach, relevant target entities are first retrieved and then used to prompt the LLM to predict the final matches. Despite their potential, these systems still present limited performance and high computational overhead. To address these issues, we introduce MILA, a novel approach that embeds a retrieve-identify-prompt pipeline within a prioritized depth-first search (PDFS) strategy. This approach efficiently identifies a large number of semantic correspondences with high accuracy, limiting LLM requests to only the most borderline cases. We evaluated MILA using three challenges from the 2024 edition of the Ontology Alignment Evaluation Initiative. Our method achieved the highest F-Measure in five of seven unsupervised tasks, outperforming state-of-the-art OM systems by up to 17%. It also performed better than or comparable to the leading supervised OM systems. MILA further exhibited task-agnostic performance, remaining stable across all tasks and settings, while significantly reducing runtime. These findings highlight that high-performance LLM-based OM can be achieved through a combination of programmed (PDFS), learned (embedding vectors), and prompting-based heuristics, without the need of domain-specific heuristics or fine-tuning.}
}
@article{ALBAYRAK2025100409,
title = {Enhancing human phenotype ontology term extraction through synthetic case reports and embedding-based retrieval: A novel approach for improved biomedical data annotation},
journal = {Journal of Pathology Informatics},
volume = {16},
pages = {100409},
year = {2025},
issn = {2153-3539},
doi = {https://doi.org/10.1016/j.jpi.2024.100409},
url = {https://www.sciencedirect.com/science/article/pii/S2153353924000488},
author = {Abdulkadir Albayrak and Yao Xiao and Piyush Mukherjee and Sarah S. Barnett and Cherisse A. Marcou and Steven N. Hart},
keywords = {Human phenotype ontology, PhenoTagger, Vector embeddings},
abstract = {With the increasing utilization of exome and genome sequencing in clinical and research genetics, accurate and automated extraction of human phenotype ontology (HPO) terms from clinical texts has become imperative. Traditional methods for HPO term extraction, such as PhenoTagger, often face limitations in coverage and precision. In this study, we propose a novel approach that leverages large language models (LLMs) to generate synthetic sentences with clinical context, which were semantically encoded into vector embeddings. These embeddings are linked to HPO terms, creating a robust knowledgebase that facilitates precise information retrieval. Our method circumvents the known issue of LLM hallucinations by storing and querying these embeddings within a true database, ensuring accurate context matching without the need for a predictive model. We evaluated the performance of three different embedding models, all of which demonstrated substantial improvements over PhenoTagger. Top recall (sensitivity), precision (positive-predictive value, PPV), and F1 are 0.64, 0.64, and 0.64, respectively, which were 31%, 10%, and 21% better than PhenoTagger. Furthermore, optimal performance was achieved when we combined the best performing embedding model with PhenoTagger (a.k.a. Fused model), resulting in recall (sensitivity), precision (PPV), and F1 values of 0.7, 0.7, and 0.7, respectively, which are 10%, 10%, and 10% better than the best embedding models. Our findings underscore the potential of this integrated approach to enhance the precision and reliability of HPO term extraction, offering a scalable and effective solution for biomedical data annotation.}
}
@article{MUNOZCADIZ2025e00431,
title = {A methodology for integrating the CIDOC-CRMba ontology into the IFC schema to support spatial analysis in archaeological heritage},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {37},
pages = {e00431},
year = {2025},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2025.e00431},
url = {https://www.sciencedirect.com/science/article/pii/S2212054825000335},
author = {Jesús Muñoz-Cádiz and Chiara Mariotti and Romina Nespeca and Letizia Bolognese},
keywords = {Interoperability, IFC schema, HBIM, Spatial analysis, Ontology},
abstract = {The paper introduces a methodology for mapping the CIDOC-CRMba ontology into the Industry Foundation Classes (IFC) schema to enhance building reuse. The methodology leverages semantic enrichment of three-dimensional (3D) reality-based objects, incorporating CIDOC-CRM relationships for advanced metadata. The proposed approach has been tested on the Roman Theater of Hadrianopolis in Sofratikë (Albania). The results demonstrate increased interoperability, semantic robustness, and scalability in Heritage Building Information Modeling (HBIM) workflows when applying openBIM standards. Furthermore, the obtained IFC data library A2Heritage bridges the gap between object-based schemas and ontology-driven approaches, providing a standardized framework for managing cultural heritage (CH) data across disciplines.}
}
@article{SHAN2025103655,
title = {Large language Models-empowered automatic knowledge graph development based on multi-modal data for building health resilience},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103655},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103655},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625005488},
author = {Tianlong Shan and Fan Zhang and Albert P.C. Chan and Shiyao Zhu and Kaijian Li},
keywords = {Building health resilience, Knowledge graph, Large language models, Multi-modal data, Rainstorm},
abstract = {Improving the health resilience of building (BHR) helps keep stable health status of both the building and its occupants under disasters. As BHR is an emerging concept, there is no structured knowledge graph to understand the whole process of BHR under disasters. Therefore, this study aims to build a structured BHR knowledge graph based on multi-modal data, providing sufficient structured knowledge for BHR enhancement. An automated knowledge graph construction approach is proposed to empower the ontology design and triple extraction by large language models (LLMs), and validation processes based on In-context Learning (ICL) prompts. A case study is conducted to construct the knowledge graph of BHR under rainstorms in Hong Kong. The performance of the proposed LLMs-empowered knowledge extraction is also validated based on natural language processing metrics and LLMs-based Evaluation (LLMs-Eval). BHR knowledge graph indicates the potential relations between disasters, factors, response actions, and the health status of the building and occupants, and provides insight to guide the BHR enhancement. The superiority of the proposed LLMs-empowered automated knowledge graph construction approach is proven, implying LLMs have great potential in knowledge graph construction, not only for BHR but also for other concepts that require structured knowledge for further explorations and analyses.}
}
@article{HWANG2025106338,
title = {Automated inference of context-specific hazards in construction using BIM and Ontology},
journal = {Automation in Construction},
volume = {177},
pages = {106338},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106338},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525003784},
author = {Seongyeon Hwang and Seoyoung Jung and Seulki Lee},
keywords = {Building information modeling, Ontology, Hazard identification, Construction accidents},
abstract = {To address the high rate of workplace accidents in the construction industry, this paper proposed an automated hazard identification process using building information modeling (BIM) and ontology. In South Korea, legislation mandates risk assessment and safety documentation to prevent construction accidents by identifying potential hazards. Current methods rely on the experience of personnel, which limits hazard recognition. The proposed approach leverages BIM to automatically infer construction methods, tasks, tools, and materials, identifying related hazards and mitigation measures through ontology. Validation experiments focused on waterproofing work revealed alignment between inferred risks and expected outcomes. By comparing the ontology-derived risk factors with those identified by safety managers, this study confirmed the consistency and adequacy of the ontology. The method improves accuracy, efficiency, and consistency in hazard identification in various construction projects.}
}
@article{HEIMANN2025101349,
title = {Circling the void: Using Heidegger and Lacan to think about large language models},
journal = {Cognitive Systems Research},
volume = {91},
pages = {101349},
year = {2025},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2025.101349},
url = {https://www.sciencedirect.com/science/article/pii/S1389041725000294},
author = {Marc Heimann and Anne-Friederike Hübener},
keywords = {AI, LLM, Continental philosophy, Psychoanalytic language theory, Heidegger, Lacan},
abstract = {The essay aims to unite two currently distinct lines of thinking and working with language. Large Language Models and continental philosophy, especially Martin Heidegger’s thinking about language and, building upon Sigmund Freud, Jacques Lacan’s structural psychoanalysis. We show that the concept of language that Heidegger, Freud and Lacan discuss and utilize in clinical frameworks is matched quite strongly by modern LLMs. This allows us to discuss a problem of negation and negativity that is central to the continental discourse but missing in current LLM research. This also means that we offer a radically different approach than is usual in the philosophy of artificial intelligence, since we base our concepts on thinkers that are often disregarded in the analytic philosophy discourse that is closer linked to AI research. To this end we also highlight, where the ontological differences of the proposed approach lie. However, our aim is to address AI researcher and continental philosophers.}
}
@article{BONATTI2025104402,
title = {Enhancing cooperativity in controlled query evaluation over ontologies},
journal = {Artificial Intelligence},
volume = {348},
pages = {104402},
year = {2025},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2025.104402},
url = {https://www.sciencedirect.com/science/article/pii/S0004370225001213},
author = {Piero Bonatti and Gianluca Cima and Domenico Lembo and Francesco Magliocca and Lorenzo Marconi and Riccardo Rosati and Luigi Sauro and Domenico Fabio Savo},
keywords = {Description logics, Ontologies, Confidentiality preservation, Query answering, Data complexity},
abstract = {Controlled Query Evaluation (CQE) is a methodology designed to maintain confidentiality by either rejecting specific queries or adjusting responses to safeguard sensitive information. In this investigation, our focus centers on CQE within Description Logic ontologies, aiming to ensure that queries are answered truthfully as long as possible before resorting to deceptive responses, a cooperativity property which is called the “longest honeymoon”. Our work introduces new semantics for CQE, denoted as MC-CQE, which enjoys the longest honeymoon property and outperforms previous methodologies in terms of cooperativity. We study the complexity of query answering in this new framework for ontologies expressed in the Description Logic DL-LiteR. Specifically, we establish data complexity results under different maximally cooperative semantics and for different classes of queries. Our results identify both tractable and intractable cases. In particular, we show that the evaluation of Boolean unions of conjunctive queries is the same under all the above semantics and its data complexity is in Image 1. This result makes query answering amenable to SQL query rewriting. However, this favorable property does not extend to open queries, even with a restricted query language limited to conjunctions of atoms. While, in general, answering open queries in the MC-CQE framework is intractable, we identify a sub-family of semantics under which answering full conjunctive queries is tractable.}
}
@article{WANG2024105293,
title = {Enhancement and validation of ifcOWL ontology based on Shapes Constraint Language (SHACL)},
journal = {Automation in Construction},
volume = {160},
pages = {105293},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105293},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524000293},
author = {Chaoyue Wang and Liang Zhang and Wei Yan},
keywords = {Industry foundation classes (IFC), Shapes constraint language (SHACL), IFC-OWL conversion, Web ontology language (OWL), EXPRESS rules},
abstract = {In the domain of Building Information Modeling (BIM), although ifcOWL ontology has already achieved a fundamental conversion from the Industry Foundation Classes (IFC) schema to the Web Ontology Language (OWL) format, deeper research is still needed to address complex attributes and constraints in the IFC schema. This paper aims to achieve the conversion and validation of WHERE rules used for defining data integrity constraints in the IFC schema. A method is introduced, which entails converting WHERE rules into SPARQL-based Shapes Constraint Language (SHACL) constraints. The application of SHACL constraints for validating the ifcOWL instance graph enables the effective identification and resolution of data errors and inconsistencies related to WHERE rules. The implementation of comprehensive validation and quality control mechanisms guarantees the reliability and accuracy of ifcOWL instance graphs. Furthermore, high-quality data supports subsequent research related to ifcOWL, such as quality control and cross-domain interoperability.}
}
@article{GUEDDES2025114332,
title = {Semantically enhanced community detection in social networks: Integrating BERT with a comprehensive ontology and SWRL rules},
journal = {Knowledge-Based Systems},
volume = {329},
pages = {114332},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114332},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125013711},
author = {Abdelweheb Gueddes and Borhen Louhichi and Mohamed Ali Mahjoub},
keywords = {Community detection, Social networks, BERT, Ontology, SWRL, Semantic enrichment, Graph-based analysis},
abstract = {Community detection in social networks is crucial for understanding online interactions. Traditional methods often overlook semantic information. This paper introduces a novel framework that significantly enhances community detection accuracy and interpretability by integrating deep semantic representation with formal knowledge and logical reasoning. Our primary contributions are threefold: (1) a synergistic framework combining fine-tuned BERT embeddings, a comprehensive domain-specific ontology, and SWRL rules; (2) an ontology-guided attention mechanism that directs BERT to focus on semantically relevant concepts during fine-tuning; and (3) the application of logical inference via SWRL to refine community boundaries and identify implicit user relationships. We evaluated our framework on diverse datasets from Facebook, Twitter, and Reddit. Experiments demonstrate significant improvements in modularity, NMI, and F1-score over strong baselines, including Louvain, graph attention networks (GAT), and other embedding-based methods. An ablation study confirms the critical contributions of both the ontology-guided attention and the SWRL rules. A case study on Twitter political discussions further illustrates the framework’s ability to uncover semantically coherent communities, influential users, and fine-grained thematic structures. This research establishes a new paradigm for community detection that effectively merges structural analysis with semantic knowledge, delivering more accurate, interpretable, and scalable results.}
}
@article{SCHEFFER202513,
title = {An ontological framework for AR-enhanced maintenance management},
journal = {Procedia CIRP},
volume = {134},
pages = {13-18},
year = {2025},
note = {58th CIRP Conference on Manufacturing Systems 2025},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2025.03.044},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125004974},
author = {Sara Scheffer and Fazel Ansari},
keywords = {Opportunistic maintenance, AR decision-making, Game theory, Ontology},
abstract = {This paper presents an ontology design framework that integrates opportunistic maintenance (OM) strategies, Stackelberg game theory models, Nash equilibrium concepts, and augmented reality (AR) to enhance decision-making, information visualization and contextualization in industrial settings. Traditional OM approaches often lack real-time, context-aware decision support, efficient resource allocation, and multi-agent collaboration, essential for dynamic optimization strategies. Integrating Stackelberg game theory models into OM enhances resource allocation and scheduling through hierarchical decision-making, allowing a leader-follower to optimally distribute resources while providing data-rich visualization and supporting adaptive, strategic planning in maintenance management. In dynamic multi-agent environments with multiple stakeholders, achieving Nash equilibrium leads to stable and efficient resource allocation, as no participant can unilaterally improve their outcome without impacting others. By incorporating Stackelberg game dynamics, Nash equilibrium concepts, and AR, the conceptual framework facilitates structured strategic planning, balancing leadership-driven optimization with equilibrium-based stability in decision-making and visualization. Evaluation of this ontology is proposed through a case study in a laboratory setting. The proposed ontology serves as a knowledge base for improving decision-making and provides a replicable framework for future advancements in industrial maintenance management by enabling the integration of emerging technologies, such as foundation models and large language models (LLMs), to refine maintenance strategies.}
}
@article{ZHAN2025110107,
title = {MIO: An ontology for annotating and integrating medical knowledge in myocardial infarction to enhance clinical decision making},
journal = {Computers in Biology and Medicine},
volume = {190},
pages = {110107},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.110107},
url = {https://www.sciencedirect.com/science/article/pii/S0010482525004585},
author = {Chaoying Zhan and Shumin Ren and Yuxin Zhang and Xiaojun Lv and Yalan Chen and Xin Zheng and Rongrong Wu and Erman Wu and Tong Tang and Jiao Wang and Cheng Bi and Mengqiao He and Xingyun Liu and Ke Zhang and Yingbo Zhang and Bairong Shen},
keywords = {Myocardial infarction, Ontology, Data standardization, Data interaction, Precision medicine},
abstract = {As biotechnology and computer science continue to advance, there's a growing amount of biomedical data worldwide. However, standardizing and consolidating these data remains challenging, making analysis and comprehension more difficult. To enhance research on complex diseases like myocardial infarction (MI), an ontology is necessary to ensure consistent data labeling and knowledge representation. This will facilitate data management and the application of artificial intelligence techniques in this field, ultimately advancing precision medicine research for MI. This study introduced the MI Ontology (MIO), which was developed using Stanford's seven-step method and Protégé. MIO aims to support precision medicine research on MI by effectively modeling and representing MI-related concepts and relationships. The validation of the MIO model involved employing Ontology Web Language (OWL) reasoners and comparing it with other disease-specific ontologies. MIO is an ontology model comprising of 3090 classes, 14 object attributes, 3494 individuals, 9415 synonyms and 49263 axioms, which encompass knowledge related to MI such as anatomical entities, clinical findings, drugs, genes, influencing factors, pathogenesis, patients-related concepts, procedures, and disease types. Furthermore, MIO has passed logical consistency validation and exhibits a broader conceptual scope and deeper knowledge structure than other disease-specific ontologies. Additionally, clinical use scenarios for MIO were developed to help address specific clinical problems. This study constructed the first comprehensive disease-specific ontology in cardiovascular diseases, named MIO, to promote precision medicine research on MI. MIO integrates and standardizes medical data, addressing complexity and standardization challenges. This promotes the use of big data analysis, explainable AI, and deep phenotype research in precision medicine. Future efforts will focus on enhancing and expanding MIO's applicability and scalability for superior services in this field.}
}
@article{SCHONFELDER2025103761,
title = {Ontology-based reasoning in automatic floor plan analysis},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103761},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103761},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625006548},
author = {Phillip Schönfelder and Markus König},
keywords = {Building information modeling, As-built modeling, Floor plan analysis, Knowledge graphs, Semantic enrichment, SPARQL},
abstract = {The growing need for digital representations of existing buildings in the Architecture, Engineering, Construction & Operations (AECO) domain necessitates efficient methods to retrospectively create Building Information Modeling (BIM) models. One prominent approach to obtain the necessary information is Plan-to-BIM, i.e., analyzing building documentation such as floor plans. However, the storage of this information is not standardized which leads to compatibility issues in collaborative scenarios. To address this, the paper presents the Drawing Analysis Ontology (DAnO), which is designed to standardize the representation of technical drawing data extracted through computer vision techniques. Focusing on floor plans, DAnO enables the aggregation, integration, and validation of extracted elements by defining key concepts, such as DrawingElement, DisplayElement, and DescriptionElement, and their relationships. By means of real floor plans, a case study demonstrates the ontology’s effectiveness in facilitating the generation of building models from legacy drawings, highlighting its potential to streamline BIM reconstruction workflows and to enhance interoperability in the AECO industry.}
}
@article{KUMAR2025S77,
title = {P03-15 From Pathways to Predictions: An Ontology-Driven Data Integration Framework for Integrating AOP and PBPK in Chemical Risk Assessment},
journal = {Toxicology Letters},
volume = {411},
pages = {S77},
year = {2025},
note = {Abstracts of the 59th Congress of the European Societies of Toxicology (EUROTOX 2025) TOXICOLOGY ADDRESSES SOCIETY'S REAL LIFE RISKS FOR SUSTAINABLE HEALTH AND WELL BEING},
issn = {0378-4274},
doi = {https://doi.org/10.1016/j.toxlet.2025.07.210},
url = {https://www.sciencedirect.com/science/article/pii/S037842742501793X},
author = {V. Kumar and S. Kumar and D. Deepika and S. Sharma and J. Kruisselbrink and P. Panov},
abstract = {Chemical risk assessment demands a harmonized integration of qualitative and quantitative methodologies to address challenges in exposure modeling, hazard prediction, and regulatory decision-making. Ontologies, widely recognized for their role in structured data mining and knowledge representation, offer a powerful solution for formalizing biological knowledge and enabling machine-readable workflows. This work undertaken within the EU Partnership for Risk Assessment (PARC) presents an ontology-driven framework that bridges Adverse Outcome Pathways (AOPs) and Physiologically Based Pharmacokinetic (PBPK) models to advance chemical risk assessment through consistent data integration and automation. Physiologically Based Pharmacokinetic Ontology (PBPKO), comprising approximately 700 terms, has been developed to annotate PBPK models in SBML format. Submitted to the OBO Foundry and undergoing revision, PBPKO standardizes toxicokinetic terminology, facilitating interoperability across regulatory applications. Case studies on selected PBPK models demonstrate its utility for annotation of model with tools like PBK- inspector and harmonized workflow. This ontology enhances translational modeling by supporting quantitative in vitro-in vivo extrapolation (QIVIVE) and probabilistic exposure predictions. The development of an AOP ontology led by the PARC partners in collaboration with major AOPs stakeholders and regulatory agencies, aims to formalize AOP knowledge representation at a granular level, maintaining a balance between information depth and abstraction. The foundational terms and logical structure will be established to support future quantitative applications of AOP. Harmonizing qualitative and quantitative methods used in chemical risk assessment enables consistent reporting and facilitates knowledge sharing among regulatory bodies. This ontology-driven harmonization represents a significant step toward unifying fragmented data streams in chemical risk assessment. It enables systematic evidence evaluation, reduces reliance on animal testing through mechanistic insights, and supports regulatory decisionmaking by providing a cohesive framework for integrating exposure, toxicokinetics, and hazard information. Ongoing case studies highlight the potential of this approach to address emerging challenges in chemical safety assessment while fostering collaboration among stakeholders in academia, industry, and regulatory agencies.}
}
@article{LAOUAR2025109361,
title = {How to tractably compute a productive repair for possibilistic partially ordered DL-LiteR ontologies?},
journal = {Fuzzy Sets and Systems},
volume = {510},
pages = {109361},
year = {2025},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2025.109361},
url = {https://www.sciencedirect.com/science/article/pii/S0165011425001009},
author = {Ahmed Laouar and Sihem Belabbes and Salem Benferhat},
keywords = {Inconsistency, Ontologies, Data repairs, Partial orders},
abstract = {The lightweight description logic dialect DL-LiteR offers a framework for specifying and reasoning with formal inconsistent ontologies. Basically, an ontology is a knowledge base composed of a TBox, modelling conceptual knowledge of some domain of interest, and an ABox, asserting factual knowledge about specific entities of the domain. Inconsistency in an ontology is usually handled by evaluating queries over maximal conflict-free subsets of the ABox, called data repairs. Several inconsistency-tolerant semantics, with different levels of cautiousness and computational cost, propose strategies for selecting the repairs to consider when deriving new conclusions from an inconsistent ontology. In this paper, we focus on partially ordered ontologies where a partial order relation captures the reliability levels of the ABox elements. We propose a new tractable method, called “Cπ-repair”, which leverages possibility theory in repairing a partially ordered ABox. It proceeds in four steps as follows. First, the partial order relation is extended into a family of total orders, thus inducing as many compatible totally ordered ABoxes. Second, a single repair is computed for each compatible ABox. Third, these repairs are closed deductively in order to improve their productivity, i.e., to derive more facts. Finally, the closed repairs are intersected to produce a single repair for the initial partially ordered ABox. The main contribution of this paper is an equivalent characterization that determines the validity of the conclusions drawn with the “Cπ-repair” method, but without eliciting the compatible ABoxes or computing their repairs. This allows us to establish the tractability of the method by reformulating the problem using the notions of support for an assertion and dominance over the conflicts that arise between the ABox elements. Essentially, the valid conclusions are those derived from the supports that dominate all conflicts. In the last part of the paper, we explore the rationality properties of our method. We show that increasing repair productivity does not alter the satisfaction of the rationality properties. We also discuss the applicability of our proposed method to languages richer than DL-LiteR and to other inconsistency-tolerant semantics.}
}
@article{ELGHOSH2025102419,
title = {CriMOnto: A generalized domain-specific ontology for modeling procedural norms of the Lebanese criminal law},
journal = {Data & Knowledge Engineering},
volume = {158},
pages = {102419},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102419},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X2500014X},
author = {Mirna {El Ghosh} and Hala Naja and Habib Abdulrab and Mohamad Khalil},
keywords = {AI & Law, Criminal law, Procedural norms, Generalized ontology, UFO, UFO-L, Ontology-Driven Conceptual Modeling, Ontology Patterns, Formal rules},
abstract = {Criminal (or penal) law regulates offenses, offenders, and legal punishments. Modeling criminal law is gaining much attention in the ontology engineering community. However, a significant aspect is neglected: the explicit representation of procedural knowledge. Procedural norms, such as regulative norms, are addressed to agents in the normative system. They govern the different interactions among these agents. In this study, we propose a formal and faithful representation of the procedural aspect of legal norms in the context of the Lebanese Criminal Code. A modular domain-specific ontology named CriMOnto is developed for this purpose. CriMOnto is grounded in the Unified Foundational Ontology (UFO) and the legal core ontology UFO-L by applying the Ontology-Driven Conceptual Modeling (ODCM) process. Conceptual Ontology Patterns (COPs) are reused from UFO and UFO-L to build the hierarchical and procedural content of the ontology. CriMOnto is validated as a formal ontology and evaluated using a dual evaluation approach. The potential use of CriMOnto for lightweight rule-based decision support is discussed in this study.}
}
@article{SHI2025127817,
title = {A multi-model approach to construction site safety: Fault trees, Bayesian networks, and ontology reasoning},
journal = {Expert Systems with Applications},
volume = {288},
pages = {127817},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127817},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425014393},
author = {Donghui Shi and Shuling Gan and Jozef Zurada and Jian Guan and Feilong Wang and Pawel Weichbroth},
keywords = {Earthwork foundation pit collapse (EFPC), Text mining, Fault tree, Bayesian network, Ontology, Knowledge graph},
abstract = {With the rapid expansion of the construction industry, accidents on construction sites have been increasingly common. Among these, collapses of earthwork foundation pits are particularly significant due to the sheer weight of the collapsed materials and the vast affected areas, resulting in substantial casualties and economic losses. This study aims to determine the causes of these construction safety and collapse incidents and understand their relationships to enable effective supervision and prevention during construction. Initially, the LDA model is used to categorize historical construction safety accident reports, and text mining is applied utilizing R language platform and the TF-IDF measure to extract keywords related to accident causative factors from historical accident reports. Following this, risk factors are evaluated to find the basic, intermediate, and top events of the accident, constructing a fault tree of casualties from earthwork foundation pit collapse (EFPC) accidents and analyzing the structural significance of risk factors. The fault tree is converted into a Bayesian network through image and numerical mapping, allowing the analysis of node sensitivity and the prediction of top event probability for informed construction accident prediction and prevention. Lastly, the study constructs an ontology knowledge base and knowledge graph in the realm of building safety, and establishes an ontology reasoning model using the Pellet reasoning machine and SWRL reasoning rules. The novel approach in this study involves integrating multiple advanced methodologies and provides a comprehensive framework and knowledge foundation for enhancing construction accident prediction and prevention as well as understanding and mitigating the risk-causing factors of earthwork foundation pit collapses in the construction industry.}
}
@article{OYEKAN2025104329,
title = {From Ontologies to Knowledge Augmented Large Language Models for Automation: A decision-making guidance for achieving human–robot collaboration in Industry 5.0},
journal = {Computers in Industry},
volume = {171},
pages = {104329},
year = {2025},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2025.104329},
url = {https://www.sciencedirect.com/science/article/pii/S0166361525000946},
author = {John Oyekan and Christopher Turner and Michael Bax and Erich Graf},
keywords = {Language models, Generative pre-trained transformers, Robotics, Manufacturing, Reasoning},
abstract = {The rapid advancement of Large Language Models (LLMs) has resulted in interest in their potential applications within manufacturing systems, particularly in the context of Industry 5.0. However, determining when to implement LLMs versus other Natural Language Processing (NLP) techniques, ontologies or knowledge graphs, remains an open question. This paper offers decision-making guidance for selecting the most suitable technique in various industrial contexts, emphasizing human–robot collaboration and resilience in manufacturing. We examine the origins and unique strengths of LLMs, ontologies, and knowledge graphs, assessing their effectiveness across different industrial scenarios based on the number of domains or disciplines required to bring a product from design to manufacture. Through this comparative framework, we explore specific use cases where LLMs could enhance robotics for human–robot collaboration, while underscoring the continued relevance of ontologies and knowledge graphs in low-dependency or resource-constrained sectors. Additionally, we address the practical challenges of deploying these technologies, such as computational cost and interpretability, providing a roadmap for manufacturers to navigate the evolving landscape of Language based AI tools in Industry 5.0. Our findings offer a foundation for informed decision-making, helping industry professionals optimize the use of Language Based models for sustainable, resilient, and human-centric manufacturing. We also propose a Large Knowledge Language Model architecture that offers the potential for transparency and configuration based on complexity of task and computing resources available.}
}
@article{EDDINEMEFTAH2025440,
title = {An Intelligent Arabic Legal Assistant system (IALAS) based on Ontology},
journal = {Transportation Research Procedia},
volume = {84},
pages = {440-447},
year = {2025},
note = {Smart Mobility and Logistics Ecosystems},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2025.03.094},
url = {https://www.sciencedirect.com/science/article/pii/S2352146525001425},
author = {Mohammed Charaf {Eddine Meftah} and Abdelhak Soussa and Adel Herzallah},
keywords = {Legal texts, Information search, Natural language processing (Arabic), Ontology, An intelligent system},
abstract = {Laws and regulations can be modified by experts in the legal field in response to various changes in the lives of individuals and communities. Massive changes and updates are constantly being made to laws to adapt to societal changes. This creates a huge database of legal information. Manually searching for information in this database takes a lot of time and effort and affects the efficiency and governance of all administrative and community affairs. To solve this problem, this paper proposes a solution based on one of the types of artificial intelligence. It is an ontology-based solution. This paper explains the design and development of a computer advisory system that helps in making legal decisions based on a proposed ontological structure using Protégé. A set of tools were also chosen to develop the proposed system. For operation, OwlReady2 with SPARQL query language was also used to extract content from the proposed ontology, Camel tools as a natural language processing (Arabic) tool, and SQLite for the database. This work contributes to filling a gap regarding the Arab cognitive modeling of Arab laws to keep pace in sustainable cognitive cities.}
}
@article{CAO2024,
title = {An Automatic and End-to-End System for Rare Disease Knowledge Graph Construction Based on Ontology-Enhanced Large Language Models: Development Study},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/60665},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424001881},
author = {Lang Cao and Jimeng Sun and Adam Cross},
keywords = {rare disease, clinical informatics, LLM, natural language processing, machine learning, artificial intelligence, large language models, data extraction, ontologies, knowledge graphs, text mining},
abstract = {Background
Rare diseases affect millions worldwide but sometimes face limited research focus individually due to low prevalence. Many rare diseases do not have specific International Classification of Diseases, Ninth Edition (ICD-9) and Tenth Edition (ICD-10), codes and therefore cannot be reliably extracted from granular fields like “Diagnosis” and “Problem List” entries, which complicates tasks that require identification of patients with these conditions, including clinical trial recruitment and research efforts. Recent advancements in large language models (LLMs) have shown promise in automating the extraction of medical information, offering the potential to improve medical research, diagnosis, and management. However, most LLMs lack professional medical knowledge, especially concerning specific rare diseases, and cannot effectively manage rare disease data in its various ontological forms, making it unsuitable for these tasks.
Objective
Our aim is to create an end-to-end system called automated rare disease mining (AutoRD), which automates the extraction of rare disease–related information from medical text, focusing on entities and their relations to other medical concepts, such as signs and symptoms. AutoRD integrates up-to-date ontologies with other structured knowledge and demonstrates superior performance in rare disease extraction tasks. We conducted various experiments to evaluate AutoRD’s performance, aiming to surpass common LLMs and traditional methods.
Methods
AutoRD is a pipeline system that involves data preprocessing, entity extraction, relation extraction, entity calibration, and knowledge graph construction. We implemented this system using GPT-4 and medical knowledge graphs developed from the open-source Human Phenotype and Orphanet ontologies, using techniques such as chain-of-thought reasoning and prompt engineering. We quantitatively evaluated our system’s performance in entity extraction, relation extraction, and knowledge graph construction. The experiment used the well-curated dataset RareDis2023, which contains medical literature focused on rare disease entities and their relations, making it an ideal dataset for training and testing our methodology.
Results
On the RareDis2023 dataset, AutoRD achieved an overall entity extraction F1-score of 56.1% and a relation extraction F1-score of 38.6%, marking a 14.4% improvement over the baseline LLM. Notably, the F1-score for rare disease entity extraction reached 83.5%, indicating high precision and recall in identifying rare disease mentions. These results demonstrate the effectiveness of integrating LLMs with medical ontologies in extracting complex rare disease information.
Conclusions
AutoRD is an automated end-to-end system for extracting rare disease information from text to build knowledge graphs, addressing critical limitations of existing LLMs by improving identification of these diseases and connecting them to related clinical features. This work underscores the significant potential of LLMs in transforming health care, particularly in the rare disease domain. By leveraging ontology-enhanced LLMs, AutoRD constructs a robust medical knowledge base that incorporates up-to-date rare disease information, facilitating improved identification of patients and resulting in more inclusive research and trial candidacy efforts.}
}
@article{SAHBI2025102392,
title = {Semantic vs. LLM-based approach: A case study of KOnPoTe vs. Claude for ontology population from French advertisements},
journal = {Data & Knowledge Engineering},
volume = {156},
pages = {102392},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102392},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24001162},
author = {Aya Sahbi and Céline Alec and Pierre Beust},
keywords = {Ontology population, LLM, Textual descriptions},
abstract = {Automatic ontology population is the process of identifying, extracting, and integrating relevant information from diverse sources to instantiate the classes and properties specified in an ontology, thereby creating a Knowledge Graph (KG) for a particular domain. In this study, we evaluate two approaches for ontology population from text: KOnPoTe, a semantic technique that employs textual and domain knowledge analysis, and a generative AI method leveraging Claude, a Large Language Model (LLM). We conduct comparative experiments on three French advertisement domains: real estate, boats, and restaurants to assess the performance of these techniques. Our analysis highlights the respective strengths and limitations of the semantic approach and the LLM-based one in the context of the ontology population process.}
}
@article{HNAINI2025104605,
title = {Enhancing security requirements specification with SECRET-SCORE: A template-driven and ontology-based approach},
journal = {Computers & Security},
volume = {157},
pages = {104605},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2025.104605},
url = {https://www.sciencedirect.com/science/article/pii/S0167404825002949},
author = {Hiba Hnaini and Raúl Mazo and Paola Vallejo and Andrés López and Joël Champeau},
keywords = {Security requirements engineering, Ontology, Cyber security, Security-by-design},
abstract = {In our rapidly changing world, where technology is integral to every aspect of our lives, ensuring our systems’ security is paramount. As industries become increasingly interconnected, the risk of security vulnerabilities and targeted attacks increases. Establishing robust security requirements is crucial to safeguard sensitive information and protect against malicious threats. To simplify and improve the quality of these requirements, researchers have proposed templates or boilerplates that can guide requirements engineers when defining requirements. However, this approach can help define the requirement structure without suggesting what security requirements to define to have a well-secured system. This paper proposes a guided strategy that combines (i) SECRET (SECurity REquirements specification Template) for a guided specification of each requirement and (ii) SCORE (Security Criteria Ontology for security Requirements Engineering) to suggest additional security requirements. We implemented the SECRET-SCORE approach using an autocomplete service that connects the SECRET template to the SCORE ontology. We then used the service to create a new language for security requirements in the VariaMos online tool. Finally, to test the usability of the implemented language, we conducted a usability test that reported high results in usability and user satisfaction with the developed tool.}
}
@article{BIERNACKA20244825,
title = {Modeling of Medical Procedures with Petri Nets over Ontological Graphs: an Example of Triage},
journal = {Procedia Computer Science},
volume = {246},
pages = {4825-4832},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.348},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924023731},
author = {Beata Biernacka and Krzysztof Pancerz},
keywords = {modeling, Petri nets, OWL ontology, medical procedures, triage},
abstract = {We present theoretical rudiments of Petri nets over ontological graphs as well as their possible application to model medical procedures on the example of triage. Petri nets are a graphical and formal tool of modelling structures and dynamics of various types of systems. In Petri nets over ontological graphs, the domain knowledge is enclosed in a form of ontologies. In the presented approach, ontological graphs are obtained from ontologies built in accordance with the OWL 2 Web Ontology Language. Ontologies specify, among others, concepts and the relations between them in individual areas of life, for example in medicine as it is shown in the paper. Therefore, Petri nets over ontological graphs combine the power of graphical and formal description of the structure and behavior of systems with operations on linguistic data in the form of concepts and relations between them.}
}
@article{TSEDURA2025100449,
title = {Towards the design of a particle swarm optimization ontology for object classification},
journal = {Array},
volume = {27},
pages = {100449},
year = {2025},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2025.100449},
url = {https://www.sciencedirect.com/science/article/pii/S2590005625000761},
author = {Nyaradzo Alice Tsedura and Ernest Bhero and Colin Chibaya},
keywords = {Scoping review, Ontology, Object classification, Particle swarm system},
abstract = {This article proposes an ontology blueprint inspired by key components of the particle swarm system to address the object classification problem. The identified key components particle, swarm, search space, goal, environment and fitness measures were independently evaluated based on their sub-entities, relationships, data flow and storage. These unit designs were integrated into a comprehensive particle swarm system ontology. A technology assessment model, in the form of a questionnaire, was distributed to 15 software engineering experts to evaluate the ontology based on 10 metrics, including completeness, correctness, usefulness and scalability. Results showed that 88 % of responses rated the designs as good, while 12 % found them to be average or poor. These findings confirm the proposed ontology designs as valid, with potential for further refinement based on expert feedback.}
}
@article{LUDWIG2025104289,
title = {An ontology-based retrieval augmented generation procedure for a voice-controlled maintenance assistant},
journal = {Computers in Industry},
volume = {169},
pages = {104289},
year = {2025},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2025.104289},
url = {https://www.sciencedirect.com/science/article/pii/S0166361525000545},
author = {Heiner Ludwig and Thorsten Schmidt and Mathias Kühn},
keywords = {Retrieval augmented generation (RAG), Large Language Model (LLM), Web Ontology Language (OWL), Maintenance assistant},
abstract = {This paper presents a novel approach to support complex maintenance procedures through a dialogue-driven digital assistant using an ontology-based retrieval augmented generation method. The core of the proposed system relies on the strong formalisation capabilities of the graph-based Web Ontology Language (OWL), combined with various retrieval algorithms and different Large Language Models (LLMs) to determine the most useful context for answering user queries. To do this, we use the popular principle of Retrieval Augmented Generation (RAG). Graph traversal enriches the contextual knowledge, enabling more accurate and context-aware responses. An evaluation using an OWL example ontology and an extensive Q&A dataset demonstrates the improved retrieval quality achieved by combining classical and vector-based semantic matching methods. The community-driven analysis of generation quality illustrates the usability of an OWL-based assistant for maintenance procedures on the basis of contexts and LLMs of varying configurations.}
}
@article{SEI2025106268,
title = {Ontology-based representation of quality assurance and inspection planning in construction},
journal = {Automation in Construction},
volume = {177},
pages = {106268},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106268},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525003085},
author = {Sebastian Seiß and Yuan Zheng and Jürgen Melzner and Jan Wium and Olli Seppänen},
keywords = {Quality assurance (QA), Inspection planning, Construction, Ontology, Knowledge representation},
abstract = {Inspections based on detailed quality inspection plans are crucial for minimizing construction failures and improving construction quality. However, the manual inspection planning process could be aided through the use of a formalized representation, which is currently missing. To address this gap, this paper introduces the Ontology for Construction Quality Assurance (OCQA), which aims to provide formal, comprehensive, and modular knowledge representation for quality inspection planning in construction. The development of the OCQA follows the Linked Open Terms methodology and is encoded using Semantic Web Ontology Language to ensure machine-readability and alignment with other ontologies. The OCQA offers support to inspection planners and inspectors by providing relevant inspection planning knowledge and information to enable project-specific inspection planning. Future research could involve extending the OCQA to specific trades or automating the inspection planning process.}
}
@article{FH2025112762,
title = {BIM ontology for information management (BIM-OIM)},
journal = {Journal of Building Engineering},
volume = {107},
pages = {112762},
year = {2025},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2025.112762},
url = {https://www.sciencedirect.com/science/article/pii/S2352710225009994},
author = {Abanda F.H and Akintola A and Tuhaise V.V and Tah J.H.M},
keywords = {BIM, BIM execution plan, Information management, ISO 19650, Ontology},
abstract = {The adoption of Building Information Modelling (BIM) in the construction industry has been hindered by numerous barriers, notably the limited understanding of its concepts, protocols, and the intricate interplay between processes, people, and technologies. To address these challenges, a range of standards and guidelines have been developed, most notably the ISO 19650 series, which offer a comprehensive framework for implementing various aspects of BIM in construction projects. However, despite the BIM's collaborative philosophy, the standards and specifications that guide its adoption and implementation seldom reveal and explain the relationships between their key elements and concepts. This lack of clarity limits understanding and undermines the very essence of collaboration that BIM seeks to promote in construction projects. The text-based nature of the standards and specifications makes it difficult to identify common concepts that cut across the different project phases, their relationships, and interdependencies. This study proposes a BIM ontology for information management (BIM-OIM) that makes BIM process data more available and easily useable, allowing other researchers and practitioners to implement, and extend its use within their domains of practice. To achieve the practice-driven goal of BIM-OIM, Yet Another Methodology for Ontology (YAMO), one of the leading ontology engineering methodologies, was used to develop BIM-OIM. BIM-OIM is a formal and structured representation of ISO 19650 knowledge that is machine-processable. This representation enhances understanding, promotes reusability, and supports practical applications throughout the information management lifecycle. Key applications include the development of BIM Execution Plans, compliance checking for information containers, and identifying the roles of various stakeholders within a project.}
}
@article{WEI2025100836,
title = {Knowledge-enhanced ontology-to-vector for automated ontology concept enrichment in BIM},
journal = {Journal of Industrial Information Integration},
volume = {45},
pages = {100836},
year = {2025},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2025.100836},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X25000603},
author = {Yinyi Wei and Xiao Li},
keywords = {Semantic representation, Natural language processing, Pre-trained language models, Ontology embedding, Ontology concept enrichment, BIM ontology},
abstract = {Building Information Modeling (BIM) relies on standardized ontologies like IfcOWL to address interoperability. However, the increasing complexity and diversity of construction information requirements demand automated enrichment of BIM ontologies, which is hindered by several factors, including complexity in ontology structure, scalability limitations, and domain-specific issues. Manual curation and maintenance of ontologies are labor-intensive and time-consuming, particularly as the scope of BIM projects expands. Despite these challenges, the construction industry lacks an effective automated approach for ontology concept enrichment. Thus, this study proposes a knowledge-enhanced ontology-to-vector (Keno2Vec) approach for automated BIM ontology concept enrichment, which can (1) encode ontology elements into meaningful and semantically rich embeddings by employing the BERT model to integrate both ontological information (names and labels) and external knowledge (definitions from authoritative knowledge bases), effectively addressing the domain expression specificity and complexity of BIM ontologies; and (2) provide a flexible framework that supports various downstream tasks of ontology concept enrichment by utilizing the resulting embeddings, thereby improving the task-specific adaptability and variability. Experimental results on datasets derived from the large-scale ifcOWL and two smaller BIM ontologies demonstrate that Keno2Vec significantly outperforms existing ontology embedding approaches in terms of accuracy and adaptability. For example, Keno2Vec achieves F1 scores on ifcOWL of nearly 87 % for subsumption prediction, 60 % for property identification, 95 % for membership recognition, and 100 % and 90 % for category-based and schema-based concept classification, respectively. Additional analysis highlights the potential of Keno2Vec for improving BIM ontology encoding and benefiting downstream applications.}
}
@article{ZHAO20254447,
title = {Intelligent Spatial Anomaly Activity Recognition Method Based on Ontology Matching},
journal = {Computers, Materials and Continua},
volume = {83},
number = {3},
pages = {4447-4476},
year = {2025},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2025.063691},
url = {https://www.sciencedirect.com/science/article/pii/S1546221825004849},
author = {Longgang Zhao and Seok-Won Lee},
keywords = {Context awareness, activity recognition, ontological reasoning, complex context, anomaly detection},
abstract = {This research addresses the performance challenges of ontology-based context-aware and activity recognition techniques in complex environments and abnormal activities, and proposes an optimized ontology framework to improve recognition accuracy and computational efficiency. The method in this paper adopts the event sequence segmentation technique, combines location awareness with time interval reasoning, and improves human activity recognition through ontology reasoning. Compared with the existing methods, the framework performs better when dealing with uncertain data and complex scenes, and the experimental results show that its recognition accuracy is improved by 15.6% and processing time is reduced by 22.4%. In addition, it is found that with the increase of context complexity, the traditional ontology inference model has limitations in abnormal behavior recognition, especially in the case of high data redundancy, which tends to lead to a decrease in recognition accuracy. This study effectively mitigates this problem by optimizing the ontology matching algorithm and combining parallel computing and deep learning techniques to enhance the activity recognition capability in complex environments.}
}
@article{SHIMIZU2025100862,
title = {Accelerating knowledge graph and ontology engineering with large language models},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100862},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2025.100862},
url = {https://www.sciencedirect.com/science/article/pii/S1570826825000022},
author = {Cogan Shimizu and Pascal Hitzler},
keywords = {Knowledge graph engineering, Ontology engineering, Large language models, Modular ontologies, Ontology modeling, Ontology population, Ontology alignment, Entity disambiguation},
abstract = {Large Language Models bear the promise of significant acceleration of key Knowledge Graph and Ontology Engineering tasks, including ontology modeling, extension, modification, population, alignment, as well as entity disambiguation. We lay out LLM-based Knowledge Graph and Ontology Engineering as a new and coming area of research, and argue that modular approaches to ontologies will be of central importance.}
}
@article{XUE2025105525,
title = {How to realize the knowledge reuse and sharing from accident reports? A knowledge-driven modeling method combining ontology and deep learning},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {94},
pages = {105525},
year = {2025},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2024.105525},
url = {https://www.sciencedirect.com/science/article/pii/S0950423024002833},
author = {Nannan Xue and Wei Zhang and Huayu Zhong and Wenbin Liao and Tingsheng Zhao},
keywords = {Process safety, Knowledge graph, Ontology design, Joint extraction model, Knowledge application},
abstract = {The exploration and understanding of past accidents are of great significance in enhancing the process safety. However, manually reading and analyzing a large number of accident reports is a time-consuming and inefficient task. In this study, a novel modeling method is developed to build the knowledge graph of process safety accidents, aiming to overcome the problem of knowledge reuse and sharing. Firstly, the dataset consists of 409 process safety accident reports selected from the official website of the Ministry of Emergency Management of China. Secondly, the ontology design schema is defined based on the seven-step method, including 34 ontology classes and 11 relations. Then, a new joint extraction model for the process domain is proposed based on the CasRel framework, which achieves 95.85% in precision, 61.54% in recall, and 74.95% in F1-score. Finally, the knowledge graph containing 9192 nodes and 11,257 edges is constructed in the Neo4j graph database, followed by the discussion of various related applications such as query, statistics, and analysis. The results indicate that the proposed method is a useful tool for obtaining valuable knowledge from accident reports, contributing to analysis and prevention of accidents.}
}
@article{HAGEDORN2025103369,
title = {OntoBPR: An ontology-based framework for performing building permit reviews using standardized information containers},
journal = {Advanced Engineering Informatics},
volume = {66},
pages = {103369},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103369},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625002629},
author = {Philipp Hagedorn and Judith Fauth and Sven Zentgraf and Sebastian Seiß and Markus König and Ioannis Brilakis},
keywords = {Digital building permit, Building permit review, Ontology & semantic web, Information Container for linked Document Delivery (ICDD), Compliance checking, Shapes Constraint Language (SHACL)},
abstract = {Building permitting is essential for ensuring the safety, sustainability, and societal alignment of construction projects. Despite interest from both practitioners and researchers, the process remains largely manual and fragmented. Ontologies offer a promising solution by managing complexity and enabling automation through semantic information, though current ontologies in the building permit domain are limited to specific aspects like building code checking. On the process level, the OntoBPR framework integrates multiple domain-specific ontologies for a seamless digital permitting process and provides a workflow to automate the lifecycle of the permit review. Therefore, it suggests integrating the submitted building application using standardized information containers. The paper explores how digital applications can be submitted, reviewed, verified for completeness, and forwarded to authorities, and how permit review results can be gathered to support decision-making and automate notification issuance, and it provides a demonstration in a case study. In conclusion, OntoBPR formalizes a multi-layered ontology that advances and aligns the partitioned building permit process and provides an adaptable framework to harmonize diverse legal, informatics, and procedural aspects.}
}
@article{MCINNIS2025293,
title = {Integrating Knowledge: The Power of Ontologies in Psychiatric Research and Clinical Informatics},
journal = {Biological Psychiatry},
volume = {98},
number = {4},
pages = {293-301},
year = {2025},
note = {Neurodevelopmental Perspectives on Bipolar Disorder and Treatment Outcomes},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2025.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0006322325012132},
author = {Melvin G. McInnis and Ben Coleman and Eric Hurwitz and Peter N. Robinson and Andrew E. Williams and Melissa A. Haendel and Julie A. McMurry},
keywords = {Bipolar disorder, Nosology, Ontology, Phenotype characterization, Precision psychiatry, Semantics},
abstract = {Ontologies are structured frameworks for representing knowledge by systematically defining concepts, categories, and their relationships. While widely adopted in biomedicine, ontologies remain largely absent in mental health research and clinical care, where the field continues to rely heavily on existing classification systems (e.g., the DSM). Although useful for clinical communication and administrative purposes, they lack the semantic structure, computational properties, and reasoning properties needed to integrate diverse data sources or support artificial intelligence–enabled analysis. This reliance on classification systems limits efforts to analyze and interpret complex, heterogeneous psychiatric data. In mood disorders, particularly bipolar disorder, the lack of formalized semantic models contributes to diagnostic inconsistencies, fragmented data structures, and barriers to precision medicine. By contrast, ontologies provide a standardized, machine-readable foundation for linking multimodal data sources, such as electronic health records, genetic and neuroimaging data, and social determinants of health, while enabling secure, deidentified computation. In this review, we survey the current landscape of mental health ontologies and highlight the Human Phenotype Ontology (HPO) as a promising framework for bridging psychiatric and medical phenotypes. We describe ongoing efforts to enhance the HPO through curated psychiatric terms, refined definitions, and structured mappings of observed phenomena. The Global Bipolar Cohort (GBC), an international collaboration, exemplifies this approach through the development of a consensus-driven ontology tailored to bipolar disorder. By supporting semantic interoperability, reproducible research, and individualized care, ontology-based approaches provide essential infrastructure for overcoming the limitations of classification systems and advancing data-driven precision psychiatry.}
}
@article{LECU2024443,
title = {Using LLMs and ontologies to extract causal relationships from medical abstracts},
journal = {Procedia Computer Science},
volume = {244},
pages = {443-452},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.219},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924030205},
author = {Alexandru Lecu and Adrian Groza and Lezan Hawizy},
keywords = {Causal Relation Extraction, Knowledge Graphs, Large Language Models, Age-Related Macular Degeneration},
abstract = {The substantiation of the causal relationships behind its development is very important in identifying possible interventions and early treatment. Knowledge Graphs (KG) play a crucial role in the medical research domain by organizing data into interconnected structures that represent relationships between entities such as disease, treatments, and progressions. This paper shows a complete workflow that demonstrates the extraction of causal relationships from medical abstracts using a fine-tuned GPT-based model and the integration of these relationships into a KG.}
}
@article{PONTE2025104484,
title = {Multi-task visual food recognition by integrating an ontology supported with LLM},
journal = {Journal of Visual Communication and Image Representation},
volume = {110},
pages = {104484},
year = {2025},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2025.104484},
url = {https://www.sciencedirect.com/science/article/pii/S1047320325000987},
author = {Daniel Ponte and Eduardo Aguilar and Mireia Ribera and Petia Radeva},
keywords = {Food ontology, Food image analysis, Multi-task learning, Large language models},
abstract = {Food image analysis is a crucial task with far-reaching implications across various domains, including culinary arts, nutrition, and food technology. This paper presents a novel approach to multi-task visual food analysis, using large language models to obtain recipes and support the creation of a comprehensive food ontology. The approach integrates the food ontology into an end-to-end model, with prior knowledge on the relationships of food concepts at different semantic levels, within a multi-task deep learning visual food analysis approach, to generate better and more consistent class predictions. Evaluated on two benchmark datasets, MAFood-121 and VireoFood-172, this method demonstrates its effectiveness in single-label food recognition and multi-label food group classification. The ontology enhances accuracy, consistency, and generalization by effectively transferring knowledge to the learning model. This study underscores the potential of ontology-based methods to address food image classification complexities, with implications for broad applications, including automated recipe generation and nutritional assessment.}
}
@article{KONE2025101174,
title = {LLM-driven semantic explanations for soil moisture prediction models},
journal = {Smart Agricultural Technology},
volume = {12},
pages = {101174},
year = {2025},
issn = {2772-3755},
doi = {https://doi.org/10.1016/j.atech.2025.101174},
url = {https://www.sciencedirect.com/science/article/pii/S277237552500406X},
author = {Bamory Ahmed Toru Koné and Khouloud Boukadi and Rima Grati and Emna Ben Abdallah and Massimo Mecella},
keywords = {LLM, Machine learning, Ontology, Soil moisture, XAI},
abstract = {Efficient soil moisture prediction is crucial for sustainable agricultural practices, especially in the face of climate change and increasing water scarcity. However, the adoption of machine learning (ML) models in this context is frequently limited by their lack of interpretability, particularly among non-expert users such as farmers. This study proposes a novel approach to soil moisture prediction that combines high predictive performance with enhanced explainability. We propose a framework that leverages large language models (LLMs) to generate textual explanations based on a proposed irrigation and soil moisture ontology, thus making the model's predictions more understandable to farmers. The ontology formalizes essential agricultural concepts and their interrelationships, enabling semantically rich explanations to bridge the gap between sophisticated model results and practical decision-making. Our approach is exemplified by a prototype system that provides both predictions and user-friendly explanations. The findings highlight the potential of combining advanced ML techniques with semantic reasoning to improve the interpretability and adoption of Artificial Intelligence in agriculture.}
}
@article{ZHAO20241855,
title = {Integrating Ontology-Based Approaches with Deep Learning Models for Fine-Grained Sentiment Analysis},
journal = {Computers, Materials and Continua},
volume = {81},
number = {1},
pages = {1855-1877},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.056215},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824007331},
author = {Longgang Zhao and Seok-Won Lee},
keywords = {Deep learning, ontology, fine-grained sentiment analysis, online reviews},
abstract = {Although sentiment analysis is pivotal to understanding user preferences, existing models face significant challenges in handling context-dependent sentiments, sarcasm, and nuanced emotions. This study addresses these challenges by integrating ontology-based methods with deep learning models, thereby enhancing sentiment analysis accuracy in complex domains such as film reviews and restaurant feedback. The framework comprises explicit topic recognition, followed by implicit topic identification to mitigate topic interference in subsequent sentiment analysis. In the context of sentiment analysis, we develop an expanded sentiment lexicon based on domain-specific corpora by leveraging techniques such as word-frequency analysis and word embedding. Furthermore, we introduce a sentiment recognition method based on both ontology-derived sentiment features and sentiment lexicons. We evaluate the performance of our system using a dataset of 10,500 restaurant reviews, focusing on sentiment classification accuracy. The incorporation of specialized lexicons and ontology structures enables the framework to discern subtle sentiment variations and context-specific expressions, thereby improving the overall sentiment-analysis performance. Experimental results demonstrate that the integration of ontology-based methods and deep learning models significantly improves sentiment analysis accuracy.}
}
@article{GU2025114216,
title = {Ontology-based data federation and query optimization},
journal = {Knowledge-Based Systems},
volume = {329},
pages = {114216},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114216},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125012572},
author = {Zhenzhen Gu and Davide Lanti and Francesco Corcoglioniti and Marco {Di Panfilo} and Alessandro Mosca and Diego Calvanese and Guohui Xiao},
keywords = {OBDA, VKG, OBDF, Data federation, Query optimization},
abstract = {Ontology-based data access (OBDA), also known as virtual knowledge graphs (VKG), is a well-established approach to information management that facilitates the access to a (single) relational data source through the mediation of a high-level ontology, and the use of a declarative mapping linking the data layer to the ontology. In order to integrate multiple, possibly distributed and heterogeneous, data sources, in this work we formally introduce an extension of OBDA, called ontology-based data federation (OBDF), by combining OBDA with a data federation layer, which can expose multiple data sources as a single relational database. We discuss opportunities and challenges of OBDF, and provide techniques to deliver efficient query answering in OBDF by exploiting inter-source relations (called data hints) in the federated sources. Such techniques are validated through an extensive experimental evaluation based on the Berlin SPARQL Benchmark.}
}
@article{HUETTEMANN2025102901,
title = {Designing ontology-based search systems for research articles},
journal = {International Journal of Information Management},
volume = {83},
pages = {102901},
year = {2025},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2025.102901},
url = {https://www.sciencedirect.com/science/article/pii/S0268401225000337},
author = {Sebastian Huettemann and Roland M. Mueller and Barbara Dinter},
keywords = {Search engines, Ontologies, Domain ontologies, Large language models, Knowledge extraction, Design science research, Literature review},
abstract = {The process of conducting scientific literature reviews is becoming increasingly complex and time-consuming due to the rapid expansion of available research. Popular academic search engines offer limited filtering capabilities and suffer from low precision. Machine learning-enhanced approaches tend to target rather specific areas, and novel approaches based on generative artificial intelligence suffer from hallucinations. Drawing on information foraging theory, this article presents a design science research project aimed at generating design knowledge for developing domain-specific search systems for research articles. Our contributions include: (1) integrating domain ontologies with large language models to design ontology-based search systems, (2) generating descriptive design knowledge by exploring the problem space, (3) generating prescriptive design knowledge for developing domain-specific search systems, and (4) presenting an ontology-based search engine prototype. Our results indicate that the proposed solution supports researchers in conducting literature reviews by increasing information gain while reducing interaction costs.}
}
@article{MACILENTI20241289,
title = {Prompting is not all you need Evaluating GPT-4 performance on a real-world ontology alignment use case},
journal = {Procedia Computer Science},
volume = {246},
pages = {1289-1298},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.557},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924026036},
author = {Giulio Macilenti and Armando Stellato and Manuel Fiorelli},
keywords = {Semantic technologies, Ontology alignment, Large language models},
abstract = {Ontology Alignment (OA) is a complex, demanding and error-prone task, requiring the intervention of domain and Semantic Web experts. Automating the alignment process thus becomes a must-do, especially when involving large datasets, to at least produce a first input for human experts. Automated ontology alignment could benefit from the outstanding language ability of Large Language Models (LLMs), which could implicitly provide the background knowledge that has been the Achilles’ heel of traditional alignment systems. However, this requires a correct evaluation of the performance of LLMs and understanding the best way to incorporate them into more specific tools. In this paper, we show that a naive prompting approach on the popular GPT-4 model could face several problems when transferred to real-world use cases. To this end, we replicated the methods of Norouzi et al. (2023), applied to the OAEI 2022 conference track, on a reference alignment between a pair of datasets (reduced versions of two popular thesauri: European Commission’s EuroVoc and TESEO, from the Italian Senate of the Republic), which has never been tested in OAEI evaluation campaigns. This reference alignment has several features common to real-world use cases: it is has a larger size than those considered in the study we replicated, it is not published online and is therefore not subject to data contamination and it involves relations between concepts that are more complex than simple equivalence. The replicated methods achieved a significantly lower performance on our reference alignment than on the OAEI 2022 conference track, suggesting that size, data contamination, and semantic complexity need to be considered when using LLMs for the alignment task.}
}
@article{AHAGGACH2024200411,
title = {Enhancing car damage repair cost prediction: Integrating ontology reasoning with regression models},
journal = {Intelligent Systems with Applications},
volume = {23},
pages = {200411},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2024.200411},
url = {https://www.sciencedirect.com/science/article/pii/S2667305324000851},
author = {Hamid Ahaggach and Lylia Abrouk and Eric Lebon},
keywords = {Cost prediction, Regression models, SWRL, Ontology reasoning, Named entity recognition, Relation extraction},
abstract = {The estimation of repair costs for car damage is a critical yet challenging task for insurance companies and repair shops. Accurate and the rapid predictions are essential for providing reliable cost estimates to customers. Traditional methods in this domain face multiple challenges, including manual processes and inaccuracies in repair cost estimation, as outlined in our article. This paper introduces a novel approach that combines regression models with ontology reasoning to enhance the accuracy of car damage repair cost predictions. An Ontology for Car Damage (OCD)11industryportal.enit.fr/ontologies/OCD. ,22github.com/OntologyCarDamage/OCD. has been developed, which is meticulously structured and populated using Named Entity Recognition (NER) and Relation Extraction (RE) techniques. This ontology provides a comprehensive framework for organizing and understanding the complex domain of car damage, capturing essential semantic relationships and variables that significantly influence repair costs. By integrating OCD with seven regression models, such as Random Forest and Decision Tree, we have proposed a hybrid methodology that leverages both structured data and semantic understanding. Our approach not only accounts for typical variables such as the type and severity of damage, and labor costs but also identifies novel features through the use of SWRL (Semantic Web Rule Language) rules, enhancing the model’s predictive capabilities. The performance of our models was evaluated using a substantial real-world dataset comprising over 300,000 records. This evaluation used metrics such as mean absolute error (MAE), root mean squared error (RMSE), and R-squared. The results indicate that our hybrid approach, which incorporates ontology reasoning, significantly outperforms traditional regression models. The Random Forest model, especially when combined with the OCD ontology, showcased superior performance, exhibiting a minimal average deviation from the actual repair costs and achieving a low MAE. This study’s findings demonstrate the potential of combining ontology reasoning with machine learning techniques for precise cost prediction in the automotive repair industry. Our methodology offers a robust tool for insurance companies and repair shops to generate more accurate, reliable, and automated cost estimates, ultimately benefiting both businesses and customers.}
}
@article{FILIPOVSKA2025S214,
title = {P18-65 Advancing the Grouping and Harmonization of Similar Key Events in the AOP Wiki: Ontology-Based Key Event Component Combinations as Catalysts for Integration},
journal = {Toxicology Letters},
volume = {411},
pages = {S214-S215},
year = {2025},
note = {Abstracts of the 59th Congress of the European Societies of Toxicology (EUROTOX 2025) TOXICOLOGY ADDRESSES SOCIETY'S REAL LIFE RISKS FOR SUSTAINABLE HEALTH AND WELL BEING},
issn = {0378-4274},
doi = {https://doi.org/10.1016/j.toxlet.2025.07.514},
url = {https://www.sciencedirect.com/science/article/pii/S0378427425020971},
author = {J. Filipovska and K. Audouze and J. Blum and L.-A. Clerbaux and E. Coerek and E. Demuynck and S. Edwards and E. Fritsche and L. Gerner and K. Groh and G. Hench and N. Jeliazkova and I. Katsiadaki and J. Malinowska and B. Mertens and M. Martens and H. Mortensen and P. Nymark and I. Sovadinová and S. Tanabe and K.E. Tollefsen and L. Wiklund and C. Wittwehr},
abstract = {Regulatory toxicology is shifting away from an observational towards predictive discipline increasingly embracing New Approach Methodologies (NAMs) that focus on mechanistic understanding of stressorinduced biological perturbations. Adverse Outcome Pathways (AOPs) provide a structured framework for organizing evidence and knowledge to support this transition, with the AOP Wiki serving as the primary platform for documenting Key Events (KEs) and Key Event Relationships (KERs). The Society for the Advancement of AOPs Knowledgebase Interest Group (SKIG) is a global assembly of scientists, regulators, and policymakers dedicated to advancing AOP development and application. A recurring challenge identified by SKIG is the proliferation of distinct yet similar KEs in the AOP Wiki [1]. The free-text approach used to define KEs has resulted in inconsistencies, complicating AOP development and hindering the emergence of functional AOP networks (AOPN). Addressing this issue requires improved tools and methodologies. One proposed solution is the use of Ontology-Based Key Event Components (KECs) which utilize structured and predefined terms from 22 biological ontologies currently integrated into the AOP Wiki [2]. They encode KE descriptions into three machine-readable components: ‘Process,’ ‘Object,’ and ‘Action’ (the type/direction of perturbation). Depending on complexity at higher organizational levels, a KE can be represented using one or more combinations of these components as machine-readable tokens. Selecting appropriate KECs or their combinations is not trivial. A proposed approach involves leveraging (i) measurement methods associated with KEs for more precise identification of the ‘Object’ and ‘Process’ and (ii) Large Language Models (LLMs) to analyze free-text descriptions and suggest the most relevant KEC tokens objectively. Although predefined KECs have been considered in AOPN development [3] and network analytics [4], they have yet to be systematically applied for defining or grouping similar KEs and integration at higher level of biological organisation. Using two- or three-component tokens to define and search KEs – and sub-events in complex cases – could significantly enhance KE identification, harmonisation and integration of KEs into AOPNs. This approach aims to facilitate more objective, high-throughput, and machine-assisted KE description while preserving the integrity and creativity of human-generated content. It focuses on embracing complexity particularly in the context of KEs and KERs at the higher level of biological organisation. As such, it is complementary to the work of the Omics2AOP [1], Methods2AOP [5], FAIR_AOP [6], Mystery of ROS [7], PARC [8] and Elixir toxicology [9] communities, working towards better use of ontologies and developing optimal programmatic applications of KECs for AOP developers and users. Standing on the shoulders of these giants the authors will explore some practical ways to address the challenges of using KECs in the AOP framework.}
}
@article{KOLLAPALLY2025104865,
title = {Ontology enrichment using a large language model: Applying lexical, semantic, and knowledge network-based similarity for concept placement},
journal = {Journal of Biomedical Informatics},
volume = {168},
pages = {104865},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2025.104865},
url = {https://www.sciencedirect.com/science/article/pii/S1532046425000942},
author = {Navya Martin Kollapally and James Geller and Vipina Kuttichi Keloth and Zhe He and Julia Xu},
keywords = {Ontology enrichment, Large language model, SemMedDB database, Semantic MEDLINE, Semantic MEDLINE database, Similarity search, Ontology evaluation, Social determinants of health},
abstract = {Objective
Ontologies are essential for representing the knowledge of a domain. To make ontologies useful, they must encompass a comprehensive domain view. To achieve ontology enrichment, there is a need to discover new concepts to be added, either because they were missed in the first place, or the state-of-the-art has advanced to develop new real-world concepts. Our goal is to develop an automatic enrichment pipeline using a seed ontology, a Large Language Model (LLM), and source of text. The pipeline is applied to the domain of Social Determinants of Health (SDoH), using PubMed as a source of concepts. In this work, the applicability and effectiveness of the enrichment pipeline is demonstrated by extending the SDoH Ontology called SOHOv1, however our methodology could be used in other domains as well.
Methods
We first retrieved PubMed abstracts of candidate articles with existing SOHOv1 concepts as search terms. Next, we used GPT-4-1201 to extract semantic triples from the abstracts. We identified concepts from these triples utilizing lexical, semantic, and knowledge network-based filtering. We also compared the granularity of semantic triples extracted with our method to the triples in the SemMedDB (Semantic MEDLINE Database). The results were evaluated by human experts and standard ontology tools for checking consistency and semantic correctness.
Results
We expanded SOHOv1, which contained 173 concepts and 585 axioms, including 207 logical axioms to SOHOv2, which contains 572 concepts, 1,542 axioms, including 725 logical axioms. Our methods identified more concepts than those extracted from SemMedDB for the same task. While we have shown the feasibility of our approach for an SDoH ontology, the methodology is generalizable to other ontologies with an existing seed ontology and text corpus.
Conclusions
The contributions of this work are: Extracting semantic triples from PubMed abstracts using GPT-4-1201 utilizing prompt chaining; showing the superiority of triples from GPT-4-1201 over triples from SemMedDB for SDoH; using lexical and semantic similarity search techniques with knowledge network-based search to identify the concepts to be added to the ontology; confirming the quality of the new concepts with human experts.}
}
@article{NAQVI2025100835,
title = {Enhancing semantic search using ontologies: A hybrid information retrieval approach for industrial text},
journal = {Journal of Industrial Information Integration},
volume = {45},
pages = {100835},
year = {2025},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2025.100835},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X25000597},
author = {Syed Meesam Raza Naqvi and Mohammad Ghufran and Christophe Varnier and Jean-Marc Nicod and Noureddine Zerhouni},
keywords = {Industry 4.0, Industrial information integration, Machine documentation, Multi-modal learning, Semantic search, Large Language Models (LLMs)},
abstract = {Despite the increased focus on data in Industry 4.0, textual data has received little attention in the production and engineering management literature. Data sources such as maintenance records and machine documentation usually are not used to help maintenance decision-making. Available studies mainly focus on categorizing maintenance records or extracting meta-data, such as time of failure, maintenance cost, etc. One of the main reasons behind this underutilization is the complexity and unstructured nature of the industrial text. In this study, we propose a novel hybrid information retrieval approach for industrial text using multi-modal learning. Maintenance operators can use the proposed system to query maintenance records and find similar solutions to a given problem. The proposed system utilizes heterogeneous (multi-modal) data, a combination of maintenance records, and machine ontology to enhance semantic search results. We used the state-of-the-art Large Language Models (LLMs); BERT (Bidirectional Encoder Representations from Transformers) for textual similarity. For similarity among ontology labels, we used a modified version of Wu-Palmer’s similarity. A hybrid weighted similarity is proposed, incorporating text and ontology similarities to enhance semantic search results. The proposed approach was validated using an open-source dataset of real maintenance records from excavators collected over ten years from different mining sites. A retrieval comparison using only text and multi-modal data is performed to estimate the proposed system’s effectiveness. Quantitative and qualitative analysis of results indicates a performance improvement of 8% using the proposed hybrid similarity approach compared to only text-based retrieval. To the best of our knowledge, this is the first study to combine LLMs and machine ontology for semantic search in maintenance records.}
}
@article{GOLNARI2025105942,
title = {Ontology accelerates few-shot learning capability of large language model: A study in extraction of drug efficacy in a rare pediatric epilepsy},
journal = {International Journal of Medical Informatics},
volume = {201},
pages = {105942},
year = {2025},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2025.105942},
url = {https://www.sciencedirect.com/science/article/pii/S1386505625001595},
author = {Pedram Golnari and Katrina Prantzalos and Veronica Hood and Mary Anne Meskis and Lori L. Isom and Karen Wilcox and Jack M. Parent and Dennis Lal and Samden D. Lhatoo and Howard P. Goodkin and Elaine C. Wirrell and Kelly G. Knupp and Manisha Patel and Jeffrey A. Loeb and Joseph E. Sullivan and Lauren Harte-Hargrove and Brandy E. Fureman and Jeffrey Buchhalter and Satya S. Sahoo},
keywords = {Optimizing Large Language Models, Few-shot Learning, Biomedical Ontology, Dravet Syndrome, Drug Efficacy},
abstract = {Objective
Dravet Syndrome (DS) is a developmental and epileptic encephalopathy that is characterized by severe, prolonged motor seizures and high resistance to multiple antiseizure medications (ASMs) with multiple comorbidities. Evaluating the efficacy of new drugs in DS preclinical models and mapping them to human phenotypes of DS through analysis of published literature is an important goal for improving outcomes in this rare pediatric epilepsy.
Materials and Methods
Large language models (LLM) have demonstrated great promise in parsing published literature; however, the performance of LLMs falls short in medical applications. In this study, we investigate the effectiveness of domain ontology developed by human experts to optimize LLMs for medical text processing in a rare disease. Utilizing a benchmark dataset that describes the efficacy of 17 ASMs tested in preclinical models and DS patients, we define a new ontology-augmented phased in-context learning (PCL) approach to process 4935 full-text DS articles. We expand this analysis to 7 new drugs that demonstrate efficacy in reducing seizures to identify gaps in current knowledge for designing new experimental studies for drug discovery in DS.
Results
Few-shot or in-context learning is a foundational capability of LLMs and the few-shot learning capability of the Gemini 1.0 Pro version LLM dramatically increases when we augment prompts with the DS epilepsy ontology. The DS epilepsy ontology is the largest epilepsy and seizure ontology in clinical use that was developed by DS basic scientists and clinical neurologists. The ontology-augmented PCL prompt achieves 100% accuracy in reproducing the benchmark drug efficacy dataset for 17 ASMs with only two examples for in-context learning.
Conclusion
The new ontology-augmented PCL approach significantly accelerates the few-shot learning capabilities of the Gemini LLM, thereby reducing the number of required examples and time needed to optimize LLMs for medical applications.}
}
@article{CHAN2025106305,
title = {Context-aware vision-language model agent enriched with domain-specific ontology for construction site safety monitoring},
journal = {Automation in Construction},
volume = {177},
pages = {106305},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106305},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525003450},
author = {Chak-Fu Chan and Peter Kok-Yiu Wong and Xiaowen Guo and Jack C.P. Cheng and Jolly Pui-Ching Chan and Pak-Him Leung and Xingyu Tao},
keywords = {Construction safety ontology, Construction site safety monitoring, Domain-tailored prompt engineering, Context-aware vision-language model, Virtual construction safety assistant},
abstract = {Traditional approaches of construction site safety monitoring heavily rely on manual on-site inspection, which are prone to overlooked incidents. Existing computer vision methods require time-consuming and case-by-case data labeling, and lack high-level reasoning capability. This paper develops a human-alike virtual assistant agent by integrating a multi-modal vision-language model into video analytics: (1) To efficiently generate image-text data for model development, a semi-automatic image-text labeling pipeline based on in-context learning is designed; (2) To optimize a virtual agent from pre-trained to domain-tailored, a two-stage curriculum learning paradigm is designed to enhance model fine-tuning effectiveness toward domain-specific tasks; (3) To inject construction-domain knowledge more effectively into the virtual agent, a hierarchical prompting framework driven by a construction safety ontology is developed for more domain-tailored reasoning capability. The virtual agent has been deployed on a real construction site for real-time video analytics, with over 90 % accuracy in identifying violations of work-at-height safety regulations.}
}
@article{SPEISER2025106293,
title = {From fragmented data to unified construction safety knowledge: A process-based ontology framework for safer work},
journal = {Automation in Construction},
volume = {176},
pages = {106293},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106293},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525003334},
author = {Kilian Speiser and Sebastian Seiß and Frank Boukamp and Jürgen Melzner and Jochen Teizer},
keywords = {Construction safety, Data interoperability, Hazard and mitigation planning, Knowledge management, Ontology development, Querying, Reasoning, Risk assessment, Semantic web, Unified ontology for construction safety},
abstract = {Effective knowledge management in construction safety is essential yet challenging. Despite emerging technologies to collect valuable data automatically, it continues to rely on manual input. The heterogeneity of data sources in construction makes it additionally difficult, resulting in a high number of incidents due to late changes in the design. Presented is a unified ontology for construction safety named UNOCS that shares safety knowledge between stakeholders during the construction processes. The UNOCS ontology follows the Linked Open Terms methodology and integrates established concepts, ensuring interoperability with other domain-specific knowledge for multiple use cases: (1) hazard and mitigation planning, (2) conformance checking and control, and (3) incident logging. UNOCS was evaluated through automatic consistency checks, criteria-based assessment, and task-based evaluation. The ontology meets the defined requirements and represents safety-related concepts. Implemented in a machine-readable format, it enables reasoning and seamless knowledge transfer between mitigation planning, safety inspections, and incident reporting.}
}
@article{ABELLANOSA2025,
title = {Integrating knowledge management and large language models to advance construction Job Hazard Analysis: A systematic review and conceptual framework},
journal = {Journal of Safety and Sustainability},
year = {2025},
issn = {2949-9267},
doi = {https://doi.org/10.1016/j.jsasus.2025.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S2949926725000332},
author = {Abbey Dale Abellanosa and Estacio Pereira and Lianne Lefsrud and Yasser Mohamed},
keywords = {Job Hazard Analysis, Knowledge management, Construction safety management, Large language models, Systematic review},
abstract = {Conducting a Job Hazard Analysis (JHA) remains essential for managing safety risks in construction; However, the process is often manual, subjective, and knowledge-intensive. While numerous studies have proposed tools and techniques to enhance JHA, a comprehensive synthesis through the lens of construction safety knowledge management (CSKM) has been lacking. This systematic review fills that gap by: (1) Critically examining recent advancements in JHA practices with a focus on how tacit and explicit safety knowledge is acquired, integrated, and applied; (2) Analyzing the emerging role of interoperable and semantic technologies – such as building information modeling (BIM), ontologies, knowledge graphs (KGs), and semantic reasoning – in supporting JHA through CSKM; and (3) Proposing a novel conceptual framework that outlines the potential integration of large language models (LLMs) to automate and enhance JHA processes. Using the preferred reporting items for systematic reviews and meta-analyses (PRISMA) methodology, 90 peer-reviewed studies were systematically reviewed and thematically analyzed. The results reveal actionable patterns in how digital technologies and knowledge management strategies are converging to address longstanding issues in hazard identification and decision-making. By embedding institutional knowledge into LLM-supported CSKM, this review contributes to developing safer, more adaptive, and ultimately more sustainable construction practices.}
}
@article{DU2025106362,
title = {Ontological reasoning for automated tunnel defect diagnosis and root cause identification},
journal = {Automation in Construction},
volume = {178},
pages = {106362},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106362},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525004029},
author = {Juan Du and Anshuang Yu and Vijayan Sugumaran and Yuqing Hu and Gang Yu and Jiajun Zhang},
keywords = {Ontology technology, Reasoning rules, Multisource data integration, Tunnel defect diagnosis, Root cause identification},
abstract = {Defect diagnosis and root cause identification are pivotal to safe tunnel operation and the formulation of maintenance strategies. Due to the heterogeneous and multisource nature of defect data, manual integration and analysis remain challenging and inefficient. This paper proposes a framework that includes a Tunnel Defect Analysis Ontology (TDAO) for knowledge extraction from heterogeneous data and a rule-based reasoning procedure that supports automated defect diagnosis and root cause identification. The framework's accuracy and efficiency are demonstrated through a case study on the Shanghai Yangtze River Tunnel, and its practical value is further confirmed by expert interviews. The result shows that the proposed approach offers an automated, economical, and high-efficiency solution that advances intelligent tunnel operation. Future studies can embed maintenance-decision knowledge and additional defect types into the ontology and implement automatic rule generation to enhance the framework's general applicability.}
}
@article{SILVA2025301845,
title = {An ontology for promoting controlled experimentation in digital forensics},
journal = {Forensic Science International: Digital Investigation},
volume = {52},
pages = {301845},
year = {2025},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2024.301845},
url = {https://www.sciencedirect.com/science/article/pii/S2666281724001720},
author = {Thiago J. Silva and Ana H.B. Mazur and Edson OliveiraJr and Avelino F. Zorzo and Monalessa P. Barcellos},
keywords = {Controlled experimentation, Digital forensics, Evidence, Ontology},
abstract = {Experimentation is a crucial method in empirical inquiry and is widely applied in Computer Science. Controlled experimentation ensures reproducibility, transparency, and reliability of findings, making the process more formal. Digital forensics (DF) lacks formalization of controlled experimental processes, leading to inadequate and informal research, making findings less transparent, reproducible, and reliable. Furthermore, existing works in this area often lack detailed descriptions of the controlled experimental decision-making procedures. To address these issues, we developed an ontology to formalize the concepts and terms used in DF-controlled experiments. The ontology was constructed based on an existing conceptual model for DF-controlled experiments. The ontology's conceptual model is represented by UML class diagrams, and the OWL language was employed to code it. Moreover, the ontology underwent evaluation by researchers and experts in DF experimentation, with the results indicating the capability of the ontology to formalize DF experimental concepts. The contribution of this ontology is to assist DF researchers and practitioners in properly documenting their controlled experiments. This will enhance the formality of the experimental process and promote the findings' reproducibility, transparency, and reliability. For researchers, the ontology's main contribution lies in influencing how these experiments are conducted, potentially impacting their transfer to industry. Practitioners stand to benefit by adopting formal experimental procedures for testing, assessing, and acquiring DF-related technology.}
}
@article{CASTIGLIONE2025104617,
title = {Guiding cybersecurity compliance: An ontology for the NIS 2 directive},
journal = {Computers & Security},
volume = {157},
pages = {104617},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2025.104617},
url = {https://www.sciencedirect.com/science/article/pii/S0167404825003062},
author = {Gianpietro Castiglione and Daniele Francesco Santamaria and Giampaolo Bella and Laura Brisindi and Gaetano Puccia},
keywords = {Semantic Web, Ontology, NIS 2, Compliance, Cybersecurity},
abstract = {Security compliance constitutes a significant source of concern for many corporate decision-makers due to its complexity and cost. These may be due, first and foremost, to the style of juridical language, which is often challenging to translate into concrete operational procedures. To facilitate such a translation and ultimately optimise the compliance effort, this article presents “NIS2Onto”, an Web Ontology Language (OWL) ontology designed to translate the Network and Information Security Directive version 2 (NIS 2) into an ontological format aimed to favour unambiguous understanding and security operations of cybersecurity professionals, legal experts, and all organisational stakeholders. Through the semantic representation of the NIS 2 entities, relationships, and security measures, NIS2Onto enables automated compliance verification, streamlined risk assessments, and effective policy implementation. Our evaluation employs both metrical and qualitative analysis through a real case study to witness the robustness and practical applicability of NIS2Onto. The ontology not only supports the accurate interpretation of complex legal texts but also aids in systematically enforcing cybersecurity measures. Furthermore, the extensibility of NIS2Onto allows for integration with other regulatory frameworks, thereby fostering a comprehensive and unified approach to cybersecurity governance.}
}
@article{MELO20251649,
title = {Towards an ontology on project portfolio management},
journal = {Procedia Computer Science},
volume = {256},
pages = {1649-1657},
year = {2025},
note = {CENTERIS - International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.02.302},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925006702},
author = {Héctor Melo and Oscar Avila and María del Pilar Villamil},
keywords = {Ontology, portfolio, project, management},
abstract = {Project Portfolio Management (PPM) is essential for organizations aiming to align projects with strategic goals. Different organizations adopt diverse PPM frameworks and standards to manage project portfolios each employing its own terminology. This semantic heterogeneity leads to communication barriers, knowledge silos, and difficulty in integrating information across various platforms in an interorganizational context. This article proposes a PPM ontology to establish a common language encapsulating key concept, addressing this challenge. The methodology involves systematic revision of three prominent PPM standards - ISO 21504, PMI Standard for Portfolio Management, and AXELOS Management of Portfolios, and employs a novel algorithm to identify equivalences and containment relationships between terms across all three standards, resolving semantic ambiguities and enriching the ontology’s expressiveness. This contribution benefits researchers, academics, portfolio managers, project managers, and PPM practitioners by providing a common vocabulary and framework for understanding and improving PPM practices, facilitates knowledge representation, improves communication and collaboration among stakeholders, and lays the groundwork for developing intelligent PPM systems capable of leveraging shared semantic understanding.}
}
@article{MARTINLAMMERDING2025114074,
title = {Dronetology: A domain ontology for UAS applications},
journal = {Knowledge-Based Systems},
volume = {327},
pages = {114074},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114074},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125011190},
author = {David Martín-Lammerding and José Javier Astrain and Alberto Córdoba},
keywords = {Ontology, Semantic web, Unmanned aerial system, UAS, Unmanned aerial vehicle, UAV, Semantic modelling, Autonomous system, Expert system},
abstract = {The increasing importance of Unmanned Aerial Systems (UAS) has led to the development of various applications, resulting in significant fragmentation in technologies, components, and data formats. The UAS domain must meet stringent requirements regarding safety, regulatory compliance, and interoperability with other systems. It also faces challenges such as efficiency, airspace sharing, scalability, awareness of other aircraft, and effective data analysis. Semantic technologies provide solutions for these challenges, offering interoperability and intelligent decision-making. Ontologies stand out for their ability to model knowledge and facilitate understanding within the UAS domain. This paper introduces Dronetology, https://dronetology.net/dronetology, a domain ontology designed to provide a common framework for UAS concepts. It aims to improve interoperability, data quality, and knowledge integration. Dronetology has been implemented in a decision support prototype using an expert system architecture to address complex UAS use cases, including flight efficiency, collision avoidance, and compliance with regulations. We also outline how Dronetology can be specialized to create application ontologies for specific UAS needs.}
}
@incollection{AGAPITO2025374,
title = {Ontology: Querying Languages and Development},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {374-379},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00152-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027001524},
author = {Giuseppe Agapito and Pietro Cinaglia},
keywords = {Biological ontology, BioPAX, Ontology, OWL, RDF, SPARQL},
abstract = {Ontologies are a way to organize knowledge in different areas, allowing for seamless automatic reasoning and data integration. This paper focuses on ontology querying languages and development tools, particularly the SPARQL query language and its use in querying RDF data. We׳ll cover SPARQL׳s key features and provide examples to demonstrate its capabilities. Additionally, we׳ll give an overview of popular ontology management tools, emphasizing their importance in creating, editing, visualizing, and managing ontological structures. This paper aims to offer a comprehensive understanding of ontology querying languages and development tools, as well as insights into their real-world applications.}
}
@article{YIN2023107066,
title = {An ontology-aided, natural language-based approach for multi-constraint BIM model querying},
journal = {Journal of Building Engineering},
volume = {76},
pages = {107066},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.107066},
url = {https://www.sciencedirect.com/science/article/pii/S2352710223012457},
author = {Mengtian Yin and Llewellyn Tang and Chris Webster and Shen Xu and Xiongyi Li and Huaquan Ying},
keywords = {Building information modeling (BIM), Data query, Project information retrieval, Natural language processing (NLP), Semantic web technologies},
abstract = {Construction project stakeholders often have to retrieve the required information in Building Information Models (BIMs) to support their design, engineering, and management activities. Natural language interface (NLI) systems are emerging as a time- and cost-effective way to query complex BIM models. However, the existing attempts cannot logically combine different constraints to perform fine-grained queries, dampening the usability of BIM-oriented NLIs. This paper presents a novel ontology-aided semantic parser to automatically map natural language queries (NLQs) that contain different attribute and relational constraints into computer-readable codes for BIM model retrieval in the context of building project development. A modular ontology was first developed to represent natural language expressions of Industry Foundation Classes (IFC) concepts, relationships, and reasoning rules; it was then populated with entities from target BIM models to assimilate project-specific information. After that, the ontology-aided semantic parser progressively extracts concepts, relationships, and value restrictions from NLQs to identify multi-level constraint conditions, resulting in standard SPARQL queries to successfully retrieve IFC-based BIM models. The approach was evaluated based on 225 NLQs collected from BIM users, with a 91% accuracy rate. Finally, a case study about the design-checking of a real-world residential building demonstrates the practicability of the proposed method in the construction industry.}
}
@article{WANG2024361,
title = {Ontology-integrated tuning of large language model for intelligent maintenance},
journal = {CIRP Annals},
volume = {73},
number = {1},
pages = {361-364},
year = {2024},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2024.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S000785062400026X},
author = {Peng Wang and John Karigiannis and Robert X. Gao},
keywords = {Maintenance, Machine learning, Large language models},
abstract = {As new AI technologies such as Large Language Models (LLM) quickly evolve, the need for enhancing general-purpose LLMs with physical knowledge to better serve the manufacturing community has been increasingly recognized. This paper presents a method that tailors GPT-3.5 with domain-specific knowledge for intelligent aircraft maintenance. Specifically, aircraft ontology is investigated to curate maintenance logs with encoded component hierarchical structure to fine-tune GPT-3.5. Experimental results demonstrate the effectiveness of the developed method in accurately identifying defective components and providing consistent maintenance action recommendations, outperforming general-purpose GPT-3.5 and GPT-4.0. The method can be adapted to other domains in manufacturing and beyond.}
}
@article{TIAN2025125650,
title = {Guiding ontology translation with hubness-aware translation memory},
journal = {Expert Systems with Applications},
volume = {264},
pages = {125650},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125650},
url = {https://www.sciencedirect.com/science/article/pii/S095741742402517X},
author = {Mingjie Tian and Fausto Giunchiglia and Rui Song and Hao Xu},
keywords = {Ontology translation, Translation memory, Cross-lingual retrieval, Hubness issue, Adversarial learning},
abstract = {Ontology, as the foundational architecture for knowledge representation, necessitates multilingualization to facilitate cross-lingual knowledge sharing, posing challenges that require a domain-specific translation system capable of producing credible translations due to its specialized vocabulary and limited context. This work presents an approach to model and inject reference knowledge to enhance ontology translation by integrating translation memory-augmented neural machine translation, aiming to solve the problem of insufficient coverage that traditional neural machine translation cannot solve. Firstly, this work enhances TM retrieval from monolingual to cross-lingual, utilizing meaning equivalents in parallel data to enrich bidirectional context. Secondly, this work proposes a novel retrieval measurement to mitigate the hubness issue that occurs in similarity-based greedy search retrieval. Furthermore, this work introduces a cross-lingual agreement matching task and an adversarial learning task to enhance the retrieval and translation models in translation memory-augmented neural machine translation. Experimental results and analysis demonstrate the effectiveness of the proposed approach, outperforming strong baselines across four domains’ ontology datasets in two language pair directions.}
}
@article{MOAYYED2025106209,
title = {Systematic analysis of large language models for automating document-to-smart contract transformation},
journal = {Automation in Construction},
volume = {175},
pages = {106209},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106209},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525002493},
author = {Erfan Moayyed and Chimay Anumba and Azita Morteza},
keywords = {Large Language Models (LLMs), Smart Contracts, Document Automation, Data Conversion, Blockchain, NLP, Systematic Review},
abstract = {Fragmentation and poor collaboration in contract-heavy industries hinder innovation. While smart contracts offer promising automation for digital documents, the transformation process presents significant challenges. Current approaches are promising but are often constrained by technical limitations, domain-specific requirements, and limited flexibility, restricting widespread adoption. This paper systematically reviews the development of smart contracts using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework to examine methodologies, challenges, and solutions through a thematic analysis of 30 key studies. The findings are grouped into three categories: Natural Language Processing (NLP)-based, template-based and ontology-based, and model-driven approaches. After analyzing the cross-industrial challenges of each category, this paper proposes a Large Language Model (LLM)-based smart contract generation solution to address the identified challenges validated through real-world use cases. This comprehensive analysis contributes to the ongoing dialogue on smart contracting, offering directions for future research and practical implementation in the digital infrastructure.}
}
@article{DENG2025112562,
title = {An ontology-based approach to dynamic indoor fire emergency evacuation path planning with BIM integration},
journal = {Journal of Building Engineering},
volume = {106},
pages = {112562},
year = {2025},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2025.112562},
url = {https://www.sciencedirect.com/science/article/pii/S2352710225007995},
author = {Hui Deng and Zihao Fan and Xinyi Wei and Yichuan Deng},
keywords = {Indoor fire emergency evacuation, Ontology, Dynamic information integration, Semantic enrichment, Path planning},
abstract = {The collection and updating of critical fire information remain key challenges in indoor fire emergency evacuation. This paper presents an innovative approach that integrates an Indoor Fire Emergency Evacuation Ontology (IFEEont) with Building Information Model (BIM). IFEEont is structured into five major categories, encapsulating key elements of fire evacuation. Unlike existing ontologies, it fully utilizes reasoning capabilities to infer and update node accessibility, a crucial factor in fire escape planning. A multi-sensor semantic model collects fire scene data, while ontology-based reasoning enables dynamic updates. Fire simulations conducted in an office building validate the effectiveness of the proposed method. Case study results demonstrate that the method not only captures critical fire information but also employs ontology-based inference to update node accessibility within the indoor geometric network model (GNM), effectively supporting escape route planning. Beyond serving as a fire knowledge base, the proposed ontology dynamically models fire information. This integrated approach—combining ontology, sensors, and BIM-based information updates—continuously optimizes evacuation strategies, enhances the accuracy and responsiveness of escape routes.}
}
@article{GAN2025106739,
title = {Ontology-driven knowledge graph for decision-making in resilience enhancement of underground structures: Framework and application},
journal = {Tunnelling and Underground Space Technology},
volume = {163},
pages = {106739},
year = {2025},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2025.106739},
url = {https://www.sciencedirect.com/science/article/pii/S0886779825003773},
author = {Bin-Lin Gan and Dong-Mei Zhang and Zhong-Kai Huang and Fei-Yu Zheng and Rui Zhu and Wei Zhang},
keywords = {Underground structures, Resilience enhancement, Knowledge graph, Ontology knowledge, Decision-making},
abstract = {Enhancing the resilience of underground structures for their operation safety amidst complex disasters has become a critical societal issue. However, resilience enhancement decision-making for underground structures mainly depends on practical subjective experience currently, with insufficient integration of ontology knowledge and a clear gap in the availability of efficient and intelligent decision-making models. To address this, this paper presents a novel method for constructing a knowledge graph (KG) based on ontology to enhance the resilience of underground structures. A comprehensive resilience knowledge system considering 10 categories for underground structures is established. This system is built upon resilience quantification analysis, fault tree modeling of resilience insufficiency, and event tree analysis of disaster chain processes. A systematic approach for KG construction, integrating top-down and bottom-up strategies, is then proposed. Additionally, a multi-layered framework of KG for underground structure resilience is developed, comprising application, rule, pattern, and data layers. Resilience-related knowledge is extracted using expert empirical methods, and the data layer is constructed through semantic networks and knowledge fusion. The visualization and field application of the KG are implemented using the Neo4j graph database. Findings of this study substantially advance a methodological foundation for intelligent decision-making in resilience enhancement and safeguarding of underground infrastructures under complex disasters.}
}
@article{UTKUCU2025106197,
title = {Ontology for holistic building performance modeling and analysis},
journal = {Automation in Construction},
volume = {175},
pages = {106197},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106197},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525002377},
author = {Duygu Utkucu and Rafael Sacks},
keywords = {Building performance modeling, Building performance simulation software, Building information modeling, Information representation, Data schema, Ontology},
abstract = {Building performance modeling and analysis using Building Information Modeling (BIM) platforms remains fragmented, requiring various software applications to address different disciplines. Challenges in data extraction, transfer, and integration arise due to inconsistencies in vendor-specific data schemas and limited interoperability. Moreover, OpenBIM data schemas lack comprehensive object definitions and semantics, compromising data integrity. While ontological frameworks have been proposed to address these issues, a unified ontology that integrates multiple performance disciplines has yet to be developed. This paper designed and developed a holistic building performance ontology (HBPO) focusing on acoustic, lighting, and energy domains as subsets to represent a range of sufficiently different domains. This ontology comprises 28 classes, 26 object properties, and 183 data properties, encapsulating essential information, data requirements, and object relationships within and across these domains. Additionally, a series of proof-of-concept experiments were conducted to test, demonstrate, validate, and evaluate the feasibility and applicability of the HBPO.}
}
@article{WU2024100619,
title = {Design ontology for cognitive thread supporting traceability management in model-based systems engineering},
journal = {Journal of Industrial Information Integration},
volume = {40},
pages = {100619},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100619},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24000633},
author = {Shouxuan Wu and Guoxin Wang and Jinzhi Lu and Zhenchao Hu and Yan Yan and Dimitris Kiritsis},
keywords = {Cognitive thread, Digital thread, Traceability management, Model-based systems engineering, Open service for lifecycle collaboration, KARMA language},
abstract = {Industrial information integration engineering (IIIE) is an interdisciplinary field to facilitate the industrial information integration process. In the age of complex and large-scale systems, model-based systems engineering (MBSE) is widely adopted in industry to support IIIE. Traceability management is considered the foundation of information management in MBSE. However, a lack of integration between stakeholders, development processes, and models can decrease the effectiveness and efficiency of the system development. A modified MBSE toolchain prototype has been developed to implement traceability management; however, a lack of formal and structured specifications makes it difficult to describe the complex topology in traceability management scenarios using this MBSE toolchain, such as creating traceability between heterogeneous models, which leads to poor reusability of this MBSE toolchain in other traceability management scenarios. To formalize traceability management scenarios using the MBSE toolchain, a cognitive thread (CT) ontology is developed in this study. The CT ontology is a specification expressing the information of stakeholders, models, and development processes for traceability management, providing the cognition capability to analyze the interrelationships between them. Based on the implementation of the modified MBSE toolchain, the concepts and interrelationships in the CT ontology are identified. The CT ontology is designed to develop the MBSE toolchain prototype for building, managing, and analyzing traceability in various traceability management scenarios. A case study of an adaptive cruise control system design is used to evaluate the completeness of the CT ontology through qualitative and quantitative analyses. The results demonstrate that the proposed CT ontology formalizes the information related to traceability management while using the proposed MBSE toolchain and can also be used in common traceability management scenarios to design other complex engineered systems.}
}
@article{OCHOA2025128792,
title = {I40GO: A global ontology for industry 4.0},
journal = {Expert Systems with Applications},
volume = {294},
pages = {128792},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.128792},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425024108},
author = {William Ochoa and Javier Cuenca and Felix Larrinaga and Alain Pérez},
keywords = {Semantic ontology, Ontology reuse, Industry 4.0, Context awareness, Workflow management},
abstract = {Over the last two decades, semantic ontologies have been developed to represent manufacturing data across various domains. These ontologies constitute the knowledge base of manufacturing management systems, which primarily focus on optimizing the manufacturing process and improving its resilience. The ontologies developed in the Industry 4.0 domain are heterogeneous, hindering the interoperability of machines, devices, and applications composing manufacturing systems. Consequently, a demand arises for an ontology that provides common vocabularies to represent the data domains inherent to Industry 4.0. A global Industry 4.0 ontology must be easily reusable in different application contexts. This paper presents I40GO: a global ontology tailored to the Industry 4.0 domain. I40GO structures in layers and modules the knowledge represented in the Industry 4.0 most relevant ontologies. The MODDALS methodology is followed to classify knowledge into different layers. This methodology classifies ontology knowledge into common, variant, and application-specific layers following a similar approach to that of Software Product Lines (SPL). I40GO assists ontology engineers in developing domain-specific ontologies for manufacturing systems and enhances interoperability among applications. This work provides an overview of I40GO, emphasizing its development methodology and its modular and layered structure. Furthermore, it demonstrates the reuse of the I40GO ontology within an Industry 4.0 use case—an architecture for context-aware workflow management.}
}
@article{LANGE2025100330,
title = {Ontologies Relevant for Improving Data Interoperability for Food Loss and Waste: A Review and Research Agenda},
journal = {Cleaner and Responsible Consumption},
pages = {100330},
year = {2025},
issn = {2666-7843},
doi = {https://doi.org/10.1016/j.clrc.2025.100330},
url = {https://www.sciencedirect.com/science/article/pii/S2666784325000816},
author = {Matthew C. Lange and Ran Li and John W. Apolzan and Patrick R. Huber and Emily Steliotes and Kai Robertson and Norbert L.W. Wilson and Karthik Jain and Rajiv Ramnath and Brian E. Roe and Edward S. Spang},
keywords = {ontology, food loss and waste, data interoperability, food system, large language models},
abstract = {Food loss and waste (FLW) is a global challenge. Interoperable FLW ontologies will foster more comprehensive data sharing and inform better solutions to reduce and recover excess food and to valorize wasted food and food byproducts. This review reveals that only eight ontologies currently address FLW with most emphasizing valorization. Notably, few are designed explicitly to support FLW reduction, and none facilitate food recovery, which is critical given that reduction and recovery are the preferred means of mitigating FLW. Furthermore, existing FLW ontologies show limited alignment with recognized gold-standard frameworks, for example the Open Biological and Biomedical Ontology (OBO) Foundry, and none support ongoing connectivity to external ontologies, restricting their utility across stakeholder domains. Looking ahead, there is a pressing need to create or expand ontologies that adhere to best practices from relevant foundries to ensure robust linkage and interoperability and undergird structured data ecosystems that support food systems stakeholders in FLW prevention and mitigation. Achieving this goal will require active collaboration among a diverse range of stakeholders, including builders of food systems cyberinfrastructure, scientists, innovators, regulators, public and private funders, community-based organizations, policymakers, and international NGOs as each rely on critical ontological elements to inform decision-making, measure impact, and drive improvement across the food supply chain. Finally, large language models offer promising capabilities for expediting ontology creation, broadening inclusivity in ontology creation, and enhancing the accuracy of resulting data infrastructures.}
}
@article{KULIKOV2025129622,
title = {An approach for linking dynamic network information models based on ontology matching},
journal = {Expert Systems with Applications},
pages = {129622},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.129622},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425032373},
author = {Igor Kulikov and Jiafeng Yang and Nataly Zhukova and Tianxing Man},
keywords = {Dynamic networks, Ontology matching, Knowledge graphs, Context-aware matching, Telecommunications networks},
abstract = {Dynamic network information models are typically heterogeneous and isolated systems that impede effective interoperability, significantly hindering end-to-end service integration and data sharing across network segments. To address this challenge, we propose a new approach for linking heterogeneous dynamic network models based on ontology matching, which can be applied in various domains utilizing dynamic networks. For ontologies matching we use different existing duplicate detection algorithms but we reduce the computational complexity of ontology matching due to splitting initial set of matched entities into a number of subsets using domain knowledge. Using telecommunications as case study, we represent operator networks as knowledge graphs and match them with standardized model ontologies using business process context to create an Extended Operator Network Ontology. Our approach ensures linking of dynamic network models used in operators information systems that is of primary importance for implementing complex business processes, and providing integrated services while maintaining existing models.}
}
@article{XU202449,
title = {GeoPredict-LLM: Intelligent tunnel advanced geological prediction by reprogramming large language models},
journal = {Intelligent Geoengineering},
volume = {1},
number = {1},
pages = {49-57},
year = {2024},
issn = {3050-6190},
doi = {https://doi.org/10.1016/j.ige.2024.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S3050619024000053},
author = {Zhenhao Xu and Zhaoyang Wang and Shucai Li and Xiao Zhang and Peng Lin},
keywords = {Advanced geological prediction, Large language model, Data diffusion, Multisource data, Multimodal data, Knowledge graph},
abstract = {With the improvement of multisource information sensing and data acquisition capabilities inside tunnels, the availability of multimodal data in tunnel engineering has significantly increased. However, due to structural differences in multimodal data, traditional intelligent advanced geological prediction models have limited capacity for data fusion. Furthermore, the lack of pre-trained models makes it difficult for neural networks trained from scratch to deeply explore the features of multimodal data. To address these challenges, we utilize the fusion capability of knowledge graph for multimodal data and the pre-trained knowledge of large language models (LLMs) to establish an intelligent advanced geological prediction model (GeoPredict-LLM). First, we develop an advanced geological prediction ontology model, forming a knowledge graph database. Using knowledge graph embeddings, multisource and multimodal data are transformed into low-dimensional vectors with a unified structure. Secondly, pre-trained LLMs, through reprogramming, reconstruct these low-dimensional vectors, imparting linguistic characteristics to the data. This transformation effectively reframes the complex task of advanced geological prediction as a "language-based" problem, enabling the model to approach the task from a linguistic perspective. Moreover, we propose the prompt-as-prefix method, which enables output generation, while freezing the core of the LLM, thereby significantly reduces the number of training parameters. Finally, evaluations show that compared to neural network models without pre-trained models, GeoPredict-LLM significantly improves prediction accuracy. It is worth noting that as long as a knowledge graph database can be established, GeoPredict-LLM can be adapted to multimodal data mining tasks with minimal modifications.}
}
@article{RAJESWARI2025106810,
title = {Ontological modeling with recursive recurrent neural network and crayfish optimization for reliable breast cancer prediction},
journal = {Biomedical Signal Processing and Control},
volume = {99},
pages = {106810},
year = {2025},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.106810},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424008681},
author = {V. Rajeswari and K. {Sakthi Priya}},
keywords = {Crayfish optimization algorithm, Ontology model, Recursive Recurrent Neural Network, Semantic Web Rule Language, Spline Adaptive Filtering Algorithm},
abstract = {Breast cancer is predominantly a female illness, even though it affects men less frequently than women. Breast cancer prediction provides significant challenges in the medical field due to the complex and heterogeneous nature of disease. Traditional methods for breast cancer prediction, such as statistical analysis and conventional machine learning algorithms, often fall short in delivering high accuracy and reliability. In this manuscript, Ontological Modeling with Recursive Recurrent Neural Network and Crayfish Optimization for Reliable Breast Cancer Prediction (OM-RRNN-CFO-RBCP) is proposed to develop the ontological based breast cancer prediction model. The input data is collected from Breast Cancer Wisconsin dataset. The input data is pre-processed using SAFA for noise removal. Then, the pre-processed data are given to RRNN to categorize the data as Benign and Malignant. For accurate classification the RRNN weight parameter is optimized with Crayfish optimization algorithm (CFO). The ontology technique is built using Semantic Web Language Rules (SWRL) extracted from RRNN. Finally, the ontology classifier predict the data as Benign and Malignant. The proposed method is examined using performance metrics like f-score, accuracy, precision, and sensitivity are examined. The proposed OM-RRNN-CFO-RBCP method attains 23.54%, 21.43% and 21.76% higher Accuracy, 31.23%, 32.41% and 31.25% higher Precision when compared with existing techniques like effectiveness of applying Machine Learning techniques and Ontologies in Breast Cancer detection (EA-MLT-OBCD), a new enhanced hybrid clinical decision support scheme for appropriate breast cancer prediction (NE-HCDS-ABCP) and an enhanced soft-computing based technique for effectual feature selection for timely breast cancer prediction (ESC-EFS-TBCP-WDBC) respectively.}
}
@article{ROSNER2025,
title = {An Ontology for Digital Medicine Outcomes: Development of the Digital Medicine Outcomes Value Set (DOVeS)},
journal = {JMIR Medical Informatics},
volume = {13},
year = {2025},
issn = {2291-9694},
doi = {https://doi.org/10.2196/67589},
url = {https://www.sciencedirect.com/science/article/pii/S2291969425000250},
author = {Benjamin Rosner and Matthew Horridge and Guillen Austria and Tiffany Lee and Andrew Auerbach},
keywords = {digital health, digital medicine, digital therapeutics, ontology, medical informatics, value set, ontology, development, digital health tool, DHT, health systems, digital medicine outcomes value set, prototype, users},
abstract = {Background
Over the last 10-15 years, US health care and the practice of medicine itself have been transformed by a proliferation of digital medicine and digital therapeutic products (collectively, digital health tools [DHTs]). While a number of DHT classifications have been proposed to help organize these tools for discovery, retrieval, and comparison by health care organizations seeking to potentially implement them, none have specifically addressed that organizations considering their implementation approach the DHT discovery process with one or more specific outcomes in mind. An outcomes-based DHT ontology could therefore be valuable not only for health systems seeking to evaluate tools that influence certain outcomes, but also for regulators and vendors seeking to ascertain potential substantial equivalence to predicate devices.
Objective
This study aimed to develop, with inputs from industry, health care providers, payers, regulatory bodies, and patients through the Accelerated Digital Clinical Ecosystem (ADviCE) consortium, an ontology specific to DHT outcomes, the Digital medicine Outcomes Value Set (DOVeS), and to make this ontology publicly available and free to use.
Methods
From a starting point of a 4-generation–deep hierarchical taxonomy developed by ADviCE, we developed DOVeS using the Web Ontology Language through the open-source ontology editor Protégé, and data from 185 vendors who had submitted structured product information to ADviCE. We used a custom, decentralized, collaborative ontology engineering methodology, and were guided by Open Biological and Biomedical Ontologies (OBO) Foundry principles. We incorporated the Mondo Disease Ontology (MONDO) and the Ontology of Adverse Events. After development, DOVeS was field-tested between December 2022 and May 2023 with 40 additional independent vendors previously unfamiliar with ADviCE or DOVeS. As a proof of concept, we subsequently developed a prototype DHT Application Finder leveraging DOVeS to enable a user to query for DHT products based on specific outcomes of interest.
Results
In its current state, DOVeS contains 42,320 and 9481 native axioms and distinct classes, respectively. These numbers are enhanced when taking into account the axioms and classes contributed by MONDO and the Ontology of Adverse Events.
Conclusions
DOVeS is publicly available on BioPortal and GitHub, and has a Creative Commons license CC-BY-SA that is intended to encourage stakeholders to modify, adapt, build upon, and distribute it. While no ontology is complete, DOVeS will benefit from a strong and engaged user base to help it grow and evolve in a way that best serves DHT stakeholders and the patients they serve.}
}
@article{LUNIG2025103650,
title = {Reducing construction quality costs through ontology-based inspection planning},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103650},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103650},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625005439},
author = {Jan Niklas Lünig and Sebastian Seiß and Patrick Schwerdtner and Jürgen Melzner},
keywords = {Inspection planning, Quality assurance, Total quality cost, Appraisal costs, Nonconformity costs, Ontology, Knowledge representation},
abstract = {Building construction constitutes a specialized and project-specific procedure, encompassing numerous risks. In particular, risks related to execution and the associated costs of defects are a constant challenge for the architectural engineering and construction (AEC) industry. To achieve a balance between the optimum level of quality and the associated costs, knowledge of the composition of quality-related costs is required. In this paper, the objective is to automatically determine expected appraisal costs (ACs) and nonconformity costs (NCCs) from building information modeling (BIM) objects by using semantic web technologies. The ontology for construction quality assurance (OCQA), which is used for the automated planning of quality inspections serves as the initial ontology in this paper. Based on model data, historical quality data, and potential NCCs, the expected quality defects are determined and evaluated using shapes constraint language (SHACL) rules, with the ACs for the corresponding preventive inspections calculated simultaneously. The functionality of the ontology and its rules is demonstrated by defect type unevenness of screed.}
}
@article{TURCHET2025100871,
title = {The Musician’s Context Ontology: Modeling the context for smart musical applications},
journal = {Journal of Web Semantics},
volume = {87},
pages = {100871},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2025.100871},
url = {https://www.sciencedirect.com/science/article/pii/S1570826825000125},
author = {Luca Turchet and Jacopo Tomelleri and Andrea Molinari and Paolo Bouquet},
keywords = {Semantic audio, Smart musical instruments, Internet of Musical Things, Context-aware computing, Music information retrieval},
abstract = {The paradigm of context-aware computing allows storing situational and environmental information in such a way that its interpretation can be done easily and more meaningfully. In turn, this understanding is used to anticipate users’ needs, and proactively provide them with situation-aware content and experiences. Whereas context-awareness has been investigated extensively in the computer science and IoT disciplines, it has been largely overlooked by the research community dealing with musical interfaces design. Existing musical instruments are not equipped with the ability to understand the context around them, namely who is the musician playing them, what musical activity is being conducted, as well as where and when. Enhancing musical instruments with context-awareness has the concrete potential to enable novel kinds of interactions between musicians and musical content in a large variety of situations, from playing alone to playing in a group, from music learning to music composition. To accomplish such a vision of intelligence embedded in musical instruments it is necessary to model the context around their users. In this paper, we present an ontology devised to represent the knowledge related to musicians and musical activities, the “Musician’s Context Ontology” (MUSICO) to facilitate the development of context-aware musical applications. There was no previous comprehensive data model for the domain of musicians’ context, nevertheless, the new ontology relates to several existing ontologies, including the Internet of Musical Things Ontology to represent Internet of Musical Things ecosystems and the Music Ontology that deals with the description of the music value-chain from production to consumption. This paper documents the design of the ontology and its evaluation with respect to specific requirements gathered from an extensive literature review and interviews with musicians. The utility of the ontology is demonstrated by a smartphone application that enables to search for musicians based on both textual and content-based musical queries. MUSICO can be accessed at: https://w3id.org/musico#.}
}
@article{ALHARBI2024100659,
title = {An ontology-based agriculture decision-support system with an evidence-based explanation model},
journal = {Smart Agricultural Technology},
volume = {9},
pages = {100659},
year = {2024},
issn = {2772-3755},
doi = {https://doi.org/10.1016/j.atech.2024.100659},
url = {https://www.sciencedirect.com/science/article/pii/S2772375524002648},
author = {Amani Falah Alharbi and Muhammad Ahtisham Aslam and Khalid Ali Asiry and Naif Radi Aljohani and Yury Glikman},
keywords = {Ontology modeling, Decision support systems, Machine reasoning, Smart agriculture, Semantic-web},
abstract = {Effective management of plant diseases and pests requires knowledge that covers multiple domains. At the same time, retrieving the relevant information in a timely manner is always challenging, due to the unstructured nature of agricultural data. Over the years, efforts have been made to develop an ontology-based Decision-Support System (DSS) to facilitate the diagnosis and control of plant diseases. Some major issues with these systems are that: (1) they do not adopt the full extent of the ontological constructs to represent domain entities, which, in turn, reduces reasoning capabilities and prevents systems from being more intelligent, (2) they do not adequately provide the desired level of knowledge to support complex decisions, which requires many factors to be considered, (3) they do not adequately explain or provide evidence to demonstrate the validity of the system's outputs. To address these limitations, we present a novel system termed Agriculture Ontology Based Decision Support System (AgrODSS), which aims to assist in plant disease and pest identification and control. AgrODSS architecture consists of two semantic-based models. First, we developed Plant Diseases and Pests Ontology (PDP-O) to capture, model, and represent diseases and pest knowledge in a machine-understandable format. Second, we designed and developed an Evidence-Based Explanation Model (EBEM) that points to related evidence from the literature to demonstrate the validity of the system outputs. We demonstrate the effectiveness of AgrODSS by executing various queries via AgrODSS SPARQL Endpoint and obtaining valuable information to support decision-making. Finally, we evaluated AgrODSS practically with domain experts (including entomologists and pathologists) and it produced similar answers to those given by the experts, with an overall accuracy of 80.66%. These results demonstrate AgrODSS's ability to assist agricultural stakeholders in making proper disease or pest diagnoses and choosing the appropriate control methods.}
}
@article{DEMURO2025101743,
title = {Language ontologies and the worlding of language(s)/languaging: does language create the world or does worlding create language?},
journal = {Language Sciences},
volume = {111},
pages = {101743},
year = {2025},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2025.101743},
url = {https://www.sciencedirect.com/science/article/pii/S0388000125000385},
author = {Eugenia Demuro and Laura Gurney},
keywords = {Language ontologies, language(s)/languaging, Ethnographic encounter, Posthumanism, Languaging, Assemblage},
abstract = {This article argues for the importance of understanding language ontologies in applied linguistics. We draw on a range of related fields to further develop this framework, questioning the notion of a singular and homogeneous understanding of language. We not only move beyond a monolingual focus to embrace a more inclusive understanding of language(s) and their sociocultural underpinnings, but, drawing on the ‘ontological turn’ and its exploration of diverse worlds, we theorise language as a dynamic assemblage of elements that emerges uniquely in each performance or practice, grounded in specific contexts. Our contribution aims to expand the understanding of language as multifaceted and ever evolving phenomena, reflecting on a range of themes and questions, including how we might pluralise language within Western modernity. Is language the same across all contexts of contemporary western societies or within Western modernity? Does language apply only to human groups? How do nonhuman others perform language(s)/languaging? In challenging existing assumptions regarding what language is or what it might be, our research invites others to explore new possibilities in their examination of language(s), languaging, and semiotic practices.}
}
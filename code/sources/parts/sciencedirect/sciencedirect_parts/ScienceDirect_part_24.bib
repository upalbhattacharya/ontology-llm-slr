@article{VALLE2025107736,
title = {Unraveling the pain points of domain modeling},
journal = {Information and Software Technology},
volume = {183},
pages = {107736},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107736},
url = {https://www.sciencedirect.com/science/article/pii/S0950584925000758},
author = {Isadora Valle and Tiago Prince Sales and Eduardo Guerra and Maya Daneva and Renata Guizzardi and Luiz Olavo {Bonino da Silva Santos} and Henderik A. Proper and Giancarlo Guizzardi},
keywords = {Conceptual modeling, Pain points, Modeling costs, Customer journey map, Multi-method research approach},
abstract = {Conceptual models offer numerous benefits but require significant investments, requiring modelers to strive to balance costs and benefits. Understanding the modeling process and the frustrations experienced by modelers can provide valuable insights for this assessment. While research acknowledges certain instances of modelers’ dissatisfaction, its scope often limits detailed examination. This study seeks to identify and analyze the main pain points associated with domain modeling through a five-phase empirical study using a multi-method approach. We identified 71 pain points, synthesized them to 41, and prioritized 16 as the most significant and prevalent in domain modeling. We then refined, documented, and exemplified the prioritized pain points, analyzed their potential causes, and discussed their practical implications. Our findings provide valuable insights for improving modelers’ experiences and optimizing the modeling process.}
}
@article{NAJAFABADI2025,
title = {Hybrid AI in synthetic biology: next era in agriculture},
journal = {Trends in Plant Science},
year = {2025},
issn = {1360-1385},
doi = {https://doi.org/10.1016/j.tplants.2025.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S1360138525002377},
author = {Mohsen Yoosefzadeh Najafabadi and Scott A. Jackson},
keywords = {crop innovation, data integration, generative AI, predictive AI, sustainable agriculture},
abstract = {Synthetic biology holds great potential to transform agriculture, yet its progress is constrained by the complexity of multigenomic, multitrait, and multi-environment data. Desirable traits often arise from complex gene networks acting across diverse conditions, making them difficult to predict and optimize manually. In the past decade, artificial intelligence (AI) has supported this process, but its large data needs and poor integration limit its role to pattern recognition rather than explanatory trait design. We argue that hybrid AI can more effectively navigate multiomics complexity to engineer climate-smart, high-yield crops. Already reducing trial and error in crop engineering, guiding gRNA design, and identifying key regulators, hybrid AI has outperformed traditional data-driven approaches, but its full potential requires clear pipelines, curated datasets, and automation platforms.}
}
@article{EHRING20231103,
title = {A first step towards automatic identification and provision of user-specific knowledge: A verification of the feasibility of automatic text classification using the example of standards},
journal = {Procedia CIRP},
volume = {119},
pages = {1103-1108},
year = {2023},
note = {The 33rd CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.02.183},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123006406},
author = {Dominik Ehring and Philipe Ferraz-Doughty and Janosch Luttmer and Arun Nagarajah},
keywords = {user-specific knowlegde, ontology, semantic, text classification, knowlegde management, engineering documents},
abstract = {The product development process faces major challenges. One major challenge is the time-efficient retrieval of information from engineering documents. In times of machine learning - as an enabler as well as a driver of digitization - new possibilities are opening up to meet the need for intelligent provision of knowledge and to make the flood of information in the product development process more manageable. To achieve the superordinate – future – goal of automatic and user-specific identification and provision of knowledge, a step-by-step approach is required. Thus, existing approaches dealing with the solution of classification problems by ontology-based methods are evaluated, transferred to the domain of standards, and extended. As an initial step, the technical feasibility of automatic text classification for the assignment of business departments to corresponding contents of documents will be examined within the scope of this research work, so that users are provided with the possibility of support when searching for information. Based on this, future research activities will primarily deal with the development of a role model so that, with the help of defined criteria, an unambiguous assignment can be made between content from different engineering documents and a "person of interest".}
}
@article{ZHU2024305,
title = {A knowledge graph and BiLSTM-CRF-enabled intelligent adaptive learning model and its potential application},
journal = {Alexandria Engineering Journal},
volume = {91},
pages = {305-320},
year = {2024},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2024.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S1110016824001418},
author = {Yiting Zhu},
keywords = {KRO model, Knowledge graph, BiLSTM-CRF, Intelligent adaptive learning model},
abstract = {The significant advancement of Artificial Intelligence (AI) technology has led to the gradual emergence of intelligent adaptive learning as a pivotal modality for acquiring knowledge. The knowledge graph, as a crucial component of the intelligent adaptive learning model, assumes the vital responsibility of guiding and facilitating the learning process while evaluating its outcomes. The current knowledge graph, however, presents several challenges including fragmented knowledge content, inadequate ability characterization, and limited automatic construction capabilities, which hinder its effectiveness in assisting learners to achieve their learning objectives. The paper proposes the knowledge-resource-objective (KRO) model of knowledge graph, based on a robust correlation between knowledge, learning resources, and learning objectives. In the KRO model, Bidirectional Encoder Representations from Transformers (BERT) is utilized for data pre-processing. Protége is employed for ontology construction. Entities and relations are synchronously annotated using the Main-Entity+ Relation +Begin-Inside-End-Single-Other (ME+R+BIESO) corpus labeling model to complete the triplet construction. Additionally, entities and relations are extracted synchronously utilizing a Bi-directional Long-Short Term Memory-Conditional Random Field (BiLSTM-CRF) model. Knowledge visualization and reasoning are achieved through Neo4j graph database. The study employs the Letter of Credit as a case study to examine the application of the KRO model, and assesses its efficacy through metrics such as accuracy, precision, recall, and F1-score. The research demonstrates that the KRO model effectively targets learning objectives, restructures knowledge and resources, optimizes learning pathways, provides recommendations for learning resources, and visualizes learners' cognitive states. As a result, it enhances the effectiveness of intelligent adaptive learning. The subsequent research will continue to focus on the visual representation of learners' learning progress and explore the practical application of learners' profiles in diverse disciplines, aiming to facilitate the realization of personalized learning for individuals.}
}
@article{CORNELIUS2025,
title = {From literature to biodiversity data: mining arthropod organismal traits with machine learning},
journal = {Biodiversity Data Journal},
volume = {13},
year = {2025},
issn = {1314-2836},
doi = {https://doi.org/10.3897/BDJ.13.e153070},
url = {https://www.sciencedirect.com/science/article/pii/S1314283625001733},
author = {Joseph Cornelius and Harald Detering and Oscar Lithgow-Serrano and Donat Agosti and Fabio Rinaldi and Robert M Waterhouse},
keywords = {arthropods, biodiversity, natural language processing, text and data mining, trait database},
abstract = {The fields of taxonomy and biodiversity research have witnessed an exponential growth in published literature. This vast corpus of articles holds information on the diverse biological traits of organisms and their ecologies. However, access to and extraction of relevant data from this extensive resource remain challenging. Advances in text and data mining (TDM) and Natural Language Processing (NLP) techniques offer new opportunities for liberating such information from literature. Testing and using such approaches to annotate articles in machine-actionable formats is, therefore, necessary to enable the exploitation of existing knowledge in new biology, ecology and evolution research. Here, we explore the potential of these methods to annotate and extract organismal trait data for the most diverse animal group on Earth, the arthropods. The article processing workflow uses manually curated trait dictionaries with trained NLP models to perform labelling of entities and relationships of thousands of articles. A subset of manually annotated documents facilitated the formal evaluation of the performance of the workflow in terms of entity recognition and normalisation and relationship extraction, highlighting several important technical challenges. The results are made available to the scientific community through an interactive web tool and queryable resource, the ArTraDB Arthropod Trait Database. These methodological explorations provide a framework that could be extended beyond the arthropods, where TDM and NLP approaches applied to the taxonomy and biodiversity literature will greatly facilitate data synthesis studies and literature reviews, the identification of knowledge gaps and biases, as well as the data-informed investigation of ecological and evolutionary trends and patterns.}
}
@article{ZHAO2023110069,
title = {Multi-task learning with graph attention networks for multi-domain task-oriented dialogue systems},
journal = {Knowledge-Based Systems},
volume = {259},
pages = {110069},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.110069},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122011625},
author = {Meng Zhao and Lifang Wang and Zejun Jiang and Ronghan Li and Xinyu Lu and Zhongtian Hu},
keywords = {Task-oriented dialogue system, Natural response generation, Graph attention network, Memory network},
abstract = {A task-oriented dialogue system (TOD) is an important application of artificial intelligence. In the past few years, works on multi-domain TODs have attracted increased research attention and have seen much progress. A main challenge of such dialogue systems is finding ways to deal with cross-domain slot sharing and dialogue act temporal planning. However, existing studies seldom consider the models’ reasoning ability over the dialogue history; moreover, existing methods overlook the structure information of the ontology schema, which makes them inadequate for handling multi-domain TODs. In this paper, we present a multi-task learning framework equipped with graph attention networks (GATs) to probe the above two challenges. In the method, we explore a dialogue state GAT consisting of a dialogue context subgraph and an ontology schema subgraph to alleviate the cross-domain slot sharing issue. We further construct a GAT-enhanced memory network using the updated nodes in the ontology subgraph to filter out the irrelevant nodes to acquire the needed dialogue states. For dialogue act temporal planning, a similar GAT and corresponding memory network are proposed to obtain fine-grained dialogue act representation. Moreover, we design an entity detection task to improve the capability of soft gate, which determines whether the generated tokens are from the vocabulary or knowledge base. In the training phase, four training tasks are combined and optimized simultaneously to facilitate the response generation process. The experimental results for automatic and human evaluations show that the proposed model achieves superior results compared to the state-of-the-art models on the MultiWOZ 2.0 and MultiWOZ 2.1 datasets.}
}
@article{MORSHEDZADEH2022100369,
title = {Managing virtual factory artifacts in the extended PLM context},
journal = {Journal of Industrial Information Integration},
volume = {28},
pages = {100369},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2022.100369},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X22000383},
author = {Iman Morshedzadeh and Amos H.C. Ng and Manfred Jeusfeld and Jan Oscarsson},
keywords = {Virtual model, Product lifecycle management, Information model, Knowledge management, Ontology},
abstract = {Virtual engineering increases the rate of and diversity of models being created; hence requires maintenance in a product lifecycle management (PLM) system. This also induces the need to understand their creation contexts, known as historical or provenance information, to reuse the models in other engineering projects. PLM systems are specifically designed to manage product- and production-related data. However, they are less capable of handling the knowledge about the contexts of the models without an appropriate extension. Therefore, this research proposes an extension to PLM systems by designing a new information model to contain virtual models, their related data and knowledge generated from them through various engineering activities so that they can be effectively used to manage historical information related to all these virtual factory artifacts. Such an information model is designed to support a new Virtual Engineering ontology for capturing and representing virtual models and engineering activities, tightly integrated with an extended provenance model based on the W7 model. In addition, this paper presents how an application prototype, called Manage-Links, has been implemented with these extended PLM concepts and then used in several virtual manufacturing activities in an automotive company.}
}
@article{NEBELUNG2022548,
title = {Towards Real-Time Machining Tool Failure Forecast Approach for Smart Manufacturing Systems},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {2},
pages = {548-553},
year = {2022},
note = {14th IFAC Workshop on Intelligent Manufacturing Systems IMS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.04.251},
url = {https://www.sciencedirect.com/science/article/pii/S240589632200252X},
author = {Nicolas Nebelung and Mario D.S. {de Oliveira Santos} and Sofia T. Helena and Athon F.C.S. {de Moura Leite} and Matheus B. Canciglieri and Anderson L. Szejka},
keywords = {Industry 4.0, Smart System, Artificial Intelligence, Machine Learning, Ontology, Machine Failure Forecast},
abstract = {Industry 4.0 is characterized by a dynamic market that constantly looking for new methods to optimize and integrate manufacturing processes. In this context, Artificial Intelligence has gained prominence in problem-solving, such as failure prediction and decision making, thus improving product quality, and consequently bringing competitiveness to the company. Aiming to contribute to this scenario, this research develops a data treatment system that, from an intelligent tool and an interoperable ontological model, automates the prediction and detection of failures in machining machines lines. The system was developed for the prediction of faults in machining lines includes an artificial intelligence formed from prediction algorithms and inferences, it is possible to guarantee the correct treatment and communication of data at different stages of the process. For the experimental research, was used data collected from a machining line of a public dataset. The information is collected and classified by Artificial Intelligence that supports a decision system. The prediction of tool wear would enable the system to infer the type of problem that is causing this wear, a possible root cause, and the needed maintenance based on the ontological inference tool. By this classification of data, it is possible to achieve, through inferences, a reduction in the decision scope, bringing the possible problems caused by the incoming value. The semantic interoperability ensures correct data exchange and processing, which generates a more assertive view of production failures. The system may help companies to increase their productive process by helping them identify future failures in production if applied in a real scenario.}
}
@incollection{ARNULF202169,
title = {Chapter 3 - Semantic and ontological structures of psychological attributes},
editor = {Dustin Wood and Stephen J. Read and P.D. Harms and Andrew Slaughter},
booktitle = {Measuring and Modeling Persons and Situations},
publisher = {Academic Press},
pages = {69-101},
year = {2021},
isbn = {978-0-12-819200-9},
doi = {https://doi.org/10.1016/B978-0-12-819200-9.00013-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128192009000132},
author = {Jan Ketil Arnulf and Kai Rune Larsen},
keywords = {Surveys, Organizational behavior, Latent semantic analysis, Language, Groups differences},
abstract = {This chapter reviews the person-situation dimension in behavior prediction through the semantic theory of survey responses (STSR). This theory proposes that the most likely source of variation in correlations between scores on Likert-scale items is overlap in meaning. We review and explain a growing number of empirical studies that support this: Up to 86% of the variation in correlation matrices may be explained using text algorithms. Also, semantics seem to predetermine the relationships between different scales, including those cast as “predictors” and “outcomes” of one another. The studies seek to establish semantic properties on population, group, and individual levels, showing that comparisons of score levels across groups are affected by predictable differences in their interpretation of items. The findings relativize the importance of data collected by semantically influenced surveys. On the bright side, they open new ways of matching individual and group level characteristics to the general population.}
}
@article{BENDOUCH2023102243,
title = {A visual-semantic approach for building content-based recommender systems},
journal = {Information Systems},
volume = {117},
pages = {102243},
year = {2023},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2023.102243},
url = {https://www.sciencedirect.com/science/article/pii/S0306437923000790},
author = {Mounir M. Bendouch and Flavius Frasincar and Tarmo Robal},
keywords = {Semantics-driven recommendation, Ontology, Computer vision, Visual semantic features, Large-scale recommendation},
abstract = {The topic of recommending items based on multimodal content has been addressed to a limited extent, and yet this could be a potential solution to the data bottleneck problem. Content-based semantics-driven recommender systems are often applied in the small-scale news recommendation domain, founded on the TF-IDF measure but also taking into account domain semantics through semantic lexicons or ontologies. In this work, we explore the application of content-based semantics-driven recommender systems to large-scale recommendations and focus on using both textual information and visual information to recommend items that have multimodal content. We propose methods to extract semantic features from various item descriptions, including digital images. In particular, we use computer vision to extract visual-semantic features from images and use these for movie recommendation together with various features extracted from textual information. The visual-semantic approach is scaled up with pre-computation of the cosine similarities and gradient learning of the model. The results of the study on a large-scale MovieLens dataset of user ratings demonstrate that semantics-driven recommenders can be extended to visual-semantic recommenders suitable for more complex domains than news recommendation, and which outperform TF-IDF-based recommenders on ROC, PR, F1, and Kappa metrics.}
}
@article{JIN2024122289,
title = {WordTransABSA: Enhancing Aspect-based Sentiment Analysis with masked language modeling for affective token prediction},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122289},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122289},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423027914},
author = {Weiqiang Jin and Biao Zhao and Yu Zhang and Jia Huang and Hang Yu},
keywords = {Natural Language Processing, Aspect-based Sentiment Analysis, Masked Language Model, Pre-trained Language Model, Few-shot supervised learning},
abstract = {In recent years, Aspect-based Sentiment Analysis (ABSA) has been a crucial yet challenging task in recognizing textual emotions from text. ABSA has numerous application across various fields, such as social media, commodity review, and movie comment, making it an attractive area of research. Many researchers are working to develop more powerful sentiment analysis models. Currently, most existing ABSA models use the generic pre-trained language models (PLMs) based fine-tuning paradigm, which only utilizes the encoder parameters while discarding the decoder parameters of PLMs. However, this approach fails to leverage the prior knowledge revealed in PLMs effectively. To address these issues, we investigate the potential of the initial pre-training scheme of PLMs to conduct ABSA and thus propose a novel approach in this paper, namely Target Word Transferred ABSA (WordTransABSA). In WordTransABSA, we propose “Word Transferred LM”, a novel sequence-level optimization strategy that transferred target words in sentence into pivot tokens to stimulate better PLM semantic understanding capability. Given a sentence with aspect terms as input, WordTransABSA generates contextually appropriate semantics and predicts the affective tokens on the corresponding positions of the aspect terms. The final sentiment polarity of each aspect term is determined through several sentiment identification strategies that we selected. WordTransABSA takes full advantage of the versatile linguistic knowledge of Pre-trained Language Model, resulting in competitive accuracy compared with recent baselines. The WordTransABSA demonstrates its superiority and effectiveness through extensive experiments in both data-sufficient (full-data supervised learning) and data-insufficient (few-shot learning) scenarios. We have made our code publicly available on GitHub: https://github.com/albert-jin/WordTransABSA.}
}
@article{XIE2025105090,
title = {Are humans (higher) animals? On the rational awakening and life transcendence of death cognition},
journal = {Acta Psychologica},
volume = {257},
pages = {105090},
year = {2025},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2025.105090},
url = {https://www.sciencedirect.com/science/article/pii/S0001691825004032},
author = {Sherman XIE},
keywords = {Religious biology, Death and dying, Cognitive regression, Instrumental rationality, Metaphysical neglect, Ontological crisis, Epistemic fragmentation},
abstract = {This article delves into the fundamental cognitive question of whether humans are (superior) animals from a utilitarian perspective. Modern biology classifies humans as animals, yet nearly all religious doctrines assert a distinctiveness of humans from animals. Humans, as higher animals, indeed possess a certain degree of rationality and intelligence compared to other lower animals. However, the innate ignorance of humans has not transcended that of lower animals. When it comes to judging the priority, importance, and urgency of their interests, humans, as higher animals, often exhibit a similar narrow-mindedness and short-sightedness as lower animals. This is primarily evident in the fact that most people have no alertness whatsoever to the constant possibility of their own death, and they show a dismissive attitude towards the fundamental metaphysical issues that could help them address the inevitability of death. From a utilitarian perspective, the predominance of human instrumental rationality over value rationality will inevitably lead to ultimate destruction through war and ecological disasters.}
}
@article{LI2025110543,
title = {A large-scale mobile application knowledge graph for the research of cybersecurity: Construction and application},
journal = {Engineering Applications of Artificial Intelligence},
volume = {149},
pages = {110543},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110543},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625005433},
author = {Weizhuo Li and Heng Zhou and Yiming Tan and Weiqi Luo and Qiu Ji and Yuyang Bian},
keywords = {Knowledge graph, Mobile applications, Knowledge extraction, Knowledge alignment, Applications relevance discovery},
abstract = {Large-scale datasets for mobile applications (terms as “apps”) have been developed and become important assets for malware identification and other tasks of cybersecurity. However, existing datasets focus on extending the scale of apps, while ignoring the relevance among apps. On the other hand, several works try to integrate different metadata of apps to discover the relevance among apps, but most of them pay little attention to the roles such as normal users, developers, cybersecurity analysts, and they do not take full advantage of these metadata so that the fine-grained correlations among apps are difficult to be captured. To fill these gaps, we present a mobile application knowledge graph, which collects millions of apps’ information from various resources, including application markets, encyclopedias and news. Precisely, a lightweight ontology is designed for our knowledge graph. It defines a unified semantic schema of collected apps so that more linkages of these apps can be shared with each other. Moreover, we employ several promising algorithms of information extraction and knowledge alignment, and evaluate their performances during the process of construction. To detect more relevance with respect to sensitive apps, we propose a hybrid embedding-based method, in which the vector representations of apps are iteratively encoded with knowledge graph embedding methods and network embedding models. Experimental results show that our hybrid method can obtain better performances than several existing models for the relevance detection of sensitive apps. Finally, we list three use cases of mobile application knowledge graph for cybersecurity and discuss their limitations that would be improved in future works.}
}
@article{MAO2025102712,
title = {A survey on pragmatic processing techniques},
journal = {Information Fusion},
volume = {114},
pages = {102712},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102712},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524004901},
author = {Rui Mao and Mengshi Ge and Sooji Han and Wei Li and Kai He and Luyao Zhu and Erik Cambria},
keywords = {Pragmatic processing, Metaphor understanding, Sarcasm detection, Personality recognition, Aspect extraction, Sentiment polarity detection},
abstract = {Pragmatics, situated in the domains of linguistics and computational linguistics, explores the influence of context on language interpretation, extending beyond the literal meaning of expressions. It constitutes a fundamental element for natural language understanding in machine intelligence. With the advancement of large language models, the research focus in natural language processing has predominantly shifted toward high-level task processing, inadvertently downplaying the importance of foundational pragmatic processing tasks. Nevertheless, pragmatics serves as a crucial medium for unraveling human language cognition. The exploration of pragmatic processing stands as a pivotal facet in realizing linguistic intelligence. This survey encompasses important pragmatic processing techniques for subjective and emotive tasks, such as personality recognition, sarcasm detection, metaphor understanding, aspect extraction, and sentiment polarity detection. It spans theoretical research, the forefront of pragmatic processing techniques, and downstream applications, aiming to highlight the significance of these low-level tasks in advancing natural language understanding and linguistic intelligence.}
}
@article{MAKSIMOV202081,
title = {Knowledge Representation Models and Cognitive Search Support Tools},
journal = {Procedia Computer Science},
volume = {169},
pages = {81-89},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.118},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920302416},
author = {Nikolay Maksimov and Olga Golitsina and Kirill Monankov and Anastasia Gavrilkina},
keywords = {information retrieval, cognitive search, knowledge extraction, text processing, interactive interface, ontology graph, aspect projection},
abstract = {The purpose of the work is to develop a cognitive-process-oriented model of data search, as well as tools of human interaction in according to this model. Cognitive search is considered as process of formation of ontology of the subject area (of target object) as a system of three systems (functional, conceptual and terminological). Accordingly, the results and trajectory of information retrieval are a reflection and component of the process of cognition. In order to reduce the dimension of the graph (to perception possibilities) aspect projections operation based on the taxonomy of relationships and entities is used.}
}
@article{HASSANZADEH2020103406,
title = {Matching patients to clinical trials using semantically enriched document representation},
journal = {Journal of Biomedical Informatics},
volume = {105},
pages = {103406},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103406},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420300344},
author = {Hamed Hassanzadeh and Sarvnaz Karimi and Anthony Nguyen},
keywords = {Clinical document classification, Clinical trials, Cohort selection, Artificial neural networks, Deep learning, Natural language processing},
abstract = {Recruiting eligible patients for clinical trials is crucial for reliably answering specific questions about medical interventions and evaluation. However, clinical trial recruitment is a bottleneck in clinical research and drug development. Our goal is to provide an approach towards automating this manual and time-consuming patient recruitment task using natural language processing and machine learning techniques. Specifically, our approach extracts key information from series of narrative clinical documents in patient’s records and collates helpful evidence to make decisions on eligibility of patients according to certain inclusion and exclusion criteria. Challenges in applying narrative clinical documents such as differences in reporting styles and sub-languages are addressed by enriching them with knowledge from domain ontologies in the form of semantic vector representations. We show that a machine learning model based on Multi-Layer Perceptron (MLP) is more effective for the task than five other neural networks and four conventional machine learning models. Our approach achieves overall micro-F1-Score of 84% for 13 different eligibility criteria. Our experiments also indicate that semantically enriched documents are more effective than using original documents for cohort selection. Our system provides an end-to-end machine learning-based solution that achieves comparable results with the state-of-the-art which relies on hand-crafted rules or data-centric engineered features.}
}
@article{KHUDHAIR2023105144,
title = {Knowledge-based OpenBIM data exchange for building design},
journal = {Automation in Construction},
volume = {156},
pages = {105144},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105144},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523004041},
author = {Ali Khudhair and Haijiang Li and Guoqian Ren},
keywords = {Building information modelling (BIM), Industry foundation classes (IFC), Data exchange, Interoperability, Semantic web, Multi-objective knowledge base},
abstract = {Building design is highly complex as it involves numerous professionals and their interactions, hence with diverse tools used and multi-resources and different structured data and information required to be processed. Despite the existing efforts to develop multi-objective decision making tools to support complex design, most of the research face difficulties to provide holistic, dynamic and collaborative knowledge base due to the complexity of the information interoperability issues across different parties and throughout life cycle. This paper developed an automatic data exchange framework that combines only the necessary data from BIM models using semantic web technology to eliminate inefficiencies in data exchange and improve decision-making early in the design stage. The proposed data acquisition method can produce a dynamic knowledge base to connect both static and dynamic information. A multi-objective knowledge base was developed to assist engineers associated with sustainability and cost in comparing different design options based on the existing BIM data. The proposed ontology was developed using a machine-readable format, allowing the ability to add more concepts to it in the future and work with other automated tools. The validated framework could reduce human involvement and errors while providing more efficient ways to leverage diverse information sources together to support holistic decision-making for building design.}
}
@article{XU2025103736,
title = {Language teachers’ use of research in a textbook writing community of practice},
journal = {System},
volume = {133},
pages = {103736},
year = {2025},
issn = {0346-251X},
doi = {https://doi.org/10.1016/j.system.2025.103736},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X25001460},
author = {Jinfen Xu and Wenbo Liu},
keywords = {Language teachers, Research use, Textbook writing, Community of practice},
abstract = {The need for the use of research in practice to bridge the research-practice divide in language teacher education has long been debated by researchers. This case study explored how six language teachers, who collaborated with researchers in writing a tertiary English textbook series in China, used research in this textbook writing community of practice (CoP). Data from interviews, documents and artifacts showed that the textbook writing CoP provided a venue to facilitate the teachers' instrumental use of research that directly informed their material selection and task design. Moreover, through increased research exposure, the teachers demonstrated enriched understandings and knowledge about English as a Foreign Language (EFL) materials and language teaching, which is indicative of their conceptual research use. Further analysis revealed that the key dimensions of the textbook writing CoP, including mutual engagement, joint enterprise, and shared repertoire, contributed to the teachers' effective engagement with research. The social significance of the joint textbook writing enterprise provided a fresh impetus for their research engagement; Collaborative negotiation within the CoP promoted their research uptake; The shared repertoire offered tangible and conceptual resources that enabled their research use. This study suggests that a textbook writing CoP may serve as an effective mechanism to enhance practitioners’ use of research.}
}
@article{CORRADI2022100061,
title = {Natural language processing in toxicology: Delineating adverse outcome pathways and guiding the application of new approach methodologies},
journal = {Biomaterials and Biosystems},
volume = {7},
pages = {100061},
year = {2022},
issn = {2666-5344},
doi = {https://doi.org/10.1016/j.bbiosy.2022.100061},
url = {https://www.sciencedirect.com/science/article/pii/S266653442200023X},
author = {Marie P.F. Corradi and Alyanne M. {de Haan} and Bernard Staumont and Aldert H. Piersma and Liesbet Geris and Raymond H.H. Pieters and Cyrille A.M. Krul and Marc A.T. Teunis},
keywords = {Natural Language Processing, Adverse Outcome Pathways, New Approach Methodologies, Toxicology},
abstract = {Adverse Outcome Pathways (AOPs) are conceptual frameworks that tie an initial perturbation (molecular initiating event) to a phenotypic toxicological manifestation (adverse outcome), through a series of steps (key events). They provide therefore a standardized way to map and organize toxicological mechanistic information. As such, AOPs inform on key events underlying toxicity, thus supporting the development of New Approach Methodologies (NAMs), which aim to reduce the use of animal testing for toxicology purposes. However, the establishment of a novel AOP relies on the gathering of multiple streams of evidence and information, from available literature to knowledge databases. Often, this information is in the form of free text, also called unstructured text, which is not immediately digestible by a computer. This information is thus both tedious and increasingly time-consuming to process manually with the growing volume of data available. The advancement of machine learning provides alternative solutions to this challenge. To extract and organize information from relevant sources, it seems valuable to employ deep learning Natural Language Processing techniques. We review here some of the recent progress in the NLP field, and show how these techniques have already demonstrated value in the biomedical and toxicology areas. We also propose an approach to efficiently and reliably extract and combine relevant toxicological information from text. This data can be used to map underlying mechanisms that lead to toxicological effects and start building quantitative models, in particular AOPs, ultimately allowing animal-free human-based hazard and risk assessment.}
}
@article{ABBASI20231,
title = {Crop diagnostic system: A robust disease detection and management system for leafy green crops grown in an aquaponics facility},
journal = {Artificial Intelligence in Agriculture},
volume = {10},
pages = {1-12},
year = {2023},
issn = {2589-7217},
doi = {https://doi.org/10.1016/j.aiia.2023.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S2589721723000314},
author = {R. Abbasi and P. Martinez and R. Ahmad},
keywords = {Computer vision, Deep learning, Disease detection, Leafy crops, Aquaponics, Digital farming},
abstract = {Crops grown on aquaponics farms are susceptible to various diseases or biotic stresses during their growth cycle, just like traditional agriculture. The early detection of diseases is crucial to witnessing the efficiency and progress of the aquaponics system. Aquaponics combines recirculating aquaculture and soilless hydroponics methods and promises to ensure food security, reduce water scarcity, and eliminate carbon footprint. For the large-scale implementation of this farming technique, a unified system is needed that can detect crop diseases and support researchers and farmers in identifying potential causes and treatments at early stages. This study proposes an automatic crop diagnostic system for detecting biotic stresses and managing diseases in four leafy green crops, lettuce, basil, spinach, and parsley, grown in an aquaponics facility. First, a dataset comprising 2640 images is constructed. Then, a disease detection system is developed that works in three phases. The first phase is a crop classification system that identifies the type of crop. The second phase is a disease identification system that determines the crop's health status. The final phase is a disease detection system that localizes and detects the diseased and healthy spots in leaves and categorizes the disease. The proposed approach has shown promising results with accuracy in each of the three phases, reaching 95.83%, 94.13%, and 82.13%, respectively. The final disease detection system is then integrated with an ontology model through a cloud-based application. This ontology model contains domain knowledge related to crop pathology, particularly causes and treatments of different diseases of the studied leafy green crops, which can be automatically extracted upon disease detection allowing agricultural practitioners to take precautionary measures. The proposed application finds its significance as a decision support system that can automate aquaponics facility health monitoring and assist agricultural practitioners in decision-making processes regarding crop and disease management.}
}
@article{ZONG2024104716,
title = {Advancing Chinese biomedical text mining with community challenges},
journal = {Journal of Biomedical Informatics},
volume = {157},
pages = {104716},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104716},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424001345},
author = {Hui Zong and Rongrong Wu and Jiaxue Cha and Weizhe Feng and Erman Wu and Jiakun Li and Aibin Shao and Liang Tao and Zuofeng Li and Buzhou Tang and Bairong Shen},
keywords = {Biomedical text mining, Health information processing, Natural language processing, Artificial intelligence, Large language model},
abstract = {Objective
This study aims to review the recent advances in community challenges for biomedical text mining in China.
Methods
We collected information of evaluation tasks released in community challenges of biomedical text mining, including task description, dataset description, data source, task type and related links. A systematic summary and comparative analysis were conducted on various biomedical natural language processing tasks, such as named entity recognition, entity normalization, attribute extraction, relation extraction, event extraction, text classification, text similarity, knowledge graph construction, question answering, text generation, and large language model evaluation.
Results
We identified 39 evaluation tasks from 6 community challenges that spanned from 2017 to 2023. Our analysis revealed the diverse range of evaluation task types and data sources in biomedical text mining. We explored the potential clinical applications of these community challenge tasks from a translational biomedical informatics perspective. We compared with their English counterparts, and discussed the contributions, limitations, lessons and guidelines of these community challenges, while highlighting future directions in the era of large language models.
Conclusion
Community challenge evaluation competitions have played a crucial role in promoting technology innovation and fostering interdisciplinary collaboration in the field of biomedical text mining. These challenges provide valuable platforms for researchers to develop state-of-the-art solutions.}
}
@article{SHAIKH2022127a,
title = {Integrated models, model languages, model repositories, simulation experiments, simulation tools and data visualizations enable facile model reuse with biosimulations},
journal = {Biophysical Journal},
volume = {121},
number = {3, Supplement 1},
pages = {127a},
year = {2022},
issn = {0006-3495},
doi = {https://doi.org/10.1016/j.bpj.2021.11.2118},
url = {https://www.sciencedirect.com/science/article/pii/S0006349521030939},
author = {Bilal Shaikh and Lucian P. Smith and Michael L. Blinov and Herbert M. Sauro and Ion I. Moraru and Jonathan R. Karr}
}
@article{XIA2025103013,
title = {A knowledge graph construction and causal structure mining approach for non-stationary manufacturing systems},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {95},
pages = {103013},
year = {2025},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2025.103013},
url = {https://www.sciencedirect.com/science/article/pii/S0736584525000675},
author = {Mingyuan Xia and Xuandong Mo and Yahui Zhang and Xiaofeng Hu},
keywords = {Non-stationary manufacturing system, Manufacturing knowledge graph, Knowledge modeling, Causal inference},
abstract = {Knowledge graph (KG) is a method for managing multi-source heterogeneous data and forming knowledge for reasoning using graph structure. It has been extensively utilized in manufacturing systems to promote the advancement of intelligent manufacturing. In non-stationary manufacturing systems, the machining performance of individual elements demonstrates variability and dynamic fluctuations. The significant dynamics and uncertainties of a manufacturing system bring great challenges to KG's modeling, construction, and reasoning. To overcome these challenges, this paper proposes a Digital-Physical Manufacturing Knowledge Graph (DPMKG) construction and reasoning method. Firstly, an ontology-based knowledge representation model is developed to facilitate the integration of digital domain knowledge with the description of physical domain performance fluctuations, thereby establishing the schema layer of DPMKG. Secondly, a SysML model-driven construction pipeline is proposed to facilitate the correlation and integration of multi-source data from both digital and physical domains, thereby establishing the instance layer of DPMKG. Thirdly, a causal structure mining method for DPMKG is developed to enhance the analytical and reasoning capabilities in non-stationary manufacturing systems. Finally, an aero-engine casing machining system is employed as a case study to establish the DPMKG, and reasoning is performed on the process quality prediction task. The case study reveals that the proposed DPMKG modeling, construction, and reasoning approach can effectively describe and analyze performance fluctuations in the physical domain of a non-stationary manufacturing system. By integrating digital and physical domain knowledge, the extensive data can be effectively leveraged to generate knowledge for reasoning, thereby facilitating intelligent and refined control of non-stationary manufacturing systems.}
}
@article{MESSAS2025986,
title = {Topography of depressive experiences. A dialectic approach},
journal = {Journal of Affective Disorders},
volume = {369},
pages = {986-994},
year = {2025},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2024.10.064},
url = {https://www.sciencedirect.com/science/article/pii/S0165032724017580},
author = {Guilherme Messas and Francesca Brencio},
keywords = {Affective experiences, Melancholia, Melancholic depression, Phenomenology, Psychopathology, Dialectics, Depression},
abstract = {This article aims to offer clinical descriptions and philosophical interpretations of the ontological nature of depressive experiences trying to address issues related to the contemporary diagnostic paradigm and values-based practices which shape clinical decisions. At the core of this contribution there is the idea that the variety of depressive experiences might be understood acknowledging the qualitative difference in the ontology underlying each form of depression. We argue that there is a fundamental difference between melancholia as a disease and those melancholic traits which characterizes human existence as such: while the former indicates a qualitative alteration of the global human experience, constituting psychopathological experience, the latter describes a style of disproportion of existence not pathological per se at all. Moreover, we defend the hypothesis that melancholia (in a medical sense) is a distinct unity of mental alteration, and should not be conflated with the multivarious kind of experiences merged under the name of depression, the variety of which may be understood as disorders of the personal development. It is in this context that the leading element of anthropological disproportions impact the role of pre-reflective and transcendental structures, polarizing the self and the world in a three-way topography: 1. depression as an excessive symmetric proportion between self and world; 2. depression as a disproportion between the self and the world (detriment of the self); 3. depressions as impoverishment of the transcendental value of the world.}
}
@article{WANG2023112,
title = {Knowledge graph representation learning model based on meta-information and logical rule enhancements},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {4},
pages = {112-125},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823000769},
author = {Ling Wang and Jicang Lu and Yepeng Sun},
keywords = {Knowledge graph, Representation learning, Meta-information, Logical rule enhancement, Knowledge embedding},
abstract = {Existing knowledge graph representation learning (KGRL) models rely on explicit semantic information of triple structure and cannot fully mine the implicit semantic information in the knowledge graph (KG). Aiming to improve KGRL model performance and accuracy, making up for the disadvantages of existing research, we propose Melo (Meta-information and Logical rules), a novel KGRL model that leverages meta-information and logical rules of entities and relations. Melo first utilizes neighborhood structures of entities to obtain meta-information and ontological information, then it mines logical rules from the KG to infer high-confidence triples and expand the training samples. Finally, Melo realizes accurate and reliable representations of entities and relations with help of meta-, logical, and triple structure information. Experimental results on regular and sparse datasets show its enhanced performance when compared with baselines in terms of multiple evaluation metrics. Visualization methods are also utilized to demonstrate how meta-information, logical rules, and triple structure mutually and separately enhance training.}
}
@article{KERMANY202330,
title = {Incorporating user rating credibility in recommender systems},
journal = {Future Generation Computer Systems},
volume = {147},
pages = {30-43},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.04.029},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2300170X},
author = {Naime Ranjbar Kermany and Weiliang Zhao and Tseesuren Batsuuri and Jian Yang and Jia Wu},
keywords = {Recommender system, Collaborative filtering, Users’ credibility, Neighbor selection, Ontology, Rating behavior},
abstract = {There have been many research efforts aimed at improving recommendation accuracy with Collaborative Filtering (CF). Yet there is still a lack of investigation into the integration of CF algorithms with the analysis of users’ rating behaviors. In this work, we develop an integrated CF-based recommendation solution by incorporating the credibility of users’ ratings, the demographic information of people, and the ontological semantics of items. The users’ credibility values are calculated based on their ratings and they are used in finding credible neighbors to improve the accuracy of recommendations. The demographic information and ontological semantics are used in the similarity measurement of users/items to alleviate the issues of sparsity and cold start in CF algorithms. Experiments are conducted on real-world datasets of MovieLens and Yahoo!Movie. Compared with baseline methods, a set of experiments shows that the proposed approach improves the recommendation quality significantly.}
}
@article{FRID2023104505,
title = {Evaluation of OMOP CDM, i2b2 and ICGC ARGO for supporting data harmonization in a breast cancer use case of a multicentric European AI project},
journal = {Journal of Biomedical Informatics},
volume = {147},
pages = {104505},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104505},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423002265},
author = {Santiago Frid and Guillem {Bracons Cucó} and Jessyca {Gil Rojas} and Antonio López-Rueda and Xavier {Pastor Duran} and Olga Martínez-Sáez and Raimundo Lozano-Rubí},
keywords = {Health information interoperability, Health research, Secondary use of health data, OMOP CDM, I2b2, ICGC ARGO},
abstract = {Objective
Observational research in cancer poses great challenges regarding adequate data sharing and consolidation based on a homogeneous data semantic base. Common Data Models (CDMs) can help consolidate health data repositories from different institutions minimizing loss of meaning by organizing data into a standard structure. This study aims to evaluate the performance of the Observational Medical Outcomes Partnership (OMOP) CDM, Informatics for Integrating Biology & the Bedside (i2b2) and International Cancer Genome Consortium, Accelerating Research in Genomic Oncology (ICGC ARGO) for representing non-imaging data in a breast cancer use case of EuCanImage.
Methods
We used ontologies to represent metamodels of OMOP, i2b2, and ICGC ARGO and variables used in a cancer use case of a European AI project. We selected four evaluation criteria for the CDMs adapted from previous research: content coverage, simplicity, integration, implementability.
Results
i2b2 and OMOP exhibited higher element completeness (100% each) than ICGC ARGO (58.1%), while the three achieved 100% domain completeness. ICGC ARGO normalizes only one of our variables with a standard terminology, while i2b2 and OMOP use standardized vocabularies for all of them. In terms of simplicity, ICGC ARGO and i2b2 proved to be simpler both in terms of ontological model (276 and 175 elements, respectively) and in the queries (7 and 20 lines of code, respectively), while OMOP required a much more complex ontological model (615 elements) and queries similar to those of i2b2 (20 lines). Regarding implementability, OMOP had the highest number of mentions in articles in PubMed (130) and Google Scholar (1,810), ICGC ARGO had the highest number of updates to the CDM since 2020 (4), and i2b2 is the model with more tools specifically developed for the CDM (26).
Conclusion
ICGC ARGO proved to be rigid and very limited in the representation of oncologic concepts, while i2b2 and OMOP showed a very good performance. i2b2′s lack of a common dictionary hinders its scalability, requiring sites that will share data to explicitly define a conceptual framework, and suggesting that OMOP and its Oncology extension could be the more suitable choice. Future research employing these CDMs with actual datasets is needed.}
}
@article{BONATTI2020103389,
title = {Real-time reasoning in OWL2 for GDPR compliance},
journal = {Artificial Intelligence},
volume = {289},
pages = {103389},
year = {2020},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2020.103389},
url = {https://www.sciencedirect.com/science/article/pii/S0004370220301399},
author = {Piero A. Bonatti and Luca Ioffredo and Iliana M. Petrova and Luigi Sauro and Ida R. Siahaan},
keywords = {Tractable OWL2 fragments, Structural subsumption, Import-by-query, Knowledge compilation, Semantic policy languages, GDPR},
abstract = {This paper shows how knowledge representation and reasoning techniques can be used to support organizations in complying with the GDPR, that is, the new European data protection regulation. This work is carried out in a European H2020 project called SPECIAL. Data usage policies, the consent of data subjects, and selected fragments of the GDPR are encoded in a fragment of OWL2 called PL (policy language); compliance checking and policy validation are reduced to subsumption checking and concept consistency checking. This work proposes a satisfactory tradeoff between the expressiveness requirements on PL posed by the modeling of the GDPR, and the scalability requirements that arise from the use cases provided by SPECIAL's industrial partners. Real-time compliance checking is achieved by means of a specialized reasoner, called PLR, that leverages knowledge compilation and structural subsumption techniques. The performance of a prototype implementation of PLR is analyzed through systematic experiments, and compared with the performance of other important reasoners. Moreover, we show how PL and PLR can be extended to support richer ontologies, by means of import-by-query techniques. We prove novel tractability and intractability results related to PL, and some negative results about the restrictions posed on ontology import.}
}
@article{RUBIO2024101766,
title = {SOSAc-Reasoner: An ASP inference engine for automatic IoT context knowledge generation},
journal = {SoftwareX},
volume = {27},
pages = {101766},
year = {2024},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2024.101766},
url = {https://www.sciencedirect.com/science/article/pii/S2352711024001377},
author = {Ana Rubio and Rubén Cantarero and David Villa and Juan C. López},
keywords = {Knowledge generation, Answer Set Programming, Commonsense reasoning, Smart environments, Internet of Things},
abstract = {The SOSAc-Reasoner is a commonsense reasoning engine, implemented using Answer Set Programming. It is designed to automatically generate IoT context knowledge, representing the capabilities of system devices, from a simple smart scenario description. The inference engine is fed with knowledge about device types and generates knowledge according to two ontologies derived from the SOSA (Sensor, Observation, Sample, and Actuator) ontology. The SOSAc-Reasoner comprises two ASP rule modules: the basic and advanced inference modules, which perform reasoning with different objectives. Implemented with Potassco, the SOSAc-Reasoner effectively generates context knowledge within a reasonable timeframe. This significantly facilitates the task of modeling a highly valuable type of knowledge in intelligent environments, a task that traditionally involves manual efforts, is prone to errors, and consumes a significant amount of time.}
}
@article{TSUJI2021,
title = {Developing a RadLex-Based Named Entity Recognition Tool for Mining Textual Radiology Reports: Development and Performance Evaluation Study},
journal = {Journal of Medical Internet Research},
volume = {23},
number = {10},
year = {2021},
issn = {1438-8871},
doi = {https://doi.org/10.2196/25378},
url = {https://www.sciencedirect.com/science/article/pii/S1438887121010785},
author = {Shintaro Tsuji and Andrew Wen and Naoki Takahashi and Hongjian Zhang and Katsuhiko Ogasawara and Gouqian Jiang},
keywords = {named entity recognition (NER), natural language processing (NLP), RadLex, ontology, stem term},
abstract = {Background
Named entity recognition (NER) plays an important role in extracting the features of descriptions such as the name and location of a disease for mining free-text radiology reports. However, the performance of existing NER tools is limited because the number of entities that can be extracted depends on the dictionary lookup. In particular, the recognition of compound terms is very complicated because of the variety of patterns.
Objective
The aim of this study is to develop and evaluate an NER tool concerned with compound terms using RadLex for mining free-text radiology reports.
Methods
We leveraged the clinical Text Analysis and Knowledge Extraction System (cTAKES) to develop customized pipelines using both RadLex and SentiWordNet (a general purpose dictionary). We manually annotated 400 radiology reports for compound terms in noun phrases and used them as the gold standard for performance evaluation (precision, recall, and F-measure). In addition, we created a compound terms–enhanced dictionary (CtED) by analyzing false negatives and false positives and applied it to another 100 radiology reports for validation. We also evaluated the stem terms of compound terms by defining two measures: occurrence ratio (OR) and matching ratio (MR).
Results
The F-measure of cTAKES+RadLex+general purpose dictionary was 30.9% (precision 73.3% and recall 19.6%) and that of the combined CtED was 63.1% (precision 82.8% and recall 51%). The OR indicated that the stem terms of effusion, node, tube, and disease were used frequently, but it still lacks capturing compound terms. The MR showed that 71.85% (9411/13,098) of the stem terms matched with that of the ontologies, and RadLex improved approximately 22% of the MR from the cTAKES default dictionary. The OR and MR revealed that the characteristics of stem terms would have the potential to help generate synonymous phrases using the ontologies.
Conclusions
We developed a RadLex-based customized pipeline for parsing radiology reports and demonstrated that CtED and stem term analysis has the potential to improve dictionary-based NER performance with regard to expanding vocabularies.}
}
@article{YU2025147,
title = {Language, disciplinarity and identity: an autoethnography of an international interdisciplinary doctoral student},
journal = {Qualitative Research Journal},
volume = {25},
number = {2},
pages = {147-159},
year = {2025},
issn = {1443-9883},
doi = {https://doi.org/10.1108/QRJ-02-2024-0047},
url = {https://www.sciencedirect.com/science/article/pii/S1443988325000165},
author = {Chengyuan Yu},
keywords = {Disciplinary literacy, Interdisciplinary identity, International doctoral student, Autoethnography},
abstract = {Purpose
While higher education has been encouraging interdisciplinary research, few studies have been conducted to understand how interdisciplinarity shapes the identity construction of scholars, especially doctoral students who may already strive to socialize into academia.
Design/methodology/approach
Therefore, this study adopts the approach of autoethnography to analyze my lived experience of developing disciplinary literacy and constructing interdisciplinary identity as a Chinese international doctoral student at a North American university. Communication theory of identity (CTI) is the theoretical framework through which I understand the negotiation among my personal, enacted, relational and communal identities while communicating my research through diverse literacy practices.
Findings
This autoethnography reveals that interdisciplinary doctoral students can flexibly use discursive resources from different disciplines and literacy practices in both English and their first language to dynamically create interdisciplinary identities communicable to different discourse communities. Their identities in different disciplines can develop simultaneously, rather than suppressing one for the development of the other as they do interdisciplinary research.
Originality/value
This study first extends current scholarly discussion of disciplinary literacy to a less-investigated setting, i.e. doctoral education in higher education. Second, it adds an additive and current layer of interdisciplinarity to the existing understanding of international doctoral students’ identity construction. Third, it helps to understand how the development of disciplinary literacy can facilitate disciplinary identity construction and how disciplinary identity construction can facilitate the development of disciplinary literacy.}
}
@article{CHEN2022,
title = {The Generation of a Lung Cancer Health Factor Distribution Using Patient Graphs Constructed From Electronic Medical Records: Retrospective Study},
journal = {Journal of Medical Internet Research},
volume = {24},
number = {11},
year = {2022},
issn = {1438-8871},
doi = {https://doi.org/10.2196/40361},
url = {https://www.sciencedirect.com/science/article/pii/S1438887122007464},
author = {Anjun Chen and Ran Huang and Erman Wu and Ruobing Han and Jian Wen and Qinghua Li and Zhiyong Zhang and Bairong Shen},
keywords = {lung cancer, risk factor, patient graph, UMLS knowledge graph, Unified Medical Language System, connection delta ratio, EMR, electronic health record, EHR, electronic health record, cancer},
abstract = {Background
Electronic medical records (EMRs) of patients with lung cancer (LC) capture a variety of health factors. Understanding the distribution of these factors will help identify key factors for risk prediction in preventive screening for LC.
Objective
We aimed to generate an integrated biomedical graph from EMR data and Unified Medical Language System (UMLS) ontology for LC, and to generate an LC health factor distribution from a hospital EMR of approximately 1 million patients.
Methods
The data were collected from 2 sets of 1397 patients with and those without LC. A patient-centered health factor graph was plotted with 108,000 standardized data, and a graph database was generated to integrate the graphs of patient health factors and the UMLS ontology. With the patient graph, we calculated the connection delta ratio (CDR) for each of the health factors to measure the relative strength of the factor’s relationship to LC.
Results
The patient graph had 93,000 relations between the 2794 patient nodes and 650 factor nodes. An LC graph with 187 related biomedical concepts and 188 horizontal biomedical relations was plotted and linked to the patient graph. Searching the integrated biomedical graph with any number or category of health factors resulted in graphical representations of relationships between patients and factors, while searches using any patient presented the patient’s health factors from the EMR and the LC knowledge graph (KG) from the UMLS in the same graph. Sorting the health factors by CDR in descending order generated a distribution of health factors for LC. The top 70 CDR-ranked factors of disease, symptom, medical history, observation, and laboratory test categories were verified to be concordant with those found in the literature.
Conclusions
By collecting standardized data of thousands of patients with and those without LC from the EMR, it was possible to generate a hospital-wide patient-centered health factor graph for graph search and presentation. The patient graph could be integrated with the UMLS KG for LC and thus enable hospitals to bring continuously updated international standard biomedical KGs from the UMLS for clinical use in hospitals. CDR analysis of the graph of patients with LC generated a CDR-sorted distribution of health factors, in which the top CDR-ranked health factors were concordant with the literature. The resulting distribution of LC health factors can be used to help personalize risk evaluation and preventive screening recommendations.}
}
@article{DAGA2025100846,
title = {Process Knowledge Graphs (PKG): Towards unpacking and repacking AI applications},
journal = {Journal of Web Semantics},
volume = {84},
pages = {100846},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100846},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000325},
author = {Enrico Daga},
keywords = {Knowledge graphs, Prompt engineering, Data science pipelines, Data pipelines documentation, Data pipelines design},
abstract = {In the past years, a new generation of systems has emerged, which apply recent advances in generative Artificial Intelligence (AI) in combination with traditional technologies. Specifically, generative AI is being delegated tasks in natural language or vision understanding within complex hybrid architectures that also include databases, procedural code, and interfaces. Process Knowledge Graphs (PKG) have a long-standing tradition within symbolic AI research. On the one hand, PKGs can play an important role in describing complex, hybrid applications, thus opening the way for addressing fundamental challenges such as explaining and documenting such systems (unpacking). On the other hand, by organising complex processes in simpler building blocks, PKGs can potentially increase accuracy and control over such systems (repacking). In this position paper, we discuss opportunities and challenges of PGRs and their potential role towards a more robust and principled design of AI applications.}
}
@incollection{LIU2024,
title = {Feedback Literacy for Language Teachers and Learners},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00008-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041000089},
author = {Qi Liu and Sin Wang Chong},
keywords = {Feedback, Language education, Student feedback literacy, Teacher feedback literacy},
abstract = {Feedback has been an important topic in language education. The notion of feedback has evolved from static information transmission to a learner-centered, sense-making, and action-driven process, necessitating the emphasis on creating a learning ecology, or environment, where shared responsibilities between feedback receivers and feedback providers are emphasized. The concepts of student and teacher feedback literacies encapsulate characteristics that are conducive to such an effective and learning-oriented feedback process. This article traces the development of the concepts and related research, focusing specifically on Language Education.}
}
@article{ZHANG20233993,
title = {Application Research on Two-Layer Threat Prediction Model Based on Event Graph},
journal = {Computers, Materials and Continua},
volume = {77},
number = {3},
pages = {3993-4023},
year = {2023},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.044526},
url = {https://www.sciencedirect.com/science/article/pii/S1546221823006847},
author = {Shuqin Zhang and Xinyu Su and Yunfei Han and Tianhui Du and Peiyu Shi},
keywords = {Knowledge graph, multi-source data fusion, network security, threat modeling, event graph, absorbing Markov chain, threat propagation path},
abstract = {Advanced Persistent Threat (APT) is now the most common network assault. However, the existing threat analysis models cannot simultaneously predict the macro-development trend and micro-propagation path of APT attacks. They cannot provide rapid and accurate early warning and decision responses to the present system state because they are inadequate at deducing the risk evolution rules of network threats. To address the above problems, firstly, this paper constructs the multi-source threat element analysis ontology (MTEAO) by integrating multi-source network security knowledge bases. Subsequently, based on MTEAO, we propose a two-layer threat prediction model (TL-TPM) that combines the knowledge graph and the event graph. The macro-layer of TL-TPM is based on the knowledge graph to derive the propagation path of threats among devices and to correlate threat elements for threat warning and decision-making; The micro-layer ingeniously maps the attack graph onto the event graph and derives the evolution path of attack techniques based on the event graph to improve the explainability of the evolution of threat events. The experiment’s results demonstrate that TL-TPM can completely depict the threat development trend, and the early warning results are more precise and scientific, offering knowledge and guidance for active defense.}
}
@article{ZHAO2025103055,
title = {Unlocking ancient wisdom with modern tools: A new approach to the revitalization of ancient texts based on generative artificial intelligence},
journal = {The Journal of Academic Librarianship},
volume = {51},
number = {3},
pages = {103055},
year = {2025},
issn = {0099-1333},
doi = {https://doi.org/10.1016/j.acalib.2025.103055},
url = {https://www.sciencedirect.com/science/article/pii/S0099133325000515},
author = {YueYan Zhao and WenJie Zhou},
keywords = {Generative artificial intelligence, Chinese classics, Multi-granularity knowledge deconstruction, Knowledge services},
abstract = {In the fields of library science and information science, the gap between the obscure language of ancient texts and the cognitive abilities of the average reader is the primary obstacle to the widespread dissemination of the rich wisdom in traditional cultural heritage. The rapid development of generative artificial intelligence technology has provided the conditions for researchers to transition from sequential organization of bibliographic references to the deconstruction and reorganization of knowledge elements. This study, reveals the intrinsic relationship patterns between multi-granularity knowledge in ancient texts from perspectives such as logical relationships, frame semantic associations, hierarchical structures, and feature associations, and constructs a multidimensional knowledge representation model for ancient texts. Furthermore, a YAML prompt template was designed by integrating large language models with the deconstruction of multi-granularity knowledge elements from ancient texts, and experiments were conducted using the Chinese classic “Records of the Grand Historian: Biographies.” The results indicate that the method developed in this study for the deconstruction and reorganization of knowledge elements in ancient texts exhibits notable characteristics such as multi-granularity, cross-document application, refinement, multiple perspectives, efficiency improvement, and scalability, demonstrating potential for application in the field of knowledge services for ancient texts.}
}
@article{LONG20225197,
title = {A Prototype for Diagnosis of Psoriasis in Traditional Chinese Medicine},
journal = {Computers, Materials and Continua},
volume = {73},
number = {3},
pages = {5197-5217},
year = {2022},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2022.029365},
url = {https://www.sciencedirect.com/science/article/pii/S1546221822000856},
author = {Hai Long and Zhe Wang and Yidi Cui and Junhui Wang and Bo Gao and Chao Chen and Yan Zhu and Heinrich Herre},
keywords = {Clinical guideline, knowledge base, GFO, ontology, TCM, CBR, fuzzy pattern recognition, psoriasis vulgaris},
abstract = {Psoriasis is a chronic, non-communicable, painful, disfiguring and disabling disease for which there is no cure, with great negative impact on patients’ quality of life (QoL). Diagnosis and treatment with traditional Chinese medical technique based on syndrome differentiation has been used in practice for a long time and proven effective, though, up to now, there are only a few available studies about the use of semantic technologies and the knowledge systems that use Traditional Chinese Medicine (TCM)-syndrome differentiation for information retrieval and automated reasoning. In this paper we use semantic techniques based on ontologies to develop a prototypical system for the diagnosis of Psoriasis. For this purpose, a domain ontology is developed for syndrome differentiation of psoriasis vulgaris (PV). This ontology is founded on an adapted version of the general formal ontology (GFO), with the evidence-based clinical practice guideline of TCM for psoriasis vulgaris (Guideline 2013) as the primary data sources. The implemented prototype, called ONTOPV, contains this domain ontology and is aimed at a decision support system for diagnosis and treatment of PV. This system uses a case-database for Case Based Reasoning (CBR), combined with fuzzy pattern recognition. Experimental results show that the ONTOPV realizes the basic functionalities of data collection, querying, browsing and navigation, and supports rule-based knowledge reasoning, and integrates fuzzy pattern recognition. It can provide users with clinical decision support for TCM syndrome differentiation in diagnosis of psoriasis.}
}
@article{RODRIGUEZCARDOS2019803,
title = {A method for the semantic analysis of documents in a business context in Spanish},
journal = {Procedia Computer Science},
volume = {162},
pages = {803-810},
year = {2019},
note = {7th International Conference on Information Technology and Quantitative Management (ITQM 2019): Information technology and quantitative management based on Artificial Intelligence},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.053},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919320642},
author = {Ruben Rodriguez-Cardos and Francisco Pascual Romero Chicharro and Jose Angel Olivas Varela and Jesus Serrano-Guerrero},
keywords = {Sentiment Analysis, Natural Language Processing, Semantics, Ontologies},
abstract = {Due to the ease of generating and storing written text documents, Natural Language Processing tools are increasingly integrated into software products on decision making in various fields. In this paper, we work with real requests from customers of a real estate company. The objective of the paper is to design a preliminary approximation of new intelligent algorithms that can carry out a semantic analysis of text documents in the business environment. For this purpose, different prototypes are developed, based on techniques and methods from different disciplines, such as Natural Language Processing, to process and structure text, Text Mining to find the most relevant terms and Knowledge Engineering, to create an ontology with the objective of measuring semantic similarity in business documents. The prototypes obtained are capable of determining the semantic similarity between simple phrases, structuring existing terms, and determining the subject on which they deal with requests from clients of a real estate company.}
}
@article{MAHARJAN2025,
title = {Differential Analysis of Age, Gender, Race, Sentiment, and Emotion in Substance Use Discourse on Twitter During the COVID-19 Pandemic: A Natural Language Processing Approach},
journal = {JMIR Infodemiology},
volume = {5},
year = {2025},
issn = {2564-1891},
doi = {https://doi.org/10.2196/67333},
url = {https://www.sciencedirect.com/science/article/pii/S2564189125000477},
author = {Julina Maharjan and Ruoming Jin and Jennifer King and Jianfeng Zhu and Deric Kenne},
keywords = {substance use, social media, deep learning, natural language processing, NLP, COVID-19, age, gender, race, sentiment, emotion, artificial intelligence, AI},
abstract = {Background
User demographics are often hidden in social media data due to privacy concerns. However, demographic information on substance use (SU) can provide valuable insights, allowing public health policy makers to focus on specific cohorts and develop efficient prevention strategies, especially during global crises such as the COVID-19 pandemic.
Objective
This study aimed to analyze SU trends at the user level across different demographic dimensions, such as age, gender, race, and ethnicity, with a focus on the COVID-19 pandemic. The study also establishes a baseline for SU trends using social media data.
Methods
The study was conducted using large-scale English-language data from Twitter (now known as X) over a 3-year period (2019, 2020, and 2021), comprising 1.13 billion posts. Following preprocessing, the SU posts were identified using our custom-trained deep learning model (Robustly Optimized Bidirectional Encoder Representations From Transformers Pretraining Approach [RoBERTa]), which resulted in the identification of 9 million SU posts. Then, demographic attributes, such as user type, age, gender, race, and ethnicity, as well as sentiments and emotions associated with each post, were extracted via a collection of natural language processing modules. Finally, various qualitative analyses were performed to obtain insight into user behaviors based on demographics.
Results
The highest level of user participation in SU discussions was observed in 2020, with a 22.18% increase compared to 2019 and a 25.24% increase compared to 2021. Throughout the study period, male users and teenagers increasingly dominated the SU discussions across all substance types. During the COVID-19 pandemic, user participation in prescription medication discussions was notably higher among female users compared to other substance types. In addition, alcohol use increased by 80% within 2 weeks after the global pandemic declaration in 2020.
Conclusions
This study presents a large-scale, fine-grained analysis of SU on social media data, examining trends by age, gender, race, and ethnicity before, during, and after the COVID-19 pandemic. Our findings, contextualized with sociocultural and pandemic-specific factors, provide actionable insights for targeted public health interventions. This study establishes social media data (powered with artificial intelligence and natural language processing tools) as a valuable platform for real-time SU surveillance and prevention during crises.}
}
@incollection{GYRARD2022143,
title = {Chapter 8 - A naturopathy knowledge graph and recommendation system to boost the immune system: KISS: Knowledge-based Immune System Suggestion},
editor = {Sanju Tiwari and Fernando {Ortiz Rodriguez} and M.A. Jabbar},
booktitle = {Semantic Models in IoT and eHealth Applications},
publisher = {Academic Press},
pages = {143-169},
year = {2022},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-323-91773-5},
doi = {https://doi.org/10.1016/B978-0-32-391773-5.00014-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323917735000145},
author = {Amelie Gyrard and Karima Boudaoud},
keywords = {Food knowledge graph food ontology, Naturopathy, Healthy diet, Nutrition, Immune system, Recommender system, Preventive health, Well-being, Wellness, Rule-based reasoning, Inference engine, FAIR principles},
abstract = {Because of COVID-19 worldwide pandemic, there is a need for any complementary solutions to boost the immune system. Nowadays, healthy lifestyle, fitness, and diet habits have become central applications in our daily life. We designed a Naturopathy Knowledge Graph for a recommender system to boost the immune system (KISS: Knowledge-based Immune System Suggestion). The Naturopathy Knowledge Graph is built from more than 50 ontology-based food projects, also released as the LOV4IoT-Food ontology catalog. The naturopathy data set is referenced on the Linked Open Data (LOD) cloud. The LOV4IoT-Food ontology catalog encourages researchers to follow FAIR principles and share their reproducible experiments by publishing online their ontologies, data sets, rules, etc. The set of the ontology code shared online can be semiautomatically processed, if not available, the scientific publications describing the food ontologies are semiautomatically processed with Natural Language Processing (NLP) techniques. We build the naturopathy recommender system that will suggest food to boost the immune system. The recommender system can be extended to address other advice such as aromatherapy and take into consideration medical devices to monitor patients' vital signals.}
}
@article{QU2025100912,
title = {Industrial digital twins based on enterprise modeling: architecture, methodology, and engineering applications},
journal = {Journal of Industrial Information Integration},
volume = {47},
pages = {100912},
year = {2025},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2025.100912},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X25001359},
author = {Mengjin Qu and Yining Yao and Minhui Sun and Xinran Chang and Qing Li},
keywords = {Enterprise architecture, Digital twin, Cyber-physical systems (CPS), System modeling, Business process management (BPM)},
abstract = {As AI technology increasingly permeates industrial applications, scenario management within the context of industrial intelligence presents a complex, multidisciplinary, and spatiotemporally coupled challenge. The integration characteristics of cyber-physical-social systems make Digital Twins (DT) a promising solution. However, constructing an effective DT model for such scenarios necessitates the incorporation of detailed industrial knowledge related to the corresponding data. Formal modeling serves as a unified cognitive approach and a robust foundation for system development. Hence, this paper focuses on the scenario management challenges within the industrial intelligence context and introduces the concept of using formal modeling languages as the foundation for DTs. We propose a model management architecture and a modeling methodology tailored for scenario-specific digital twins and provide a corresponding metamodel for the modeling approach. To validate the efficacy of our architecture and models, we analyze a multi-enterprise network collaboration scenario in remote maintenance of engineering machinery. This exploration not only demonstrates the validity of the proposed models and architecture but also offers a conceptual DT model for enhancing remote machinery maintenance.}
}
@article{LINNA2022104779,
title = {Applications of natural language processing in radiology: A systematic review},
journal = {International Journal of Medical Informatics},
volume = {163},
pages = {104779},
year = {2022},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2022.104779},
url = {https://www.sciencedirect.com/science/article/pii/S1386505622000934},
author = {Nathaniel Linna and Charles E. Kahn},
keywords = {Natural Language Processing, Radiology, Systematic Review, Artificial Intelligence},
abstract = {Background
Recent advances in performance of natural language processing (NLP) techniques have spurred wider use and more sophisticated applications of NLP in radiology. This study systematically reviews the trends and applications of NLP in radiology within the last five years.
Methods
A search of three databases of peer-reviewed journal articles and conference papers from January 1, 2016 to April 21, 2021 resulted in a total of 228 publications included in the review. Manuscripts were analyzed by several factors, including clinical application, study setting, NLP technique, and performance.
Results
Of the 228 included publications, there was an overall increase in number of studies published with an increase in use of machine learning models. NLP models showed high performance: >50% of publications reported F1 > 0.91. There was variable sample size across the studies with a median of 3708 data points, most commonly radiology reports. 145 studies utilized data from a single academic center. Applications were classified as clinical (n = 87), technical (n = 66), quality improvement (n = 61), research (n = 9), and education (n = 5).
Discussion
There has been a continued increase in number of studies involving NLP in radiology. Newer NLP techniques, including word embedding, deep learning, and transformers, are being applied and show improved performance. There has been growth in the interpretative and non-interpretative use of NLP techniques in radiology and has great capacity to improve patient care and delivery. Although the performance and breadth of NLP applications is impressive, there is an overall lack of high-level evidence for actual clinical application of published tools.
Conclusion
NLP applications in radiology has been increasing studied and more accurate in the last 5 years. More direct clinical application and portability of the NLP pipelines is need to reach the technology’s full potential.}
}
@article{HARBER2020605,
title = {Informatics Approaches for Recognition, Management, and Prevention of Occupational Respiratory Disease},
journal = {Clinics in Chest Medicine},
volume = {41},
number = {4},
pages = {605-621},
year = {2020},
note = {Advances in Occupational and Environmental Lung Diseases},
issn = {0272-5231},
doi = {https://doi.org/10.1016/j.ccm.2020.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0272523120300836},
author = {Philip Harber and Gondy Leroy},
keywords = {Occupational lung disease, Asthma, Informatics, Computer systems, Occupational health surveillance, Natural language processing, Machine learning, Ontology}
}
@article{BENSASSI20233123,
title = {Fuzzy knowledge based assessment system for K-12 Scientific Reasoning Competencies},
journal = {Procedia Computer Science},
volume = {225},
pages = {3123-3132},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.306},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923014643},
author = {Manel BenSassi and Henda Ben Ghezala},
keywords = {Learning analytics, educational recommendations, fuzzy competencies assessment, knowledge representation, fuzzy ontology, Scientific Reasoning competencies},
abstract = {Developing Scientific Reasoning (SR) competencies at an early age, are challenging to meet expectation of the 4th sustainable development goal. Hence, educators and educational decision-makers try to embed these competencies into such subjects as the arts, language, technology, economics, mathematics and science, using an inter-disciplinary approach. In this context, this paper proposes a fuzzy knowledge-based solution to build practical pupils, educators, and decision-makers recommender system to support the development of SR competencies in a data driver manner. Our system consists of:(1) inferring and computational module that calculates in a fuzzy manner the global appreciation to each SR-competencies. (2) recommendation module that aims to help learners, educators and decision makers to assess the degree of development of SR competencies and to get alternative suggestion of remediation. The proposed solution has been tested on the last two levels of science education in four Tunisian elementary schools in different regions. A preliminary analysis showed that the learning process should be more focused on Tunisian pupil's profile, and that investigation and collaborative based learning should be applied further in Tunisian classroom.}
}
@article{DENG2023101921,
title = {Research on the construction of event logic knowledge graph of supply chain management},
journal = {Advanced Engineering Informatics},
volume = {56},
pages = {101921},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.101921},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623000496},
author = {Jianfeng Deng and Chong Chen and Xinyi Huang and Wenyan Chen and Lianglun Cheng},
keywords = {Event logic knowledge graph, Supply chain management, Event knowledge autonomous extraction, Event argument entity recognition, GAN active learning},
abstract = {Knowledge graph technology plays an important role in knowledge supporting for efficient supply chain management (SCM) of manufacturing enterprises, a SCM knowledge graph can be constructed based on the relevant case corpus. In order to solve the problems of coarse concept granularity of event ontology knowledge in SCM cases, potential words of characters in the existing entity recognition models may not be matched or matched incorrectly, key features of entities with different lengths may interfere with each other in the expression of attention mechanism, and lack of a large number of annotation training samples, a top-down construction method of SCM event logic knowledge graph (ELKG) is proposed. Firstly, SCM event argument classes and class relations are defined, an event logic ontology model is built, and the event argument entities according to event logic ontology are labeled. Then, an active learning event argument entity recognition (EAER) model based on two-stage generative adversarial network (GAN) is proposed. In GAN generator, an EAER model based on binocular attention-based stacked BiLSTM with CNN (BACSBN) is proposed, which combines word-level character feature attention mechanism and n-gram pooling feature attention mechanism to improve the attention to character features of constituent words and highlight entity key information with different lengths, respectively. Two-stage GAN adversarial training and label space attention mechanism are introduced to select the correct predicted label samples for active learning training. The experimental results show that BACSBN can improve the entity recognition accuracy, and the two-stage GAN’s active learning can further improve the recognition effect on the basis of full annotation sample training, and can still maintain a high accuracy in the absence of a large number of manual annotation data. Further, according to the sentence pattern and keyword matching, the matching relations of argument entities are completed, and the SCM ELKG is constructed to provide knowledge support for autonomous SCM.}
}
@article{BONATTI2025104345,
title = {Effective and fast module extraction for nonempty ABoxes},
journal = {Artificial Intelligence},
volume = {344},
pages = {104345},
year = {2025},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2025.104345},
url = {https://www.sciencedirect.com/science/article/pii/S0004370225000645},
author = {Piero Andrea Bonatti and Francesco Magliocca and Iliana Mineva Petrova and Luigi Sauro},
keywords = {Description logics, Module extraction},
abstract = {A deductive module of a knowledge base KB is a subset of KB that preserves a specified class of consequences. Module extraction is applied in ontology design, debugging, and reasoning. The locality-based module extractors of the OWL API are less effective when the knowledge base contains facts such as ABox assertions. The competing module extractor PrisM computes smaller modules at the cost of higher computation time. In this paper, we introduce and study a novel module extraction technique, called conditional module extraction, that can be applied to satisfiable SRIQ(D) knowledge bases. Experimental analysis shows that conditional module extraction constitutes an appealing alternative to PrisM and to the locality-based extractors of the OWL API, when the ABox is nonempty.}
}
@article{AGUIRRE2025863,
title = {Towards the Development of a Conversational CNC Assistant: Compiling a Sample of Key Questions and Procedures for Benchmarking Purposes},
journal = {Procedia Computer Science},
volume = {253},
pages = {863-873},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.01.148},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925001565},
author = {Maia Aguirre and Ariane Méndez and Cristina Aceta and Manuel Torralbo and Arantza {del Pozo} and Izaskun Fernández and Aitor Etxalar and María Inés Torres and Joseba Andoni Agirre and Egoitz Artetxe and Iker Altuna and Iker Etxebeste},
keywords = {Conversational CNC Assistant, Industrial Corpus Compilation, Question Answering, Task-Oriented Dialogue Systems},
abstract = {Smart manufacturing, characterized by human-machine collaboration, is transforming traditional industrial processes. Effective human-machine communication is pivotal for this transformation, making natural language interaction technologies such as Question Answering (QA) and task-oriented Dialogue Systems (DS) essential in factory settings. Despite the undergoing revolution in the field powered by Large Language Models, annotated corpora are still needed to tailor applications to specific tasks and use cases. To address this need, this work describes the steps taken to compile a sample of key questions and procedures for the development of a Computerized Numerical Control (CNC) assistant aimed at helping users operate a milling machine. As a result, the compiled corpus includes 221 question-answer pairs and 10 detailed procedures from an industrial CNC programming setting in Spanish and Basque. This corpus is made publicly available, with the objective of promoting research and serving as a benchmark for the development of natural language assistants in the industrial field. Additionally, preliminary outcomes are presented for a CNC assistant integrating a similarity-based QA module and a task-oriented procedural DS developed using the compiled sample, showing promising results.}
}
@article{MEIER2025104202,
title = {Structured knowledge-based causal discovery: Agentic streams of thought},
journal = {Information Processing & Management},
volume = {62},
number = {5},
pages = {104202},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104202},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325001438},
author = {Sven Meier and Pratik Narendra Raut and Felix Mahr and Nils Thielen and Jörg Franke and Florian Risch},
keywords = {Large Language Models, Agentic Systems, Causal discovery, Expert systems},
abstract = {Causal discovery—the systematic identification of cause-and-effect relationships among variables—forms the cornerstone of causal inference. Its application enables reliable predictions and targeted interventions across complex systems, from medical treatments to engineering processes. Traditional statistical causal discovery methods face significant limitations with high-dimensional data structures, while existing knowledge-based approaches rely on single large-scale models that raise fundamental concerns about computational efficiency and result reliability. The Agentic Stream of Thought (ASoT) addresses these limitations through a novel architecture that orchestrates multiple smaller open-source language models. The framework integrates hierarchical query decomposition with Model Compiler refinement, while dual-stream thought processing enables balanced analysis through parallel evaluation of competing hypotheses. Dedicated Direction and Transitive Processors enhance reasoning by resolving bidirectional relationships and refining transitive pathways. A two-tiered quality gate system and complementary consensus mechanisms—Delphi protocol and Ensemble Synthesis Method—iteratively refine outputs while mitigating hallucination risks. Empirical evaluations across causal discovery benchmarks and question-answering tasks demonstrate that this approach matches or exceeds state-of-the-art models while enabling local deployment, establishing that sophisticated orchestration of smaller models provides a more sustainable path than increasing model scale alone.}
}
@article{DOLATABADI2023,
title = {Using Social Media to Help Understand Patient-Reported Health Outcomes of Post–COVID-19 Condition: Natural Language Processing Approach},
journal = {Journal of Medical Internet Research},
volume = {25},
year = {2023},
issn = {1438-8871},
doi = {https://doi.org/10.2196/45767},
url = {https://www.sciencedirect.com/science/article/pii/S143888712300729X},
author = {Elham Dolatabadi and Diana Moyano and Michael Bales and Sofija Spasojevic and Rohan Bhambhoria and Junaid Bhatti and Shyamolima Debnath and Nicholas Hoell and Xin Li and Celine Leng and Sasha Nanda and Jad Saab and Esmat Sahak and Fanny Sie and Sara Uppal and Nirma Khatri Vadlamudi and Antoaneta Vladimirova and Artur Yakimovich and Xiaoxue Yang and Sedef Akinli Kocak and Angela M Cheung},
keywords = {long COVID, post–COVID-19 condition, PCC, social media, natural language processing, transformer models, bidirectional encoder representations from transformers, machine learning, Twitter, Reddit, PRO, patient-reported outcome, patient-reported symptom, health outcome, symptom, entity extraction, entity normalization},
abstract = {Background
While scientific knowledge of post–COVID-19 condition (PCC) is growing, there remains significant uncertainty in the definition of the disease, its expected clinical course, and its impact on daily functioning. Social media platforms can generate valuable insights into patient-reported health outcomes as the content is produced at high resolution by patients and caregivers, representing experiences that may be unavailable to most clinicians.
Objective
In this study, we aimed to determine the validity and effectiveness of advanced natural language processing approaches built to derive insight into PCC-related patient-reported health outcomes from social media platforms Twitter and Reddit. We extracted PCC-related terms, including symptoms and conditions, and measured their occurrence frequency. We compared the outputs with human annotations and clinical outcomes and tracked symptom and condition term occurrences over time and locations to explore the pipeline’s potential as a surveillance tool.
Methods
We used bidirectional encoder representations from transformers (BERT) models to extract and normalize PCC symptom and condition terms from English posts on Twitter and Reddit. We compared 2 named entity recognition models and implemented a 2-step normalization task to map extracted terms to unique concepts in standardized terminology. The normalization steps were done using a semantic search approach with BERT biencoders. We evaluated the effectiveness of BERT models in extracting the terms using a human-annotated corpus and a proximity-based score. We also compared the validity and reliability of the extracted and normalized terms to a web-based survey with more than 3000 participants from several countries.
Results
UmlsBERT-Clinical had the highest accuracy in predicting entities closest to those extracted by human annotators. Based on our findings, the top 3 most commonly occurring groups of PCC symptom and condition terms were systemic (such as fatigue), neuropsychiatric (such as anxiety and brain fog), and respiratory (such as shortness of breath). In addition, we also found novel symptom and condition terms that had not been categorized in previous studies, such as infection and pain. Regarding the co-occurring symptoms, the pair of fatigue and headaches was among the most co-occurring term pairs across both platforms. Based on the temporal analysis, the neuropsychiatric terms were the most prevalent, followed by the systemic category, on both social media platforms. Our spatial analysis concluded that 42% (10,938/26,247) of the analyzed terms included location information, with the majority coming from the United States, United Kingdom, and Canada.
Conclusions
The outcome of our social media–derived pipeline is comparable with the results of peer-reviewed articles relevant to PCC symptoms. Overall, this study provides unique insights into patient-reported health outcomes of PCC and valuable information about the patient’s journey that can help health care providers anticipate future needs.
International Registered Report Identifier (IRRID)
RR2-10.1101/2022.12.14.22283419}
}
@article{VILA2023100855,
title = {Critical infrastructure awareness based on IoT context data},
journal = {Internet of Things},
volume = {23},
pages = {100855},
year = {2023},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2023.100855},
url = {https://www.sciencedirect.com/science/article/pii/S2542660523001786},
author = {Marc Vila and Maria-Ribera Sancho and Ernest Teniente and Xavier Vilajosana},
keywords = {Interoperability, Context-awareness, Semantics, Cloud continuum, Internet of things},
abstract = {The Internet of Things (IoT) represents a powerful new paradigm for connecting and communicating with the world around us. It has the potential to transform the way we live, work, and interact with our surroundings. IoT devices are transmitting information over the Internet, most of them with different data formats, despite they may be communicating similar concepts. This often leads to data incompatibilities and makes it difficult to extract the knowledge underlying that data. Because of the heterogeneity of IoT devices and data, interoperability is a challenge, and efforts are underway to overcome this through research and standardization. While data collection and monitoring in IoT systems are becoming more prevalent, contextualizing the data and taking appropriate actions to address issues in the monitored environment is still an ongoing concern. Context Awareness is a highly relevant topic in IoT, as it aims to provide a deeper understanding of the data collected and enable more informed decision-making. In this paper, we propose a semantic ontology designed to monitor global entities in the IoT. By leveraging semantic definitions, it enables end-users to model the entire process from detection to action, including context-aware rules for taking appropriate actions. The advantages of using semantic definitions include more accurate and consistent data interpretation, which improves the overall monitoring process and enables more effective decision-making based on the collected insights. Our proposal includes semantic models for defining the entities responsible for monitoring and executing actions, as well as the elements that need to be considered for an effective monitoring process. Additionally, we provide a new definition for the components known as gateways, which enable the connection and communication between devices and the Internet. Finally, we show the benefits of our ontology by applying it to a critical infrastructure domain where a rapid response is vital to prevent accidents and malfunction of the entities.}
}
@article{WALLACE2022115352,
title = {Tackling communication and analytical problems in environmental planning: Expert assessment of key definitions and their relationships},
journal = {Journal of Environmental Management},
volume = {317},
pages = {115352},
year = {2022},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2022.115352},
url = {https://www.sciencedirect.com/science/article/pii/S0301479722009252},
author = {Kenneth J. Wallace and Christian Wagner and David J. Pannell and Milena Kiatkoski Kim and Abbie A. Rogers},
keywords = {Environmental planning, Values, Assessing definitions, Interval-valued analysis, Framework evaluation, Ontology},
abstract = {Inadequate definition of key terms and their relationships generates significant communication and analytical problems in environmental planning. In this work, we evaluate an ontological framework for environmental planning designed to combat these problems. After outlining the framework and issues addressed, we describe its evaluation by a group of experts representing a range of expertise and institutions. Experts rated their level of agreement with 12 propositions concerning the definitions and models underpinning the framework. These propositions, in turn, were used to assess three assumptions regarding the expected effectiveness of the framework and its contribution to addressing the abovementioned planning problems. In addition to point-based best estimates of their agreement with propositions, expert ratings were also captured on a continuous interval-valued scale. The use of intervals addresses the challenge of measuring and modelling uncertainty associated with complex assessments such as those provided by experts. Combined with written anonymous expert comments, these data provide multiple perspectives on the level of support for the approach. We conclude that the framework can complement existing planning approaches and strengthen key definitions and related models, thus helping avoid communication and analytical problems in environmental planning. Finally, experts highlighted areas that require further development, and we provide recommendations for improving the framework.}
}
@article{DAMICO20222725,
title = {Detecting failure of a material handling system through a cognitive twin},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {2725-2730},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.128},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322021383},
author = {R.D. D'Amico and A. Sarkar and H. Karray and S. Addepalli and J.A. Erkoyuncu},
keywords = {Digital twin, cognitive twin, ontology, BFO, IOF, CCO, knowledge graph, SPARQL, material handling systems, Festo MPS},
abstract = {This paper describes a methodology for developing a digital twin (DT) based on a rich semantic model and principles of system engineering. The aim is to provide a general model of digital twins (DT) that can improve decision making based on semantic reasoning on real-time system monitoring. The methodology has been tested on a laboratory pilot plant that acts as a material handling system. The key contribution of this research is to propose a generic information model for DT using foundational ontology and principles of systems engineering. The efficacy of the proposed methodology is demonstrated by the automatic detection of a component level failure using semantic reasoning.}
}
@article{CANEPARO2022104012,
title = {Semantic knowledge in generation of 3D layouts for decision-making},
journal = {Automation in Construction},
volume = {134},
pages = {104012},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.104012},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521004635},
author = {Luca Caneparo},
keywords = {Ontology (computer science), Semantic knowledge, Generative design, Layout generation, Layout planning, Site management, Multiobjective performance optimisation, Pareto set, AEC, Smart City},
abstract = {Generative computation has the potential to enhance the accuracy, effectiveness, and creativity of spatial layout in design and planning. The paper proposes a methodology to separate the knowledge about objects, spatial relationships, and constraints from the generative process. The separation between the knowledge in a domain and its possible practical uses is an important achievement of semantic technologies, because it grants access to a large body of knowledge, spanning various aspects and processes across buildings and cities, which is being codified into formal ontologies. The present study has reused existing knowledge from two established ontologies. An illustrative case-project demonstrates the suitability of the methodology for a complex layout planning problem, involving a large number of decision-makers, with multiple competing objectives and criteria. The system implements multidimensional visual interactive tools to assist designers, planners, and decision-makers in exploring the layouts and the criteria, to develop their confidence in what qualifies as a good and effective solution.}
}
@article{WIDMER2023100807,
title = {Towards human-compatible XAI: Explaining data differentials with concept induction over background knowledge},
journal = {Journal of Web Semantics},
volume = {79},
pages = {100807},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2023.100807},
url = {https://www.sciencedirect.com/science/article/pii/S1570826823000367},
author = {Cara Leigh Widmer and Md Kamruzzaman Sarker and Srikanth Nadella and Joshua Fiechter and Ion Juvina and Brandon Minnery and Pascal Hitzler and Joshua Schwartz and Michael Raymer},
keywords = {Concept induction, Explainable AI, Class hierarchy},
abstract = {Concept induction, which is based on formal logical reasoning over description logics, has been used in ontology engineering in order to create ontology (TBox) axioms from the base data (ABox) graph. In this paper, we show that it can also be used to explain data differentials, for example in the context of Explainable AI (XAI), and we show that it can in fact be done in a way that is meaningful to a human observer. Our approach utilizes a large class hierarchy, curated from the Wikipedia category hierarchy, as background knowledge. To make the explanations easily understandable for non-specialists, the complex description logic explanations generated by our concept induction system (ECII) were presented as a word list consisting of the concept names occurring in the highest rated system responses.}
}
@article{KOHLER2023e19694,
title = {Geometric feature extraction in manufacturing based on a knowledge graph},
journal = {Heliyon},
volume = {9},
number = {9},
pages = {e19694},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e19694},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023069025},
author = {Tobias Köhler and Buchao Song and Jean Pierre Bergmann and Diana Peters},
keywords = {Knowledge graph, Feature technology, Ontology, Manufacturing, Geometry analysis},
abstract = {In times of global crises, the resilience of production chains is becoming increasingly important. If a supply chain is interrupted, a cost-effective solution must be established quickly. In the context of Industry 4.0, the concept of smart manufacturing offers a solution for fast and automated decision-making in production planning. The core idea of smart manufacturing is the digitalization of the product life cycle and the linking of individual phases of this cycle. Computer Aided Process Planning (CAPP) plays an important role as the connecting element between design and manufacturing. An important prerequisite for CAPP is the automated analysis of 3D models of components. The aim of this work is the development of an automatic feature recognition (AFR) -method to recognize geometric manufacturing features and their properties from 3D-models and then store them in a knowledge base. In that way, the result of the design can be automatically analysed and compared with manufacturing information afterwards in order to achieve an automated process planning. Geometric and topological information of a 3D model (STEP-AP242 format) generated by CAD systems is extracted by a Python-script developed and stored in an ontology-based knowledge base. The extracted product data is analysed using a Python-script to identify manufacturing features. To provide a comprehensive extensibility of the model, geometric features are defined according to a layered and hierarchical structure.}
}
@article{LIN2024,
title = {Artificial Intelligence–Augmented Clinical Decision Support Systems for Pregnancy Care: Systematic Review},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/54737},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124005727},
author = {Xinnian Lin and Chen Liang and Jihong Liu and Tianchu Lyu and Nadia Ghumman and Berry Campbell},
keywords = {artificial intelligence, biomedical ontologies, clinical decision support systems, implementation science, obstetrics, pregnancy, AI, systematic review, CDSS, functionality, methodology, implementation, database query, database queries, bibliography, record, records, eligibility, literature review, prenatal, early pregnancy, obstetric care, postpartum care, pregnancy care, diagnostic support, clinical prediction, knowledge base, therapeutic, therapeutics, recommendation, recommendations, diagnosis, abnormality, abnormalities, cost-effective, surveillance, ultrasound, ontology},
abstract = {Background
Despite the emerging application of clinical decision support systems (CDSS) in pregnancy care and the proliferation of artificial intelligence (AI) over the last decade, it remains understudied regarding the role of AI in CDSS specialized for pregnancy care.
Objective
To identify and synthesize AI-augmented CDSS in pregnancy care, CDSS functionality, AI methodologies, and clinical implementation, we reported a systematic review based on empirical studies that examined AI-augmented CDSS in pregnancy care.
Methods
We retrieved studies that examined AI-augmented CDSS in pregnancy care using database queries involved with titles, abstracts, keywords, and MeSH (Medical Subject Headings) terms. Bibliographic records from their inception to 2022 were retrieved from PubMed/MEDLINE (n=206), Embase (n=101), and ACM Digital Library (n=377), followed by eligibility screening and literature review. The eligibility criteria include empirical studies that (1) developed or tested AI methods, (2) developed or tested CDSS or CDSS components, and (3) focused on pregnancy care. Data of studies used for review and appraisal include title, abstract, keywords, MeSH terms, full text, and supplements. Publications with ancillary information or overlapping outcomes were synthesized as one single study. Reviewers independently reviewed and assessed the quality of selected studies.
Results
We identified 30 distinct studies of 684 studies from their inception to 2022. Topics of clinical applications covered AI-augmented CDSS from prenatal, early pregnancy, obstetric care, and postpartum care. Topics of CDSS functions include diagnostic support, clinical prediction, therapeutics recommendation, and knowledge base.
Conclusions
Our review acknowledged recent advances in CDSS studies including early diagnosis of prenatal abnormalities, cost-effective surveillance, prenatal ultrasound support, and ontology development. To recommend future directions, we also noted key gaps from existing studies, including (1) decision support in current childbirth deliveries without using observational data from consequential fetal or maternal outcomes in future pregnancies; (2) scarcity of studies in identifying several high-profile biases from CDSS, including social determinants of health highlighted by the American College of Obstetricians and Gynecologists; and (3) chasm between internally validated CDSS models, external validity, and clinical implementation.}
}
@article{DUVERGER2024510,
title = {Early concurrent engineering in the aerospace industry supported by a Digital Thread framework},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {19},
pages = {510-515},
year = {2024},
note = {18th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.09.263},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324016926},
author = {Eliott Duverger and Alexis Aubry and Eric Levrat and Rebeca Arista},
keywords = {Concurrent Engineering, Manufacturing System Engineering, MBSE, Digital Thread, Ontology for Enterprise Interoperability, Knowledge Management},
abstract = {The traditional sequential aircraft design process is reaching its limits in an increasingly complex environment. Therefore, it has become an industrial necessity to implement radical change in working methods. This paper focuses on the conceptual design phase, where concurrent engineering offers the greatest potential to improve the costs, quality, and development time of a new aircraft. It pays particular attention to the knowledge management and modeling activities used to provide global perspectives that are inherently lacking in early phases. A digital thread framework is presented linking model-based systems engineering artifacts from product and manufacturing systems, coupled with knowledge capture methods. This framework aims to enable early trade-of analysis to support concurrent engineering during the conceptual design phase.}
}
@article{HORO2024102427,
title = {A Machine learning approach for Post-Disaster data curation},
journal = {Advanced Engineering Informatics},
volume = {60},
pages = {102427},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102427},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624000752},
author = {Sun {Ho Ro} and Yitong Li and Jie Gong},
keywords = {Natural disaster, Machine learning, Data curation, Image search},
abstract = {Image data collected after natural disasters play an important role in the forensics of structure failures. However, curating and managing large amounts of post-disaster imagery data is challenging. In most cases, data users still have to spend much effort to find and sort images from the massive amounts of images archived for past decades in order to study specific types of disasters. This paper proposes a new machine learning based approach for automating the labeling and classification of large volumes of post-natural disaster image data to address this issue. More specifically, the proposed method couples pre-trained computer vision models and a natural language processing model with an ontology tailed to natural disasters to facilitate the search and query of specific types of image data. The resulting process returns each image with five primary labels and similarity scores, representing its content based on the developed word-embedding model. Validation and accuracy assessment of the proposed methodology was conducted with ground-level residential building panoramic images from Hurricane Harvey. The computed primary labels showed a minimum average difference of 13.32% when compared to manually assigned labels. This versatile and adaptable solution offers a practical and valuable solution for automating image labeling and classification tasks, with the potential to be applied to various image classifications and used in different fields and industries. The flexibility of the method means that it can be updated and improved to meet the evolving needs of various domains, making it a valuable asset for future research and development.}
}
@article{WANG2024108316,
title = {An improved case-based reasoning approach for sustainable rural development applied to strategic responses},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108316},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108316},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624004743},
author = {Yameng Wang and Yuqiang Feng and Luning Liu},
keywords = {Case-based reasoning, Sustainable rural development, Strategic response, Case retrieval, Case reuse},
abstract = {Sustainable development has become a worldwide consensus in recent years. It has become increasingly important to study how to make effective management decisions in rural areas to achieve sustainable development. In this paper, we develop a new approach to strategic responses for sustainable rural development by using case-based reasoning (CBR), the bi-direction Choquet integral (BDCI), and the Shapley index (SI). First, to deal with the complexity of sustainable rural development, we develop an ontology-based sustainable rural development domain model. Second, we propose a case retrieval algorithm based on BDCI to improve the case retrieval method used in CBR to solve the problem of the relationship between the features. Third, we propose a case reuse method that is inspired by SI, in which each solution is evaluated relative to its value, and the implementation step for the response strategy is determined based on its priority. The proposed strategy response method for sustainable rural development is demonstrated through a case study and its effectiveness is demonstrated empirically.}
}
@article{JBENE202380,
title = {Tracking Dialog States in Goal-Oriented Dialogues using a BERT-Based Siamese Network},
journal = {Procedia Computer Science},
volume = {225},
pages = {80-87},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.09.094},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923011493},
author = {Mourad Jbene and Smail Tigani and Abdellah Chehri and Hasna Chaibi and Rachid Saadane},
keywords = {Task-Oriented Dialog Systems, Dialog management, Dialog state tracking, Conversational systems, Transformer-based models},
abstract = {A dialogue state tracker is a component in a task-oriented dialogue system that monitors the current state of a conversation and gives information about its context and history to other system components. The dynamic and open-ended character of human interactions is one of the primary obstacles in dialogue state monitoring, necessitating robust and adaptable models to keep up with the quick context changes. Recently, numerous deep learning-based algorithms have been developed for this purpose. Still, these models are typically heavily-engineered and conceptually sophisticated, making them challenging to deploy, debug, and maintain in a production environment. To overcome these challenges, we offer the BERT-SIAM-DST model, a unique way to dialogue state monitoring employing a Siamese network with BERT as the base network. This model uses the robust representation capabilities of BERT and the ability of Siamese networks to record correlations between inputs to make accurate predictions regarding the current state of the discussion. In addition, the number of parameters does not increase proportionally with the size of the ontology, and the model is adaptable to alterations in the domain ontology. We test the performance of the BERT-SIAM-DST model on the standard WoZ 2.0 dataset of annotated dialogues and compare it to other approaches. Compared to numerous baseline models, the BERT-SIAM-DST model is effective at tracking the state of discussions, demonstrating the promise of BERT-based Siamese networks for this purpose.}
}
@article{NGUYEN2023,
title = {Knowledge Management for Information Querying System in Education via the Combination of Rela-Ops Model and Knowledge Graph},
journal = {Journal of Cases on Information Technology},
volume = {25},
number = {1},
year = {2023},
issn = {1548-7717},
doi = {https://doi.org/10.4018/JCIT.324113},
url = {https://www.sciencedirect.com/science/article/pii/S1548771723000209},
author = {Hien D. Nguyen and Duc Truong and Sang Vu and Diem Nguyen and Hung Nguyen and Nha Thanh Tran},
keywords = {E-Learning, Fundamentals of Database Systems, Information System, Intelligent Searching, Knowledge Graph, Knowledge Management, Knowledge-Based System, Ontology},
abstract = {ABSTRACT
E-learning is an online educational system that uses electronic and technology tools and the internet. To facilitate the acquisition of knowledge, students can consult materials and interact directly with lecturers via e-learning educational systems. The method for developing an intelligent querying system for e-learning is suggested in this research. The knowledge base of the system is structured using the knowledge model combining an ontology and the two-layer knowledge graph, called Rela-KG model. The searching problems for knowledge content are studied and solved. The technique is applied to create an intelligent querying system on the course of Database Foundation in Information Technology (IT) at the university. This technique attempts to assist students in reviewing lectures and better comprehending course material through independent study. The experimental results indicate that it would be anticipated to support student access to online learning resources.}
}
@article{LEE2019325,
title = {Improving process safety: What roles for Digitalization and Industry 4.0?},
journal = {Process Safety and Environmental Protection},
volume = {132},
pages = {325-339},
year = {2019},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2019.10.021},
url = {https://www.sciencedirect.com/science/article/pii/S0957582019317057},
author = {John Lee and Ian Cameron and Maureen Hassall},
keywords = {Process safety, Digital twin, Digitalization, Industry 4.0, Models, Life cycle, ISO15926},
abstract = {Process safety and risk management remain a significant challenge for the process and manufacturing industries. Digital systems have been applied over many decades to assist in process safety management throughout the lifecycle of a process plant. There has been much hype in recent years regarding Industry 4.0, digitalization and digital twins regarding the transformative potential that exists within these technologies to improve operational performance and reduce process safety accidents. In this article, a fundamental systems thinking approach is applied to the implementation of the digital twin within the process industries. The importance of having a standardized language and ontology, such as ISO15926, enables the use of reasoning engines and the ability to interconnect models and systems across the process and product lifecycle. We discuss use-cases and forms of the digital twin to improve safety within the process industries. A specific focus shows how an operator training simulator and its embedded dynamic models are applied within this environment. The article concludes with a summary of process safety related opportunities and threats associated with the application of digitalized dynamic models in industry.}
}
@incollection{BUSSEMAKER20232679,
title = {Knowledge modelling framework for per-and poly-fluoroalkyl substances (PFAS) treatment solutions},
editor = {Antonios C. Kokossis and Michael C. Georgiadis and Efstratios Pistikopoulos},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {52},
pages = {2679-2684},
year = {2023},
booktitle = {33rd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-15274-0.50426-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443152740504261},
author = {Madeleine Bussemaker and Nikos Trokanas and Ioannis Kavakiotis and Franjo Cecelja},
keywords = {Per-and poly-fluoroalkyl substances (PFAS), remediation, ontology, knowledge modelling, treatment trains},
abstract = {Remediation of per-and poly-fluoroalkyl substances (PFAS) is challenged with complexities of solutions, recalcitrance of end products and stringent, evolving regulations. Grouping, characterization and classification of PFAS compounds, environmental contaminations and treatment technologies through knowledge modelling has potential to overcome these challenges. Treatment technologies are often required to work in sequences, called treatment trains to achieve complete removal of PFAS from the environment i.e. a removal/separation stage followed by a degradation stage. Here, an ontology framework is presented to classify PFAS compounds and treatment technologies. Potential applications for the knowledge model to support decision making in environmental remediation and technology research and development is discussed.}
}
@article{HOFELE2025102823,
title = {Societal implications of bioinspired technologies: Introduction to the special issue},
journal = {Technology in Society},
volume = {81},
pages = {102823},
year = {2025},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2025.102823},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X25000132},
author = {Philipp Höfele and Louisa Estadieu and Oliver Müller and Lore Hühn and Andrea Kiesel},
keywords = {Bioinspired technologies, Sustainability, Technology acceptance, Societal acceptance, Ethical responsibility},
abstract = {This Special Issue seeks to introduce a broad range of bioinspired technologies and provides an overview of the challenges regarding sustainability, ethical responsibility and societal acceptance. It adopts both a narrow and broad understanding of bioinspired technology. In a narrower sense, it refers to technologies that mimic natural forms, structures, or functional principles, typically developed within the field of biomimetics. More broadly, bioinspired technologies are defined as those designed with human or non-human nature in mind and adapted to meet the needs of both. The Special Issue addresses five key questions: First, it asks how the broad spectrum of bioinspired technologies can be categorized and classified. Second, it explores the extent to which bioinspired technologies contribute to a sustainable future and in particular to the development of sustainable technologies and what criteria can be used to ensure the sustainability of bioinspired technologies. Third, it presents methods for investigating and fostering societal acceptance of new bioinspired technologies. Fourth, central technical and societal areas are considered in which bioinspiration can contribute to a sustainable future. Lastly, it explores the ontological and ethical challenges associated with bioinspired technologies in so far as they undermine the previous distinction between ‘nature’ and ‘technology’.}
}
@article{XIA2022104009,
title = {Study on city digital twin technologies for sustainable smart city design: A review and bibliometric analysis of geographic information system and building information modeling integration},
journal = {Sustainable Cities and Society},
volume = {84},
pages = {104009},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.104009},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722003298},
author = {Haishan Xia and Zishuo Liu and Maria Efremochkina and Xiaotong Liu and Chunxiang Lin},
keywords = {BIM, GIS, Digital twins, Smart city, Sustainable city design, Data value extension, Urban rail transit},
abstract = {Geographic information system (GIS) data provide geospatial data on cities and spatial analysis functions that are essential for urban design. Building information modeling (BIM) includes a digital entity of construction, a passive presentation of micro-digital information on real entities, and an active application of models in the entire life cycle realization of the architecture, engineering, and construction industries. A combination of these technologies could provide a core technology for the urban digital twin to support sustainable smart city design. Through an insightful literature review, this paper summarizes the different disciplinary classifications of GIS and BIM functional integration, distills the value of data, and discusses the ontology-based data integration approach that GIS and BIM should take in the future to conduct research on integration applications in smart cities. To verify this view, keyword analysis, co-country analysis, and co-citation and coupling analyses are conducted using CiteSpace. GIS and BIM integration has attracted much attention. However, a professional disconnect and fragmented composition pose challenges in the field of GIS and BIM integration. Future research should focus on smart city planning, updating, management; ontology-based GIS and BIM data integration platform; and operation; and the collaborative management of urban rail transportation engineering.}
}
@article{CAO2025124297,
title = {Integrating the REA model and multisig architecture for blockchain-based supply chain traceability: An organisational-technical approach},
journal = {Technological Forecasting and Social Change},
volume = {219},
pages = {124297},
year = {2025},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2025.124297},
url = {https://www.sciencedirect.com/science/article/pii/S0040162525003282},
author = {Shoufeng Cao and Thomas Miller and Marcus Foth and Warwick Powell},
keywords = {Blockchain, Supply chain, Traceability, Accountability, Harmonisation, Organisational-technical integration},
abstract = {Blockchain-enabled supply chain traceability has transitioned from a new concept into a business practice and a customer expectation across industries. This paper explores an organisational-technical approach that integrates the structured business logic of the Resource-Event-Agent (REA) model with the security and accountability of multisig blockchain architecture to enhance blockchain-enabled supply chain traceability. This approach combines (i) an identity-based multisig mechanism, and (ii) unique REA-based traceable units to coordinate supply chain actors to propose, verify and register accountable data onto blockchain networks, while adhering to structured business practices for harmonised data traceability. The novelty of this approach lies in bridging blockchain's technical capabilities with business logic to mitigate garbage in and garbage out problems in achieving reliable and structured traceability in blockchain-enabled supply chains. The organisational-technical approach was developed and implemented to demonstrate its viability through an empirical pilot study in a food supply chain scenario. This paper offers theoretical advancements by integrating the REA ontological model with a multisig architecture to establish a robust framework for structured, secure, and accountable supply chain traceability, while providing practical insights through a scalable solution for industry adoption. By aligning technical innovations with organisational practices, this work makes a significant contribution to the digital transformation of supply chain ecosystems, fostering greater transparency, security, and trust to maximise social and economic impacts.}
}
@article{ALSAYED2020438,
title = {An intelligent cloud service discovery framework},
journal = {Future Generation Computer Systems},
volume = {106},
pages = {438-466},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.12.027},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19322630},
author = {Mustafa M. Al-Sayed and Hesham A. Hassan and Fatma A. Omara},
keywords = {Cloud computing, Cloud ontology, OBDA, Information retrieval, Cloud service discovery, Functional features, Non-functional features},
abstract = {Due to the heterogeneity nature and the huge number of published cloud services, as well as, the continuous increase of the users’ complexity demands, the discovery process of cloud services remains an open issue. According to our conducted comprehensive survey study, the existing cloud service discovery solutions suffer from a set of drawbacks; missing a standardized and comprehensive specification of cloud services, not considering the lack of knowledge about cloud concepts, and not considering the continuous increase of the complex cloud services published to meet the complexity of the users’ demands. In this paper, a comprehensive, standardized, flexible, and intelligent cloud service discovery framework is proposed to overcome these drawbacks. According to this framework, a comprehensive ontology has been developed to provide a standardized semantic specification of cloud services based on their functional features and non-functional features. For exploiting the high performance of the relational databases in managing the large amounts of data and improving the effectiveness of the proposed framework, instances of the non- functional features’ ontological concepts are separated from the developed ontology to be stored in a relational database, where Ontop OBDA platform is used to translate the complex queries over the ontology to SQL queries that are understood by the underlying relational database source. Also, our proposed framework provides a user-centric interface that enables users with low knowledge about cloud concepts to compose complex queries using their natural English language. The effectiveness evaluation shows that the proposed framework provides better results than other solutions. According to the implementation results, the average amount of error expected to identify a service by using the proposed framework is 11% compared to 31% by using the Cloudle service discovery solution. Also, the framework achieves entirely the cloud service discovery evaluation criteria compared to 66% of these criteria for the current cloud service discovery solutions. For efficiency, the proposed framework achieves 88% F-Score for mapping cloud services into suitable functional features and 80% for discovering services that match the users’ queries.}
}
@incollection{DAPAZ20232673,
title = {Development of a framework for simulation of biotechnological processes},
editor = {Antonios C. Kokossis and Michael C. Georgiadis and Efstratios Pistikopoulos},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {52},
pages = {2673-2678},
year = {2023},
booktitle = {33rd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-15274-0.50425-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044315274050425X},
author = {Priscila Marques {da Paz} and Caroline Satye Martins Nakama and Galo Antonio Carrillo {Le Roux}},
keywords = {bioprocesses, object-oriented programming, prototype, systems biology, unified modelling language},
abstract = {The development of Process Systems Engineering tools that can integrate experimental information with models is fundamental to obtain higher yields and increase productivity of a bioprocess. This is not a simple task, and there is a need for tools that facilitate the collaboration in a multidisciplinary group. In this context, developing computational frameworks for simulation and parameter estimation of bioprocesses that could be used comprehensively by researchers with different backgrounds is the goal of this work. The structure of such frameworks is carefully designed based on an ontology to describe bioprocesses. A helpful tool is the Unified Modeling Language and its use for software generalization is demonstrated along with a case study, showing that the software could serve as a model or example of good practice of software development for guiding simulations and parameter estimations of bioprocess in a structured way.}
}
@article{XUE2024127242,
title = {Relation-oriented few-shot knowledge graph prototype networks},
journal = {Neurocomputing},
volume = {575},
pages = {127242},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.127242},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224000134},
author = {Yingying Xue and Aibo Song and Jiahui Jin and Hui Peng and Jingyi Qiu and Xiaolin Fang and Xiaorui Zhai},
keywords = {Knowledge graph embedding, Ontology, Prototype network, Relation-oriented},
abstract = {Knowledge graph (KG) embedding has been widely researched, but it suffers from some problems in the few-shot scenarios. The topological and semantic connection among entities cannot satisfy the assumption of samples’ independent identically distribution. Additionally, these relations between entity pairs are various and complex. But most of the previous methods are node-oriented, which is weak in modeling complex relations. In order to solve the above problems, we propose a few-shot relation prototype network (FRPN). In our method, each relation is regarded as a learning object, instead of just entity pairs’ concatenation. A relation-oriented two-channel embedding mechanism is designed to achieve multi-scale information aggregation at the entity level and the relation level respectively. For the entity level, it aggregates information at different scales according to the relation type and the neighbors under each relation. For the relation level, our model obtains each kind of relation’s prototype from the ontology and the entity layer. The multi-scale aggregation contributes to KG embedding in the few-shot scenario. Compared with existing models, our model has significantly improved the performance on both the link prediction and triple classification tasks in two few-shot datasets.}
}
@article{FAN202332,
title = {Multimodal knowledge graph construction of Chinese traditional operas and sentiment and genre recognition},
journal = {Journal of Cultural Heritage},
volume = {62},
pages = {32-44},
year = {2023},
issn = {1296-2074},
doi = {https://doi.org/10.1016/j.culher.2023.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S1296207423000584},
author = {Tao Fan and Hao Wang and Tobias Hodel},
keywords = {Digital humanities, Intangible cultural heritage, Traditional operas, Multimodal knowledge graph, Sentiment and genre recognition},
abstract = {The advancement of digital technologies promotes the documentation of traditional operas, leaving a large amount of data but in a state of fragmentation. Constructing a knowledge graph (KG) is an effective way to realize the knowledge integration and reduce fragmentation, which can help the public understand traditional operas. However, constructed KGs in cultural heritage are mainly unimodal, lacking the ability to give the public a comprehensive perception, especially when they do not have related in-depth knowledge. In this paper, we take Chinese nation-level traditional operas as an example and construct a traditional opera ontology OpeOnto including classes with deep semantics (topic, sentiment). Then, we adopt an OpeOnto-driven way to construct multimodal knowledge graph OpeMKG including images and music links from several data resources. Aimed at analysing sentiments in OpeMKG and the automatic genre recognition of works for the preparation of automatic updating of OpeMKG, we develop a novel unified sentiment and genre recognition model (SGRM) for traditional operas with multimodal fusion and multi-task learning (MTL). The proposed model is examined on the built dataset of traditional operas and experimental results demonstrate its superiority compared with several state-of-the-art baselines.}
}
@article{WU2022100891,
title = {A semantic web approach to uplift decentralized household energy data},
journal = {Sustainable Energy, Grids and Networks},
volume = {32},
pages = {100891},
year = {2022},
issn = {2352-4677},
doi = {https://doi.org/10.1016/j.segan.2022.100891},
url = {https://www.sciencedirect.com/science/article/pii/S2352467722001497},
author = {Jiantao Wu and Fabrizio Orlandi and Tarek AlSkaif and Declan O’Sullivan and Soumyabrata Dev},
keywords = {Heterogeneous data, Household energy systems, Linked Data, Ontology, Semantics web, Decentralized energy data},
abstract = {In a decentralized household energy system comprised of various devices such as home appliances, electric vehicles, and solar panels, end-users are able to dig deeper into the system’s details and further achieve energy sustainability if they are presented with data on the electric energy consumption and production at the granularity of the device. However, many databases in this field are siloed from other domains, including solely information pertaining to energy. This may result in the loss of information (e.g. weather) on each device’s energy use. Meanwhile, a large number of these datasets have been extensively used in computational modeling techniques such as machine learning models. While such computational approaches achieve great accuracy and performance by concentrating only on a local view of datasets, model reliability cannot be guaranteed since such models are very vulnerable to data input fluctuations when information omission is taken into account. This article tackles the data isolation issue in the field of smart energy systems by examining Semantic Web methods on top of a household energy system. We offer an ontology-based approach for managing decentralized data at the device-level resolution in a system. As a consequence, the scope of the data associated with each device may easily be expanded in an interoperable manner throughout the Web, and additional information, such as weather, can be obtained from the Web, provided that the data is organized according to W3C standards.}
}
@article{AHMED20226505,
title = {Arabic Knowledge Graph Construction: A close look in the present and into the future},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {9},
pages = {6505-6523},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2022.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S1319157822001240},
author = {Ibrahim A. Ahmed and Fatima N. AL-Aswadi and Khaled M.G. Noaman and Wafa' Za'al Alma'aitah},
keywords = {Arabic Knowledge Graph, Knowledge Graph Construction, Knowledge Representation, Ontology},
abstract = {With the widespread growth of data on the Web, the need for efficient methods to get and arrange valuable information from these big noisy data is increased. The knowledge graph (KG) is a way to represent and organize the data in a more efficient and easy way to modify, use, and understand. Recently, KG has become a new hotspot topic in academic and business research; it is used in many applications such as intelligent question-answering (QA), recommender systems, map navigation, etc. There has been a tendency to construct KG in different languages such as English, Chinese, Persian, or Arabic. Constructing KG faces many challenges and obstacles, especially constructing Arabic Knowledge Graph (AKG) due to the sparse Arabic data in online encyclopedias and academic research, as well as the lack of tools that can process the proprietary nature of the Arabic language effectively, besides other challenges. This research aims to review and discuss KG construction best practices (systems, phases, problems, and challenges) with highlighting the Arabic perspective. Besides, it elaborates a classification of the AKG challenges and investigates the potential solutions and opportunities that might define the key future research directions of constructing AKG.}
}
@article{BENTRCIA2018382,
title = {Extracting semantic relations from the Quranic Arabic based on Arabic conjunctive patterns},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {30},
number = {3},
pages = {382-390},
year = {2018},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2017.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1319157817302094},
author = {Rahima Bentrcia and Samir Zidat and Farhi Marir},
keywords = {Ontology, Text mining, Arabic AND conjunctive patterns, Arabic grammar, Semantic relations, Quranic Arabic Corpus},
abstract = {There is an immense need for information systems that rely on Arabic Quranic ontologies to provide a precise and comprehensive knowledge to the world. Since semantic relations are a vital component in any ontology and many applications in Natural Language Processing strongly depend on them, this motivates the development of our approach to extract semantic relations from the Quranic Arabic Corpus, written in Arabic script, and enrich the automatic construction of Quran ontology. We focus on semantic relations resulting from proposed conjunctive patterns which include two terms with the conjunctive AND enclosed in between. The strength of each relation is measured based on the correlation coefficient. Finally, we evaluate the significance of this method by using hypotheses testing and Student t-test. The obtained results are very promising since we combine an accurate Arabic grammar with strong statistical techniques to prove the existence and measure the strength of this type of semantic relations.}
}
@incollection{SATHIO202557,
title = {Chapter 4 - Integration of blockchain, IoT, fog computing, and semantic technologies},
editor = {Tuan Anh Nguyen},
booktitle = {Digital Twin and Blockchain for Sensor Networks in Smart Cities},
publisher = {Elsevier},
pages = {57-105},
year = {2025},
isbn = {978-0-443-30076-9},
doi = {https://doi.org/10.1016/B978-0-443-30076-9.00005-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443300769000054},
author = {Anwar Ali Sathio and Muhammad Malook Rind and Shafique Ahmed Awan and Allah Rakhio Junejo and Sameer Ali},
keywords = {Fog computing, IoT, semantic-driven, blockchain, 5G, integration},
abstract = {Integrating Blockchain, Fog Computing, semantic technologies, and the Internet of Things (IoT) have driven capabilities, particularly in banking, healthcare, and supply chain management. In the framework of a post-5G network, this chapter examines these technologies’ current state and potential future paths. The chapter begins with an overview of fog computing integration with IoT systems, emphasizing how it facilitates edge computing and reduces data processing latency. Next, the evolution of semantic technologies—like the Resource Description Framework and Web Ontology Language—and its potential applications to enhance data interoperability and semantic comprehension in IoT ecosystems are discussed. In addition, the chapter explores how Blockchain technology may be integrated with fog-IoT networks, emphasizing how this might improve security, openness, and confidence in decentralized systems. It details the idea of Semantic-Driven Blockchains, in which semantic annotations improve transaction data and make it possible for smart contracts and automated decision-making to become more complex. The chapter also addresses how these technologies are now being used in different sectors, describing the difficulties that still need to be solved and possible future study areas. Additionally, it offers information about how these developments are expected to affect network design, edge computing, data governance, and future 5G and beyond networks. This chapter thoroughly examines the state of play and future possibilities of Semantic-driven Blockchain and Fog-IoT technologies within changing network paradigms beyond 5G. Fully utilizing these technologies to address new issues and open up new opportunities across various application areas emphasizes the significance of interdisciplinary research and cooperation.}
}
@article{MAKONI202280,
title = {Southern perspectives of language and the construction of the common},
journal = {Language & Communication},
volume = {86},
pages = {80-86},
year = {2022},
issn = {0271-5309},
doi = {https://doi.org/10.1016/j.langcom.2022.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0271530922000374},
author = {Sinfree Makoni and Cristine Severo},
keywords = {Southern perspectives, Common, Decoloniality, Social voices},
abstract = {In this paper, we build on the decolonial integrational linguistic perspective proposed by Makoni and Pablé. Based on integrationist principles, through which we avoid all-encompassing interpretations and methodologies that do not adequately engage with local experiences and voices, we construe decolonization as an ongoing project that interrogates the epistemological and political boundaries that isolate the Other from the common world. We argue that public or shared experiences recognized as ‘Global South/s’ can contribute to our ways of approaching the common and expand our understanding of the meaning of development. We provide an overview of the notion of the common and discuss the role played by languages and Southern epistemologies in its construction. We then propose an active and dynamic notion of the common that recognizes our capacity to build collective and plural spaces through common acts of learning and sharing.}
}
@article{ALAOUI20223262,
title = {An original Data, Information and Knowledge management approach for model-based engineering projects},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {3262-3267},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.135},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322021486},
author = {M. El Alaoui and S. Rabah and V. Chapurlat and V. Richet and R. Plana},
keywords = {System Engineering, Model Based System Engineering (MBSE), Knowledge management, SE Ontology, Large engineering projects, data, information, knowledge, model},
abstract = {Data, Information and Knowledge (DIK) problematics are of undeniable growing actuality; many efforts were made to explore and make progress in this domain. Therefore, these DIK have been defined by several characteristics conventionally studied such as volume, variety, variability… and a few solutions have been uncovered and revealed. In parallel, the systems Engineering (SE) and specifically Model-Based Systems Engineering (MBSE) approaches, that encourage, among other things, the use of models instead of documents in critical infrastructure engineering namely in the energy infrastructure domain, participate in the growing volume and complexity of DIK. In this particular context, it is important to enrich MBSE by adapting the existing DIK advances to the MBSE needs, which is initiated in this paper, first by providing DIK definition, second, by checking what are the main issues that shall be solved in order to help stakeholders use and manage them in large MBSE-driven projects, involving various business actors during more or less long periods. At last, this article proposes a methodological contribution bridging the supposed gap between MBSE and so-called data management fields of research.}
}
@article{LIU2023103973,
title = {SMFM-based analogy retrieval tool for the conceptual design of innovative products},
journal = {Computers in Industry},
volume = {151},
pages = {103973},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.103973},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523001239},
author = {Hongwei Liu and Yan Li and Ziqian Bai and Yimin Wang},
keywords = {Engineering design, Design knowledge representation, Analogy retrieval, Design-by-analogy, Computer-aided innovation},
abstract = {Design-by-Analogy (DbA) is a powerful approach to innovative conceptual design. Cross-domain analogies are the stimulated sources of inspiration for generating new concepts. Therefore, how to efficiently retrieve them becomes an essential issue to be solved. This paper proposes a tool based on a structure-mapping function model (SMFM) for analogy retrieval. Inspired by the structure-mapping theory, SMFM considers two in-depth features: functional relation and causal relation between functions, which are expressed by function and meta-function concepts, respectively. SMFM serves to capture design knowledge from instances (i.e., engineering or biological systems) to establish a case database. The SMFM ontology is constructed for linking knowledge representation and analogy retrieval. Its meta-function, function, and flow terms are used to index the modeled instances so as to establish an index database. Analogy retrieval is realized by the ontology-based query expansion, vector space model and weighted cosine similarity. By inputting function or meta-function queries, we retrieve the required cross-domain analogies, whose design knowledge is displayed by function models to inspire target functions or meta-functions realization. Finally, the tool is evaluated for its effectiveness in cross-domain analogies retrieval. An application example illustrates its practicality in aiding innovative design. Additionally, an experiment was conducted to test the tool. The results indicate that our tool can assist users (e.g., engineering designers) to more quickly generate new schemes to realize target meta-functions and the quality of schemes is higher.}
}
@article{ECCLES2025284,
title = {OP0349 Pathological mechanisms in fibromyalgia and hypermobility; a novel human model using an inflammatory challenge: insights from transcriptomics},
journal = {Annals of the Rheumatic Diseases},
volume = {84},
pages = {284-285},
year = {2025},
note = {EULAR 2025: European Congress of Rheumatology},
issn = {0003-4967},
doi = {https://doi.org/10.1016/j.ard.2025.05.351},
url = {https://www.sciencedirect.com/science/article/pii/S0003496725013846},
author = {J. Eccles and J. Porter and M. Amato and K. Themelis and H. Critchley and S. Newbury and N. Harrison and B. Towler and K. Davies},
keywords = {Cognitive Function and Mental Health, -omics, Pain, Fatigue},
abstract = {Background:
Fibromyalgia, characterised by chronic pain and fatigue constitutes a major clinical problem in Europe and elsewhere. Hypermobile conditions, including Ehlers-Danlos syndrome and joint hypermobility syndrome, encompass a broad spectrum of heterogenous clinical entities characterised by excessive range of joint movements and variable extra-articular features. The etiological basis of these conditions is thought to relate to abnormal extra cellular matrix (e.g. collagen) assembly, which impairs the structural integrity of connective tissue. All rheumatologists will recognise a clinical phenotype characterised by fibromyalgia, neuropsychiatric features (eg anxiety, bipolar disorder, ADHD, autism), hypermobility, and autonomic dysfunction (eg POTS). We have also recently described a strong association between ‘Long COVID' (in which condition chronic pain and fatigue are major features) and hypermobility [1]. However, the pathologic mechanisms underlying these clinical associations remain both debated and incompletely understood [2].
Objectives:
In this study we applied a transcriptomic approach to explore gene expression in fibromyalgia, at baseline, and after a controlled experimental inflammatory challenge. Our hypothesis is that hypermobility is constitutionally associated with potentially deranged immunological function and following an inflammatory insult, a range of different pathways are triggered that result in neuropsychiatric, rheumatological and cardiovascular sequelae.
Methods:
We tested for the effects of hypermobility on gene expression, both at baseline and following an inflammatory challenge, by analysing existing transcriptomic data gathered from healthy subjects and patients with fibromyalgia and ME/CFS (ISRCTN78820481) using total RNA sequencing of blood samples (Illumina NextSeq 500). From each participant, two blood samples were taken on separate occasions: one after a saline injection and the other after an inflammatory challenge, typhoid vaccination, in a double-blind, randomised, cross-over design. Participants were assigned to hypermobile or non-hypermobile group according to Criterion 1 of the 2017 for Hypermobile Ehlers-Danlos Syndrome (hEDS) diagnostic criteria. Differential gene expression analysis comparing hypermobile and non-hypermobile participants was performed using EdgeR software in R studio. Baseline differences were determined by comparing post-saline samples between groups. Differential responses to the inflammatory challenge were detected by comparison of post-typhoid samples between groups. Gene ontology on differentially expressed genes was performed using DAVID's Functional Annotation Clustering tool.
Results:
Gene ontology analysis of differentially expressed genes (hypermobile versus non-hypermobile) at baseline were found to associate with six diseases (AIDS, leiomyoma, cardiomegaly, eczema, atopic dermatitis and SLE). After inflammatory challenge compared to baseline, the hypermobile group displayed a gene expression profile associated with 27 diseases including substance use disorders, bipolar disorder, autism, ADHD and Parkinson's disease, myocardial infarction and atrial fibrillation. (Figure 1. Gene ontology analysis in hypermobile versus non- hypermobile individuals following an inflammatory challenge.). 
Conclusion:
Here, we note at baseline, multiple associations with hypermobility and immunological disease, both immunodeficiency and autoimmunity, including SLE and atopic dermatitis, which suggests impaired immunological regulation in hypermobile patients. After inflammatory challenge there were associations with a large number of cardiovascular and neuropsychiatric conditions, notably ADHD and bipolar disorder. This suggests a two-factor model whereby hypermobility is indeed constitutionally associated with deranged immunological function. Following an inflammatory insult, neuropsychiatric and cardiovascular sequelae may occur. This model potentially explains the common association of fibromyalgia/CFS and mental health disorders with what may be an acute, often infective, inflammatory precipitant, or in some instances a more chronic auto-immune stimulus. Further research is clearly needed to delineate the nature of such mechanistic links, with a view to identifying potential therapeutic targets, and/or developing a personalized medicine approach which may mitigate the risk of developing chronic sequelae after say, COVID or other viral infection, in vulnerable individuals.
REFERENCES:
[1] Jessica A Eccles, Dorina Cadar, Lisa Quadt, Alan J Hakim, Nicholas Gall, Covid Symptom Survey Biobank Consortium, Vicky Bowyer, Nathan Cheetham, Claire J Steves, Hugo D Critchley, Kevin A Davies - Is joint hypermobility linked to self-reported non-recovery from COVID-19? Case–control evidence from the British COVID Symptom Study Biobank: BMJ Public Health 2024; 2:e000478. https://hdl.handle.net/10779/uos.25461169.v1 [2] Jessica A Eccles, Beth Thompson, Kristy Themelis, Marisa L Amato, Robyn Stocks, Amy Pound, Anna-Marie Jones, Zdenka Cipinova, Lorraine Shah-Goodwin, Jean Timeyin, Charlotte R Thompson, Thomas Batty, Neil A Harrison, Hugo D Critchley, Kevin A Davies. Beyond bones: the relevance of variants of connective tissue (hypermobility) to fibromyalgia, ME/CFS and controversies surrounding diagnostic classification: an observational study. Clin Med (Lond) 2021;21:53–8.
Acknowledgements:
Versus Arthritis.
Disclosure of Interests:
None declared. © The Authors 2025. This abstract is an open access article published in Annals of Rheumatic Diseases under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/). Neither EULAR nor the publisher make any representation as to the accuracy of the content. The authors are solely responsible for the content in their abstract including accuracy of the facts, statements, results, conclusion, citing resources etc.}
}
@article{WANG2022105159,
title = {Safe medicine recommendation via star interactive enhanced-based transformer model},
journal = {Computers in Biology and Medicine},
volume = {141},
pages = {105159},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.105159},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521009537},
author = {Nanxin Wang and Xiaoyan Cai and Libin Yang and Xin Mei},
keywords = {Medicine recommendation, Star interactive enhanced-based transformer, Graph embedding},
abstract = {With the rapid development of electronic medical records (EMRs), most existing medicine recommendation systems based on EMRs explore knowledge from the diagnosis history to help doctors prescribe medication correctly. However, due to the limitations of the EMRs’ content, recommendation systems cannot explicitly reflect relevant medical data, such as drug interactions. In recent years, medicine recommendation approaches based on medical knowledge graphs and graph neural networks have been proposed, and the methods based on the Transformer model have been widely used in medicine recommendation systems. Transformer-based medicine recommendation approaches are readily applicable to inductive problems. Unfortunately, traditional Transformer-based medicine recommendation approaches require complex computing power and suffer information loss among the multi-heads in Transformer model, which causes poor performance. At the same time, these approaches have rarely considered the side effects of drug interaction in traditional medical recommendation approaches. To overcome the drawbacks of the current medicine recommendation approaches, we propose a Star Interactive Enhanced-based Transformer (SIET) model. It first constructs a high-quality heterogeneous graph by bridging EMR (MIMIC-III) and a medical knowledge graph (ICD-9 ontology and DrugBank). Then, based on the constructed heterogeneous graph, it extracts a disease homogeneous graph, a medicine homogeneous graph, and a negative factors homogeneous graph to get auxiliary information of disease or drug (named enhanced neighbors). These are fed into the SIET model in conjunction with the relevant information in the EMRs to obtain representations of diseases and drugs. It finally generates the recommended drug list by calculating the cosine similarity between disease combination representations and drug combination representations. Extensive experiments on the MIMIC-III, DrugBank, and ICD-9 ontology datasets demonstrate the outstanding performance of our proposed model. Meanwhile, we show that our SIET model outperforms strong baselines on an inductive medicine recommendation task.}
}
@article{WANG2021906,
title = {Data modeling and evaluation of deep semantic annotation for cultural heritage images},
journal = {Journal of Documentation},
volume = {77},
number = {4},
pages = {906-925},
year = {2021},
issn = {0022-0418},
doi = {https://doi.org/10.1108/JD-06-2020-0102},
url = {https://www.sciencedirect.com/science/article/pii/S0022041821000278},
author = {Xiaoguang Wang and Ningyuan Song and Xuemei Liu and Lei Xu},
keywords = {Cultural heritage, Deep semantic annotation, Information modeling, Narrative image, Dunhuang murals},
abstract = {Purpose
To meet the emerging demand for fine-grained annotation and semantic enrichment of cultural heritage images, this paper proposes a new approach that can transcend the boundary of information organization theory and Panofsky's iconography theory.
Design/methodology/approach
After a systematic review of semantic data models for organizing cultural heritage images and a comparative analysis of the concept and characteristics of deep semantic annotation (DSA) and indexing, an integrated DSA framework for cultural heritage images as well as its principles and process was designed. Two experiments were conducted on two mural images from the Mogao Caves to evaluate the DSA framework's validity based on four criteria: depth, breadth, granularity and relation.
Findings
Results showed the proposed DSA framework included not only image metadata but also represented the storyline contained in the images by integrating domain terminology, ontology, thesaurus, taxonomy and natural language description into a multilevel structure.
Originality/value
DSA can reveal the aboutness, ofness and isness information contained within images, which can thus meet the demand for semantic enrichment and retrieval of cultural heritage images at a fine-grained level. This method can also help contribute to building a novel infrastructure for the increasing scholarship of digital humanities.}
}
@article{JIANG2024102530,
title = {Product innovation design approach driven by implicit relationship completion via patent knowledge graph},
journal = {Advanced Engineering Informatics},
volume = {61},
pages = {102530},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102530},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624001782},
author = {Shaofei Jiang and Jingwei Yang and Jing Xie and Xuesong Xu and Yubo Dou and Liting Jing},
keywords = {Product innovation design, Patent text, Knowledge graph, RFSB ontology model, Implicit relationship completion},
abstract = {Product innovation design process involves a great deal of discrete engineering knowledge, limiting the ability of designers to quickly utilize this knowledge to support design innovation. Nowadays, innovation design based on knowledge graphs has enhanced the ability to explore design knowledge, improving the efficiency of knowledge retrieval. Previous studies have focused on mining more design knowledge to enrich the knowledge graph overlooks the implicit relationships with potential value among design knowledge, wasting design resources. To address these issues, an approach for product innovation design based on implicit knowledge relationship completion in the patent knowledge graph is proposed, which explores the implicit relationships between design knowledge to provide new knowledge satisfying design preferences and enhance the innovativeness of solutions. First, a requirements-function-structure-benefit (RFSB) knowledge ontology is constructed and extracted from the benefit knowledge of patents to build the knowledge graph. Second, an implicit relationship completion model based on the similarity of function or benefit entities explores the implicit relationships, replacing structure entities directly connected to similar function or benefit entities to generate new relationships and outputs novel ideas. Third, a scheme improvement process based on the co-occurrence frequency of requirement and structure knowledge supplements neglected design preferences. Final, a pipeline inspection robot case study is further employed to verify the proposed approach, and a patent knowledge graph assisted design solution prototype system is developed to assist in the utilization of innovative design knowledge. Evaluation results show the significant design potential of the proposed approach in inspiring innovative thinking and knowledge reuse.}
}
@article{HANGLOO2025130827,
title = {Multimodal fusion techniques: Review, data representation, information fusion, and application areas},
journal = {Neurocomputing},
volume = {649},
pages = {130827},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.130827},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225014997},
author = {Sakshini Hangloo and Bhavna Arora},
keywords = {Representation learning, Multimodal fusion, Heterogeneity gap, Multimodality, Natural Language Processing (NLP), Computer Vision (CV), Vision-Language Pre-training (VLP)},
abstract = {Multimodality refers to a model's ability to process and integrate information from various sources, such as visual, audio, and textual data. In real-world scenarios, data is often collected across multiple modalities, necessitating effective techniques for their fusion. The task of fusion of these features is called multimodal fusion. While multimodal learning aims to combine complementary information from multiple modalities to form a unified representation or improve task performance, cross-modal learning emphasizes the mapping, alignment, or translation between modalities—such as generating images from textual descriptions or retrieving videos based on audio cues. This survey provides a comprehensive overview of unimodal and multimodal representation learning, identifying cross-modal correlations. Models like LSTM, GRU, and BERT are highlighted for semantic feature extraction, while architectures such as VGGNet, MobileNet, InceptionNet, and ResNet are discussed for visual features, along with recent advancements in Large Language Models (LLMs) and Vision Transformers (ViTs). The early, late, and intermediate fusion techniques for presenting a combined multimodal representation are discussed, with further analysis of the joint, coordinated, and encoder-decoder representation frameworks. Various conventional and contemporary fusion models, such as Attention networks, Transformers are discussed in depth, encompassing different application domains that revolve around text and visual data. In the contemporary scenario where big data is the trend and multimodality is a concern, the various aspects of volume, varsity, and variety of multimodal Big Data and its challenges have also been discussed and addressed. Various Vision-Languge Pre-trained (VLP) models and Multimodal Large Language Models (MLLMs) such as ALIGN, Florence, ALBEF, ViLT, and VLMo are highlighted. Key application areas, benchmark datasets, and evaluation metrics are outlined to provide a holistic understanding of the field. The survey concludes by exploring pressing challenges and outlining potential directions for future research in multimodal fusion.}
}
@article{NUNDLOLL2021100064,
title = {A semantic approach to enable data integration for the domain of flood risk management},
journal = {Environmental Challenges},
volume = {3},
pages = {100064},
year = {2021},
issn = {2667-0100},
doi = {https://doi.org/10.1016/j.envc.2021.100064},
url = {https://www.sciencedirect.com/science/article/pii/S2667010021000433},
author = {Vatsala Nundloll and Rob Lamb and Barry Hankin and Gordon Blair},
keywords = {Ontologies, Structured data, Unstructured data, Semantic integration, Natural language processing, Flood risk management},
abstract = {With so many things around us continuously producing and processing data, be it mobile phones, or sensors attached to devices, or satellites sitting thousands of kilometres above our heads, data is becoming increasingly heterogeneous. Scientists are inevitably faced with data challenges, coined as the 4 V’s of data - volume, variety, velocity and veracity. In this paper, we address the issue of data variety. The task of integrating and querying such heterogeneous data is further compounded if the data is in unstructured form. We hence propose an approach using Semantic Web and Natural Language Processing techniques to resolve the heterogeneity arising in data formats, bring together structured and unstructured data and provide a unified data model to query from disparate data sets.}
}
@article{PATEL2022,
title = {InBiodiv-O:},
journal = {International Journal of Information System Modeling and Design},
volume = {13},
number = {7},
year = {2022},
issn = {1947-8186},
doi = {https://doi.org/10.4018/IJISMD.315021},
url = {https://www.sciencedirect.com/science/article/pii/S1947818622000333},
author = {Archana Patel and Sarika Jain and Narayan C. Debnath and Vishal Lama},
keywords = {Biodiversity, Evaluation, Methodology, Ontology, Semantic Model},
abstract = {ABSTRACT
To present the biodiversity information, a semantic model is required that connects all kinds of data about living creatures and their habitats. The model must be able to encode human knowledge for machines to be understood. Ontology offers the richest machine-interpretable semantics that are being extensively used in the biodiversity domain. Various ontologies are developed for the biodiversity domain; however, these ontologies are not capable to define the Indian biodiversity information though India is one of the megadiverse countries. To semantically analyze the Indian biodiversity information, it is crucial to build an ontology that describes all the terms of this domain. Since the curation of the ontology depends on the domain where these are used, there is no ideal methodology defined yet. The aim of this article is to develop an ontology that semantically encodes all the terms of Indian biodiversity information in all its dimensions based on the proposed methodology. The evaluation of the proposed ontology depicts that ontology is well built in the specified domain.}
}
@article{AKHTAR2025109660,
title = {Multilingual entity alignment by abductive knowledge reasoning on multiple knowledge graphs},
journal = {Engineering Applications of Artificial Intelligence},
volume = {139},
pages = {109660},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109660},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624018189},
author = {Muhammad Usman Akhtar and Jin Liu and Zhiwen Xie and Xiaohui Cui and Xiao Liu and Bo Huang},
keywords = {Entity alignment, Abductive reasoning, Structural semantics, Knowledge retrieval, Information fusion},
abstract = {Objectives:
Entity alignment (EA) seeks to identify similar real-world objects in different multilingual knowledge graphs (KGs), also known as ontology alignment. EA assists in handling a wide range of language semantics and in building integrated knowledge bases. However, most mainstream studies have focused on structural information, paying little attention to insufficient contextual information and limited handling of complex relationships. This paper aims to address these limitations and improve EA performance and efficiency.
Methods:
This paper investigates multilingual EA techniques and proposes a novel Abductive Knowledge Reasoning (AKR) model to address these issues. AKR can compute complex relationship semantics context by reasoning and enrich counterpart entity contextual information through centrality calculation, which helps connect distant entities in multilingual KGs.
Novelty:
The proposed AKR model introduces a new approach to EA by integrating centrality calculation and relational semantics reasoning. This method overcomes the limitations of existing EA techniques by effectively handling insufficient contextual information and complex relationships in multilingual KGs.
Findings:
AKR outperforms all state-of-the-art EA models across five datasets. AKR achieves Hit@1 score of 79.4%, for entity alignment between Chinese-to-English knowledge graphs representing 19.9% improvement over the best-performing translation-based model, Neighborhood-Aware Attentional Representation Entity Alignment, and a 5.0% improvement over the best-performing graph neural network-based model, Relational Semantics Augmentation.}
}
@article{CHEN2025103124,
title = {Knowledge Graphs for Multi-modal Learning: Survey and Perspective},
journal = {Information Fusion},
volume = {121},
pages = {103124},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103124},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525001976},
author = {Zhuo Chen and Yichi Zhang and Yin Fang and Yuxia Geng and Lingbing Guo and Jiaoyan Chen and Xiaoze Liu and Jeff Z. Pan and Ningyu Zhang and Huajun Chen and Wen Zhang},
keywords = {Knowledge Graphs, Multi-modal Learning, Knowledge-based Information Fusion, Visual Question Answering, Large Language Model, Literature review},
abstract = {Integrated with multi-modal learning, knowledge graphs (KGs) as structured knowledge repositories, can enhance AI for processing and understanding complex, real-world data. This paper provides a comprehensive survey of cutting-edge research on KG-aware multi-modal learning. For these core areas, we provide task definitions, evaluation benchmarks, and comprehensive insights into key breakthroughs, offering detailed explanations critical for conducting related research. Furthermore, we also discuss current challenges, highlighting emerging trends and future research directions. The repository for this paper can be found at https://github.com/zjukg/KG-MM-Survey.}
}
@article{FOTOPOULOU2021100015,
title = {EmoSocio: An open access sociometry-enriched Emotional Intelligence model},
journal = {Current Research in Behavioral Sciences},
volume = {2},
pages = {100015},
year = {2021},
issn = {2666-5182},
doi = {https://doi.org/10.1016/j.crbeha.2021.100015},
url = {https://www.sciencedirect.com/science/article/pii/S2666518221000024},
author = {Eleni Fotopoulou and Anastasios Zafeiropoulos and Symeon Papavassiliou},
keywords = {Emotional Intelligence model, Sociometry, Social network analysis, Psychological assessment, EmoSocio ontology, Collective emotional intelligence},
abstract = {Significant efforts have been allocated over the last thirty years towards the definition and measurement of the Emotional Intelligence (EI) construct. Several EI theories and models have been produced to support psychological assessment processes. However, barriers are identified for their wide adoption and exploitation by social scientists. The absence of a common structured format to represent concepts in EI models has resulted in lack of clarity and consistency, while hindering comparison, validation and extensive evaluation processes. Provision of open access to such models and measurement instruments has not been promoted so far, however, considered crucial for their wide adoption. Furthermore, the inclusion of indexes from the sociometry domain can facilitate participatory modeling by multidisciplinary scientists during the development of social and emotional training programs. To address these challenges, we propose EmoSocio, an open access Emotional Intelligence Model, built upon a detailed comparison and synthesis of the main constructs represented in widely accepted EI models and enriched with sociometric indexes at an individual and group level. Upon detailing the methodological approach followed for the development of the EmoSocio model, we present the EI and social constructs of the model, followed by an assessment of the EI part in terms of reliability and validity. EmoSocio is also represented in a semantically-enriched format in the form of an ontology. Our ambition is to provide an open access EI model that can be used by multidisciplinary scientists to evaluate psychological assessment processes and develop interventions, aiming to strengthen interpersonal and intrapersonal competencies.}
}
@article{BENAVENT2019195,
title = {How to Restructure PPDRC and MIRC According to DOLCE},
journal = {Procedia Manufacturing},
volume = {28},
pages = {195-200},
year = {2019},
note = {7th International conference on Changeable, Agile, Reconfigurable and Virtual Production (CARV2018)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.12.032},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918313751},
author = {Sergio Benavent and Pedro Rosado and Lorenzo Solano and Nicola Guarino and Emilio Sanfilippo},
keywords = {DOLCE, Manufacturing, Ontology},
abstract = {The use of ontologies in information systems for engineering applications calls for ontologies representing experts’ knowledge in a principled and robust manner. The work we present in the paper brings together, on the one hand, two ontologies developed for the engineering domain, namely PPDRC and MIRC and, on the other hand, the DOLCE foundational ontology. The former two have been developed by taking DOLCE as reference model for distinguishing between the high-level entities they quantify over. However, the relation with DOLCE calls for improvements. The ultimate purpose of the paper is to show how the conceptual transparency of domain ontologies can be enhanced by (properly) using a foundational ontology.}
}
@article{KREMEN20191,
title = {Improving discoverability of open government data with rich metadata descriptions using semantic government vocabulary},
journal = {Journal of Web Semantics},
volume = {55},
pages = {1-20},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300714},
author = {Petr Křemen and Martin Nečaský},
keywords = {Linked data, Open data, Ontology, Data catalog, Dataset discovery},
abstract = {The descriptive metadata gathered by open data catalogs are often simple key–value pairs that describe provenance information, but not concepts from the domain of the described dataset. Search engines relying on such metadata cannot make use of semantic connections among datasets. In this paper, we present a Semantic Government Vocabulary that is used for creating rich annotations of Open Government Data, allowing to find their mutual interconnections, as well as document their meaning in the machine readable form. We discuss how the Semantic Government Vocabulary is layered based on the different ontological types of terms occurring in the Open Government Data. Next, we show how the vocabularies can be used to annotate Open Government Data on different levels of detail and how to formalize the whole stack in the Web Ontology Language. We evaluate feasibility and usability of our approach using a study in the elections domain.}
}
@article{ROSPOCHER2020100617,
title = {Knowledge-driven joint posterior revision of named entity classification and linking},
journal = {Journal of Web Semantics},
volume = {65},
pages = {100617},
year = {2020},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2020.100617},
url = {https://www.sciencedirect.com/science/article/pii/S1570826820300500},
author = {Marco Rospocher and Francesco Corcoglioniti},
abstract = {In this work we address the problem of extracting quality entity knowledge from natural language text, an important task for the automatic construction of knowledge graphs from unstructured content. More in details, we investigate the benefit of performing a joint posterior revision, driven by ontological background knowledge, of the annotations resulting from natural language processing (NLP) entity analyses such as named entity recognition and classification (NERC) and entity linking (EL). The revision is performed via a probabilistic model, called jpark, that given the candidate annotations independently identified by NERC and EL tools on the same textual entity mention, reconsiders the best annotation choice performed by the tools in light of the coherence of the candidate annotations with the ontological knowledge. The model can be explicitly instructed to handle the information that an entity can potentially be NIL (i.e., lacking a corresponding referent in the target linking knowledge base), exploiting it for predicting the best NERC and EL annotation combination. We present a comprehensive evaluation of jpark along various dimensions, comparing its performances with and without exploiting NIL information, as well as the usage of three different background knowledge resources (YAGO, DBpedia, and Wikidata) to build the model. The evaluation, conducted using different tools (the popular Stanford NER and DBpedia Spotlight, as well as the more recent Flair NER and End-to-End Neural EL) with three reference datasets (AIDA, MEANTIME, and TAC-KBP), empirically confirms the capability of the model to improve the quality of the annotations of the given tools, and thus their performances on the tasks they are designed for.}
}
@article{XUE202111,
title = {Relation-based multi-type aware knowledge graph embedding},
journal = {Neurocomputing},
volume = {456},
pages = {11-22},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.05.021},
url = {https://www.sciencedirect.com/science/article/pii/S092523122100758X},
author = {Yingying Xue and Jiahui Jin and Aibo Song and Yingxue Zhang and Yangyang Liu and Kaixuan Wang},
keywords = {Knowledge graph embedding, Graph attention network, Ontology, Taxonomy tree, Multi-type},
abstract = {Knowledge graph (KG) embedding projects the graph into a low-dimensional space and preserves the graph information. An essential part of a KG is the ontology, which always is organized as a taxonomy tree, depicting the type (or multiple types) of each entity and the hierarchical relationships among these types. The importance of considering the ontology during KG embedding lies in its ability to provide side-information, improving the downstream applications’ accuracy (e.g., link prediction, entity alignment or recommendation). However, the ontology has yet to receive adequate attention during the KG embedding, especially for instances where each entity may belong to multiple types. This ontology-enhanced KG embedding’s main challenges are twofold: determining how to discover the relationships among these types and how to integrate them with the entities’ relationship network. Although it is common to see attention-based models used in KG embedding, they cannot settle the issues raised simultaneously. Only a single type is assigned to each entity and the correlation among types are ignored in those models, leading to information loss and encumbered downstream tasks. To overcome these challenges, we propose a composite multi-type aware KG embedding model, whose main components are a multi-type layer and entity embedding layer. We model it as a natural language processing task at the multi-type layer to discover each entity’s multi-type feature and automatically capture their correlations. Additionally, a relation-based attention mechanism is conducted at the entity embedding layer, which aggregates neighborhoods’ information and integrates the multi-type layer’s information through common entities of these two layers. Through extensive experiments on two real KGs, we demonstrate that, compared to several state-of-the-art baselines, our Multi-Type aware Embedding (MTE) model achieves substantial gain in both Mean Rank and Hit@N for the link prediction task and accuracy for multi-type classification.}
}
@article{GOMEZCABRERA2024100567,
title = {ViLanIoT: A visual language for improving Internet of Things systems representation},
journal = {Journal of Industrial Information Integration},
volume = {38},
pages = {100567},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100567},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24000116},
author = {Alain Gomez-Cabrera and Ponciano J. Escamilla-Ambrosio and Jassim Happa},
keywords = {Internet of Things, Visual language, Language design, Physics of notation},
abstract = {With the rapidly evolving and changing nature of Internet of Things (IoT) technologies, there is an absence of tools to support the IoT system design and development. For example, there is not a specialized tool for representing IoT systems. The common solution to this problem is to use general-purpose modeling tools, such as the Unified Modeling Language (UML) and its extensions, which have some disadvantages when representing IoT systems. This paper proposes a new visual language, called ViLanIoT, for making representations of IoT systems conceived as cyber–physical systems. We argue that diagrams obtained with ViLanIoT are more understandable and intuitive compared to improvised representations found in literature. To illustrate the use of ViLanIoT, we represent a smart campus system with visual elements according to our proposal. In addition, we also present a study of the application of ViLanIoT. In ViLanIoT, the inclusion of visual elements for each component reflecting semantic meaning improves the understandability, clarity, and simplicity of IoT system representations.}
}
@article{UNGER2023,
title = {Data provenance - from experimental data to trustworthy simulation models and standards},
journal = {Materials Today: Proceedings},
year = {2023},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2023.08.081},
url = {https://www.sciencedirect.com/science/article/pii/S2214785323043341},
author = {Jörg F. Unger and Annika Robens-Radermacher and Erik Tamsen},
keywords = {FAIR data, Semantic web, Scientific workflows, Metadata, Reproducible data processing},
abstract = {FAIR (findable, accessible, interoperable and reusable) data usage is one of the main principals that many of the research and funding organizations include in their strategic plans, which means that following the main principals of FAIR data is required in many research projects. The definition of data being FAIR is very general. When implementing that for a specific application or project or even setting a standardized procedure within a working group, a company or a research community, many challenges arise. In this contribution, an overview about our experience with different methods and tools is outlined. We begin with a motivation on potential use cases for the application of FAIR data with increasing complexity starting from a reproducible research paper over collaborative projects with multiple participants such as Round-Robin tests up to data-based models within standardization codes, applications in machine learning or parameter estimation of physics-based simulation models. In a second part, different options for structuring the data (including metadata schema) are discussed. The first one is the openBIS system, which is an open-source lab notebook and PostgreSQL based data management system. A second option is a semantic representation using RDF based on ontologies for the domain of interest. In a third section, requirements for workflow tools to automate data processing are discussed and their integration into reproducible data analysis is presented with an outlook on required information to be stored as metadata in the database. Finally, the presented procedures are exemplarily demonstrated for the calibration of a temperature dependent constitutive model for additively manufactured mortar. A metadata schema for a rheological measurement setup is derived and implemented in an openBIS database. After a short review of a potential numerical model predicting the structural build-up behavior, the automatic workflow to use the stored data for model parameter estimation is demonstrated.}
}
@article{PRIESNITZFILHO20191,
title = {Privacy-preserving attribute aggregation in eID federations},
journal = {Future Generation Computer Systems},
volume = {92},
pages = {1-16},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.09.025},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17327966},
author = {Walter {Priesnitz Filho} and Carlos Ribeiro and Thomas Zefferer},
keywords = {Electronic identity, Identity federation, Attribute aggregation, Interoperability, Ontologies, Privacy},
abstract = {Personalized electronic services, e.g. from the e-government domain, need to reliably identify and authenticate users. During user-authentication processes, the electronic identity of the respective user is determined and required additional attributes, e.g. name and date of birth, linked to this identity are collected. This attribute-collection process can become complex, especially if required attributes are distributed over various attribute providers that are organized in a federated identity-management system. In many cases, these identity management systems rely on different ontologies and make use of different languages. Hence, identity federations, such as the one currently established across the European Union, require effective solutions to collect user attributes from different heterogeneous sources and aggregate them to a holistic user facet. At the same time, these solutions need to comply with minimum disclosure rules to preserve users’ privacy. In this article, we propose and introduce a solution for privacy-preserving attribute aggregation. Our solution combines attributes from different domains using ontology alignment and makes use of locality sensitive hashing functions to preserve users’ privacy. Evaluation results obtained from conducted experiments demonstrate our solution’s advantages for both, service providers and users. While service providers can be provided with a larger set of attributes, users remain in full control of their data and can decide on which of their attributes shall be revealed.}
}
@article{MANDEL2023897,
title = {Implementation and Assessment of a Comprehensive Model-Based Systems Engineering Methodology with Regard to User Acceptance in Practice},
journal = {Procedia CIRP},
volume = {119},
pages = {897-902},
year = {2023},
note = {The 33rd CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.03.135},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123005346},
author = {Constantin Mandel and Jerome Kaspar and Rebecca Heitmann and Sarah Horstmeyer and Alex Martin and Albert Albers},
keywords = {Model-Based Systems Engineering, modeling, product development, user-orientation, evaluation},
abstract = {Model-Based Systems Engineering (MBSE) is becoming increasingly popular, not only in research but also in industrial companies. However, MBSE approaches (i.e. methods, frameworks, ontologies, and tools) are usually developed at a scientific level, so that they are too generic and formal for a company's internal use leading to acceptance issues in industrial practice. Against this background, this contribution presents a comprehensive user-oriented MBSE methodology tackling this lack of acceptance in industrial practice. Based on ten previously derived fields of action for individual and organizational acceptance of MBSE approaches, a first positive evaluation of the MBSE methodology has been received. In this contribution, a further developed MBSE methodology is presented, which is created in cooperation of partners from research and industry. Assessment of this further developed MBSE methodology with six companies across different industries shows a positive impact on acceptance in comparison to existing MBSE approaches across the identified fields of action. Major improvements are seen regarding the perceived performance and benefit of MBSE, the usability of the modeling tool, and the communication within a development team. Smaller improvements are noted regarding the establishment of a clear target picture and modeling process, as well as in tackling ambiguity when modeling in a development team. In addition, research at one of the industrial partners shows that company-specific tailoring and implementation of the developed MBSE methodology can be performed in a fast and straightforward way.}
}
@article{OHLSEN2024,
title = {PCEtoFHIR: Decomposition of Postcoordinated SNOMED CT Expressions for Storage as HL7 FHIR Resources},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/57853},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424001236},
author = {Tessa Ohlsen and Josef Ingenerf and Andrea Essenwanger and Cora Drenkhahn},
keywords = {SNOMED CT, HL7 FHIR, TermInfo, postcoordination, semantic interoperability, terminology, OWL, semantic similarity},
abstract = {Background
To ensure interoperability, both structural and semantic standards must be followed. For exchanging medical data between information systems, the structural standard FHIR (Fast Healthcare Interoperability Resources) has recently gained popularity. Regarding semantic interoperability, the reference terminology SNOMED Clinical Terms (SNOMED CT), as a semantic standard, allows for postcoordination, offering advantages over many other vocabularies. These postcoordinated expressions (PCEs) make SNOMED CT an expressive and flexible interlingua, allowing for precise coding of medical facts. However, this comes at the cost of increased complexity, as well as challenges in storage and processing. Additionally, the boundary between semantic (terminology) and structural (information model) standards becomes blurred, leading to what is known as the TermInfo problem. Although often viewed critically, the TermInfo overlap can also be explored for its potential benefits, such as enabling flexible transformation of parts of PCEs.
Objective
In this paper, an alternative solution for storing PCEs is presented, which involves combining them with the FHIR data model. Ultimately, all components of a PCE should be expressible solely through precoordinated concepts that are linked to the appropriate elements of the information model.
Methods
The approach involves storing PCEs decomposed into their components in alignment with FHIR resources. By utilizing the Web Ontology Language (OWL) to generate an OWL ClassExpression, and combining it with an external reasoner and semantic similarity measures, a precoordinated SNOMED CT concept that most accurately describes the PCE is identified as a Superconcept. In addition, the nonmatching attribute relationships between the Superconcept and the PCE are identified as the “Delta.” Once SNOMED CT attributes are manually mapped to FHIR elements, FHIRPath expressions can be defined for both the Superconcept and the Delta, allowing the identified precoordinated codes to be stored within FHIR resources.
Results
A web application called PCEtoFHIR was developed to implement this approach. In a validation process with 600 randomly selected precoordinated concepts, the formal correctness of the generated OWL ClassExpressions was verified. Additionally, 33 PCEs were used for two separate validation tests. Based on these validations, it was demonstrated that a previously proposed semantic similarity calculation is suitable for determining the Superconcept. Additionally, the 33 PCEs were used to confirm the correct functioning of the entire approach. Furthermore, the FHIR StructureMaps were reviewed and deemed meaningful by FHIR experts.
Conclusions
PCEtoFHIR offers services to decompose PCEs for storage within FHIR resources. When creating structure mappings for specific subdomains of SNOMED CT concepts (eg, allergies) to desired FHIR profiles, the use of SNOMED CT Expression Templates has proven highly effective. Domain experts can create templates with appropriate mappings, which can then be easily reused in a constrained manner by end users.}
}
@article{CAMPOS2020104813,
title = {Finding reusable structured resources for the integration of environmental research data},
journal = {Environmental Modelling & Software},
volume = {133},
pages = {104813},
year = {2020},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2020.104813},
url = {https://www.sciencedirect.com/science/article/pii/S1364815219307078},
author = {Patricia M.C. Campos and Cassio C. Reginato and João Paulo A. Almeida and Monalessa P. Barcellos and Ricardo {de Almeida Falbo} and Vítor E. {Silva Souza} and Giancarlo Guizzardi},
keywords = {Data integration, Environmental research data, Knowledge resources, Reuse, Systematic search, Ontology},
abstract = {Successful data integration requires careful examination of data semantics, a task that has often been approached with the use of ontologies. However, there are some barriers to build ontologies for data integration in complex domains such as the environmental one. A relevant problem is the development of new ontologies disregarding previous knowledge resources such as reference models and vocabularies. This paper addresses this challenge by proposing a systematic approach (dubbed CLeAR) for the identification and selection of reusable artifacts for building ontologies with the purpose of research data integration. CLeAR follows some principles of the systematic literature reviews, supporting the search for structured resources in the scientific literature. We apply CLeAR to the environmental domain. A total of 543 publications were surveyed. The results obtained provide a set of 75 structured resources for the environmental domain, evaluated according domain coverage and some quality attributes (e.g., proper documentation, community acceptance).}
}
@article{TERZIYAN20241388,
title = {Taxonomy-Informed Neural Networks for Smart Manufacturing},
journal = {Procedia Computer Science},
volume = {232},
pages = {1388-1399},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.01.137},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924001376},
author = {Vagan Terziyan and Oleksandra Vitko},
keywords = {neural networks, machine learning, informed machine learning, physics-informed neural networks, taxonomy, Industry 4.0},
abstract = {A neural network (NN) is known to be an efficient and learnable tool supporting decision-making processes particularly in Industry 4.0. The majority of NNs are data-driven and, therefore, depend on training data quantity and quality. The current trend in enhancing data-driven models with knowledge-based models promises to enable effective NNs with less data. So-called physics-informed NNs use additional knowledge from computational science to improve NN training. Quite much of the knowledge is available as logical constraints from domain ontologies, and NNs may benefit from using it. In this paper, we study the concept of Taxonomy-Informed NN (TINN), which combines data-driven training of NNs with ontological knowledge. We study different patterns of NN training with additional knowledge on class-subclass hierarchies and instance-class relationships with potential for federated learning. Our experiments show that additional knowledge, which influences TINNs’ training process through the loss function at backpropagation, improves the quality of trained models.}
}
@article{WANG2023104979,
title = {Graph-based inter-domain consistency maintenance for BIM models},
journal = {Automation in Construction},
volume = {154},
pages = {104979},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.104979},
url = {https://www.sciencedirect.com/science/article/pii/S092658052300239X},
author = {Zijian Wang and Boyuan Ouyang and Rafael Sacks},
keywords = {Cloud-based Building Information Modeling (CBIM), Building Information Modeling, Design cloud collaboration, Intelligent model maintenance, Construction industry},
abstract = {Even with modern BIM software, design collaboration across disciplines for building construction remains sequential and siloed. Cloud-based BIM (CBIM) proposes an alternative approach, in which BIM models are stored as discipline-specific graphs whose nodes are linked to support across-domain coordination. This work presents a theoretical basis for systematic consistency maintenance by 1) defining constraint classes to relate objects across disciplines and 2) devising a mechanism to resolve conflicts. A case study prototype was implemented with federated building models to demonstrate the system’s response to design changes made by one discipline that impact others. It detects conflicts that violate design intent, and in simple cases, such as translations, it can resolve inconsistencies by actively propagating corrections subject to users’ approvals. The prototype demonstrates the feasibility of the approach and strengthens the growing understanding that linked graphs with meaningful relationships can open the door to intelligent applications across BIM disciplines.}
}
@article{KEWANG2023,
title = {Defining the Digital Measurement of Scratching During Sleep or Nocturnal Scratching: Review of the Literature},
journal = {Journal of Medical Internet Research},
volume = {25},
year = {2023},
issn = {1438-8871},
doi = {https://doi.org/10.2196/43617},
url = {https://www.sciencedirect.com/science/article/pii/S1438887123002893},
author = {Will {Ke Wang} and Lucia Cesnakova and Jennifer C Goldsack and Jessilyn Dunn},
keywords = {atopic dermatitis, ontologies, nocturnal scratching, quality of life, outcomes measurement, dermatitis, scratch, review method, systematic review, literature review, pruritis},
abstract = {Background
Digital sensing solutions represent a convenient, objective, relatively inexpensive method that could be leveraged for assessing symptoms of various health conditions. Recent progress in the capabilities of digital sensing products has targeted the measurement of scratching during sleep, traditionally referred to as nocturnal scratching, in patients with atopic dermatitis or other skin conditions. Many solutions measuring nocturnal scratch have been developed; however, a lack of efforts toward standardization of the measure’s definition and contextualization of scratching during sleep hampers the ability to compare different technologies for this purpose.
Objective
We aimed to address this gap and bring forth unified measurement definitions for nocturnal scratch.
Methods
We performed a narrative literature review of definitions of scratching in patients with skin inflammation and a targeted literature review of sleep in the context of the period during which such scratching occurred. Both searches were limited to English language studies in humans. The extracted data were synthesized into themes based on study characteristics: scratch as a behavior, other characterization of the scratching movement, and measurement parameters for both scratch and sleep. We then developed ontologies for the digital measurement of sleep scratching.
Results
In all, 29 studies defined inflammation-related scratching between 1996 and 2021. When cross-referenced with the results of search terms describing the sleep period, only 2 of these scratch-related papers also described sleep-related variables. From these search results, we developed an evidence-based and patient-centric definition of nocturnal scratch: an action of rhythmic and repetitive skin contact movement performed during a delimited time period of intended and actual sleep that is not restricted to any specific time of the day or night. Based on the measurement properties identified in the searches, we developed ontologies of relevant concepts that can be used as a starting point to develop standardized outcome measures of scratching during sleep in patients with inflammatory skin conditions.
Conclusions
This work is intended to serve as a foundation for the future development of unified and well-described digital health technologies measuring nocturnal scratching and should enable better communication and sharing of results between various stakeholders taking part in research in atopic dermatitis and other inflammatory skin conditions.}
}
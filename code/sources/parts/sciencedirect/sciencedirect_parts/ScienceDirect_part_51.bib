@article{BUKOWIEC2025,
title = {Supervised Machine Learning and Clinical Decision Support},
journal = {Hand Clinics},
year = {2025},
issn = {0749-0712},
doi = {https://doi.org/10.1016/j.hcl.2025.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0749071225000678},
author = {Lainey G. Bukowiec and Yining Lu},
keywords = {Clinical decision support, Supervised, Machine learning, Deep learning, Artificial intelligence}
}
@article{CHOUCHANI2020825,
title = {Automatic generation of personalized applications based on social media},
journal = {Procedia Computer Science},
volume = {170},
pages = {825-830},
year = {2020},
note = {The 11th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 3rd International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.149},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920306050},
author = {Nadia Chouchani and Mourad Abed},
keywords = {Model-Driven Engineering, Domain-Specific Language, Social media, Personalization},
abstract = {In this work, we present a contextual personalization method based on users’ interests and social neighborhoods. It is implemented by proposing an approach to automatically generate personalized interactive applications based on social media platforms. The shared contents, designed by documents, are used to extract users’ contexts and profiles. Thus, extensive functionality is needed to exploit these information and develop social applications adapted to end-users interests. An automated model-driven engineering approach, using Domain-Specific Language, is proposed for the generation of what we call personalized document-based applications by exploiting the interaction power of online social media.}
}
@article{NOOR2022,
title = {Deployment of a Free-Text Analytics Platform at a UK National Health Service Research Hospital: CogStack at University College London Hospitals},
journal = {JMIR Medical Informatics},
volume = {10},
number = {8},
year = {2022},
issn = {2291-9694},
doi = {https://doi.org/10.2196/38122},
url = {https://www.sciencedirect.com/science/article/pii/S229196942200206X},
author = {Kawsar Noor and Lukasz Roguski and Xi Bai and Alex Handy and Roman Klapaukh and Amos Folarin and Luis Romao and Joshua Matteson and Nathan Lea and Leilei Zhu and Folkert W Asselbergs and Wai Keong Wong and Anoop Shah and Richard JB Dobson},
keywords = {natural language processing, text mining, information retrieval, electronic health record system, clinical support},
abstract = {Background
As more health care organizations transition to using electronic health record (EHR) systems, it is important for these organizations to maximize the secondary use of their data to support service improvement and clinical research. These organizations will find it challenging to have systems capable of harnessing the unstructured data fields in the record (clinical notes, letters, etc) and more practically have such systems interact with all of the hospital data systems (legacy and current).
Objective
We describe the deployment of the EHR interfacing information extraction and retrieval platform CogStack at University College London Hospitals (UCLH).
Methods
At UCLH, we have deployed the CogStack platform, an information retrieval platform with natural language processing capabilities. The platform addresses the problem of data ingestion and harmonization from multiple data sources using the Apache NiFi module for managing complex data flows. The platform also facilitates the extraction of structured data from free-text records through use of the MedCAT natural language processing library. Finally, data science tools are made available to support data scientists and the development of downstream applications dependent upon data ingested and analyzed by CogStack.
Results
The platform has been deployed at the hospital, and in particular, it has facilitated a number of research and service evaluation projects. To date, we have processed over 30 million records, and the insights produced from CogStack have informed a number of clinical research use cases at the hospital.
Conclusions
The CogStack platform can be configured to handle the data ingestion and harmonization challenges faced by a hospital. More importantly, the platform enables the hospital to unlock important clinical information from the unstructured portion of the record using natural language processing technology.}
}
@article{LIU20241465,
title = {SOX1 promotes osteosarcoma metastasis by modulating TSPAN12 expression},
journal = {Biocell},
volume = {48},
number = {10},
pages = {1465-1473},
year = {2024},
issn = {0327-9545},
doi = {https://doi.org/10.32604/biocell.2024.052670},
url = {https://www.sciencedirect.com/science/article/pii/S0327954524001312},
author = {HEYI LIU and WENHAO CHENG and JINGLIANG HE and LUYAO ZHANG and KADIRYA ASAN and YULU CHEN and JIAYUN WANG and QI GAO and SENG WANG and ZIEN YU and SHAOJIE MA and LAN ZHU and JING JI},
keywords = {Metastasis, Osteosarcoma, SOX1, TSPAN12, Transcriptional factor},
abstract = {Background
Osteosarcoma is the most common primary bone malignancy, with a strong tendency towards local invasion and metastasis. The SRY-Box Transcription Factor 1 (SOX1) gene, a member of the HMG-box family of DNA-binding transcription factors, plays a crucial role in embryogenesis and tumorigenesis. However, its role in osteosarcoma, particularly in relation to metastatic potential, is not well understood.
Methods
The GSE14359 dataset containing five samples of conventional osteosarcoma and four samples of lung metastatic osteosarcoma was obtained from the Gene Expression Omnibus (GEO) database and analyzed for differential gene expression using the R language. Gene expression was detected using qPCR and Western blotting. Transcriptional activity was assessed by Luciferase reporter gene assays, and cell metastatic ability was assessed by migration and invasion assays.
Results
The study demonstrated that SOX1 binds to a specific response element within the Transmembrane 4 Superfamily Member 12 (TSPAN12) promoter, upregulating TSPAN12 and its associated inflammatory pathways. Silencing TSPAN12 markedly reduces SOX1-mediated osteosarcoma cell invasion and inflammatory response, while TSPAN12 overexpression reverses these effects in SOX1-suppressed cells.
Conclusion
In this study, our findings elucidate SOX1’s role in enhancing osteosarcoma metastasis via TSPAN12 upregulation, offering new insights into the molecular mechanisms of osteosarcoma progression.}
}
@article{THAPA2025101203,
title = {Strategies to include prior knowledge in omics analysis with deep neural networks},
journal = {Patterns},
volume = {6},
number = {3},
pages = {101203},
year = {2025},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2025.101203},
url = {https://www.sciencedirect.com/science/article/pii/S2666389925000510},
author = {Kisan Thapa and Meric Kinali and Shichao Pei and Augustin Luna and Özgün Babur},
keywords = {deep learning, multi-omics, biological prior knowledge, graph neural networks},
abstract = {Summary
High-throughput molecular profiling technologies have revolutionized molecular biology research in the past decades. One important use of molecular data is to make predictions of phenotypes and other features of the organisms using machine learning algorithms. Deep learning models have become increasingly popular for this task due to their ability to learn complex non-linear patterns. Applying deep learning to molecular profiles, however, is challenging due to the very high dimensionality of the data and relatively small sample sizes, causing models to overfit. A solution is to incorporate biological prior knowledge to guide the learning algorithm for processing the functionally related input together. This helps regularize the models and improve their generalizability and interpretability. Here, we describe three major strategies proposed to use prior knowledge in deep learning models to make predictions based on molecular profiles. We review the related deep learning architectures, including the major ideas in relatively new graph neural networks.}
}
@article{CONG2025103131,
title = {Enhancing novel product iteration: An integrated framework for heuristic ideation via interpretable conceptual design knowledge graph},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103131},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103131},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625000242},
author = {Yangfan Cong and Suihuai Yu and Jianjie Chu and Yuexin Huang and Ning Ding and Cong Fang and Stephen Jia Wang},
keywords = {Novel product iteration, Conceptual product design, Design knowledge, Interpretable knowledge graph, Heuristic product ideation},
abstract = {Novel products emerge over time to survive the competitive landscape as no existing product can perpetually satisfy all evolving customer expectations. These products are often characterized by groundbreaking solutions previously unavailable on the market. However, the swift imitation of successful novel products by competitors underscores the need for sustained iteration and continuous improvement. Designers increasingly face challenges in keeping up to date with the growing volume and fragmented nature of design information from diverse sources. While knowledge graphs show promise in structuring and organizing complex design information, their effective application in the ideation process remains limited due to difficulties in automatic knowledge extraction and the lack of interpretability aligned well with designers’ cognitive processes. This study proposes an integrated method to construct an interpretable conceptual design knowledge graph (I-CDKG) that features both inherent and acquired interpretability for heuristic product ideation. First, the schema layer models product design knowledge and governs the semantic connection of design information reinforced by design cognition principles to create a reasonable organizational framework to foster intuitive knowledge exploration. Second, the data layer mainly fulfills automatic and smooth design knowledge extraction for I-CDKG construction through the deep learning ERNIE-BiGRU-CRF model combined with BIESO labeling mode and triple-extracting algorithm. Third, the application layer empowers designers to visually delve into interpretable design knowledge to locate inspiration from cluster, relation, and nest levels and enable constant I-CDKG expansion as design schemes proliferate. A case study on the smart cat litter box demonstrates the feasibility of the proposed methodology. The evaluation results confirm the I-CDKG’s advantages as a productive design tool for inspiring creative, practical, and cost-effective product ideations, thereby empowering the iterative development of competitive novel products.}
}
@article{MAVROPOULOS2019101743,
title = {Apparatus: A framework for security analysis in internet of things systems},
journal = {Ad Hoc Networks},
volume = {92},
pages = {101743},
year = {2019},
note = {Special Issue on Security of IoT-enabled Infrastructures in Smart Cities},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2018.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S1570870518305936},
author = {Orestis Mavropoulos and Haralambos Mouratidis and Andrew Fish and Emmanouil Panaousis},
keywords = {IoT Security, Security requirements, Smart cities security, Apparatus framework},
abstract = {Internet of Things (IoT) systems are ubiquitous, highly complex and dynamic event-based systems. These characteristics make their security analysis challenging. Security in IoT requires domain-specific methodologies and tools. The proposed methodologies need to be able to capture information from software and hardware constructs to security and social constructs. In this paper, in addition to refining the modeling language of the Apparatus Framework, we propose a class-based notation of the modeling language and a structured approach to transition between different models. Apparatus is a security framework developed to facilitate security analysis in IoT systems. We demonstrate the application of the framework by analyzing the security of smart public transport system. The security analysis and visualization of the system are facilitated by a software application that is developed as part of the Apparatus Framework.}
}
@article{DASILVA2024202,
title = {Bacterial endometritis-induced changes in the endometrial proteome in mares: Potential uterine biomarker for bacterial endometritis},
journal = {Theriogenology},
volume = {226},
pages = {202-212},
year = {2024},
issn = {0093-691X},
doi = {https://doi.org/10.1016/j.theriogenology.2024.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0093691X24002425},
author = {E. {Da Silva} and F.E. Martín-Cano and V. Gómez-Arrones and G. Gaitskell-Phillips and J.M. Alonso and J. Rey and L. Becerro and M.C. Gil and F.J. Peña and C. Ortega-Ferrusola},
keywords = {NET, Immune system, Energy metabolism, Equine, Subfertility, Proteomic, Endometritis, Uterine infectious, Antimicrobial peptides, Secretome, Uterine proteome},
abstract = {Equine endometritis is one of the main causes of subfertility in the mare. Unraveling the molecular mechanisms involved in this condition and pinpointing proteins with biomarker potential could be crucial in both diagnosing and treating this condition. This study aimed to identify the endometritis-induced changes in the endometrial proteome in mares and to elucidate potential biological processes in which these proteins may be involved. Secondly, biomarkers related to bacterial endometritis (BE) in mares were identified. Uterine lavage fluid samples were collected from 28 mares (14 healthy: negative cytology and culture, and no clinical signs and 14 mares with endometritis: positive cytology and culture, in addition to clinical signs). Proteomic analysis was performed with a UHPLC-MS/MS system and bioinformatic analysis was carried out using Qlucore Omics Explorer. Gene Ontology enrichment and pathway analysis (PANTHER and KEGG) of the uterine proteome were performed to identify active biological pathways in enriched proteins from each group. Quantitative analysis revealed 38 proteins differentially abundant in endometritis mares when compared to healthy mares (fold changes >4.25, and q-value = 0.002). The proteins upregulated in the secretome of mares with BE were involved in biological processes related to the generation of energy and REDOX regulation and to the defense response to bacterium. A total of 24 biomarkers for BE were identified using the biomarker workbench algorithm. Some of the proteins identified were related to the innate immune system such as isoforms of histones H2A and H2B involvement in neutrophil extracellular trap (NET) formation, complement C3a, or gelsolin and profilin, two actin-binding proteins which are essential for dynamic remodeling of the actin cytoskeleton during cell migration. The other group of biomarkers were three known antimicrobial peptides (lysosome, equine cathelicidin 2 and myeloperoxidase (MPO)) and two uncharacterized proteins with a high homology with cathelicidin families. Findings in this study provide the first evidence that innate immune cells in the equine endometrium undergo reprogramming of metabolic pathways similar to the Warburg effect during activation. In addition, biomarkers of BE in uterine fluid of mares including the new proteins identified, as well as other antimicrobial peptides already known, offer future lines of research for alternative treatments to antibiotics.}
}
@article{DAVIES2020102426,
title = {A formal, scalable approach to semantic interoperability},
journal = {Science of Computer Programming},
volume = {192},
pages = {102426},
year = {2020},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2020.102426},
url = {https://www.sciencedirect.com/science/article/pii/S016764232030037X},
author = {Jim Davies and James Welch and David Milward and Steve Harris},
keywords = {Semantic interoperability, Refinement, Formal methods},
abstract = {Scientific progress is increasingly dependent upon the acquisition, processing, and analysis of large volumes of data. The validity of results and the safety of applications rely upon an adequate understanding of the real-world semantics of this data: its intended interpretation, and the context in which it is acquired and processed. This presents a challenge: interpretations vary, context is infinite, and either may change over time. This paper addresses that challenge. It introduces a language for the description of real-world semantics that allows for multiple, evolving interpretations, together with a high degree of automation in the capture and creation of contextual metadata. The language itself has a mathematical semantics, and supports a notion of semantic interoperability closely related to existing, formal notions of refinement. The language represents a scalable approach in three respects: it is compositional, in terms of composing real-world semantics piece by piece; it allows for multiple perspectives, allowing the parallel development of different interpretations; and it supports automatic transformations to and from implementation languages. The practical application of the approach is illustrated with examples from large-scale medical research.}
}
@article{AGHAYENGEJEH2025100792,
title = {Revolutionizing textual data insights: A comprehensive review of the dual relationship between transformers and clustering in textual data analysis},
journal = {Computer Science Review},
volume = {58},
pages = {100792},
year = {2025},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2025.100792},
url = {https://www.sciencedirect.com/science/article/pii/S1574013725000681},
author = {Nazila Pourhaji Aghayengejeh and M.A. Balafar and Narjes Nikzad Khasmakhi},
keywords = {Transformer, Clustering, Dual relationship},
abstract = {In recent years, the integration of transformer models and clustering techniques has gained significant attention in the research community. Transformers excel at feature extraction, representation learning, and understanding data, which helps improve the accuracy and efficiency of clustering tasks. Conversely, clustering methods play a critical role in managing data distribution, enhancing interpretability, and improving the training of transformer models. This review looks at the dual relationship between these two domains: how transformers can advance clustering methodologies and how clustering techniques can optimize transformer performance. By examining this interaction, the paper highlights promising directions for future research.}
}
@article{MICHAEL2024100100,
title = {Integrating models of civil structures in digital twins: State-of-the-Art and challenges},
journal = {Journal of Infrastructure Intelligence and Resilience},
volume = {3},
number = {3},
pages = {100100},
year = {2024},
issn = {2772-9915},
doi = {https://doi.org/10.1016/j.iintel.2024.100100},
url = {https://www.sciencedirect.com/science/article/pii/S2772991524000197},
author = {Judith Michael and Jörg Blankenbach and Jan Derksen and Berit Finklenburg and Raul Fuentes and Thomas Gries and Sepehr Hendiani and Stefan Herlé and Stefan Hesseler and Magdalena Kimm and Jörg Christian Kirchhof and Bernhard Rumpe and Holger Schüttrumpf and Grit Walther},
keywords = {Civil structures, Model-based software engineering, Life cycle, Engineering, Digital twin, Internet of Things, Intelligent civil infrastructural systems, Operations management, Geometric models, Modeling smart material, BIM, GIM, Interoperability},
abstract = {Software systems monitoring civil structures over their lifetime are exposed to the risk of aging much faster than the structures themselves. This risk can be minimized if we use models describing the structure, geometry, processes, interaction, and risk assessment as well as the data collected over the lifetime of a civil structure. They are considered as a unity together with the civil structure. These model-based systems constitute a digital twin of such a civil structure, which through appropriate operative services remain in permanent use and thus co-evolve with the civil structure even over a long-lasting lifetime. Even though research on digital twins for civil structures has grown over the last few years, digital twin engineering with heterogeneous models and data sources is still challenging. Within this article, we describe models used within all phases of the whole civil structure life cycle. We identify the models from the computer science, civil engineering, mechanical engineering, and business management domains as specifically relevant for this purpose, as they seem to cover all relevant aspects of sustainable civil structures at best, and discuss them using a dam as an example. Moreover, we discuss challenges for creating and using models within different scenarios such as improving the sustainability of civil structures, evaluating risks, engineering digital twins, parallel software and object evolution, and changing technologies and software stacks. We show how this holistic view from different perspectives helps overcome challenges and raises new ones. The consideration from these different perspectives enables the long-term software support of civil structures while simultaneously opening up new paths and needs for research on the digitalization of long-lasting structures.}
}
@article{GARCIAMENDEZ2019372,
title = {A library for automatic natural language generation of spanish texts},
journal = {Expert Systems with Applications},
volume = {120},
pages = {372-386},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2018.11.036},
url = {https://www.sciencedirect.com/science/article/pii/S0957417418307565},
author = {Silvia García-Méndez and Milagros Fernández-Gavilanes and Enrique Costa-Montenegro and Jonathan Juncal-Martínez and F. {Javier González-Castaño}},
keywords = {Natural language generation, Spanish, Text planning, Lexicon, Labelled text corpora, Augmentative and alternative communication},
abstract = {In this article we present a novel system for natural language generation (nlg) of Spanish sentences from a minimum set of meaningful words (such as nouns, verbs and adjectives) which, unlike other state-of-the-art solutions, performs the nlg task in a fully automatic way, exploiting both knowledge-based and statistical approaches. Relying on its linguistic knowledge of vocabulary and grammar, the system is able to generate complete, coherent and correctly spelled sentences from the main word sets presented by the user. The system, which was designed to be integrable, portable and efficient, can be easily adapted to other languages by design and can feasibly be integrated in a wide range of digital devices. During its development we also created a supplementary lexicon for Spanish, aLexiS, with wide coverage and high precision, as well as syntactic trees from a freely available definite-clause grammar. The resulting nlg library has been evaluated both automatically and manually (annotation). The system can potentially be used in different application domains such as augmentative communication and automatic generation of administrative reports or news.}
}
@article{HODGE201933,
title = {The semiotic diversity of doing reference in a deaf signed language},
journal = {Journal of Pragmatics},
volume = {143},
pages = {33-53},
year = {2019},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2019.01.025},
url = {https://www.sciencedirect.com/science/article/pii/S0378216618303734},
author = {Gabrielle Hodge and Lindsay N. Ferrara and Benjamin D. Anible},
keywords = {Animacy, Indexicality, Multimodal, Referential cohesion, Sign language, Semiotics},
abstract = {This article describes how deaf signers of Auslan (a deaf signed language of Australia) coordinate fully conventionalised forms (such as lexical manual signs and English fingerspelling and/or mouthing) with more richly improvised semiotics (such as indicating verbs, pointing signs, depicting signs, visible surrogates and/or invisible surrogates) to identify and talk about referents of varying agency. Using twenty retellings of Frog, Where Are You? and twenty retellings of The Boy Who Cried Wolf archived in the Auslan Corpus, we analysed 4,699 tokens of referring expressions with respect to: (a) activation status; (b) semiotic form; and (c) animacy. Statistical analysis confirmed choice of strategy was most strongly motivated by activation status: new referents were expressed with more conventionalised forms (especially lexical manual signs and English mouthing), whereas maintained and reintroduced referents typically involved fewer and more richly improvised, context-dependent semiotics. However, animacy was also a motivating factor: humans and animals were often depicted via visible surrogates (not pointing signs), whereas inanimate referents favoured depicting signs and invisible surrogates. These findings highlight the role of animacy in signed language discourse and challenge the claim that informativeness decreases as cognitive saliency increases, while demonstrating the ‘pretend world’ indexicality of signed language use and the pluralistic complexity of face-to-face communication.}
}
@article{CHEN2025110365,
title = {Optimized graph neural networks for spatial recognition to support automatic building information model semantic enrichment - Exploring node features enhancing strategies},
journal = {Engineering Applications of Artificial Intelligence},
volume = {147},
pages = {110365},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110365},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625003653},
author = {Yian Chen and Huixian Jiang},
keywords = {Building information model, Semantic enrichment, Graph neural networks, Spatial functional identification, Node feature enhancement, Bidirectional encoder representation from transformer},
abstract = {The integration of Graph Neural Networks (GNNs) with Building Information Modeling (BIM) unveils transformative potential for the semantic enrichment of BIM data, offering significant advancements to the Architecture, Engineering, and Construction (AEC) industry. This paper addresses the critical issue of spatial recognition and classification in BIM through the proposed node-enhanced, self-supervised graph neural network model, Node-Enhanced Graph- Bidirectional Encoder Representation from Transformer (NE-Graph-BERT), which incorporates edge features. This model represents each space as a node, utilizing spatial relational edges as inputs and is trained on an independently developed knowledge base consisting of 14 space types and 4 relational features across three major categories of architectural space layouts, enabling automatic identification of spatial features. Experimental results indicate that: (1) the enhanced GNNs with integrated edge features achieves precise spatial classification of building functions on extensive building diagrams; (2) NE-Graph-BERT surpasses existing GNNs models in both Precision (97.08%) and F1-score (96.75%); (3) spatial classification accuracy and interoperability are notably enhanced, reaching up to 97.05%. This study advances the application of GNNs for semantic enrichment in BIM and offers practical insights for their implementation in real AEC projects, paving the way for enhanced efficiency and automation in applications including compliance review, spatial analysis, and building evacuation.}
}
@article{KRALJEVIC2024e281,
title = {Foresight—a generative pretrained transformer for modelling of patient timelines using electronic health records: a retrospective modelling study},
journal = {The Lancet Digital Health},
volume = {6},
number = {4},
pages = {e281-e290},
year = {2024},
issn = {2589-7500},
doi = {https://doi.org/10.1016/S2589-7500(24)00025-6},
url = {https://www.sciencedirect.com/science/article/pii/S2589750024000256},
author = {Zeljko Kraljevic and Dan Bean and Anthony Shek and Rebecca Bendayan and Harry Hemingway and Joshua Au Yeung and Alexander Deng and Alfred Balston and Jack Ross and Esther Idowu and James T Teo and Richard J B Dobson},
abstract = {Summary
Background
An electronic health record (EHR) holds detailed longitudinal information about a patient's health status and general clinical history, a large portion of which is stored as unstructured, free text. Existing approaches to model a patient's trajectory focus mostly on structured data and a subset of single-domain outcomes. This study aims to evaluate the effectiveness of Foresight, a generative transformer in temporal modelling of patient data, integrating both free text and structured formats, to predict a diverse array of future medical outcomes, such as disorders, substances (eg, to do with medicines, allergies, or poisonings), procedures, and findings (eg, relating to observations, judgements, or assessments).
Methods
Foresight is a novel transformer-based pipeline that uses named entity recognition and linking tools to convert EHR document text into structured, coded concepts, followed by providing probabilistic forecasts for future medical events, such as disorders, substances, procedures, and findings. The Foresight pipeline has four main components: (1) CogStack (data retrieval and preprocessing); (2) the Medical Concept Annotation Toolkit (structuring of the free-text information from EHRs); (3) Foresight Core (deep-learning model for biomedical concept modelling); and (4) the Foresight web application. We processed the entire free-text portion from three different hospital datasets (King's College Hospital [KCH], South London and Maudsley [SLaM], and the US Medical Information Mart for Intensive Care III [MIMIC-III]), resulting in information from 811 336 patients and covering both physical and mental health institutions. We measured the performance of models using custom metrics derived from precision and recall.
Findings
Foresight achieved a precision@10 (ie, of 10 forecasted candidates, at least one is correct) of 0·68 (SD 0·0027) for the KCH dataset, 0·76 (0·0032) for the SLaM dataset, and 0·88 (0·0018) for the MIMIC-III dataset, for forecasting the next new disorder in a patient timeline. Foresight also achieved a precision@10 value of 0·80 (0·0013) for the KCH dataset, 0·81 (0·0026) for the SLaM dataset, and 0·91 (0·0011) for the MIMIC-III dataset, for forecasting the next new biomedical concept. In addition, Foresight was validated on 34 synthetic patient timelines by five clinicians and achieved a relevancy of 33 (97% [95% CI 91–100]) of 34 for the top forecasted candidate disorder. As a generative model, Foresight can forecast follow-on biomedical concepts for as many steps as required.
Interpretation
Foresight is a general-purpose model for biomedical concept modelling that can be used for real-world risk forecasting, virtual trials, and clinical research to study the progression of disorders, to simulate interventions and counterfactuals, and for educational purposes.
Funding
National Health Service Artificial Intelligence Laboratory, National Institute for Health and Care Research Biomedical Research Centre, and Health Data Research UK.}
}
@article{AFZAL20201401,
title = {Requirements modeling of Web services-based business processes},
journal = {Business Process Management Journal},
volume = {26},
number = {6},
pages = {1401-1424},
year = {2020},
issn = {1463-7154},
doi = {https://doi.org/10.1108/BPMJ-08-2019-0322},
url = {https://www.sciencedirect.com/science/article/pii/S1463715420000424},
author = {Ayesha Afzal and Basit Shafiq and Shafay Shamail and Nabil Adam},
keywords = {Business process, Web services, Requirements modeling},
abstract = {Purpose
This paper reviews existing business process (BP) modeling languages that are widely used in the industry as well as recent research work on modeling and analysis of BPs in the service-oriented environment and Internetware-based software paradigm. BPs in such environment are different from traditional BPs due to loose coupling of partner services, dynamic and on-the-fly selection of partners and run-time process adaptability. The unique characteristics of these BPs require formal modeling of the requirements and constraints in each phase of their life cycle, including design phase, implementation and deployment phase and execution phase.
Design/methodology/approach
The paper first provides a categorization of typical user requirements in each phase of the BP life cycle. Then a detailed comparison of the selected languages with respect to their requirement modeling and analysis capabilities in each of the identified categories is provided. The paper also discusses new requirements engineering research challenges arising from future software needs and emerging trends in software engineering in the context of Web-services-based BPs and Internetware.
Findings
There is a need to have a framework that provides support for user requirements modeling and analysis for all the phases of BP life cycle in an integrated manner. Such a framework would be useful not only in resolving the inconsistencies between requirements across phases but also in addressing the issues related to BP evolution due to changes in user requirements over time. Moreover, with the Internet of things (IoT) adoption in BPM, there is a need to have an integrated environment that provides support for capturing the resilience requirements of enterprise BPs as well as the mobility constraints of the underlying IoT devices.
Originality/value
This paper reviews existing BP modeling languages and frameworks and discusses the new requirements engineering research challenges arising from future software needs and the emerging trends in BP management in the service-oriented environment and Internetware-based software paradigm.}
}
@article{XIN201877,
title = {Identification of key microRNAs, transcription factors and genes associated with congenital obstructive nephropathy in a mouse model of megabladder},
journal = {Gene},
volume = {650},
pages = {77-85},
year = {2018},
issn = {0378-1119},
doi = {https://doi.org/10.1016/j.gene.2018.01.063},
url = {https://www.sciencedirect.com/science/article/pii/S0378111918300763},
author = {Guangda Xin and Rui Chen and Xiaofei Zhang},
keywords = {Congenital obstructive nephropathy, Differentially expressed miRNAs, Functional enrichment analysis, Transcription factor, Molecular mechanisms},
abstract = {Objective
The present study aimed to investigate the molecular mechanism underlying congenital obstructive nephropathy (CON).
Methods
The microarray dataset GSE70879 was downloaded from the Gene Expression Omnibus, including 3 kidney samples of megabladder mice and 4 control kidneys. Using this dataset, differentially expressed miRNAs (DEMs) were identified between the kidney samples from megabladder mice and controls, followed by identification of the target genes for these DEMs and construction of a DEM and target gene interaction network. Additionally, the target genes were subjected to Gene Ontology and pathway enrichment analyses, and were used for construction of a protein-protein interaction (PPI) network. Finally, regulatory networks were constructed to analyze transcription factors for the key miRNAs.
Results
From 17 DEMs identified between kidney samples of megabladder mice and controls, 3 key miRNAs were screened, including mmu-miR-150-5p, mmu-miR-374b-5p and mmu-miR-126a-5p. The regulatory networks identified vascular endothelial growth factor A (Vegfa) as the common target gene of mmu-miR-150-5p and five transcription factors, including nuclear receptor subfamily 4, group A, member 2 (Nr4a2), Jun dimerisation protein 2 (Jdp2), Kruppel-like factor 6 (Klf6), Neurexophilin-3 (Nxph3) and RNA binding motif protein 17 (Rbm17). The gene encoding phosphatase and tensin homolog (Pten) was found to be co-regulated by mmu-miR-374b-5p and high mobility group protein A1 (Hmga1), whereas the kirsten rat sarcoma viral oncogene (Kras) was identified as a common target gene of mmu-miR-126a-5p and paired box 6 (Pax6).
Conclusions
In summary, the above-listed key miRNAs, transcription factors and key genes may be involved in the development of CON.}
}
@article{MACANOVIC2022102784,
title = {Text mining for social science – The state and the future of computational text analysis in sociology},
journal = {Social Science Research},
volume = {108},
pages = {102784},
year = {2022},
issn = {0049-089X},
doi = {https://doi.org/10.1016/j.ssresearch.2022.102784},
url = {https://www.sciencedirect.com/science/article/pii/S0049089X22000904},
author = {Ana Macanovic},
keywords = {Text mining, Text analysis, Content analysis, Machine learning, Natural language processing, Big data},
abstract = {The emergence of big data and computational tools has introduced new possibilities for using large-scale textual sources in sociological research. Recent work in sociology of culture, science, and economic sociology has shown how computational text analysis can be used in theory building and testing. This review starts with an introduction of the history of computer-assisted text analysis in sociology and then proceeds to discuss five families of computational methods used in contemporary research. Using exemplary studies, it shows how dictionary methods, semantic and network analysis tools, language models, unsupervised, and supervised machine learning can assist sociologists with different analytical tasks. After presenting recent methodological developments, this review summarizes several important implications of using large datasets and computational methods to infer complex meaning in texts. Finally, it calls researchers from different methodological traditions to adopt text mining tools while remaining mindful of lessons learned from working with conventional data and methods.}
}
@incollection{KUILER2023429,
title = {12 - Panopticon implications of ethical AI: equity, disparity, and inequality in healthcare},
editor = {Feras A. Batarseh and Laura J. Freeman},
booktitle = {AI Assurance},
publisher = {Academic Press},
pages = {429-451},
year = {2023},
isbn = {978-0-323-91919-7},
doi = {https://doi.org/10.1016/B978-0-32-391919-7.00026-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919197000263},
author = {Erik W. Kuiler and Connie L. McNeely},
keywords = {Artificial Intelligence, ethics, healthcare, infosphere},
abstract = {Addressing issues in the healthcare domain as principal illustrations, instrumentalities of Artificial Intelligence (AI) are explored at the nexus of ubiquity and control reflected in the Internet of Things (IoT) and in various governance and policy features. Roles of ethics and ontological determinations are examined with particular attention to the effects of AI-enabled systems and policies on selected groups and related health disparities and biases. The growth of predictive data analytics and the simultaneous growth in the availability of interoperable AI-enabled devices offer opportunities to mitigate healthcare disparities currently endemic in indigent, underrepresented, and underserved communities. However, use of these devices can exacerbate inequities as well as ameliorate them. By themselves, human and AI agents operating in an IoT-sustained infosphere cannot solve problems related to the multiple forms of marginalization that affect the health and wellbeing of particular communities. They require actively engaged and empathetic governance to address complex socioeconomic issues and solve complex accessibility and distribution problems. In collaborative environments, where healthcare provision depends on the synergy of human and AI actors, regulatory models must not only address the ethical conduct of medical practitioners, but also the professional conduct of informaticists, software developers, and device vendors.}
}
@article{PORTUGAL2025103851,
title = {Enhancing microglial antioxidant capacity via the ascorbate transporter SVCT2 delays onset and modifies disease progression in mouse models of Alzheimer's disease},
journal = {Redox Biology},
volume = {86},
pages = {103851},
year = {2025},
issn = {2213-2317},
doi = {https://doi.org/10.1016/j.redox.2025.103851},
url = {https://www.sciencedirect.com/science/article/pii/S2213231725003647},
author = {Camila C. Portugal and Evelyn C.S. Santos and Ana Monteiro-Pacheco and Sara Costa-Pinto and Tiago O. Almeida and Joana Tedim-Moreira and Dora Gavin and Teresa Canedo and Fabiana Oliveira and Isabel Cardoso and Teresa Summavielle and Sandra H. Vaz and Renato Socodato and João B. Relvas},
keywords = {Ascorbate, Slc23a2, SVCT2, Neurodegenerative diseases, 5xFAD, LTP, RNA sequencing, Proteomics, APP/PS1},
abstract = {Despite clear evidence that vitamin C levels are depleted in the brains of Alzheimer's disease (AD) patients, dietary supplementation has consistently failed in clinical trials, suggesting a critical bottleneck not in systemic supply, but in its transport into brain cells. Here, we identify this bottleneck as a progressive downregulation of the ascorbate transporter, Slc23a2, also known as SVCT2, in microglia. Then we hypothesized that bypassing this cellular deficiency via targeted SVCT2 overexpression in microglia could either prevent the onset of pathology or rescue established functional deficits. Indeed, overexpressing SVCT2 in microglia before disease onset in 5xFAD mice triggered a profound redox reprogramming, resulting in a unique "hybrid" neuroprotective microglial phenotype that co-expressed both homeostatic and disease-associated markers. Functionally, this leads to decreased amyloid plaque burden and strengthens the synaptic bioenergetic capacity, which consequently prevents the development of synaptic and memory deficits. Strikingly, when employed after disease establishment, SVCT2 overexpression rescued synaptic plasticity and memory performance despite not affecting the existing amyloid burden. This rescue was driven by changes in the microglial secretory pathways. Collectively, these findings resolve a long-standing clinical paradox by establishing that neuroprotection depends not on systemic vitamin C intake but on the brain's cellular uptake machinery. This offers a mechanistic explanation for the failure of dietary supplementation in AD and identifies SVCT2 as a promising therapeutic target against the neurodegenerative process in AD.}
}
@article{RAMDANI2022109573,
title = {DEEP, a methodology for entity extraction using organizational patterns: Application to job offers},
journal = {Knowledge-Based Systems},
volume = {258},
pages = {109573},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109573},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122007936},
author = {Halima Ramdani and Armelle Brun and Eric Bonjour and Davy Monticolo},
keywords = {Entity extraction, Organizational patterns, Methodology, Sequence labelling, Recurrent neural network, Job offer entity extraction},
abstract = {Plain texts written in natural language may have several specific features, such as organizational patterns and an ambiguous and evolving vocabulary. From the literature, entity extraction approaches are not sufficient to consider these specific features jointly. To address this issue, we propose DEEP, a methodology that improves the quality of entity extraction by using organizational patterns through a sequence labelling technique. To this end, DEEP creates a high-quality corpus and relies on an appropriate learning algorithm. DEEP is validated on a real corpus of job offers. Experiments show that (1) considering organizational patterns improves the quality of entity extraction, (2) vocabulary evolution is taken into consideration and ambiguity in vocabulary is reduced, (3) DEEP provides clear guidelines for the creation of a high-quality corpus for entity extraction, (4) the Bidirectional Long Short-Term Memory + Conditional Random Field architecture for sequence labelling is the one that takes the most advantage of the organizational patterns.}
}
@article{BEKELE2025102742,
title = {Societal relevance of higher education: An Afrocentric theoretical framework},
journal = {International Journal of Educational Research},
volume = {133},
pages = {102742},
year = {2025},
issn = {0883-0355},
doi = {https://doi.org/10.1016/j.ijer.2025.102742},
url = {https://www.sciencedirect.com/science/article/pii/S0883035525002150},
author = {Teklu Abate Bekele and Samuel Amponsah},
keywords = {African epistemologies, African knowledge systems, African philosophies, Higher education, Higher education societal relevance},
abstract = {Recognizing the colonial legacy of African higher education, which predominantly embraced Western knowledge systems, this study calls for a paradigm shift toward incorporating African Indigenous knowledge systems to rejuvenate and sustain the sector's societal relevance. Employing philosophical methods of explication and conceptual re-engineering on African individual and communal epistemologies, an Afrocentric framework of higher education societal relevance is proposed. The framework acknowledges the multidimensional, multiscalar and dynamic nature of the phenomenon. The core thesis is that higher education societal relevance is ubiquitous- all university missions and support structures embody it. The framework is conceived around five domains each highlighting different aspects of higher education societal relevance: substantive (attuning to the what of education), methodological (the how of education), teleological (the why of education), procedural (university governance), and relational (university-society engagements). Overall, the framework aims to foster a comprehensive understanding of higher education that is responsive to Africa's unique cultural, economic, historical, and social contexts and emerging needs. It signifies a critical first step towards redefining higher education to ensure its relevance and contribution to human and societal development, and to the transformation as well as preservation of African knowledge systems. This original and novel approach challenges the conceptual and philosophical underpinnings of the colonial higher education model and aligns with decolonization movements, discourses of Pan Africanism and African renaissance, and sustainable development. Further scholarship is warranted to further qualify the framework across diverse African contexts, thereby contributing to a more contextually relevant higher education landscape on the continent.}
}
@article{DAUSCH20241364,
title = {Semantic Integration and Interdisciplinary Collaboration in Production Planning: A Graph-Based Approach for Enhanced Data Consistency},
journal = {Procedia CIRP},
volume = {130},
pages = {1364-1371},
year = {2024},
note = {57th CIRP Conference on Manufacturing Systems 2024 (CMS 2024)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.10.253},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124014100},
author = {Valesko Dausch and Joachim Lentes and Oliver Riedel and Matthias Kreimeyer},
keywords = {Data driven development, knowledge management, data continuity, method},
abstract = {Based on increasing individualization, shorter innovation cycles and integrating services into products complexity in the development of products and production processes increases. In combination with digitalization, this results in an increased need for interdisciplinary work. A high level of data consistency is required to cope with the resulting diverse and complex data structures, leading to the need for a consistent data model that orchestrates information flows. PLM-systems fall short in their ambition of reaching an overarching data management across the product lifecycle. This results in a lot of administrative work in planning instead of value-adding as well as in inconsistencies and incomplete change processes. To improve interdisciplinary collaboration in production planning, goal of this work is to develop a method leading to a comprehensive data model and to show it’s applicability by means of a case study. The steps of the research performed follow the typical circle of action research: diagnosis, planning, action, evaluation, and reflection. The data model resulting of the application of the method uses semantics to make relationships understandable for both humans and machines. This is necessary for downstream automation and the use of artificial intelligence. The three essential steps of the method are, firstly, an as-is modeling of the processes of interest. Then, the process models are used to evaluate the consistency and efficiency of the planning processes. The method is completed by a guideline for setting up the data model. The proposed approach aims to improve efficiency and quality in assembly planning processes, resulting in its importance for industrial companies.}
}
@article{DUONG2023102198,
title = {From scattered sources to comprehensive technology landscape : A recommendation-based retrieval approach},
journal = {World Patent Information},
volume = {73},
pages = {102198},
year = {2023},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2023.102198},
url = {https://www.sciencedirect.com/science/article/pii/S0172219023000285},
author = {Chi Thang Duong and Dimitri Perica David and Ljiljana Dolamic and Alain Mermoud and Vincent Lenders and Karl Aberer},
keywords = {Technology monitoring, Information retrieval, Entity-based retrieval, Technology classifier, Recommender system},
abstract = {Mapping the technology landscape is crucial for market actors to take informed investment decisions. However, given the large amount of data on the Web and its subsequent information overload, manually retrieving information is a seemingly ineffective and incomplete approach. In this work, we propose an end-to-end recommendation based retrieval approach to support automatic retrieval of technologies and their associated companies from raw Web data. This is a two-task setup involving (i) technology classification of entities extracted from company corpus, and (ii) technology and company retrieval based on classified technologies. Our proposed framework approaches the first task by leveraging DistilBERT which is a state-of-the-art language model. For the retrieval task, we introduce a recommendation-based retrieval technique to simultaneously support retrieving related companies, technologies related to a specific company and companies relevant to a technology. To evaluate these tasks, we also construct a data set that includes company documents and entities extracted from these documents together with company categories and technology labels. Experiments show that our approach is able to return 4 times more relevant companies while outperforming traditional retrieval baseline in retrieving technologies.}
}
@article{WANG2022146170,
title = {Comparative transcriptome analysis reveals distinct responsive biological processes in radish genotypes contrasting for Plasmodiophora brassicae interaction},
journal = {Gene},
volume = {817},
pages = {146170},
year = {2022},
issn = {0378-1119},
doi = {https://doi.org/10.1016/j.gene.2021.146170},
url = {https://www.sciencedirect.com/science/article/pii/S0378111921007654},
author = {Jinglei Wang and Tianhua Hu and Wuhong Wang and Haijiao Hu and Qingzhen Wei and Yaqin Yan and Jiangming He and Jingfeng Hu and Chonglai Bao},
keywords = {, Clubroot disease, Radish, Transcriptome, Hormone},
abstract = {Plasmodiophora brassicae is a protozoan pathogen that causes clubroot disease, which is one of the most destructive diseases for Brassica crops, including radish. However, little is known about the molecular mechanism of clubroot resistance in radish. In this study, we performed a comparative transcriptome analysis between resistant and susceptible radish inoculated with P. brassicae. More differentially expressed genes (DEGs) were identified at 28 days after inoculation (DAI) compared to 7 DAI in both genotypes. Gene ontology (GO) and KEGG enrichment indicated that stress/defense response, secondary metabolic biosynthesis, hormone metabolic process, and cell periphery are directly involved in the defense response process. Further analysis of the transcriptome revealed that effector-triggered immunity (ETI) plays key roles in the defense response. The plant hormones jasmonic acid (JA), ethylene (ET), and abscisic acid (ABA) related genes are activated in clubroot defense in the resistant line. Auxin (AUX) hormone related genes are activated in the developing galls of susceptible radish. Our study provides a global transcriptional overview for clubroot development for insights into the P. brassicae defense mechanisms in radish.}
}
@article{SALIMKHANI202029,
title = {The dynamical approach to spin-2 gravity},
journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
volume = {72},
pages = {29-45},
year = {2020},
issn = {1355-2198},
doi = {https://doi.org/10.1016/j.shpsb.2020.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1355219820300836},
author = {Kian Salimkhani},
abstract = {This paper engages with the following closely related questions that have recently received some attention in the literature: (a) what is the status of the equivalence principle in general relativity (GR)?; (b) how does the metric field obtain its property of being able to act as a metric?; and (c) is the metric of GR derivative on the dynamics of the matter fields? The paper attempts to complement these debates by studying the spin-2 approach to (quantum) gravity. In particular, the paper argues that three lessons can be drawn from the spin-2 approach: (1) different from what is sometimes claimed in the literature, central aspects of the non-linear theory of GR are already derivable in classical spin-2 theory; in particular, ‘universal coupling’ can be considered a derived ‘theorem’ in both the classical and the quantum spin-2 approach; this provides new insights for the investigation of the equivalence principle; (2) the ‘second miracle’ that Read et al. argue characterises GR is explained in the classical as well as in the quantum version of the spin-2 approach; (3) the spin-2 approach allows for an ontological reduction of the metrical part of spacetime to the dynamics of matter fields.}
}
@article{DONG2022109195,
title = {A guideline to document occupant behavior models for advanced building controls},
journal = {Building and Environment},
volume = {219},
pages = {109195},
year = {2022},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2022.109195},
url = {https://www.sciencedirect.com/science/article/pii/S0360132322004310},
author = {Bing Dong and Romana Markovic and Salvatore Carlucci and Yapan Liu and Andreas Wagner and Antonio Liguori and Christoph {van Treeck} and Dmitry Oleynikov and Elie Azar and Gianmarco Fajilla and Ján Drgoňa and Joyce Kim and Marika Vellei and Marilena {De Simone} and Masood Shamsaiee and Mateus Bavaresco and Matteo Favero and Mikkel Kjaergaard and Mohamed Osman and Moritz Frahm and Sanam Dabirian and Da Yan and Xuyuan Kang},
keywords = {Guideline documentation, Occupant behavior model, Building control, Model Predictive control (MPC)},
abstract = {The availability of computational power, and a wealth of data from sensors have boosted the development of model-based predictive control for smart and effective control of advanced buildings in the last decade. More recently occupant-behavior models have been developed for including people in the building control loops. However, while important objectives of scientific research are reproducibility and replicability of results, not all information is available from published documents. Therefore, the aim of this paper is to propose a guideline for a thorough and standardized occupant-behavior model documentation. For that purpose, the literature screening for the existing occupant behavior models in building control was conducted, and the occupant behavior modeling processes were studied to extract practices and gaps for each of the following phases: problem statement, data collection, and preprocessing, model development, model evaluation, and model implementation. The literature screening pointed out that the current state-of-the-art on model documentation shows little unification, which poses a particular burden for the model application and replication in field studies. In addition to the standardized model documentation, this work presented a model-evaluation schema that enabled benchmarking of different models in field settings as well as the recommendations on how OB models are integrated with the building system.}
}
@article{PICH2024100345,
title = {A measure to gauge psychological pain: Assessing convergent construct validity and internal consistency of the Psychache Scale in the Cambodian context},
journal = {SSM - Mental Health},
volume = {6},
pages = {100345},
year = {2024},
issn = {2666-5603},
doi = {https://doi.org/10.1016/j.ssmmh.2024.100345},
url = {https://www.sciencedirect.com/science/article/pii/S2666560324000501},
author = {Panha Pich and Elena Lesley and Quynh-Anh Ngoc Nguyen and Craig Higson-Smith and Sokha Sieng},
keywords = {Psychological pain, Psychache, Cambodia, Suicide, Depression, Anxiety and stress scale (DASS), Baksbat},
abstract = {The following study was conducted in order to assess convergent construct validity and internal consistency of a Khmer-language adaptation of the Psychache Scale, a 13-item self-report instrument used to assess subjective experiences of psychological pain. The Psychache Scale (PAS) was translated into Khmer and back-translated to verify accurate meaning. The Khmer translation was corrected to address cultural and linguistic nuances for the Cambodian population and piloted among 121 students and recent graduates. The final Khmer version of the PAS showed high internal consistency (Cronbach's alpha = 0.928). Results also showed that scores on the Psychache Scale were strongly associated with related concepts of depression (r2 = 0.545, p = 0.000), anxiety (r2 = 0.438, p = 0.000), stress (r2 = 0.459, p = 0.000), and Baksbat, a Cambodian-specific cultural concept of distress (r2 = 0.549, p = 0.000). Contrary to a priori hypotheses, the Psychache Scale was also negatively and statistically significantly associated with age (Kruskal-Wallis χ2 = 20.561, df = 4, p = 0.000) and was negatively statistically significantly related to education level (Kruskal-Wallis χ2 = 13.053, df = 2 p = 0.001). Given these results, the Psychache Scale shows potential clinical utility in the Cambodian context, though future research may consider how psychological pain differs according to age and education levels.}
}
@article{POWELL20241,
title = {Rooting natural climate solutions in Wahkohtowin through Indigenous guardianship: insights from a youth-led initiative in Northern Ontario, Canada},
journal = {FACETS},
volume = {9},
pages = {1-17},
year = {2024},
issn = {2371-1671},
doi = {https://doi.org/10.1139/facets-2023-0104},
url = {https://www.sciencedirect.com/science/article/pii/S2371167124000851},
author = {Lara Powell and Amberly Quakegesic and Elena McCulloch and Isabelle Allen and Ben Bradshaw and Nicole Redvers},
keywords = {Indigenous Guardians, natural climate solutions, nature-based solutions, Indigenous-led conservation, youth, Canada},
abstract = {In recent years, increasing attention has been directed to “natural climate solutions” to mitigate climate change through the protection, restoration, and improved management of carbon-storing ecosystems. In practice, Indigenous Peoples have been implementing natural climate solutions for millennia through land stewardship. As Indigenous nations and communities in Canada reassert stewardship roles through Indigenous Guardians programs, the question arises: what possibilities emerge when natural climate solutions are driven by Guardians, guided by multifaceted community priorities and Indigenous knowledge? This paper responds to this question, drawing upon collaborative research with Wahkohtowin Development, a social enterprise based in Treaty 9 territory (Ontario, Canada), made up of Chapleau Cree First Nation, Missanabie Cree First Nation, and Brunswick House First Nation. We engaged youth Guardians in workshops that generated insights on the role of youth, cross-cultural collaboration, and holistic conceptualizations of climate action rooted in Indigenous ontologies (such as the Cree philosophy of wahkohtowin, embodying kinship and interconnectedness). Our analysis reveals that Indigenous Guardians are well positioned to advance natural climate solutions and to do so in an integrative manner that addresses intersecting challenges—with benefits for communities, ecosystems, climate action, and reconciliation.}
}
@article{GNADLINGER2024,
title = {Incorporating an Intelligent Tutoring System Into a Game-Based Auditory Rehabilitation Training for Adult Cochlear Implant Recipients: Algorithm Development and Validation},
journal = {JMIR Serious Games},
volume = {12},
year = {2024},
issn = {2291-9279},
doi = {https://doi.org/10.2196/55231},
url = {https://www.sciencedirect.com/science/article/pii/S2291927924000977},
author = {Florian Gnadlinger and Maika Werminghaus and André Selmanagić and Tim Filla and Jutta G Richter and Simone Kriglstein and Thomas Klenzner},
keywords = {cochlear implant, eHealth, evidence-centered design, hearing rehabilitation, adaptive learning, intelligent tutoring system, game-based learning},
abstract = {Background
Cochlear implants are implanted hearing devices; instead of amplifying sounds like common hearing aids, this technology delivers preprocessed sound information directly to the hearing (ie, auditory) nerves. After surgery and the first cochlear implant activation, patients must practice interpreting the new auditory sensations, especially for language comprehension. This rehabilitation process is accompanied by hearing therapy through face-to-face training with a therapist, self-directed training, and computer-based auditory training.
Objective
In general, self-directed, computer-based auditory training tasks have already shown advantages. However, compliance of cochlear implant recipients is still a major factor, especially for self-directed training at home. Hence, we aimed to explore the combination of 2 techniques to enhance learner motivation in this context: adaptive learning (in the form of an intelligent tutoring system) and game-based learning (in the form of a serious game).
Methods
Following the suggestions of the evidence-centered design framework, a domain analysis of hearing therapy was conducted, allowing us to partially describe human hearing skill as a probabilistic competence model (Bayesian network). We developed an algorithm that uses such a model to estimate the current competence level of a patient and create training recommendations. For training, our developed task system was based on 7 language comprehension task types that act as a blueprint for generating tasks of diverse difficulty automatically. To achieve this, 1053 audio assets with meta-information labels were created. We embedded the adaptive task system into a graphic novel–like mobile serious game. German-speaking cochlear implant recipients used the system during a feasibility study for 4 weeks.
Results
The 23 adult participants (20 women; 3 men) fulfilled 2259 tasks. In total, 2004 (90.5%) tasks were solved correctly, and 255 (9.5%) tasks were solved incorrectly. A generalized additive model analysis of these tasks indicated that the system adapted to the estimated competency levels of the cochlear implant recipients more quickly in the beginning than at the end. Compared with a uniform distribution of all task types, the recommended task types differed (χ²6=86.713; P<.001), indicating that the system selected specific task types for each patient. This is underlined by the identified categories for the error proportions of the task types.
Conclusions
This contribution demonstrates the feasibility of combining an intelligent tutoring system with a serious game in cochlear implant rehabilitation therapies. The findings presented here could lead to further advances in cochlear implant care and aural rehabilitation in general.
Trial Registration
German Clinical Trials Register (DRKS) DRKS00022860; https://drks.de/search/en/trial/DRKS00022860}
}
@article{ASGARIBIDHENDI2021100638,
title = {FarsBase-KBP: A knowledge base population system for the Persian Knowledge Graph},
journal = {Journal of Web Semantics},
volume = {68},
pages = {100638},
year = {2021},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100638},
url = {https://www.sciencedirect.com/science/article/pii/S1570826821000135},
author = {Majid Asgari-Bidhendi and Behrooz Janfada and Behrouz Minaei-Bidgoli},
keywords = {Knowledge extraction, Knowledge Graph, Canonicalization, Natural Language Processing, Persian language},
abstract = {While most of the knowledge bases already support the English language, there is only one knowledge base for the Persian language, known as FarsBase, which is automatically created via semi-structured web information. Unlike English knowledge bases such as Wikidata, which have tremendous community support, the population of a knowledge base like FarsBase must rely on automatically extracted knowledge. Knowledge base population can let FarsBase keep growing in size, as the system continues working. In this paper, we present a knowledge base population system for the Persian language, which extracts knowledge from unlabelled raw text, crawled from the Web. The proposed system consists of a set of state-of-the-art modules such as an entity linking module as well as information and relation extraction modules designed for FarsBase. Moreover, a canonicalization system is introduced to link extracted relations to FarsBase properties. Then, the system uses knowledge fusion techniques with minimal intervention of human experts to integrate and filter the proper knowledge instances, extracted by each module. To evaluate the performance of the presented knowledge base population system, we present the first gold dataset for benchmarking knowledge base population in the Persian language, which consisting of 22015 FarsBase triples and verified by human experts. The evaluation results demonstrate the efficiency of the proposed system.}
}
@article{SHAKERIHOSSEINABAD2022,
title = {Physical Activity, Sedentary Behavior, and Sleep on Twitter: Multicountry and Fully Labeled Public Data Set for Digital Public Health Surveillance Research},
journal = {JMIR Public Health and Surveillance},
volume = {8},
number = {2},
year = {2022},
issn = {2369-2960},
doi = {https://doi.org/10.2196/32355},
url = {https://www.sciencedirect.com/science/article/pii/S2369296022000801},
author = {Zahra {Shakeri Hossein Abad} and Gregory P Butler and Wendy Thompson and Joon Lee},
keywords = {digital public health surveillance, social media analysis, physical activity, sedentary behavior, sleep, machine learning, online health information, infodemiology, public health database},
abstract = {Background
Advances in automated data processing and machine learning (ML) models, together with the unprecedented growth in the number of social media users who publicly share and discuss health-related information, have made public health surveillance (PHS) one of the long-lasting social media applications. However, the existing PHS systems feeding on social media data have not been widely deployed in national surveillance systems, which appears to stem from the lack of practitioners and the public’s trust in social media data. More robust and reliable data sets over which supervised ML models can be trained and tested reliably is a significant step toward overcoming this hurdle. The health implications of daily behaviors (physical activity, sedentary behavior, and sleep [PASS]), as an evergreen topic in PHS, are widely studied through traditional data sources such as surveillance surveys and administrative databases, which are often several months out-of-date by the time they are used, costly to collect, and thus limited in quantity and coverage.
Objective
The main objective of this study is to present a large-scale, multicountry, longitudinal, and fully labeled data set to enable and support digital PASS surveillance research in PHS. To support high-quality surveillance research using our data set, we have conducted further analysis on the data set to supplement it with additional PHS-related metadata.
Methods
We collected the data of this study from Twitter using the Twitter livestream application programming interface between November 28, 2018, and June 19, 2020. To obtain PASS-related tweets for manual annotation, we iteratively used regular expressions, unsupervised natural language processing, domain-specific ontologies, and linguistic analysis. We used Amazon Mechanical Turk to label the collected data to self-reported PASS categories and implemented a quality control pipeline to monitor and manage the validity of crowd-generated labels. Moreover, we used ML, latent semantic analysis, linguistic analysis, and label inference analysis to validate the different components of the data set.
Results
LPHEADA (Labelled Digital Public Health Dataset) contains 366,405 crowd-generated labels (3 labels per tweet) for 122,135 PASS-related tweets that originated in Australia, Canada, the United Kingdom, or the United States, labeled by 708 unique annotators on Amazon Mechanical Turk. In addition to crowd-generated labels, LPHEADA provides details about the three critical components of any PHS system: place, time, and demographics (ie, gender and age range) associated with each tweet.
Conclusions
Publicly available data sets for digital PASS surveillance are usually isolated and only provide labels for small subsets of the data. We believe that the novelty and comprehensiveness of the data set provided in this study will help develop, evaluate, and deploy digital PASS surveillance systems. LPHEADA will be an invaluable resource for both public health researchers and practitioners.}
}
@article{BISSELL201857,
title = {Automation interrupted: How autonomous vehicle accidents transform the material politics of automation},
journal = {Political Geography},
volume = {65},
pages = {57-66},
year = {2018},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2018.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0962629817303943},
author = {David Bissell},
keywords = {Automation, Posthumanism, Digital technology, Mobilities, Materiality, Labour, Politics},
abstract = {This paper develops our geographical understanding of the material politics of automation. Through the empirical site of the autonomous vehicle, the paper argues that dominant understandings of the politics of contemporary automation draw on a restricted understanding of materiality where political agency is concentrated in the hands of powerful individuals or institutions. However, this focus potentially obscures the complex material agencies of the systems of automation themselves. In response, this paper develops the conceptual potentials of the accident to bring these overlooked interruptive material agencies to the fore. This provides us with an opportunity to appreciate how the sites of power in systems of contemporary digital automation are more multiple and dispersed than is often assumed. In making this argument, this paper seeks to contribute to political geographical research that has turned to questions of ontology to pluralise the sites of politics and diversify the agents of political change.}
}
@article{VANSCHILT2024102926,
title = {Identifying the structure of illicit supply chains with sparse data: A simulation model calibration approach},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102926},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102926},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624005779},
author = {Isabelle M. {van Schilt} and Jan H. Kwakkel and Jelte P. Mense and Alexander Verbraeck},
keywords = {Illicit, Supply chain, Structural uncertainty, Sparse data, Simulation},
abstract = {Illicit supply chains for products like counterfeit Personal Protective Equipment (PPE) are characterized by sparse data and great uncertainty about the operational and logistical structure, making criminal activities largely invisible to law enforcement and challenging to intervene in. Simulation is a way to get insight into the behavior of complex systems, using calibration to tune model parameters to match its real-world counterpart. Calibration methods for simulation models of illicit supply chains should work with sparse data, while also tuning the structure of the simulation model. Thus, this study addresses the question: “To what extent can various model calibration techniques reconstruct the underlying structure of an illicit supply chain when varying the degree of data sparseness?” We evaluate the quality-of-fit of a reference technique, Powell’s Method, and three model calibration techniques that have shown promise for sparse data: Approximate Bayesian Computing, Bayesian Optimization, and Genetic Algorithms. For this, we use a simulation model of a stylized counterfeit PPE supply chain as ground truth. We extract data from this ground truth and systematically vary its sparseness. We parameterize structural uncertainty using System Entity Structure. The results demonstrate that Bayesian Optimization and Genetic Algorithms are suitable for reconstructing the underlying structure of an illicit supply chain for a varying degree of data sparseness. Both techniques identify a diverse set of optimal solutions that fit with the sparse data. For a comprehensive understanding of illicit supply chain structures, we propose to combine the results of the two techniques. Future research should focus on developing a combined algorithm and incorporating solution diversity.}
}
@article{FALCETTA2023102487,
title = {Automatic documentation of professional health interactions: A systematic review},
journal = {Artificial Intelligence in Medicine},
volume = {137},
pages = {102487},
year = {2023},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2023.102487},
url = {https://www.sciencedirect.com/science/article/pii/S0933365723000015},
author = {Frederico Soares Falcetta and Fernando Kude {de Almeida} and Janaína Conceição Sutil Lemos and José Roberto Goldim and Cristiano André {da Costa}},
keywords = {EHR, Automatic documentation, Neural networks},
abstract = {Electronic systems are increasingly present in the healthcare system and are often related to improved medical care. However, the widespread use of these technologies ended up building a relationship of dependence that can disrupt the doctor–patient relationship. In this context, digital scribes are automated clinical documentation systems that capture the physician–patient conversation and then generate the documentation for the appointment, enabling the physician to engage with the patient entirely. We have performed a systematic literature review on intelligent solutions for automatic speech recognition (ASR) with automatic documentation during a medical interview. The scope included only original research on systems that could detect speech and transcribe it in a natural and structured fashion simultaneously with the doctor–patient interaction, excluding speech-to-text-only technologies. The search resulted in a total of 1995 titles, with eight articles remaining after filtering for the inclusion and exclusion criteria. The intelligent models mainly consisted of an ASR system with natural language processing capability, a medical lexicon, and structured text output. None of the articles had a commercially available product at the time of the publication and reported limited real-life experience. So far, none of the applications has been prospectively validated and tested in large-scale clinical studies. Nonetheless, these first reports suggest that automatic speech recognition may be a valuable tool in the future to facilitate medical registration in a faster and more reliable manner. Improving transparency, accuracy, and empathy could drastically change how patients and doctors experience a medical visit. Unfortunately, clinical data on the usability and benefits of such applications is almost non-existent. We believe that future work in this area is necessary and needed.}
}
@article{ALBALWY2021,
title = {A Blockchain-Based Dynamic Consent Architecture to Support Clinical Genomic Data Sharing (ConsentChain): Proof-of-Concept Study},
journal = {JMIR Medical Informatics},
volume = {9},
number = {11},
year = {2021},
issn = {2291-9694},
doi = {https://doi.org/10.2196/27816},
url = {https://www.sciencedirect.com/science/article/pii/S2291969421000429},
author = {Faisal Albalwy and Andrew Brass and Angela Davies},
keywords = {blockchain, smart contracts, dynamic consent, clinical genomics, data sharing},
abstract = {Background
In clinical genomics, sharing of rare genetic disease information between genetic databases and laboratories is essential to determine the pathogenic significance of variants to enable the diagnosis of rare genetic diseases. Significant concerns regarding data governance and security have reduced this sharing in practice. Blockchain could provide a secure method for sharing genomic data between involved parties and thus help overcome some of these issues.
Objective
This study aims to contribute to the growing knowledge of the potential role of blockchain technology in supporting the sharing of clinical genomic data by describing blockchain-based dynamic consent architecture to support clinical genomic data sharing and provide a proof-of-concept implementation, called ConsentChain, for the architecture to explore its performance.
Methods
The ConsentChain requirements were captured from a patient forum to identify security and consent concerns. The ConsentChain was developed on the Ethereum platform, in which smart contracts were used to model the actions of patients, who may provide or withdraw consent to share their data; the data creator, who collects and stores patient data; and the data requester, who needs to query and access the patient data. A detailed analysis was undertaken of the ConsentChain performance as a function of the number of transactions processed by the system.
Results
We describe ConsentChain, a blockchain-based system that provides a web portal interface to support clinical genomic sharing. ConsentChain allows patients to grant or withdraw data requester access and allows data requesters to query and submit access to data stored in a secure off-chain database. We also developed an ontology model to represent patient consent elements into machine-readable codes to automate the consent and data access processes.
Conclusions
Blockchains and smart contracts can provide an efficient and scalable mechanism to support dynamic consent functionality and address some of the barriers that inhibit genomic data sharing. However, they are not a complete answer, and a number of issues still need to be addressed before such systems can be deployed in practice, particularly in relation to verifying user credentials.}
}
@article{XIA2024e37048,
title = {Epithelial cell-related prognostic risk model in breast cancer based on single-cell and bulk RNA sequencing},
journal = {Heliyon},
volume = {10},
number = {17},
pages = {e37048},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e37048},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024130794},
author = {Man-zhi Xia and Hai-chao Yan},
keywords = {Breast cancer, Epithelial cell, Single-cell sequencing, Bulk RNA sequencing, Prognosis},
abstract = {Objective
This study aims to construct an epithelial cell-related prognostic risk model for breast cancer (BRCA) and explore its significance.
Methods
GSE42568, GSE10780, GSE245601, and TCGA-BRCA datasets were sourced from public databases. Epithelial cell-related differentially expressed genes were identified using single-cell data analysis. Venn diagrams determined the intersecting genes between epithelial cell-related and BRCA-related genes. Batch Kaplan-Meier (K-M) survival analysis identified core intersecting genes for BRCA overall survival. Consensus clustering, enrichment, LASSO, and COX regression analyses were performed on the core intersecting genes, and then a prognostic risk model was constructed. The diagnostic and prognostic effectiveness of the risk model was subsequently evaluated and immune infiltration analysis was conducted. Finally, qRT-PCR was used to verify the expression of genes in the risk model.
Results
There were 374 intersecting genes between epithelial cell-related and BRCA-related genes, among which 51 core intersecting genes were associated with BRCA prognosis. Consensus clustering categorized TCGA-BRCA into C1 and C2, with shared regulation of the estrogen signaling pathway. Three genes (DIRC3, SLC6A2, TUBA3D) were independent predictors of BRCA prognosis, forming the basis for a risk model. Except for exhibiting satisfactory diagnostic efficacy, the risk score elevation correlated with poor prognosis, elevated matrix, immune, and ESTIMATE scores, and negative correlation with microsatellite instability. The in vitro results confirmed the differential expression levels of DIRC3, SLC6A2, and TUBA3D.
Conclusion
The prognostic risk model associated with epithelial cells demonstrates effective diagnostic performance in BRCA, serving as an independent prognostic factor for BRCA patients. Additionally, it exhibits a correlation with immune scores.}
}
@article{PETROLLINO2022101448,
title = {The Hamar cattle model: the semantics of appearance in a pastoral linguaculture},
journal = {Language Sciences},
volume = {89},
pages = {101448},
year = {2022},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2021.101448},
url = {https://www.sciencedirect.com/science/article/pii/S0388000121000954},
author = {Sara Petrollino},
keywords = {Visual semantics, Appearance, Cattle color and pattern terms, Pastoralism, Hamar},
abstract = {The rich anthropological and ethnographic literature on cattle “color” and “pattern” terms has argued for the central role of the cattle model in the visual systems of pastoral cultures. This study provides further evidence to the idea of a cattle model structuring pastoralists’ visual systems, and it explores the indigenous visual meanings of cattle “color” and “pattern” terms in Hamar, a language spoken in Southwest Ethiopia. The results show that pastoralists communicate and conceptualize all kind of visual experience in terms of cattle appearance: in the Hamar visual system features such as brightness, sheen and (de)saturation, rather than hue, are central to the meanings of at least some “color” terms; moreover, the conceptualization of categories referred to in mainstream languages as “stripes” or “dots” is based on features such as visual conspicuousness and stand-out effects rather than geometrical shape. The methods, tasks and stimuli used in the study were tailored for the collection of comparative data among different pastoral societies of East Africa. Their practical application is discussed, illustrating the effectiveness in revealing important aspects of cattle-centered meanings.}
}
@incollection{2025201,
title = {Index},
editor = {Faadiel Essop},
booktitle = {Truth Unveiled},
publisher = {Academic Press},
pages = {201-205},
year = {2025},
series = {Fundamentals of Physiology},
isbn = {978-0-443-23655-6},
doi = {https://doi.org/10.1016/B978-0-443-23655-6.09993-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443236556099937}
}
@incollection{GORANSON2025145,
title = {7 - User affordances to engineer open-world enterprise dynamics},
editor = {William Lawless and Ranjeev Mittu and Donald Sofge and Hesham Fouad},
booktitle = {Interdependent Human-Machine Teams},
publisher = {Academic Press},
pages = {145-174},
year = {2025},
isbn = {978-0-443-29246-0},
doi = {https://doi.org/10.1016/B978-0-443-29246-0.00006-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443292460000067},
author = {Ted Goranson and Beth Cardier and Mathew Hancock and Naso Evangelou-Oost and Benjamin J. Seligmann and Matthew Garcia and Glen Smith},
keywords = {Enterprise affordances, Human–machine enterprises, Possibilistic networks, Risk engineering},
abstract = {We address the challenge of engineering enterprise risk to prevent undesirable outcomes that have not previously occurred. This is generally understood as a problem of open-world modeling. Modeling open-world enterprises in the risk domain is complicated by numerous factors: These activities often span disciplines, institutional boundaries, and engage across both human and machine processes. Traditional modeling approaches generalize enterprise operations so that standard policies and procedures can be developed to prevent harm to workers and workplaces. However, these approaches can only prevent problems that have already happened, or which are otherwise expected. We report on studies toward graphical modeling techniques to allow a user flexible insight and control among different presentations while accessing deep open-world models. Our goal is to cross domains and contexts to modify models in a manner that guides outcomes in these open worlds. To support this strategy, we leverage a novel reasoning strategy that captures extended notions of cause, probability, influence, and possibility.}
}
@article{BORCHERS2025103256,
title = {TSC2CARLA: An abstract scenario-based verification toolchain for automated driving systems},
journal = {Science of Computer Programming},
volume = {242},
pages = {103256},
year = {2025},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2024.103256},
url = {https://www.sciencedirect.com/science/article/pii/S0167642324001795},
author = {Philipp Borchers and Tjark Koopmann and Lukas Westhofen and Jan Steffen Becker and Lina Putze and Dominik Grundt and Thies {de Graaff} and Vincent Kalwa and Christian Neurohr},
keywords = {Automated driving systems, Verification, Abstract scenarios, Simulation, Constraint solving},
abstract = {Transitioning automated driving systems to complex operational domains disproportionally increases demands on verification activities. In the worst case, the operational domain can not be covered by a manageable set of logical scenarios. An anticipated solution is to use abstract scenarios, which increase coverage while still enabling formal methods. However, established verification approaches must be adapted for abstract scenarios. In this work, we consider the generation of simulatable test suites from abstract scenarios. For this, we use Traffic Sequence Charts (TSCs), a visual yet formal scenario description language based on first order logic. We propose an SMT-based process for generating concrete test cases that can be simulated in e.g. CARLA. This theoretical framework is compiled into an architecture and a prototypical implementation called TSC2CARLA. An evaluation on a set of non-trivial examples yields initial evidence for the feasibility of our approach.}
}
@article{KOERICH2022100581,
title = {Gastronomy knowledge in the socio-cultural context of transformations},
journal = {International Journal of Gastronomy and Food Science},
volume = {29},
pages = {100581},
year = {2022},
issn = {1878-450X},
doi = {https://doi.org/10.1016/j.ijgfs.2022.100581},
url = {https://www.sciencedirect.com/science/article/pii/S1878450X22001160},
author = {Guilherme Henrique Koerich and Silvana Graudenz Müller},
keywords = {Gastronomy, Culture, Epistemology, Ontology, Systematic literature review},
abstract = {The term Gastronomy is defined and conceptualized from different perspectives and academic disciplines. It is an ambiguous concept that confronts common belief and scientific knowledge, which cannot be accessed from a single perspective. Thus, the present research aimed to identify the epistemological construction of Gastronomy Knowledge in the scientific literature. To achieve this objective, a systematic literature review was carried out in the Scopus, EBSCOhost, Web of Science, ProQuest, ERIC, and SCIELO databases. And in the grey literature, in the CAPES Theses and Dissertations Catalog, the Institutional Repository of the Federal University of Santa Catarina, the Brazilian Digital Library of Theses and Dissertations, and in ProQuest - Dissertations & Theses. The results are presented and discussed in two distinct sections and methods: bibliometric analysis and content analysis. Gastronomy is considered a plural and ambiguous concept, which denotes a series of meanings used in different contexts and situations, to designate a field of study, research, and practice, composed of a set of knowledge. Gastronomy Knowledge, as an epistemology, is constituted by systematized (scientific) knowledge originating from different epistemologies and scientific disciplines, therefore, a kind of knowledge of a multidisciplinary and interdisciplinary nature, and by popular (artistic) knowledge constituted by empirical knowledge originating from cultural knowledge and practices, in a sociocultural context of transformations of new transdisciplinary knowledge.}
}
@article{GREWE2018146,
title = {Exploration of language specifications by compilation to first-order logic},
journal = {Science of Computer Programming},
volume = {155},
pages = {146-172},
year = {2018},
note = {Selected and Extended papers from the International Symposium on Principles and Practice of Declarative Programming 2016},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2017.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167642317301594},
author = {Sylvia Grewe and Sebastian Erdweg and André Pacak and Michael Raulf and Mira Mezini},
keywords = {Type systems, Formal specification, Declarative languages, First-order theorem proving, Domain-specific languages},
abstract = {Exploration of language specifications helps to discover errors and inconsistencies early during the development of a programming language. We propose exploration of language specifications via application of existing automated first-order theorem provers (ATPs). To this end, we translate language specifications and exploration tasks to first-order logic, which many ATPs accept as input. However, there are several different strategies for compiling a language specification to first-order logic, and even small variations in the translation may have a large impact on the time it takes ATPs to find proofs. In this paper, we first present a systematic empirical study on how to best compile language specifications to first-order logic such that existing ATPs can solve typical exploration tasks efficiently. We have developed a compiler product line that implements 36 different compilation strategies and used it to feed language specifications to 4 existing first-order theorem provers. As benchmarks, we developed language specifications for typed SQL and for a Questionnaire Language (QL), with 50 exploration goals each. Our study empirically confirms that the choice of a compilation strategy greatly influences prover performance in general and shows which strategies are advantageous for prover performance. Second, we extend our empirical study with 4 domain-specific strategies for axiom selection and find that axiom selection does not influence prover performance in our benchmark specifications.}
}
@article{MATEUSANZ2024402,
title = {Redefining biomaterial biocompatibility: challenges for artificial intelligence and text mining},
journal = {Trends in Biotechnology},
volume = {42},
number = {4},
pages = {402-417},
year = {2024},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2023.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167779923002895},
author = {Miguel Mateu-Sanz and Carla V. Fuenteslópez and Juan Uribe-Gomez and Håvard Jostein Haugen and Abhay Pandit and Maria-Pau Ginebra and Osnat Hakimi and Martin Krallinger and Athina Samara},
keywords = {biomaterial, biocompatibility, databases, artificial intelligence, data mining, natural language processing, international organization for standardization (ISO)},
abstract = {The surge in ‘Big data’ has significantly influenced biomaterials research and development, with vast data volumes emerging from clinical trials, scientific literature, electronic health records, and other sources. Biocompatibility is essential in developing safe medical devices and biomaterials to perform as intended without provoking adverse reactions. Therefore, establishing an artificial intelligence (AI)-driven biocompatibility definition has become decisive for automating data extraction and profiling safety effectiveness. This definition should both reflect the attributes related to biocompatibility and be compatible with computational data-mining methods. Here, we discuss the need for a comprehensive and contemporary definition of biocompatibility and the challenges in developing one. We also identify the key elements that comprise biocompatibility, and propose an integrated biocompatibility definition that enables data-mining approaches.}
}
@article{GORHAM2024101352,
title = {Learning analytics of peer feedback on communicative skills in an EFL course across different learning modalities},
journal = {Studies in Educational Evaluation},
volume = {81},
pages = {101352},
year = {2024},
issn = {0191-491X},
doi = {https://doi.org/10.1016/j.stueduc.2024.101352},
url = {https://www.sciencedirect.com/science/article/pii/S0191491X24000312},
author = {Tom Gorham and Rwitajit Majumdar and Hiroaki Ogata},
keywords = {Communicative language teaching, Emergency remote teaching, Online learning, English as a foreign language, Peer feedback, Task-based learning, Feedback literacy},
abstract = {This research investigates the improvement of communicative skills in university students via peer feedback activities in an English-as-a-foreign-language (EFL) course. Utilizing a comprehensive view of contemporary feedback frameworks, the study engaged 320 students across three cohorts. The last two cohorts transitioned to online Emergency Remote Teaching (ERT) due to the COVID-19 pandemic. The final cohort further employed a prototype learning analytics system to enhance peer feedback skills. Answering calls for more research on authentic task-based learning in tech-mediated contexts, this study offers empirical evidence supporting modern feedback frameworks. Results showed that ERT cohorts, especially those using the prototype system, outperformed the face-to-face cohort in communicative speaking tests, highlighting the effectiveness of peer feedback activities in boosting communicative skills.}
}
@article{BELGHIAT2025103334,
title = {Interleaving semantics and verification of UML 2 dynamic interactions using process algebra},
journal = {Science of Computer Programming},
volume = {246},
pages = {103334},
year = {2025},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2025.103334},
url = {https://www.sciencedirect.com/science/article/pii/S0167642325000735},
author = {Aissam Belghiat},
keywords = {UML sequence diagrams, -calculus, Process algebra, Formal verification, CombinedFragments, Interleaving semantics},
abstract = {UML sequence diagrams provide a visual notation for modeling the behavior of object interactions in systems. They lack precise formal semantics due to the semi-formal nature of the UML language which hinders their automated analysis and verification. Process algebras have been widely used in the literature in order to deal with such problems. π-calculus is a well-known process algebra recognized for its rich theoretical foundation and high expressivity power. It is also characterized by its capabilities in specifying interleaving and weak sequencing which is considered by the OMG standard as the default semantics for interaction diagrams. Thus, this paper presents a novel approach to formalizing UML 2 sequence diagrams by translating them into π-calculus. The translation captures the semantics of their basic elements as well as their combined fragments. A compositional technique is adopted to gradually build the corresponding π-calculus specification which results in easy induction/recursion of elements and their meaning enabling reasoning about complex dynamic behaviors. The latter task could be done using different analysis tools such as the MWB tool used in this study. The mapping provides a formal semantics as well as formal analysis and verification for UML2 sequence diagrams according to the OMG standard. A case study is shown to illustrate the usefulness of the translation.}
}
@article{ALONSO2025100509,
title = {A novel approach for job matching and skill recommendation using transformers and the O*NET database},
journal = {Big Data Research},
volume = {39},
pages = {100509},
year = {2025},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2025.100509},
url = {https://www.sciencedirect.com/science/article/pii/S2214579625000048},
author = {Rubén Alonso and Danilo Dessí and Antonello Meloni and Diego {Reforgiato Recupero}},
keywords = {Information extraction, Transformers, Online enrolling process, Natural language processing, Course recommendation},
abstract = {Today we have tons of information posted on the web every day regarding job supply and demand which has heavily affected the job market. The online enrolling process has thus become efficient for applicants as it allows them to present their resumes using the Internet and, as such, simultaneously to numerous organizations. Online systems such as Monster.com, OfferZen, and LinkedIn contain millions of job offers and resumes of potential candidates leaving to companies with the hard task to face an enormous amount of data to manage to select the most suitable applicant. The task of assessing the resumes of candidates and providing automatic recommendations on which one suits a particular position best has, therefore, become essential to speed up the hiring process. Similarly, it is important to help applicants to quickly find a job appropriate to their skills and provide recommendations about what they need to master to become eligible for certain jobs. Our approach lies in this context and proposes a new method to identify skills from candidates' resumes and match resumes with job descriptions. We employed the O*NET database entities related to different skills and abilities required by different jobs; moreover, we leveraged deep learning technologies to compute the semantic similarity between O*NET entities and part of text extracted from candidates' resumes. The ultimate goal is to identify the most suitable job for a certain resume according to the information there contained. We have defined two scenarios: i) given a resume, identify the top O*NET occupations with the highest match with the resume, ii) given a candidate's resume and a set of job descriptions, identify which one of the input jobs is the most suitable for the candidate. The evaluation that has been carried out indicates that the proposed approach outperforms the baselines in the two scenarios. Finally, we provide a use case for candidates where it is possible to recommend courses with the goal to fill certain skills and make them qualified for a certain job.}
}
@article{WEI2025110873,
title = {The use of knowledge graphs for drug repurposing: From classical machine learning algorithms to graph neural networks},
journal = {Computers in Biology and Medicine},
volume = {196},
pages = {110873},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.110873},
url = {https://www.sciencedirect.com/science/article/pii/S0010482525012247},
author = {Siqi Wei and Christo Sasi and Jelle Piepenbrock and Martijn A. Huynen and Peter A.C. {’t Hoen}},
keywords = {Drug repositioning, Knowledge graph, Graph convolutional networks, Machine learning, Deep learning},
abstract = {Drug repurposing, the development of new therapeutic indications for existing drugs, is a promising strategy in drug development. Computational methods and artificial intelligence may be used to identify new drug repurposing candidates. Knowledge graph (KG) based methods have emerged as powerful tools for modeling and predicting drug–disease relationships, because of their intuitive way of exploiting biomedical knowledge and data. This review provides an overview of computational drug repurposing methods based on KGs. The motivation for adopting KG-based knowledge representations, traditional machine learning and deep learning approaches are discussed, followed by an analysis of selected tools, their construction, link prediction capabilities, and inherent advantages and limitations.}
}
@article{GRUENHAGEN2025124217,
title = {How do politics, news media and the public frame the discourse on coal mining? Implications for the legitimacy, (de)stabilisation and transition of an industry regime},
journal = {Technological Forecasting and Social Change},
volume = {218},
pages = {124217},
year = {2025},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2025.124217},
url = {https://www.sciencedirect.com/science/article/pii/S0040162525002483},
author = {Jan Henrik Gruenhagen and Stephen Cox},
keywords = {Coal mining, Industry regimes, Industry transitions, Discourse, Legitimacy, Natural language processing, Topic modelling},
abstract = {The use of coal for energy generation is a major contributor to greenhouse gas emissions, adversely affecting the climate and attempts to achieve net zero. Yet coal mining is also an important pillar of national economies such as Australia, provoking a contested discourse about the legitimacy of coal mining. We conceptualise the framing of coal mining as reflecting legitimacy pressures through discourse and agenda setting in the economic and socio-political environment that may lead to changes in industry regimes. We thereby examine how coal mining is framed within different arenas of discourse, and how this framing has changed over time. Specifically, we apply natural language processing and topic modelling to analyse and compare a large amount of text data capturing parliamentary documents and debates, news media reports and debates in the broader public over a period of more than 30 years. Our findings reveal that the discourse on coal mining among policymakers is dominated by economic framing, as opposed to a strong socio-political framing in the broader public. This suggests a mismatch between how coal mining is viewed by policymakers focussing on economic benefits versus parts of the broader public raising concerns over environmental and climate issues. Framing of the discourse in news media is more balanced. Our analysis demonstrates that the framing of coal mining has remained remarkably consistent, overall suggesting continuous legitimacy and a relatively stable industry regime.}
}
@article{LIN201966,
title = {A cloud-based energy data mining information agent system based on big data analysis technology},
journal = {Microelectronics Reliability},
volume = {97},
pages = {66-78},
year = {2019},
issn = {0026-2714},
doi = {https://doi.org/10.1016/j.microrel.2019.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0026271419301064},
author = {Hsueh-Yuan Lin and Sheng-Yuan Yang},
keywords = {Data mining agent systems, Big Data analysis, Web services, Energy saving agent systems},
abstract = {2019 is the first year of 5G and the information flow is growing even more; therefore, data mining technology is one of the key technologies regarding how to find useful information from the vast information flow. This paper aims to develop the cloud-based energy data mining information agent system OntoDMA, as based on the WIAS cloud environment and Big Data analysis technology, which is embedded in a cloud-based active multi-agent system to proactively provide appropriate, real-time, and fast domain information prediction. On one hand, the related technologies for constructing web service platforms are shared; on the other hand, how to widely and seamlessly integrate and support the cloud interaction paradigm handled by the data mining agent system through these technologies is explored. In order to outline the feasibility of the proposed system architecture, a case study is conducted on the energy saving information system, and the relevant R&D results are presented in detail. Then, both the preliminary system R&D interface and experimental verification are illustrated. Finally, the cache performance of the Solutions Pool is increased by 19.82%, the query workload of the Prediction Rules is reduced by 66.51%, and the overall operating time is decreased by 5.21%, which effectively and efficiently relieves the workload on the back-end servo system.}
}
@incollection{CAVALIERE201933,
title = {Data-Information-Concept Continuum From a Text Mining Perspective},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {33-49},
year = {2019},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00347-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032395502700347X},
author = {Danilo Cavaliere and Sabrina Senatore and Vincenzo Loia},
keywords = {Concept mining, Data mining, Information retrieval, Knowledge modelling, Natural language processing, Semantic web},
abstract = {The recent Web panorama reveals a tangible proliferation of “social” data, in form of posts, opinions, feelings, experiences. Most of the available data is unstructured text, unsuitable to be processed by computers, especially due to ambiguity and vagueness of the natural language. Research developments highlight the difficulty in capturing semantics of terms, linguistic expressions, and sentences and their consequent representation as a finite concept. This article presents an open-minded overview of the Text Mining approaches, targeted at transforming unstructured textual data into explicit knowledge, with a special focus on the conceptualization, i.e., the concept identification by analysing syntactic and semantic relations among terms as well as the contextual surrounding information. Different knowledge granulation is described in a layered knowledge model, where the term, the information and the concept represent the basic knowledge granules that cover most Text Mining approaches, in an evolving knowledge continuum.}
}
@article{SCHMELING2025102063,
title = {Data collaboration in digital government research: A literature review and research agenda},
journal = {Government Information Quarterly},
volume = {42},
number = {3},
pages = {102063},
year = {2025},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2025.102063},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X25000577},
author = {Juliane Schmeling and Sami {al Dakruni} and Ines Mergel},
keywords = {Data collaboration, Data ecosystem, Data space, Computational literature review, Structural topic modelling, Digital government},
abstract = {Sovereign data infrastructures are a central building block of the European Data Strategy, yet little is known about how public administrations share and collaborate on both open and restricted data. This research addresses the gap by systematically analysing the existing literature on data collaboration within the field of digital government research. We thereby make a methodological contribution to digital government research through a rigorous literature review framework that includes Structural Topic Modelling to understand the different themes of the scientific discussion in the field of digital government. We propose an innovative data collaboration framework that includes the ecosystem, the organisational, and the individual levels, enhancing our understanding of the multidimensional nature of data collaboration. Our analysis reveals that while the emphasis is on innovation and participation, critical aspects like standardisation and data management have a declining topic prevalence, despite their importance in developing federated data ecosystems. This comprehensive analysis not only sheds light on the current landscape but also informs a structured research agenda in digital government, aiming to contribute to the advancement of the field.}
}
@article{SANHOKWE2023401,
title = {Evaluating the quality of the organisational learning capability measurement model},
journal = {Journal of Workplace Learning},
volume = {35},
number = {5},
pages = {401-416},
year = {2023},
issn = {1366-5626},
doi = {https://doi.org/10.1108/JWL-08-2022-0102},
url = {https://www.sciencedirect.com/science/article/pii/S1366562623000256},
author = {Hamfrey Sanhokwe and Willie Chinyamurindi and Joe Muzurura},
keywords = {Higher-order, Bifactor analysis, Invariance testing, Predictive validity, Zimbabwe},
abstract = {Purpose
This study aims to answer pertinent questions related to the quality of the organisational learning capability measurement model.
Design/methodology/approach
A time-separated design informed data collection. The organisational learning capability was exposed to classical higher-order and bifactor confirmatory factor analyses. Multigroup confirmatory factor analysis facilitated measurement invariance testing. This study assessed the predictive validity of the organisational learning capability subscales using hierarchical regression analysis.
Findings
This study replicated the second-order organisational learning capability model with four subscales. Bifactor modelling confirmed the multidimensionality of the organisational learning capability. The organisational learning capability was invariant between gender groups. The organisational learning capability subscales accounted for a significant variance in innovative work behaviour.
Practical implications
The organisational learning capability exhibits robust properties, making it a plausible option for monitoring the quality of organisational learning. Organisations must appreciate the quality of this dynamic capability and leverage it to generate new sources of value.
Originality/value
This study fills a critical gap in organisational learning-related capabilities in sub-Saharan African contexts, providing a base to influence innovation-related trajectories positively.}
}
@article{ALYAMI20226524,
title = {Systematic literature review of arabic aspect-based sentiment analysis},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {9},
pages = {6524-6551},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2022.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1319157822002282},
author = {Salha Alyami and Areej Alhothali and Amani Jamal},
keywords = {Aspect-based sentiment analysis, Arabic sentiment analysis, Opinion target extraction, Feature-based sentiment analysis, Aspect sentiment classification, Systematic literature review},
abstract = {Aspect-based sentiment analysis (ABSA) is a natural language processing task that provides a detailed analysis of clients’ opinions about various aspects of a product or service. Although several review papers have examined Arabic ABSA studies, the number of studies covered is small, or the included studies are inadequately analyzed. Moreover, only one systematic literature review devoted to Arabic ABSA has been published to our knowledge, which covered only 21 primary studies. Therefore, this systemic literature review was conducted to analyze the existing techniques and resources used for Arabic ABSA. This review covered 47 primary studies published between 2012 and 2021 that were retrieved from eight bibliographic databases and search engines. The included studies were analyzed according to the dataset utilized, domain covered, Arabic language type, preprocessing procedures, selected features, word representation, employed techniques, and evaluation metrics used to assess the proposed techniques. As a result of this analysis, different limitations and issues were identified, and multiple future research directions were suggested. A new taxonomy was also constructed for the techniques employed, which were classified according to aspect-based sentiment analysis tasks and approaches.}
}
@article{PAEPLOW2025146144,
title = {AI startups for good: A taxonomy and archetypes of sustainable business models},
journal = {Journal of Cleaner Production},
volume = {520},
pages = {146144},
year = {2025},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2025.146144},
url = {https://www.sciencedirect.com/science/article/pii/S0959652625014945},
author = {Johanna Paeplow and Thorsten Schoormann and Frederik Möller and Gero Strobel},
keywords = {Business models, Sustainability, Artificial intelligence, Conceptualization},
abstract = {Artificial Intelligence (AI) empowers startups to have a positive impact on personal and professional lives. Given that power, research on how business models utilize AI to contribute to sustainability is needed. Knowledge about various design options can help business model developers minimize unnecessary efforts and mitigate potential risks. This paper addresses this issue through twofold contributions: First, we provide a taxonomy of AI startup business models for good. Based on the analysis of 100 real-world instances, the taxonomy captures how AI is currently used in startups to achieve sustainability value. This helps scholars and practitioners be aware of and navigate key characteristics of such business models, as well as understand the boundaries of the overarching design solution space. Second, we present five archetypical configurations: AI environmental analyser, AI healthcare improver with patient data, AI product manufacturer for farming and grocery, AI surveillant and reporter of customer-provided data, and AI Energy Improver. These archetypes reflect common business model characteristic combinations that practitioners can use to develop or redesign their businesses. Researchers can further examine these archetypes to generate new insights, for instance, by evaluating their success and contributions to sustainable development.}
}
@article{JONES20181,
title = {Karl Marx and the language sciences – critical encounters: introduction to the special issue},
journal = {Language Sciences},
volume = {70},
pages = {1-15},
year = {2018},
note = {Karl Marx and the Language Sciences – Critical Encounters},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2018.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0388000118302717},
author = {Peter E. Jones}
}
@article{CHRISTINO2024104092,
title = {ChatKG: Visualizing time-series patterns aided by intelligent agents and a knowledge graph},
journal = {Computers & Graphics},
volume = {124},
pages = {104092},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.104092},
url = {https://www.sciencedirect.com/science/article/pii/S0097849324002279},
author = {Leonardo Christino and Fernando V. Paulovich},
keywords = {Knowledge graphs, Intelligent agents, Visual analytics},
abstract = {Line-chart visualizations of temporal data enable users to identify interesting patterns for the user to inquire about. Using Intelligent Agents (IA), Visual Analytic tools can automatically uncover explicit knowledge related information to said patterns. Yet, visualizing the association of data, patterns, and knowledge is not straightforward. In this paper, we present ChatKG, a novel visual analytics strategy that allows exploratory data analysis of a Knowledge Graph that associates temporal sequences, the patterns found in each sequence, the temporal overlap between patterns, the related knowledge of each given pattern gathered from a multi-agent IA, and the IA’s suggestions of related datasets for further analysis visualized as annotations. We exemplify and informally evaluate ChatKG by analyzing the world’s life expectancy. For this, we implement an oracle that automatically extracts relevant or interesting patterns, populates the Knowledge Graph to be visualized, and, during user interaction, inquires the multi-agent IA for related information and suggests related datasets to be displayed as visual annotations. Our tests and an interview conducted showed that ChatKG is well suited for temporal analysis of temporal patterns and their related knowledge when applied to history studies.}
}
@article{GLADKAYA2024103899,
title = {Capturing the self-others dichotomy of social media use: Affordances-actualizations-outcomes model},
journal = {Information & Management},
volume = {61},
number = {1},
pages = {103899},
year = {2024},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2023.103899},
url = {https://www.sciencedirect.com/science/article/pii/S0378720623001477},
author = {Margarita Gladkaya and Fenne große Deters},
keywords = {Instagram, Self-Others Dichotomy, Features, Affordances, Actualizations, Outcomes},
abstract = {Focusing on the passive use of Instagram, we apply the affordance perspective to deeply explore its use and use-related outcomes. In the qualitative study, we uncover the affordances of focal social media features. Two distinct groups of affordances (self- and others-oriented) emerge. Following the grounded theory methodology, we develop the affordances-actualizations-outcomes model, explaining how immediate goals associated with features translate into outcomes. In the quantitative study, we test the model by applying structural equation modeling. Our findings confirm that actualizations of self- and others-oriented affordances are associated with distinct outcomes: social connectedness, positive affect, and overall satisfaction with Instagram experience.}
}
@article{HASSANZADEH2019103321,
title = {Quantifying semantic similarity of clinical evidence in the biomedical literature to facilitate related evidence synthesis},
journal = {Journal of Biomedical Informatics},
volume = {100},
pages = {103321},
year = {2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103321},
url = {https://www.sciencedirect.com/science/article/pii/S1532046419302400},
author = {Hamed Hassanzadeh and Anthony Nguyen and Karin Verspoor},
keywords = {Clinical semantic similarity, Clinical evidence, Evidence based medicine, Systematic review},
abstract = {Objective
Published clinical trials and high quality peer reviewed medical publications are considered as the main sources of evidence used for synthesizing systematic reviews or practicing Evidence Based Medicine (EBM). Finding all relevant published evidence for a particular medical case is a time and labour intensive task, given the breadth of the biomedical literature. Automatic quantification of conceptual relationships between key clinical evidence within and across publications, despite variations in the expression of clinically-relevant concepts, can help to facilitate synthesis of evidence. In this study, we aim to provide an approach towards expediting evidence synthesis by quantifying semantic similarity of key evidence as expressed in the form of individual sentences. Such semantic textual similarity can be applied as a key approach for supporting selection of related studies.
Material and methods
We propose a generalisable approach for quantifying semantic similarity of clinical evidence in the biomedical literature, specifically considering the similarity of sentences corresponding to a given type of evidence, such as clinical interventions, population information, clinical findings, etc. We develop three sets of generic, ontology-based, and vector-space models of similarity measures that make use of a variety of lexical, conceptual, and contextual information to quantify the similarity of full sentences containing clinical evidence. To understand the impact of different similarity measures on the overall evidence semantic similarity quantification, we provide a comparative analysis of these measures when used as input to an unsupervised linear interpolation and a supervised regression ensemble. In order to provide a reliable test-bed for this experiment, we generate a dataset of 1000 pairs of sentences from biomedical publications that are annotated by ten human experts. We also extend the experiments on an external dataset for further generalisability testing.
Results
The combination of all diverse similarity measures showed stronger correlations with the gold standard similarity scores in the dataset than any individual kind of measure. Our approach reached near 0.80 average Pearson correlation across different clinical evidence types using the devised similarity measures. Although they were more effective when combined together, individual generic and vector-space measures also resulted in strong similarity quantification when used in both unsupervised and supervised models. On the external dataset, our similarity measures were highly competitive with the state-of-the-art approaches developed and trained specifically on that dataset for predicting semantic similarity.
Conclusion
Experimental results showed that the proposed semantic similarity quantification approach can effectively identify related clinical evidence that is reported in the literature. The comparison with a state-of-the-art method demonstrated the effectiveness of the approach, and experiments with an external dataset support its generalisability.}
}
@article{ALULEMA2023132,
title = {SI4IoT: A methodology based on models and services for the integration of IoT systems},
journal = {Future Generation Computer Systems},
volume = {143},
pages = {132-151},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23000316},
author = {Darwin Alulema and Javier Criado and Luis Iribarne and Antonio Jesús Fernández-García and Rosa Ayala},
keywords = {Model-Driven Engineering (MDE), Domain-Specific Language (DSL), Web services, Integration patterns, Internet of Things (IoT), Smart applications},
abstract = {The Internet of Things (IoT) is a technology that is growing faster every day due to the large number of platforms and end-devices that are becoming connected to each other. As part of this wide and diverse scenario, developers are now facing various challenges, such as heterogeneity, diversity of communication protocols, discovery of things, and coordination of services, among others. A paradigm that can help to tackle these issues is the model engineering since it allows different elements to be reused which can simplify the work of developers. In this paper, we propose SI4IoT (Service Integration for IoT), a methodology based on MDE (Model-Driven Engineering) for the development of IoT systems. This methodology enables automatic code generation, making it easier for developers to design sophisticated new IoT applications. We focus on a DSL (Domain-Specific Language), a graphic editor, and a set of M2T (Model-to-Text) transformations that generate code for software artifacts on Arduino, Node-Red, Ballerina, and NCL-Lua for deployment on hardware nodes, web services, and DTV (Digital TV). Our proposal consists of a model for the integration of services made up of three layers: physical, logical, and application. To validate our proposal, a Smart Home scenario has been considered, with sensors and actuators which, when combined, allow control of lights and heating. In addition, it allows the user to receive information about their home on television based on the REST services that have been created for the IoT nodes.}
}
@article{OTTERSEN2024100334,
title = {Triplet extraction leveraging sentence transformers and dependency parsing},
journal = {Array},
volume = {21},
pages = {100334},
year = {2024},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2023.100334},
url = {https://www.sciencedirect.com/science/article/pii/S2590005623000590},
author = {Stuart Gallina Ottersen and Flávio Pinheiro and Fernando Bação},
keywords = {Triplet extraction, NLP, Natural language processing, Knowledge Graph},
abstract = {Knowledge Graphs are a tool to structure (entity, relation, entity) triples. One possible way to construct these knowledge graphs is by extracting triples from unstructured text. The aim when doing this is to maximise the number of useful triples while minimising the triples containing no or useless information. Most previous work in this field uses supervised learning techniques that can be expensive both computationally and in that they require labelled data. While the existing unsupervised methods often produce an excessive amount of triples with low value, base themselves on empirical rules when extracting triples or struggle with the order of the entities relative to the relation. To address these issues this paper suggests a new model: Unsupervised Dependency parsing Aided Semantic Triple Extraction (UDASTE) that leverages sentence structure and allows defining restrictive triple relation types to generate high-quality triples while removing the need for mapping extracted triples to relation schemas. This is done by leveraging pre-trained language models. UDASTE is compared with two baseline models on three datasets. UDASTE outperforms the baselines on all three datasets. Its limitations and possible further work are discussed in addition to the implementation of the model in a computational intelligence context.}
}
@article{PALLI2018436,
title = {Ascribing materiality and agency to strategy in interaction: A language-based approach to the material agency of strategy},
journal = {Long Range Planning},
volume = {51},
number = {3},
pages = {436-450},
year = {2018},
note = {The Performativity of Strategy: Taking Stock and Moving Ahead},
issn = {0024-6301},
doi = {https://doi.org/10.1016/j.lrp.2017.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0024630117301012},
author = {Pekka Pälli},
keywords = {Strategy-as-practice, Discourse, Communicative constitution of organization, Materiality, Nonhuman agency},
abstract = {Adding to the existent research on strategy as discourse and practice, this paper develops a language-based approach to viewing the agency and materiality of strategy. The study draws insights from the communicative constitution of organization (CCO) approach and linguistic agency to investigate how organizational members ascribe materiality and performative agency to strategy in their talk-in-interaction. The data consist of 14 video-recorded dyadic manager-to-manager conversations from one private and one public Finnish organization. The findings highlight how strategy is habitually spoken of as a material concrete entity and as a nonhuman agent that makes a difference in the course of described actions. The findings thus suggest that the performative position of strategy has been encoded in language and its use, which further suggests that object-like concreteness and agentivity are key elements of the organizational strategy discourse.}
}
@article{BI2024103721,
title = {Difficulty-controllable question generation over knowledge graphs: A counterfactual reasoning approach},
journal = {Information Processing & Management},
volume = {61},
number = {4},
pages = {103721},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103721},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324000815},
author = {Sheng Bi and Jianyu Liu and Zeyi Miao and Qizhi Min},
keywords = {Question generation, Knowledge graph, Difficulty controllable, Counterfactual reasoning, Soft template},
abstract = {Difficulty-controllable question generation (DCQG) over knowledge graphs aims to generate questions with a given subgraph and a difficulty label, such as “easy” or “hard.” However, three significant challenges currently confront DCQG: (1) limited modes for modeling difficulty, (2) the inability to ensure causality between difficulty labels and generated outcomes, and (3) lack of difficulty-annotated datasets. To overcome these challenges, we present DiffQG, a DCQG model that uses soft templates and counterfactual reasoning. DiffQG utilizes a mixture of experts as soft template selectors to enhance the diversity of difficulty representation. Soft templates can efficiently capture the similarity among questions of different difficulties, avoiding the need for constructing explicit templates. A disentanglement module is introduced to separate triple representations in the input subgraph that are pertinent and extraneous to the current question’s difficulty. Disentanglement minimizes the interference of irrelevant information on the generated output in neural networks due to entanglement. More importantly, disentangled representations enable the model to create training samples for counterfactual reasoning, strengthening causality between inputs and outputs. Additionally, we propose a question difficulty estimation method that simultaneously considers the input subgraph, question, and answering process. Extensive experiments reveal that our model can successfully generate questions at desired difficulty levels, surpassing the baselines by at least 8% in terms of difficulty control. Furthermore, DiffQG exhibits superior generalizability and interpretability.}
}
@article{LI2024113532,
title = {Biomarkers associated with papillary thyroid carcinoma and Hashimoto’s thyroiditis: Bioinformatic analysis and experimental validation},
journal = {International Immunopharmacology},
volume = {143},
pages = {113532},
year = {2024},
issn = {1567-5769},
doi = {https://doi.org/10.1016/j.intimp.2024.113532},
url = {https://www.sciencedirect.com/science/article/pii/S156757692402054X},
author = {Bingxin Li and Zhaogen Cai and Yihan Zhang and Ruihua Chen and Shanshan Tang and Feijuan Kong and Wen Li and Li Ding and Lei Chen and Huanbai Xu},
keywords = {Hashimoto’s thyroiditis, Papillary thyroid carcinoma, Biomarker, Bioinformatics, Immune infiltration, Disease association},
abstract = {Purpose
Hashimoto’s thyroiditis (HT) is widely recognized as a risk factor for papillary thyroid carcinoma (PTC). This study aimed to identify key targets involved in the progression of HT to PTC.
Methods
Microarray datasets (GSE138198) for PTC, HT, and PTC with HT in the background (PTC-W) were obtained from the Gene Expression Omnibus (GEO) database. Differentially expressed genes (DEGs) were identified and analyzed between normal and diseased groups. Functional enrichment analysis was performed using Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG). Protein-protein interaction (PPI) network analysis was conducted to identify hub genes, which were validated through qPCR and immunohistochemical (IHC) analysis. ROC analysis was then carried out based on the expression levels of hub genes in clinical plasma samples.
Results
A total of 78 shared DEGs were identified from the GEO dataset. GO and KEGG analyses highlighted pathways such as epithelial-to-mesenchymal transition (EMT) and PI3K-Akt signaling. The analysis of immune cell subtypes showed that the hub genes were commonly associated with various immune cells, particularly dendritic cells (DC) and macrophages. Ten hub genes—LYZ, FCER1G, CCL18, CXCL9, ALOX5, TYROBP, C1QB, CTSS, MET, and FAM20A—were identified from the PPI network. qPCR and IHC confirmed the overexpression of MET and FAM20A in PTC-W. The area under the curve (AUC) of the ROC analysis was 0.889 for MET and 0.825 for FAM20A.
Conclusion
This study identified two hub genes, MET and FAM20A, with potential diagnostic value in HT and PTC.}
}
@article{FU2024104623,
title = {FedFSA: Hybrid and federated framework for functional status ascertainment across institutions},
journal = {Journal of Biomedical Informatics},
volume = {152},
pages = {104623},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104623},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000418},
author = {Sunyang Fu and Heling Jia and Maria Vassilaki and Vipina K. Keloth and Yifang Dang and Yujia Zhou and Muskan Garg and Ronald C. Petersen and Jennifer {St Sauver} and Sungrim Moon and Liwei Wang and Andrew Wen and Fang Li and Hua Xu and Cui Tao and Jungwei Fan and Hongfang Liu and Sunghwan Sohn},
keywords = {Natural language processing, Functional status, Electronic health records, Federated learning, Deep learning},
abstract = {Introduction
Patients' functional status assesses their independence in performing activities of daily living, including basic ADLs (bADL), and more complex instrumental activities (iADL). Existing studies have discovered that patients’ functional status is a strong predictor of health outcomes, particularly in older adults. Depite their usefulness, much of the functional status information is stored in electronic health records (EHRs) in either semi-structured or free text formats. This indicates the pressing need to leverage computational approaches such as natural language processing (NLP) to accelerate the curation of functional status information. In this study, we introduced FedFSA, a hybrid and federated NLP framework designed to extract functional status information from EHRs across multiple healthcare institutions.
Methods
FedFSA consists of four major components: 1) individual sites (clients) with their private local data, 2) a rule-based information extraction (IE) framework for ADL extraction, 3) a BERT model for functional status impairment classification, and 4) a concept normalizer. The framework was implemented using the OHNLP Backbone for rule-based IE and open-source Flower and PyTorch library for federated BERT components. For gold standard data generation, we carried out corpus annotation to identify functional status-related expressions based on ICF definitions. Four healthcare institutions were included in the study. To assess FedFSA, we evaluated the performance of category- and institution-specific ADL extraction across different experimental designs.
Results
ADL extraction performance ranges from an F1-score of 0.907 to 0.986 for bADL and 0.825 to 0.951 for iADL across the four healthcare sites. The performance for ADL extraction with impairment ranges from an F1-score of 0.722 to 0.954 for bADL and 0.674 to 0.813 for iADL across four healthcare sites. For category-specific ADL extraction, laundry and transferring yielded relatively high performance, while dressing, medication, bathing, and continence achieved moderate-high performance. Conversely, food preparation and toileting showed low performance.
Conclusion
NLP performance varied across ADL categories and healthcare sites. Federated learning using a FedFSA framework performed higher than non-federated learning for impaired ADL extraction at all healthcare sites. Our study demonstrated the potential of the federated learning framework in functional status extraction and impairment classification in EHRs, exemplifying the importance of a large-scale, multi-institutional collaborative development effort.}
}
@article{LI20252436,
title = {Outcomes of immune tolerance induction with rituximab to eradicate high-titer inhibitor of hemophilia A: depicted by exponential decay model and the gene expression profile of different outcomes by RNA-sequencing},
journal = {Journal of Thrombosis and Haemostasis},
volume = {23},
number = {8},
pages = {2436-2448},
year = {2025},
issn = {1538-7836},
doi = {https://doi.org/10.1016/j.jtha.2025.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S153878362500265X},
author = {Zekun Li and Yongqiang Tang and Zhenping Chen and Guoqing Liu and Wanru Yao and Gang Li and Xiaoling Cheng and Yaguang Peng and Siyu Cai and Chang Cui and Di Ai and Jialu Zhang and Man−Chiu Poon and Wensheng Zhang and Runhui Wu},
keywords = {hemophilia A, FVIII inhibitor, immune tolerance induction, rituximab, RNA-sequencing},
abstract = {Background
Eradication of inhibitors is still a desirable goal for patients with hemophilia A inhibitors. Combining rituximab with immune tolerance induction (ITI) is the secondline regimen, but data and predictors are limited.
Objectives
To evaluate the efficacy of ITI-rituximab and to identify the predictors of prognosis.
Methods
In total, 76 children with high-titer inhibitor prospectively using low-dose ITI together with 1-3 round(s) of rituximab were evaluated for outcomes: success or failure and rapidity (rapid or slow) of inhibitor negativity (ie, inhibitor titers turned negative, inhibitor negativity [IN]). The whole-transcriptome RNA-sequencing (RNA-seq) was used to analyze the gene expression profile of 4 failure patients (excluding F8 large deletion) and 4 rapid success-IN patients.
Results
Success IN was achieved in 41 of 76 (53.9%) patients after first-round of rituximab, 50 of 76 (65.8%) after second-round of rituximab, and 51 of 76 (67.1%) after third-round of rituximab. Profile of inhibitor decay followed an exponential decay curve. Time to a given inhibitor titer during ITI–rituximab could be estimated by the model t=ln(Y0−PlateauY−Plateau)k. The newly observed poor prognostic factors included relapse after the first-round of rituximab and early occurence of poor-outcome events. RNA-seq analysis showed 186 upregulated differential expressed genes (DEGs) and 176 downregulated DEGs in failure subjects compared with those in patients with rapid success IN. The upregulated DEGs included CXCL8, NLRP6, CHI3L1, CLEC9A, THBD, and PROS1. The downregulated DEGs included STAT1, TLR7, C1Q, C2, IDO1, and CD38.
Conclusion
Success IN was achieved in 67% of children with hemophilia A with high-titer inhibitor treated by ITI–rituximab. A model based on the profile of inhibitor-titer decay can be used for predicting outcomes. Humoral immune response and complement and coagulation cascades may act as signals that influence ITI outcomes (ClinicalTrials.gov: NCT03598725).}
}
@article{DIMARTINO2022,
title = {A Big Data Pipeline and Machine Learning for Uniform Semantic Representation of Data and Documents From IT Systems of the Italian Ministry of Justice},
journal = {International Journal of Grid and High Performance Computing},
volume = {14},
number = {1},
year = {2022},
issn = {1938-0259},
doi = {https://doi.org/10.4018/IJGHPC.301579},
url = {https://www.sciencedirect.com/science/article/pii/S1938025922000024},
author = {Beniamino {Di Martino} and Luigi {Colucci Cante} and Salvatore D'Angelo and Antonio Esposito and Mariangela Graziano and Fiammetta Marulli and Pietro Lupi and Alessandra Cataldi},
keywords = {Big Data Representation, Data Lake, Machine Learning, Natural Language Processing, Privacy Preservation},
abstract = {ABSTRACT
In this paper, a big data pipeline is presented, taking in consideration both structured and unstructured data made available by the Italian Ministry of Justice, regarding their telematic civil process. Indeed, the complexity and volume of the data provided by the ministry requires the application of big data analysis techniques, in concert with machine and deep learning frameworks, to be correctly analysed and to obtain meaningful information that could support the ministry itself in better managing civil processes. The pipeline has two main objectives: to provide a consistent workflow of activities to be applied to the incoming data, aiming at extracting useful information for the ministry's decision making tasks, and to homogenize the incoming data, so that they can be stored in a centralized and coherent data lake to be used as a reference for further analysis and considerations.}
}
@article{SKEIDSVOLLSOLVANG2025105688,
title = {Healthcare professionals’ cross-organizational access to electronic health records: A scoping review},
journal = {International Journal of Medical Informatics},
volume = {193},
pages = {105688},
year = {2025},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2024.105688},
url = {https://www.sciencedirect.com/science/article/pii/S1386505624003514},
author = {Øivind {Skeidsvoll Solvang} and Sonja Cassidy and Conceição Granja and Terje Solvoll},
keywords = {Healthcare professionals, Electronic health records, Access, Cross-organizational, Scoping review},
abstract = {Background
Cross-organizational access to shared electronic health records can enhance integrated, people-centered health services. However, a gap remains between these potential benefits and the limited support currently offered by electronic health records. The Valkyrie research project aims to bridge this gap by developing a technical prototype of an architecture to promote healthcare service coordination.
Objective
To inform the Valkyrie project, we aimed to evaluate approaches for healthcare professionals’ access to electronic health records across healthcare providers and identify factors influencing the success and failure of these approaches.
Materials and methods
Using the Joanna Briggs Institute guidance for scoping reviews, searches were conducted in six research databases and grey literature, without limitations on year or language. Papers selected for full-text review were analyzed, and data was extracted using standardized forms that reflected the population, concept, and context framework and the categorization model used in the qualitative analysis of the barriers and facilitators reported in the included papers.
Results
Among the 290 identified papers, five were deemed eligible for full-text review. The included papers were heterogeneous in country, year of publication, study setting, implementation level, and access approaches to electronic health records, highlighting various techniques, from federated to centralized, for accessing shared electronic health records.
Discussion and conclusion
The review did not identify one single superior access approach. However, a hybrid approach incorporating components from the different approaches combined with emerging technologies may benefit the Valkyrie project. The key facilitators were identified as improved information quality and flexible and easy access. In contrast, lack of trust and poor information quality were significant barriers to successful cross-organizational access to electronic health records. Future research should explore alternative access approaches, considering information quality, user training, and collegial trust across healthcare providers.}
}
@article{KELLER202214,
title = {Connecting the multi-level-perspective and social practice approach for sustainable transitions},
journal = {Environmental Innovation and Societal Transitions},
volume = {44},
pages = {14-28},
year = {2022},
issn = {2210-4224},
doi = {https://doi.org/10.1016/j.eist.2022.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S221042242200051X},
author = {Margit Keller and Marlyne Sahakian and Léon Francis Hirt},
keywords = {Multi-level perspective, Social practice approach, Sustainability transition, Intervention design, Social change},
abstract = {The increasing sense of urgency to transition to sustainable modes of consumption and production requires an understanding of social problem framings and processes of change. We examine how two conceptual frameworks, the Multi-level Perspective (MLP), a socio-technical transition theory, and Social Practice Approaches (SPA), contribute to understanding opportunities for social change. We share the results of a systematic literature review that seeks a better understanding of how these two approaches are co-employed. We first present quantifiable results from an analysis of 118 publications; we then focus on a qualitative analysis to investigate conceptual complementarities while recognizing their ontological differences. We find further entry points where the two approaches can be fruitfully combined, such as the study of vulnerable proto-practices, research on informal regimes, and the development of landscape- and supra-practice-level meanings. We conclude with recommendations on how to further operationalize the co-employment of MLP and SPA in sustainability transition studies.}
}
@article{ALMEIDA2021100760,
title = {A methodology for cohort harmonisation in multicentre clinical research},
journal = {Informatics in Medicine Unlocked},
volume = {27},
pages = {100760},
year = {2021},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2021.100760},
url = {https://www.sciencedirect.com/science/article/pii/S2352914821002343},
author = {João Rafael Almeida and Luís Bastão Silva and Isabelle Bos and Pieter Jelle Visser and José Luís Oliveira},
keywords = {Clinical studies, Observational studies, Data harmonisation, ETL, OMOP CDM},
abstract = {Many clinical trials and scientific studies have been conducted aiming for better understanding of specific medical conditions. However, these studies are often based on a small number of participants due to the difficulty in finding people with similar medical characteristics and available to participate in the studies. This is particularly critical in rare diseases, where the reduced number of subjects hinders reliable findings. To generate more substantial clinical evidence by increasing the power of the analyses, researchers have started to perform data harmonisation and multiple cohort analyses. However, the analysis of heterogeneous data sources implies dealing with different data structures, terminologies, concepts, languages and, most importantly, the knowledge behind the data. In this paper, we present a methodology to harmonise different cohorts into a standard data schema, helping the research community to generate evidence from a wider variety of data sources. Our methodology was inspired by the OHDSI Common Data Model, which aims to harmonise EHR datasets for observational studies, leveraging on knowledge and open source tools to perform multicentric disease-specific studies. This proposal was validated using Alzheimer’s Disease cohorts from several countries, combining at the end 6,669 subjects and 172 clinical concepts. The harmonised datasets now enable multi-cohort querying and analysis, helping in the execution of new research. The methodology was implemented in Python language and is available, under the MIT licence, at https://bioinformatics-ua.github.io/CMToolkit/.}
}
@article{RAUSCH2022104394,
title = {Tolerance management domain model for semantic enrichment of BIMs},
journal = {Automation in Construction},
volume = {141},
pages = {104394},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104394},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522002679},
author = {Christopher Rausch and Saeed Talebi and Mani Poshdar and Beidi Li and Carl Schultz},
keywords = {Domain model, Dependency structure matrix, Tolerance management, Building information model, Tolerance analysis, Risk management},
abstract = {Dimensional variability of components and assemblies in construction can lead to significant defects, rework, and project risk if not managed effectively. Given the complexity of using tolerance management to control dimensional variability, an automated BIM-based approach is highly propitious, while currently elusive. This paper develops the first iteration of a domain model for tolerance management (ToleranceDM) using two case study examples within the domain of building construction. The results are shown to (1) consolidate the scattered, disparate existing “knowledge” and research on tolerance management into a single standardised, uniform framework, and (2) formalise this knowledge so that it can be unambiguously interpreted and parsed into software systems for automated tolerance management in construction. ToleranceDM functions as a key step towards benchmarking process capabilities, computing tolerance compliance automatically, and enabling in-field communication of tolerance requirements. Future research should explore case studies in different construction domains, along with developing an improved abduction framework and integrating as-built project data for tolerance compliance checking.}
}
@article{BEREA2025101711,
title = {Towards a generalized framework for planetary communication},
journal = {Space Policy},
pages = {101711},
year = {2025},
issn = {0265-9646},
doi = {https://doi.org/10.1016/j.spacepol.2025.101711},
url = {https://www.sciencedirect.com/science/article/pii/S0265964625000359},
author = {Anamaria Berea and Karen S. Lewis and Bettina Beinhoff and Arik Kershenbaum and Eng Sengsavang and Nick Searra}
}
@article{AZKUNE2020280,
title = {Cross-environment activity recognition using word embeddings for sensor and activity representation},
journal = {Neurocomputing},
volume = {418},
pages = {280-290},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.08.044},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220313230},
author = {Gorka Azkune and Aitor Almeida and Eneko Agirre},
keywords = {Smart homes, Activity recognition, Semantic representations, Cross-environment activity recognition},
abstract = {Cross-environment activity recognition in smart homes is a very challenging problem, specially for data-driven approaches. Currently, systems developed to work for a certain environment degrade substantially when applied to a new environment, where not only sensors, but also the monitored activities may be different. Some systems require manual labeling and mapping of the new sensor names and activities using an ontology. Ideally, given a new smart home, we would like to be able to deploy the system, which has been trained on other sources, with minimal manual effort and with acceptable performance. In this paper, we propose the use of neural word embeddings to represent sensor activations and activities, which comes with several advantages: (i) the representation of the semantic information of sensor and activity names, and (ii) automatically mapping sensors and activities of different environments into the same semantic space. Based on this novel representation approach, we propose two data-driven activity recognition systems: the first one is a completely unsupervised system based on embedding similarities, while the second one adds a supervised learning regressor on top of them. We compare our approaches with some baselines using four public datasets, showing that data-driven cross-environment activity recognition obtains good results even when sensors and activity labels significantly differ. Our results show promise for reducing manual effort, and are complementary to other efforts using ontologies.}
}
@article{HONG2025103673,
title = {Sequence image layout generation for construction accident simulation using domain-tuned NER by ZSL-PLM and scene graph learning},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103673},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103673},
url = {https://www.sciencedirect.com/science/article/pii/S147403462500566X},
author = {Eunbin Hong and June-Seong Yi},
keywords = {Zero-shot learning, Pre-trained LLM, Domain-tuned NER, Automated annotation, Scene graph embedding, Sequence image layout generation, Accident analysis, Construction safety management},
abstract = {Safety in the construction industry remains a critical concern, with accident analysis often relying on time-intensive, error-prone manual processes. The unstructured nature of accident reporting text complicates the extraction and interpretation of sequence information essential for understanding accident dynamics. This study addresses these challenges by proposing a domain-specific AI framework that automates the extraction, structuring, predicting, and visualization of sequence information to generate interpretable 2D image layouts. A key achievement of the framework is its ability to infer accident sequences and generate layouts without relying on large-scale or ground truth image datasets for training. By integrating domain-tuned named entity recognition (NER), scene graph learning, and graph convolutional networks (GCNs), the framework achieves a highly robust entity diversity and demonstrates accurate entity recognition. These metrics, alongside notable improvements in spatial alignment (+88.89 %) and temporal consistency (+68.75 %) over the text-based model using DALL-E API, laying a foundation for robust framework advancing domain-specific deep learning applications and enhancing sequential analysis of construction accidents.}
}
@article{ARSLAN2019102854,
title = {Semantic trajectory insights for worker safety in dynamic environments},
journal = {Automation in Construction},
volume = {106},
pages = {102854},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.102854},
url = {https://www.sciencedirect.com/science/article/pii/S0926580518311890},
author = {Muhammad Arslan and Christophe Cruz and Dominique Ginhac},
keywords = {Semantic trajectories, Building Information Modeling (BIM), Stay region, GPS, Mobility, Health and Safety (H&S), Hidden Markov Model (HMM)},
abstract = {Existing studies reveal that unsafe worker movement behaviors are one of the major reasons of construction site fatalities resulting in serious collisions with site objects and machinery. For understanding worker movements in dynamic construction environments which involve moving and changing objects, a system named ‘WoTAS’ (Worker Trajectory Analysis System) is proposed. First, a real-time Bluetooth Low Energy (BLE) beacons-based data collection and trajectory pre-processing subsystem is built for extracting multifaceted trajectory characteristics and stay regions of the workers that will help in recognizing the important regions in the building for categorizing the worker movements. Second, to enable the desired semantic insights for better understanding the underlying meaningful worker movements using the contextual data repositories related to the building environment, an ontology-based ‘STriDE’ (Semantic Trajectories for Dynamic Environments) model is applied which tracks the evolution of moving and changing building objects and outputs semantic trajectories. For extracting insights from the semantic trajectories, the Hidden Markov Model (HMM) is used which is one of the probabilistic approaches present in the literature for describing the object behavior in time. Using the HMMs, a set of trajectories belonging to a stay region is analyzed by categorizing the worker movements into four different states. In the end, the output of the Viterbi algorithm is visualized using a BIM model for identifying the most probable high-risk locations involving sharp worker movements and rotations. The developed ‘WoTAS’ system will help safety managers in monitoring and controlling building activities remotely in dynamic environments by understanding the worker movements for improved safety management in day-to-day building operations. Eventually, understanding the worker movements will contribute towards reducing the chances of near-miss incidents on sites which have the potential to cause serious accidents.}
}
@article{FESHCHENKO2025145,
title = {Mayakovsky within the LEF Circle: Life-Building, Linguistic Creativity, and Pragmatics of the Avant-Garde},
journal = {Slavic Literatures},
volume = {154-156},
pages = {145-171},
year = {2025},
note = {MAYAKOVSKY’S TIME: A CHANGE OF DISCOURSES. ON THE 132ND ANNIVERSARY OF A KEY FIGURE OF THE AVANT-GARDE ERA},
issn = {2950-3965},
doi = {https://doi.org/10.1016/j.slalit.2025.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S2950396525000195},
author = {Vladimir Feshchenko and Olga Sokolova},
keywords = {Linguistic pragmatics, Linguistic creativity, Vladimir Mayakovsky, LEF, Art-as-life-building, Revolution of language},
abstract = {The paper analyzes the correlations between life-creation and verbal creativity in post-revolutionary texts by Vladimir Mayakovsky and his fellow “LEF” / “Novy LEF” authors. Mayakovsky’s three-dimensional view of poetic language (what in modern semiotics would correspond to semantics, syntactics, and pragmatics) laid out in his early essay ‘Two Chekhovs’ (1914) paved the way for a new pragmatics of avant-garde discourse as a discourse of active impact and at the same time a meta-reflexive linguistic activity. Both in poetic texts and in critical essays of the LEF period, Mayakovsky’s linguistic innovation oscillates between the two major functions – aesthetic and appellative – to make “speech creativity” (as Boris Arvatov termed it) a powerful tool of poetic-and-political communication. This study approaches Mayakovsky’s linguistic and communicative experiments in terms of LEF members’ conceptions, such as “revolutionization of language” by Grigory Vinokur, “art-as-life-building” by Nikolai Chuzhak, “literature of fact” by Sergey Tretyakov, and “art as production” by Boris Arvatov.}
}
@article{GRIMWOOD2018204,
title = {Realism and rhetoric in the evaluation of a new care model},
journal = {Journal of Integrated Care},
volume = {27},
number = {3},
pages = {204-214},
year = {2018},
issn = {1476-9018},
doi = {https://doi.org/10.1108/JICA-11-2018-0067},
url = {https://www.sciencedirect.com/science/article/pii/S1476901818000582},
author = {Tom Grimwood},
keywords = {Integrated provision of care, Integrated care, Management of change, Health and social care, Whole systems},
abstract = {Purpose
The purpose of this paper is to discuss the methodological challenges to evaluating one of the 50 vanguard sites of the new care model (NCM) programme for integrated care in England, and make the case for a modified realist approach to this kind of evaluation.
Design/methodology/approach
The paper considers three challenges to evaluating the NCM in this particular vanguard: complexity, strategy and rhetoric. It reflects on how the realist approach negotiates these philosophical challenges to delivering integrated care, in order to provide contextualised accounts of who a programme works for, in what context, and why.
Findings
The paper argues that, in the case of this particular vanguard site, the tangible benefit of the realist approach was not in providing a firm epistemological basis for evaluation, but rather in drawing out and articulating the ontological rhetoric of such large-scale transformation programmes. By understanding the work of the NCM less as an objective “system”, and more as a dynamic form of persuasion, aimed at securing the “adherence of minds” (Perelman and Olbrechts-Tyteca, 2008, p. 8) in multiple audiences, the paper suggests that realist evaluation can be used to address both the systematic issues and localised successes the NCMs encountered.
Originality/value
The paper identifies a number of aspects of new models of integrated care for evaluators to consider. It offers ways of negotiating the challenges to conventional outcome-focused evaluation, by drawing attention to the need for contextualised, time-situated and audience-sensitive value of NCMs.}
}
@article{JOVANOVIC2022,
title = {Ambient Assisted Living: Scoping Review of Artificial Intelligence Models, Domains, Technology, and Concerns},
journal = {Journal of Medical Internet Research},
volume = {24},
number = {11},
year = {2022},
issn = {1438-8871},
doi = {https://doi.org/10.2196/36553},
url = {https://www.sciencedirect.com/science/article/pii/S1438887122006975},
author = {Mladjan Jovanovic and Goran Mitrov and Eftim Zdravevski and Petre Lameski and Sara Colantonio and Martin Kampel and Hilda Tellioglu and Francisco Florez-Revuelta},
keywords = {ambient assisted living, AAL, assisted living, active living, digital health, digital well-being, automated learning approach, artificial intelligence algorithms, human-centered AI, review, implications, artificial intelligence, mobile phone},
abstract = {Background
Ambient assisted living (AAL) is a common name for various artificial intelligence (AI)—infused applications and platforms that support their users in need in multiple activities, from health to daily living. These systems use different approaches to learn about their users and make automated decisions, known as AI models, for personalizing their services and increasing outcomes. Given the numerous systems developed and deployed for people with different needs, health conditions, and dispositions toward the technology, it is critical to obtain clear and comprehensive insights concerning AI models used, along with their domains, technology, and concerns, to identify promising directions for future work.
Objective
This study aimed to provide a scoping review of the literature on AI models in AAL. In particular, we analyzed specific AI models used in AАL systems, the target domains of the models, the technology using the models, and the major concerns from the end-user perspective. Our goal was to consolidate research on this topic and inform end users, health care professionals and providers, researchers, and practitioners in developing, deploying, and evaluating future intelligent AAL systems.
Methods
This study was conducted as a scoping review to identify, analyze, and extract the relevant literature. It used a natural language processing toolkit to retrieve the article corpus for an efficient and comprehensive automated literature search. Relevant articles were then extracted from the corpus and analyzed manually. This review included 5 digital libraries: IEEE, PubMed, Springer, Elsevier, and MDPI.
Results
We included a total of 108 articles. The annual distribution of relevant articles showed a growing trend for all categories from January 2010 to July 2022. The AI models mainly used unsupervised and semisupervised approaches. The leading models are deep learning, natural language processing, instance-based learning, and clustering. Activity assistance and recognition were the most common target domains of the models. Ambient sensing, mobile technology, and robotic devices mainly implemented the models. Older adults were the primary beneficiaries, followed by patients and frail persons of various ages. Availability was a top beneficiary concern.
Conclusions
This study presents the analytical evidence of AI models in AAL and their domains, technologies, beneficiaries, and concerns. Future research on intelligent AAL should involve health care professionals and caregivers as designers and users, comply with health-related regulations, improve transparency and privacy, integrate with health care technological infrastructure, explain their decisions to the users, and establish evaluation metrics and design guidelines.
Trial Registration
PROSPERO (International Prospective Register of Systematic Reviews) CRD42022347590; https://www.crd.york.ac.uk/prospero/display_record.php?ID=CRD42022347590}
}
@article{PADOVANO2024100256,
title = {Towards human-AI collaboration in the competency-based curriculum development process: The case of industrial engineering and management education},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100256},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100256},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000596},
author = {Antonio Padovano and Martina Cardamone},
keywords = {Engineering education, Industry 5.0, Industrial engineering and management, Curriculum development, Artificial intelligence, Competency-based education},
abstract = {In the endeavor to advance industrial engineering and management (IEM) education, this research underscores the imperative of supporting a dynamic and responsive adaptation of a competency-based curriculum (CBC) to meet the demands of an ever-evolving industrial landscape and job market. Our study contributes to competency-based education (CBE) by demonstrating how Artificial Intelligence (AI) can inform the definition of a CBC in the IEM field, thus initiating the pioneering steps towards a collaborative human-AI approach in CBC design. Through a stepwise methodology based on semantic analysis, text mining, natural language processing (NLP) models, informetrics approaches, and clustering algorithms, we provide data-driven insights to inform the curriculum development process. This approach enabled us to identify educational gap, particularly in domains such as digital twin engineering and human-centric IEM. Moreover, this study advocates for higher education institutions (HEIs) to embrace a more structured and collaborative approach to continuously developing competency-based curricula. In this perspective, AI (including generative AI) emerges as a valuable ally in curriculum design. This approach proves instrumental in crafting competitive and appealing curricula, especially at peripheral universities. This study culminates in an updated WING model showing how to build Industry 5.0 related curricula and a series of recommendations for engineering educators.}
}
@incollection{VOINOV201958,
title = {Conceptual Diagrams and Flow Diagrams☆},
editor = {Brian Fath},
booktitle = {Encyclopedia of Ecology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {58-64},
year = {2019},
isbn = {978-0-444-64130-4},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.11143-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095489111431},
author = {Alexey Voinov},
keywords = {Cognitive maps, Conceptual model, Mental model, Qualitative modeling, Space, Structure, Time},
abstract = {Diagrams are probably the next most widely used type of models after mental and verbal models. Intuitively when presenting a model we tend to start drawing diagrams to explain the assumptions and simplifications made. With the advent of new software tools it becomes easier to use the computer to design these conceptual diagrams, especially since these software packages in many cases can convert the diagrams into computer code and generate numerical models for further analysis. Most of the systems dynamics software (such as Stella, Madonna, Powersim, Simile and others) can be readily used to put together conceptual models and flow diagrams. More advanced tools are based on the UML (Unified Modeling Language) approach and can generate computer code in Java, PHP, C++, Python, and other languages.}
}
@article{PU2024122999,
title = {Modeling and application of a customized knowledge graph for railway alignment optimization},
journal = {Expert Systems with Applications},
volume = {244},
pages = {122999},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122999},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423035017},
author = {Hao Pu and Ting Hu and Taoran Song and Paul Schonfeld and Xinjie Wan and Wei Li and Lihui Peng},
keywords = {Railway design, Alignment optimization, Knowledge graph, Knowledge modeling, Knowledge query},
abstract = {Railway alignment optimization is a complex process in which human knowledge and experience are extensively used. However, the related knowledge is usually unstructured, which is difficult for computers to recognize. Moreover, related knowledge is applied in fragmented ways in existing alignment optimization methods, which are thus difficult to update with actual advancements of human experience and knowledge. To solve the above problems, the first-known knowledge graph modeling method for railway alignment optimization is proposed in this paper. First, a hierarchical and categorized semantic network modeling approach for railway alignment design knowledge is devised. Based on this, a railway alignment design knowledge graph (RAD-KG) is constructed. Then, a rapid knowledge retrieval method is proposed for improving the querying efficiency from the RAD-KG during alignment optimization. Finally, the RAD-KG integrating multiple alignment design principles is successfully applied to a real-world case. It is verified that the alignment generated by the proposed method reduces costs by 9.2% compared with conventional manual work by experienced engineers. Moreover, the RAD-KG-assisted method can rapidly update alignment design guidelines during optimization and, hence, produce several alignment alternatives satisfying various complicated requirements, which confirms the flexibility of the proposed method.}
}
@article{LI2024107783,
title = {A multi-type semantic interaction and enhancement method for tax question understanding},
journal = {Engineering Applications of Artificial Intelligence},
volume = {130},
pages = {107783},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.107783},
url = {https://www.sciencedirect.com/science/article/pii/S095219762301967X},
author = {Jing Li and Dezheng Zhang and Yonghong Xie and Aziguli Wulamu},
keywords = {Question classification, Pretraining language model, Deep semantic, Transformer},
abstract = {Problem classification serves as a fundamental process in a tax intelligence consulting system, enabling the categorization of user-posed questions according to their semantic attributes. This categorization is pivotal in ensuring accurate question comprehension. Nevertheless, the inclusion of intricate professional terminology and the frequent alterations in linguistic structures associated with tax-related matters may culminate in suboptimal classification outcomes and hinder the precise comprehension of user demands. To address these issues, we propose a multitype semantic interaction and enhancement method (MtSIEM) to classify tax related issues that integrates entity and nonentity semantics to represent the semantic features of tax-related domain issues. Specifically, a pretraining language model and multigram mechanism are adopted to enhance the feature extraction ability. A soft attention module is also simplified to allocate interaction information weights, thereby adaptively determining the importance of the feature elements. These three components are used to perform precise learning on the tax question data. Subsequently, a dynamic routing architecture is employed to capture the relationships between the different problem features, resulting in predictive vectors. A series of comparative experiments on tax question data demonstrated that the proposed model achieved a classification accuracy of approximately 94.46%, an improvement of 2.99% compared with the baseline. Therefore, the proposed model can be utilized to predict the semantic category of tax-related issues, assisting intelligent tax advisory systems in matching questions with the most relevant knowledge and professional domains, thereby enabling faster retrieval of pertinent information and enhancing the timeliness of responses.}
}
@article{LIU2022104302,
title = {Deep learning-based data analytics for safety in construction},
journal = {Automation in Construction},
volume = {140},
pages = {104302},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104302},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522001753},
author = {Jiajing Liu and Hanbin Luo and Henry Liu},
keywords = {Safety, Deep learning, Computer vision, Natural language processing, Knowledge graph},
abstract = {Deep learning has been acknowledged as being robust in managing and controlling the performance of construction safety. However, there is an absence of state-of-the-art review that examines its developments and applications from the perspective of data utilization. Our review aims to fill this void and addresses the following research question: what developments in deep learning for data mining have been made to manage safety in construction? We systematically review the extant literature of deep-learning-based data analytics for construction safety management, including: (1) image/video-based; (2) text-based; (3) non-visual sensor-based; and (4) multi-modal-based. The review revealed three challenges of existing research in the construction industry: (1) lack of high-quality database; (2) inadequate ability of deep learning models; and (3) limited application scenarios. Based on our observations for the prevailing literature and practice, we identify that future research on safety management is needed and focused on the: (1) development of dynamic multi-modal knowledge graph; and (2) knowledge graph-based decision-making for safety. The application of deep learning is an emerging line of inquiry in construction, and this study not only identifies new research opportunities to support safety management, but also facilitates practicing deep learning for construction projects.}
}
@article{LOU2023,
title = {Potential Target Discovery and Drug Repurposing for Coronaviruses: Study Involving a Knowledge Graph–Based Approach},
journal = {Journal of Medical Internet Research},
volume = {25},
year = {2023},
issn = {1438-8871},
doi = {https://doi.org/10.2196/45225},
url = {https://www.sciencedirect.com/science/article/pii/S1438887123008245},
author = {Pei Lou and An Fang and Wanqing Zhao and Kuanda Yao and Yusheng Yang and Jiahui Hu},
keywords = {coronavirus, heterogeneous data integration, knowledge graph embedding, drug repurposing, interpretable prediction, COVID-19},
abstract = {Background
The global pandemics of severe acute respiratory syndrome, Middle East respiratory syndrome, and COVID-19 have caused unprecedented crises for public health. Coronaviruses are constantly evolving, and it is unknown which new coronavirus will emerge and when the next coronavirus will sweep across the world. Knowledge graphs are expected to help discover the pathogenicity and transmission mechanism of viruses.
Objective
The aim of this study was to discover potential targets and candidate drugs to repurpose for coronaviruses through a knowledge graph–based approach.
Methods
We propose a computational and evidence-based knowledge discovery approach to identify potential targets and candidate drugs for coronaviruses from biomedical literature and well-known knowledge bases. To organize the semantic triples extracted automatically from biomedical literature, a semantic conversion model was designed. The literature knowledge was associated and integrated with existing drug and gene knowledge through semantic mapping, and the coronavirus knowledge graph (CovKG) was constructed. We adopted both the knowledge graph embedding model and the semantic reasoning mechanism to discover unrecorded mechanisms of drug action as well as potential targets and drug candidates. Furthermore, we have provided evidence-based support with a scoring and backtracking mechanism.
Results
The constructed CovKG contains 17,369,620 triples, of which 641,195 were extracted from biomedical literature, covering 13,065 concept unique identifiers, 209 semantic types, and 97 semantic relations of the Unified Medical Language System. Through multi-source knowledge integration, 475 drugs and 262 targets were mapped to existing knowledge, and 41 new drug mechanisms of action were found by semantic reasoning, which were not recorded in the existing knowledge base. Among the knowledge graph embedding models, TransR outperformed others (mean reciprocal rank=0.2510, Hits@10=0.3505). A total of 33 potential targets and 18 drug candidates were identified for coronaviruses. Among them, 7 novel drugs (ie, quinine, nelfinavir, ivermectin, asunaprevir, tylophorine, Artemisia annua extract, and resveratrol) and 3 highly ranked targets (ie, angiotensin converting enzyme 2, transmembrane serine protease 2, and M protein) were further discussed.
Conclusions
We showed the effectiveness of a knowledge graph–based approach in potential target discovery and drug repurposing for coronaviruses. Our approach can be extended to other viruses or diseases for biomedical knowledge discovery and relevant applications.}
}
@article{ZHUHADAR2019507,
title = {Leveraging learning innovations in cognitive computing with massive data sets: Using the offshore Panama papers leak to discover patterns},
journal = {Computers in Human Behavior},
volume = {92},
pages = {507-518},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2017.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S0747563217306933},
author = {Leyla Zhuhadar and Mark Ciampa},
keywords = {Cognitive computing, Learning analytics, Panama papers, Ontology, GraphDB},
abstract = {Exposing learners to cognitive computing concepts involves new learning strategies. Because cognitive computing is probabilistic, using massive sets of data is fundamental in understanding these concepts. One data set is the release of the Offshore Panama Papers Leaks Database (LeaksDB) in May 2016, in which researchers were able to access this graph database as part of the International Consortium of Investigative Journalists (ICIJ) Offshore Leaks investigation and to draw conclusions about companies, trusts, foundations, and funds incorporated in 21 tax havens. For the purpose of this research, GraphDB Server was installed and configured by faculty members from a mid-western university. In addition, DBPedia and GeoNames repositories were linked to LeaksDB, leading to the discovery of interesting patterns about these Geo facade relationships between Officers (persons or companies) and Countries.}
}
@article{DENG2022109906,
title = {Subgraph-based feature fusion models for semantic similarity computation in heterogeneous knowledge graphs},
journal = {Knowledge-Based Systems},
volume = {257},
pages = {109906},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109906},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122009996},
author = {Yuanfei Deng and Wen Bai and Yuncheng Jiang and Yong Tang},
keywords = {Semantic similarity, Semantic relatedness, Heterogeneous knowledge graphs},
abstract = {Semantic similarity is a fundamental task in natural language processing that determines the similarity between two concepts within a taxonomy. For example, a pair of words (e.g., car and bike) appear similar because they share the same category (e.g., vehicle). Numerous computation methods, such as distance-based and feature-based approaches, are proposed to precisely depict this similarity. As knowledge graphs become heterogeneous (e.g., DBpedia), existing methods have limitations on utilizing multi-view features (e.g., abstract, structure, and categories). On the one hand, some features are incomplete for various reasons, reducing the effectiveness of embedding methods. On the other hand, the hidden connections among multi-view features are omitted by existing approaches. To address the problems mentioned above, we first extract three subgraphs from a heterogeneous knowledge graph and then combine various embedding approaches to capture the global semantics of each concept. Next, we offer subgraph-based feature fusion models that improve concept representation by fusing multi-view features. Finally, we devise mixed computation methods to calculate the semantic similarity between the two concepts. Experiment results show that multi-view features, particularly the abstract feature, can effectively improve the performance of the proposed methods. Compared to existing approaches, our methods significantly improve the Pearson correlation coefficient by about 7%. The source code of this paper is available at: https://github.com/fiego/SubgraphSS.}
}
@article{GANDHI20202486,
title = {Extracting Aspect Terms using CRF and Bi-LSTM Models},
journal = {Procedia Computer Science},
volume = {167},
pages = {2486-2495},
year = {2020},
note = {International Conference on Computational Intelligence and Data Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.301},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920307675},
author = {Hetal Gandhi and Vahida Attar},
keywords = {Aspect Based Sentiment Analysis, Aspect Term Extraction, Feature Extraction, CRF Model, Recurrent Neural Network Model},
abstract = {Sentiment Analysis deals with analysing the reviews stated by its consumers for any product. If such analysis is performed at a deeper level, it enables us to identify the consumer’s sentiment towards each feature of the product. The sentiment expressed may not be same towards each feature. The analysis of this sort is called Aspect Based Sentiment Analysis (ABSA) and it has been sub-divided into four subtasks. In this paper, the detailed study of the approaches used for the first subtask of ABSA, i.e. Aspect Term Extraction (ATE) is presented. This paper discusses how ATE can be performed for the reviews in a rich morphological language, like Hindi. The models proposed for ATE of Hindi Reviews are Conditional Random Field (CRF) and Bidirectional Long-Short-Term-Memory (Bi-LSTM) models with novel architecture. The CRF based approach with novel feature, ‘Cluster-id’ improved F-measure from 41.07% to 42.71%. However, with 5-fold cross-validation, the CRF model attained an F-measure of 44.54%. By using our proposed Bi-LSTM based model with PoS vector, the F-measure obtained is 44.49%.}
}
@article{CONSTANTINOU2025103112,
title = {Reimagining self-determination: Relational, decolonial, and intersectional perspectives},
journal = {Political Geography},
volume = {118},
pages = {103112},
year = {2025},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2024.103112},
url = {https://www.sciencedirect.com/science/article/pii/S0962629824000611},
author = {Costas M. Constantinou and Fiona McConnell and Dilar Dirik and Asebe Regassa and Shona Loong and Rauna Kuokkanen},
keywords = {Self-determination, Relationality, Intersectionality, Decoloniality, State, Indigenous peoples},
abstract = {Self-determination language and practice are increasingly perplexing in the 21st century. Historically linked to decolonization processes and post-imperial transformations of the international system, self-determination has espoused both violent and non-violent resistance, and supported both existing and emergent sovereignty. With the Janus-faced relationship between self-determination and colonialism continuing to this day, the contemporary moment is an opportune time to take stock of self-determination. However, as conventional jurisprudence and international legalism framings have, in many ways, hampered its emancipatory potential, alternative ways of reimagining self-determination are needed. Bringing together scholars from the fields of political and development geography, indigenous studies, international relations, and sociology, this intervention demonstrates how articulations of self-determination in specific sites offer powerful critiques of the state system and the liberal world order and unsettle hegemonic forms of knowledge production. These articulations open up conceptual space to push self-determination beyond the realm of rights, allowing us to reimagine self-determination as a vision and practice, and to recover and reconceptualize the hopeful, emancipatory and aspirational politics that have always underpinned self-determination. This intervention seeks to re-envision self-determination from three novel and interlinked angles: decoloniality, intersectionality, and relationality. Drawing on a range of examples of contemporary and historical self-determination claims and contestations, each author focuses on one or more of these angles to examine the extent to which current practices of and visions for self-determination engender novel understandings of emancipation from ‘foreign’ domination and/or colonial systems of governance.}
}
@article{KEEVALLIK202333,
title = {Sounding for others: Vocal resources for embodied togetherness},
journal = {Language & Communication},
volume = {90},
pages = {33-40},
year = {2023},
issn = {0271-5309},
doi = {https://doi.org/10.1016/j.langcom.2023.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0271530923000071},
author = {Leelo Keevallik and Emily Hofstetter},
keywords = {Dialogism, Distributed language, Non-lexical vocalization, Theory of language, Multisensoriality, Self-other},
abstract = {Standard models of language and communication depart from the assumption that speakers encode and receive messages individually, while interaction research has shown that utterances are composed jointly (C. Goodwin, 2018), dialogically designed with and for others (Linell, 2009). Furthermore, utterances only achieve their full semantic potential in concrete interactional contexts. This SI investigates various practices of human sounding that achieve their meaning through self and others' ongoing bodily actions. One person may vocalize to enact someone else's ongoing bodily experience, to coordinate with another body, or to convey embodied knowledge about something that is ostensibly only accessible to another's individual body. This illustrates the centrality of distributed action and collaborative agency in communication.}
}
@article{THANIKACHALAM20251971,
title = {EffNet-CNN: A Semantic Model for Image Mining & Content-Based Image Retrieval},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {143},
number = {2},
pages = {1971-2000},
year = {2025},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2025.063063},
url = {https://www.sciencedirect.com/science/article/pii/S1526149225001456},
author = {Rajendran Thanikachalam and Anandhavalli Muniasamy and Ashwag Alasmari and Rajendran Thavasimuthu},
keywords = {Image mining, CBIR, semantic features, EffNet-CNN, image retrieval},
abstract = {Content-Based Image Retrieval (CBIR) and image mining are becoming more important study fields in computer vision due to their wide range of applications in healthcare, security, and various domains. The image retrieval system mainly relies on the efficiency and accuracy of the classification models. This research addresses the challenge of enhancing the image retrieval system by developing a novel approach, EfficientNet-Convolutional Neural Network (EffNet-CNN). The key objective of this research is to evaluate the proposed EffNet-CNN model’s performance in image classification, image mining, and CBIR. The novelty of the proposed EffNet-CNN model includes the integration of different techniques and modifications. The model includes the Mahalanobis distance metric for feature matching, which enhances the similarity measurements. The model extends EfficientNet architecture by incorporating additional convolutional layers, batch normalization, dropout, and pooling layers for improved hierarchical feature extraction. A systematic hyperparameter optimization using SGD, performance evaluation with three datasets, and data normalization for improving feature representations. The EffNet-CNN is assessed utilizing precision, accuracy, F-measure, and recall metrics across MS-COCO, CIFAR-10 and 100 datasets. The model achieved accuracy values ranging from 90.60% to 95.90% for the MS-COCO dataset, 96.8% to 98.3% for the CIFAR-10 dataset and 92.9% to 98.6% for the CIFAR-100 dataset. A validation of the EffNet-CNN model’s results with other models reveals the proposed model’s superior performance. The results highlight the potential of the EffNet-CNN model proposed for image classification and its usefulness in image mining and CBIR.}
}
@incollection{2025718,
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {718-800},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.09001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027090011}
}
@article{XU2025,
title = {Model-Driven Integration of Deep Learning for Artifact Classification in Museum Information Systems},
journal = {International Journal of Information Technology and Web Engineering},
volume = {20},
number = {1},
year = {2025},
issn = {1554-1045},
doi = {https://doi.org/10.4018/IJITWE.387650},
url = {https://www.sciencedirect.com/science/article/pii/S1554104525000038},
author = {Ke Xu and Qiong Wu and Yujiao Hou},
keywords = {Museum Information Systems, Deep Learning, Convolutional Neural Networks, Model-Driven Architecture, Artifact Classification, Cultural Heritage, Image Retrieval, Semantic},
abstract = {ABSTRACT
Museum Information Systems (MIS) often rely on manual classification and keyword search, limiting accuracy and scalability. Deep learning offers a solution, but effective integration requires alignment with curatorial workflows. This study proposes a model-driven framework for integrating Convolutional Neural Networks (CNNs) into MIS to enhance artifact classification and retrieval. A prototype was built using ReactJS, Django, and TensorFlow, and it was trained on a curated subset of The Met’s Open Access Images. The system employs a Hybrid-E Loss for improved classification accuracy. The model achieved 94.3% classification accuracy and real-time retrieval latency below 100 ms, with throughput exceeding 14 queries per second. The framework successfully bridges AI performance with curatorial logic, demonstrating a scalable and interpretable solution for digital heritage systems.}
}
@incollection{GEETHA2025287,
title = {15 - Emerging graphical data management methodologies for automated driving},
editor = {Rajesh Kumar Dhanaraj and M. Nalini and Malathy Sathyamoorthy and Manar Mohaisen},
booktitle = {Knowledge Graph-Based Methods for Automated Driving},
publisher = {Elsevier},
pages = {287-309},
year = {2025},
isbn = {978-0-443-30040-0},
doi = {https://doi.org/10.1016/B978-0-443-30040-0.00015-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443300400000151},
author = {Anbazhagan Geetha and S. Usha and A. Prasanth and Ahmed Elngar},
keywords = {Emerging graphical data, Automated driving systems, Machine learning models, Optimization techniques, Management approaches, Sensor fusion, Automated driving, Deep learning, Knowledge graph},
abstract = {This chapter examines emerging techniques of graphical data management approaches in enhancing the functionalities and ensuring the safety of automated driving systems. It places emphasis on the efficient arrangement, examination, and depiction of sensor data, with particular attention given to their influence on crucial elements such as sensor fusion, mapping, localization, simulation, machine learning model training, human-machine interface design, and system debugging. By conducting a thorough investigation into these areas, the study highlights the essential role of graphical data management in the developmental stages of autonomous cars. The results emphasize the ways in which these methodologies play a role in the development of strong mapping and localization algorithms, the building of realistic simulation environments for testing purposes, and the enhancement of machine learning models through optimization techniques.}
}
@article{COOK2022100117,
title = {Characterizing the extracellular matrix transcriptome of cervical, endometrial, and uterine cancers},
journal = {Matrix Biology Plus},
volume = {15},
pages = {100117},
year = {2022},
issn = {2590-0285},
doi = {https://doi.org/10.1016/j.mbplus.2022.100117},
url = {https://www.sciencedirect.com/science/article/pii/S2590028522000175},
author = {Carson J. Cook and Andrew E. Miller and Thomas H. Barker and Yanming Di and Kaitlin C. Fogg},
keywords = {RNA-seq, Gynecological cancer, TCGA, Matrisome, Extracellular matrix},
abstract = {Increasingly, the matrisome, a set of proteins that form the core of the extracellular matrix (ECM) or are closely associated with it, has been demonstrated to play a key role in tumor progression. However, in the context of gynecological cancers, the matrisome has not been well characterized. A holistic, yet targeted, exploration of the tumor microenvironment is critical for better understanding the progression of gynecological cancers, identifying key biomarkers for cancer progression, establishing the role of gene expression in patient survival, and for assisting in the development of new targeted therapies. In this work, we explored the matrisome gene expression profiles of cervical squamous cell carcinoma and endocervical adenocarcinoma (CESC), uterine corpus endometrial carcinoma (UCEC), and uterine carcinosarcoma (UCS) using publicly available RNA-seq data from The Cancer Genome Atlas (TCGA) and The Genotype-Tissue Expression (GTEx) portal. We hypothesized that the matrisomal expression patterns of CESC, UCEC, and UCS would be highly distinct with respect to genes which are differentially expressed and hold inferential significance with respect to tumor progression, patient survival, or both. Through a combination of statistical and machine learning analysis techniques, we identified sets of genes and gene networks which characterized each of the gynecological cancer cohorts. Our findings demonstrate that the matrisome is critical for characterizing gynecological cancers and transcriptomic mechanisms of cancer progression and outcome. Furthermore, while the goal of pan-cancer transcriptional analyses is often to highlight the shared attributes of these cancer types, we demonstrate that they are highly distinct diseases which require separate analysis, modeling, and treatment approaches. In future studies, matrisome genes and gene ontology terms that were identified as holding inferential significance for cancer stage and patient survival can be evaluated as potential drug targets and incorporated into in vitro models of disease.}
}
@article{IBTIHEL2018335,
title = {A Semantic Approach for Tweet Categorization},
journal = {Procedia Computer Science},
volume = {126},
pages = {335-344},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.07.267},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918312432},
author = {Ben Ltaifa Ibtihel and Hlaoua Lobna and Ben Jemaa Maher},
keywords = {Tweet Categorization (TC), semantic approach, deep enrichment, Knowledge Bases (KBs), eXtended WordNet Domain},
abstract = {The explosion of social media and microblogging services has gradually increased the microblogging data and particularly tweets data. In microblogging services such as Twitter, the users may become overwhelmed by the rise of data. Although, Twitter allows people to micro-blog about a broad range of topics in real time, it is often hard to understand what these tweets are about. In this work, we study the problem of Tweet Categorization (TC), which aims to automatically classify tweets based on their topic. The accurate TC, however, is a challenging task within the 140-character limit imposed by Twitter. The majority of TC approaches use lexical features such as Bag of Words (BoW) and Bag of Entities (BoE) extracted from a Tweet content. In this paper, we propose a semantic approach of improving the accuracy of TC based on feature expansion from external Knowledge Bases (KBs) and the use of eXtended WordNet Domain as a classifier. In particular, we propose a deep enrichment strategy to extend tweets with additional features by exploiting the concepts present in the semantic graph structures of the KBs. Then, our supervised categorization relies only on the ontological knowledge and classifier training is not required. Empirical results indicate that this enriched representation of text items can substantially improve the TC performance.}
}
@article{SOININEN2025111575,
title = {What is a data space—Logical architecture model},
journal = {Data in Brief},
volume = {60},
pages = {111575},
year = {2025},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2025.111575},
url = {https://www.sciencedirect.com/science/article/pii/S2352340925003075},
author = {Juha-Pekka Soininen and Gabriella Laatikainen},
keywords = {Data sharing, Logical model, Use case model, System architecture},
abstract = {The paper presents a use case model and a logical architecture model of a data space system. The models view the data space system from the user and operator perspectives and describe the needed functionalities and their connection on an abstract level. The core features in our data space model are collaboration networks and contract-based data sharing. The models are meant as a simple and explainable first introduction to what a data space system is, and they provide an implementation technology-independent basis for creating data space implementation specifications. The model is validated by comparing it with existing data space reference models from IDSA, GAIA-X, and the Data Space Support Centre. The modelled core features map with the technical specifications of the system. Our study enables data space system developers to think out of the box and create new innovative solutions.}
}
@article{SCHODERER2022105888,
title = {Contested water- and miningscapes – Explaining the high intensity of water and mining conflicts in a meta-study},
journal = {World Development},
volume = {154},
pages = {105888},
year = {2022},
issn = {0305-750X},
doi = {https://doi.org/10.1016/j.worlddev.2022.105888},
url = {https://www.sciencedirect.com/science/article/pii/S0305750X2200078X},
author = {Mirja Schoderer and Marlen Ott},
keywords = {Water, Mining, Extractive industries, Conflicts, Resistance, Contestation, Qualitative comparative analysis, QCA, Literature review, Social movements, Meta-study},
abstract = {Conflicts around access to, control over, and quality of water accompany mining projects all over the globe. Often, they are associated with high intensity as means of contestation range from verbal complaints to protest marches, civil disobedience and violent confrontations. While numerous case studies on water-related mining conflicts exist, scholarship that synthesizes insights remains rare. In order to better understand the dynamics that lead to the escalation of conflicts and to further theory development on the role of, e.g., political economic contexts, hydro-social conditions and social relationships, a systematic overview of the existing empirical evidence is needed. Our meta-study of 53 water and mining conflicts identifies several combinations of conditions that are tied to large-scale mobilization and the use of civil disobedience measures, sabotage or hunger strikes by environmental defenders. As our results show, raised stakes and ontological differences, e.g. in situations where water is essential for livelihoods and cultural and spiritual practices, play a role, in particular when coupled with a lack of meaningful participation. Discursive or physical coercion by the state or by private security forces also intensify mobilization rather than containing it while the role of international NGOs is more ambiguous. To identify explanatory scenarios, we conducted a two-step, fuzzy-set qualitative comparative analysis (fsQCA) based on data collected in a systematic literature review of peer-reviewed articles and book chapters. Taking its data from published research, our study identifies a geographic bias towards Latin America in academic literature on water and mining conflicts and points out topical blind spots. By looking for conditions that are consistently associated with high-intensity conflicts, it also provides insights on priority areas of engagement for community leaders, policy-makers, and private sector and civil society representatives seeking to avoid the escalation of conflicts.}
}
@article{UPADHYAY2024100088,
title = {A comprehensive survey on answer generation methods using NLP},
journal = {Natural Language Processing Journal},
volume = {8},
pages = {100088},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100088},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000360},
author = {Prashant Upadhyay and Rishabh Agarwal and Sumeet Dhiman and Abhinav Sarkar and Saumya Chaturvedi},
keywords = {Question-answering systems, Natural language processing, Question analysis, Answer extraction, Information retrieval},
abstract = {Recent advancements in question-answering systems have significantly enhanced the capability of computers to understand and respond to queries in natural language. This paper presents a comprehensive review of the evolution of question answering systems, with a focus on the developments over the last few years. We examine the foundational aspects of a question answering framework, including question analysis, answer extraction, and passage retrieval. Additionally, we delve into the challenges that question answering systems encounter, such as the intricacies of question processing, the necessity of contextual data sources, and the complexities involved in real-time question answering. Our study categorizes existing question answering systems based on the types of questions they address, the nature of the answers they produce, and the various approaches employed to generate these answers. We also explore the distinctions between opinion-based, extraction-based, retrieval-based, and generative answer generation. The classification provides insight into the strengths and limitations of each method, paving the way for future innovations in the field. This review aims to offer a clear understanding of the current state of question answering systems and to identify the scaling needed to meet the rising expectations and demands of users for coherent and accurate automated responses in natural language.}
}
@article{SUN2024145,
title = {Bipolar disorder: Construction and analysis of a joint diagnostic model using random forest and feedforward neural networks},
journal = {IBRO Neuroscience Reports},
volume = {17},
pages = {145-153},
year = {2024},
issn = {2667-2421},
doi = {https://doi.org/10.1016/j.ibneur.2024.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S2667242124000691},
author = {Ping Sun and Xiangwen Wang and Shenghai Wang and Xueyu Jia and Shunkang Feng and Jun Chen and Yiru Fang},
keywords = {Bipolar disorder, Machine learning, Neural networks, Diagnostic models},
abstract = {Background
To construct a diagnostic model for Bipolar Disorder (BD) depressive phase using peripheral tissue RNA data from patients and combining Random Forest with Feedforward Neural Network methods.
Methods
Datasets GSE23848, GSE39653, and GSE69486 were selected, and differential gene expression analysis was conducted using the limma package in R. Key genes from the differentially expressed genes were identified using the Random Forest method. These key genes' expression levels in each sample were used to train a Feedforward Neural Network model. Techniques like L1 regularization, early stopping, and dropout layers were employed to prevent model overfitting. Model performance was then validated, followed by GO, KEGG, and protein-protein interaction network analyses.
Results
The final model was a Feedforward Neural Network with two hidden layers and two dropout layers, comprising 2345 trainable parameters. Model performance on the validation set, assessed through 1000 bootstrap resampling iterations, demonstrated a specificity of 0.769 (95 % CI 0.571–1.000), sensitivity of 0.818 (95 % CI 0.533–1.000), AUC value of 0.832 (95 % CI 0.642–0.979), and accuracy of 0.792 (95 % CI 0.625–0.958). Enrichment analysis of key genes indicated no significant enrichment in any known pathways.
Conclusion
Key genes with biological significance were identified based on the decrease in Gini coefficient within the Random Forest model. The combined use of Random Forest and Feedforward Neural Network to establish a diagnostic model showed good classification performance in Bipolar Disorder.}
}
@article{DONOVAN2021102347,
title = {Colonising geology: Volcanic politics and geopower},
journal = {Political Geography},
volume = {86},
pages = {102347},
year = {2021},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2021.102347},
url = {https://www.sciencedirect.com/science/article/pii/S096262982100007X},
author = {Amy Donovan},
keywords = {Disaster risk, Volcanic risk, Science and politics, Political geology},
abstract = {This paper discusses the experiences of Christian Montserratian residents through the 1995-present eruptions of the Soufrière Hills Volcano, highlighting that while the earthly powers of the volcano are fundamentally nonhuman, they are known and understood in a diversity of ways by different actors – and they interfere in politics and the production of knowledge. Montserrat, as a UK Overseas Territory, is a non-sovereign territory with a strong Christian identity that has been enhanced by its geological experience. At the same time, the UK has used the eruption to hold greater legislative control over the island. A final group of stakeholders in the eruption is the volcanologists, mainly from the UK until 2010, whose terminology and knowledge-world has had significant impact on the island, but has also been significantly extended by it. The paper ultimately uses this example to suggest that while there are useful and important insights from the “ontological turn” (incorporating vitalism, materialism, speculative realism) in thinking about the “lively earth”, the enactment of volcanic risk management at the governmental level also constitutes a knowledge-practice that must be approached critically to avoid the pitfalls of modernist science and ensure meaningful political change.}
}
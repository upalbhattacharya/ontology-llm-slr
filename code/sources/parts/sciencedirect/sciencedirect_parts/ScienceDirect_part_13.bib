@article{PEASE2020848,
title = {An interoperable semantic service toolset with domain ontology for automated decision support in the end-of-life domain},
journal = {Future Generation Computer Systems},
volume = {112},
pages = {848-858},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19326640},
author = {Sarogini Grace Pease and Richard Sharpe and Kate {van Lopik} and Eleni Tsalapati and Paul Goodall and Bob Young and Paul Conway and Andrew West},
abstract = {In product-diverse, end-of-life (EoL) production lines the relevant markets, competitors and customer bases continuously change as new products are processed. The resale market itself changes with the influx of new products, as well as hardware and software discontinuations. Competitive business decision making is often performed by a human operator and may not be timely or fully informed. These are decisions such as whether to perform a high cost repair or recycle a product or whether to use a batch of parts in repair or sell them on. These decisions can be used to optimise product life-cycle management (PLM) and profit margins. A real-time decision making capability can reduce the risk of performing non-profitable processing. The novel contribution of this work is an interoperable semantic decision support toolset that enables a capability for timely EoL decisions based on complete knowledge on profitability, predicted pricing and cost-of-production. Many decision support systems have been proposed for the EoL domain, but a lack of interoperability and use of unstructured knowledge bases has led to decisions based on knowledge that is not up to date. Using formalised, semantic technologies offers sustainable decision making in this volatile and increasingly competitive domain.}
}
@article{ARENA2018219,
title = {The Training Data Evaluation Tool: Towards a unified ontology-based solution for industrial training evaluation},
journal = {Procedia Manufacturing},
volume = {23},
pages = {219-224},
year = {2018},
note = {“Advanced Engineering Education & Training for Manufacturing Innovation”8th CIRP Sponsored Conference on Learning Factories (CLF 2018)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S235197891830492X},
author = {Damiano Arena and Stefano Perini and Marco Taisch and Dimitris Kiritsis},
keywords = {Industry 4.0, Ontology, Training, Skill, Evaluation},
abstract = {With the increasing diffusion of the Industry 4.0 paradigm, the role of industrial training is becoming more and more relevant. Thanks to new technologies and innovative business processes now available, a great amount of data can be collected from the shop floor and elaborated in order to provide a real-time evaluation of the expertise level of the workers. In this regard, the paper aims at presenting and expanding the Training Data Evaluation Tool previously developed by the authors. Results achieved pave the way towards the final deployment of the Training Evaluation suite of Ontologies through an integrated software stack using semantic technologies for knowledge management along with a Training Analytics Model. Integration tests of the used technologies have been carried out using real industrial data showing the feasibility and robustness of the proposed solution.}
}
@incollection{GAUDET20191,
title = {The Gene Ontology},
editor = {Shoba Ranganathan and Michael Gribskov and Kenta Nakai and Christian Schönbach},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology},
publisher = {Academic Press},
address = {Oxford},
pages = {1-7},
year = {2019},
isbn = {978-0-12-811432-2},
doi = {https://doi.org/10.1016/B978-0-12-809633-8.20500-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128096338205001},
author = {Pascale Gaudet},
keywords = {Annotation evidence, Annotations, Evidence codes, Function, Gene association file (GAF), Gene ontology structure, GO files, Vocabulary},
abstract = {The Gene Ontology (GO) project provides a structured, controlled terminology’ of ‘terms’ or ‘classes’ describing the functions of gene products, as well as the association of these terms with the gene products performing these functions. It is specifically designed for supporting the computational representation of biological systems. The combination of solid conceptual underpinnings and tools for its use have made the GO a widely popular resource in the biological and bioinformatics research community and an essential resource for computational data analysis. This article describes the structure of the Gene Ontology, the semantic meaning of annotations, and an overview of typical uses of the Gene Ontology.}
}
@article{HOWEY2021101272,
title = {Landscape bundling of ceremonial Earthworks: Incorporating ethnohistoric and contemporary Indigenous ontologies to revive Great Lakes archaeological legacy datasets},
journal = {Journal of Anthropological Archaeology},
volume = {62},
pages = {101272},
year = {2021},
issn = {0278-4165},
doi = {https://doi.org/10.1016/j.jaa.2021.101272},
url = {https://www.sciencedirect.com/science/article/pii/S0278416521000052},
author = {Meghan C.L. Howey and Marieka {Brouwer Burg}},
keywords = {Bundling, Indigenous ontologies, Legacy data, Mounds, Earthworks, Landscape, GIS, Great lakes},
abstract = {The thousands of ancestral Indigenous mounds and earthworks of Eastern North America have long been a source of intrigue for diverse audiences. And although explanations have come a long way from the days of the Mound Builder Myth, there are avenues of inquiry that remain under-investigated. Here, we seek to let Indigenous ontologies lead the way by employing an interpretive lens based on bundling, a significant practice across many North American Indigenous peoples. We expand the notion of bundling to the landscape scale and, drawing on ethnohistoric and contemporary understandings of Anishinaabeg and Ho-Chunk ontologies, suggest that Great Lakes earthworks were brought into being for/through the bundling together of relationships between humans, other-than-human persons, and the land. This lens sets the scene for novel geospatial and statistical analyses of a legacy archaeological dataset of Late Precontact (ca. CE 1200–1600) earthworks in the Great Lakes, most of which have been destroyed. This study reinvigorates a fragmentary legacy data set, a practice that – during a time of pandemic-related restrictions on travel and field work – should become more prominent in archaeological investigations. This study illustrates the value of blending quantitative approaches and Indigenous ontologies to studies of landscape-scale processes and meanings.}
}
@article{NAZ2020106695,
title = {Ontology-driven advanced drug-drug interaction},
journal = {Computers & Electrical Engineering},
volume = {86},
pages = {106695},
year = {2020},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2020.106695},
url = {https://www.sciencedirect.com/science/article/pii/S0045790620305504},
author = {Tabbasum Naz and Muhammad Akhtar and Syed Khuram Shahzad and Maria Fasli and Muhammad Waseem Iqbal and Muhammad Raza Naqvi},
keywords = {Drug-drug interaction, Pharmacy semantics, Drug ontologies, Pharmaceutical informatics},
abstract = {The rapid growth of data in the pharmaceutical area has created new challenges for large-scale data mining like Drug-Drug Interaction (DDI) analysis. To meet these challenges, various types of data related to DDI must be integrated with true semantics. However, the existing tools do not provide automated DDI analysis. Interaction details are not machine readable and pharmacists need to do further processing for its extraction. This research paper proposed an ontology-driven Advanced Drug-Drug Interaction (ADDI) system to assists the physicians and pharmacists to identify the DDI effects. ADDI provides ontological definitions and semantic relations among diseases, drugs, ingredients, action mechanism, physiologic effect, dosage formation, administration methods, DDI mechanism, DDI types (Antagonism, Synergism, Potentiation, and Interaction with metabolism), DDI reactions, their frequency and duration. It can be used as Semantic Information Layer (SIL) to resolve the heterogeneity problem and can play a significant role to remove the barriers for semantic interoperability.}
}
@article{SAYAH2020169,
title = {An intelligent system for energy management in smart cities based on big data and ontology},
journal = {Smart and Sustainable Built Environment},
volume = {10},
number = {2},
pages = {169-192},
year = {2020},
issn = {2046-6099},
doi = {https://doi.org/10.1108/SASBE-07-2019-0087},
url = {https://www.sciencedirect.com/science/article/pii/S2046609920000319},
author = {Zaoui Sayah and Okba Kazar and Brahim Lejdel and Abdelkader Laouid and Ahmed Ghenabzia},
keywords = {Big data, Energy saving, Multi-agent system, Ontology, Semantics integration, Smart cities},
abstract = {Purpose
This research paper aims at proposing a framework based on semantic integration in Big Data for saving energy in smart cities. The presented approach highlights the potential opportunities offered by Big Data and ontologies to reduce energy consumption in smart cities.
Design/methodology/approach
This study provides an overview of semantics in Big Data and reviews various works that investigate energy saving in smart homes and cities. To reach this end, we propose an efficient architecture based on the cooperation between ontology, Big Data, and Multi-Agent Systems. Furthermore, the proposed approach shows the strength of these technologies to reduce energy consumption in smart cities.
Findings
Through this research, we seek to clarify and explain both the role of Multi-Agent System and ontology paradigms to improve systems interoperability. Indeed, it is useful to develop the proposed architecture based on Big Data. This study highlights the opportunities offered when they are combined together to provide a reliable system for saving energy in smart cities.
Practical implications
The significant advancement of contemporary applications (smart cities, social networks, health care, IoT, etc.) requires a vast emergence of Big Data and semantics technologies in these fields. The obtained results provide an improved vision of energy-saving and environmental protection while keeping the inhabitants’ comfort.
Originality/value
This work is an efficient contribution that provides more comprehensive solutions to ontology integration in the Big Data environment. We have used all available data to reduce energy consumption, promote the change of inhabitant’s behavior, offer the required comfort, and implement an effective long-term energy policy in a smart and sustainable environment.}
}
@article{SILVA201928,
title = {Visualization and analysis of schema and instances of ontologies for improving user tasks and knowledge discovery},
journal = {Journal of Computer Languages},
volume = {51},
pages = {28-47},
year = {2019},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2019.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X17302458},
author = {Isabel Cristina Siqueira Silva and Giuseppe Santucci and Carla Maria Dal Sasso Freitas},
keywords = {Ontology, Information visualization, Data visualization, Visual analytics, Interaction},
abstract = {Ontologies are an important resource for knowledge representation. Their structure can be complex due to role relations between several concepts, distinct attributes, and different instances. In this paper, we discuss a Visual Analytics solution, relying on the use of multiple coordinated views for exploring different ontology aspects and a novel use of the degree of interest (DoI) suppression technique to reduce the complexity of the ontology visual representation. Visual Analytics facilitates the understanding of the domain and tasks represented by ontologies, thus allowing to carry out exploratory analysis to optimize the comprehension of data semantics including non-explicit relationships between data. Through the DoI technique, we place the main concept in focus, distinguishing it from the unnecessary information and facilitating the analysis and understanding of correlated data. We evaluated all the devised solutions, and the results reinforce the importance of providing visualization and analysis techniques dedicated to the schema and instances levels of ontologies for the discovery of non-explicit information.}
}
@article{PRATAP2025100144,
title = {The fine art of fine-tuning: A structured review of advanced LLM fine-tuning techniques},
journal = {Natural Language Processing Journal},
volume = {11},
pages = {100144},
year = {2025},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2025.100144},
url = {https://www.sciencedirect.com/science/article/pii/S2949719125000202},
author = {Samar Pratap and Alston Richard Aranha and Divyanshu Kumar and Gautam Malhotra and Anantharaman Palacode Narayana Iyer and Shylaja S.S.},
keywords = {Adapter, FFT, LLM, LoRA, MoE, PEFT, Quantization},
abstract = {Transformer-based models have consistently demonstrated superior accuracy compared to various traditional models across a range of downstream tasks. However, due to their large nature, training or fine-tuning them for specific tasks has heavy computational and memory demands. This causes the creation of specialized transformer-based models to be almost impossible in the generally present constrained scenarios. To tackle this issue and to make these large models more accessible, a plethora of techniques have been developed. In this study, we will be reviewing the types of techniques developed, their impacts and benefits concerning performance and resource usage along with the latest developments in the domain. We have broadly categorized these techniques into six key areas: Changes in Training Method, Changes in Adapter, Quantization, Parameter Selection, Mixture of Experts, and Application based methods. We collated the results of various techniques on common benchmarks and also evaluated their performance on different datasets and base models.}
}
@article{DING2019101574,
title = {Ontology-based knowledge representation for malware individuals and families},
journal = {Computers & Security},
volume = {87},
pages = {101574},
year = {2019},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2019.101574},
url = {https://www.sciencedirect.com/science/article/pii/S0167404819301373},
author = {Yuxin Ding and Rui Wu and Xiao Zhang},
keywords = {Ontology, Malware, Dynamic behavior, Malware detection, Knowledge base},
abstract = {Malware consists of a large numbers of malware families and individuals, and each individual has complex behaviors. So knowledge base is urgently needed to process and store such a huge amount of information. In present the traditional signature-based database cannot represent the behavioral semantics of malicious code. Therefore, people cannot know what malware will do on a computer system. To solve this issue, we apply ontology technique into the malware domain, and propose the method for constructing malware knowledge base. We design the concept classes and object properties of malware, and propose the method for representing semantics of malware behavior. The data mining method, Apriori algorithm, is applied to extract the common behaviors of individuals belonging to the same family, and common behaviors are used to represent the knowledge of a malware family. The experimental results show that the data mining method can discover the common behaviors of the malware family, and the common behaviors mined can effectively classify the malware families.}
}
@article{EMMANOUILIDIS202010923,
title = {Context Ontology Development for Connected Maintenance Services},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {10923-10928},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2833},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320335989},
author = {C Emmanouilidis and M. Gregori and A. Al-Shdifat},
keywords = {Internet of Things, Context Management, Ontology, Interoperability, Maintenance},
abstract = {The opportunity to shift from corrective and preventive to data-driven Predictive Maintenance has received a significant boost with the deeper penetration of Internet of Things (IoT) technologies in industrial environments. Processing IoT generated data nonetheless creates challenges for data management and actionable data processing. One way to handle such complexity is to introduce context information modelling and management, wherein data and service delivery are determined upon resolving the apparent context of a service or data request. In this paper, context information management is considered on the basis of a valid knowledge construct for reliability-oriented maintenance management. The aim is to produce a viable semantic organization of data for maintenance services. It is applied on an industrial case linked to maintenance of a distributed fleet of connected production grade industrial printers. The complexity of translating the data generated by such production assets to actionable information is significant, as the status of a single asset is characterised by several hundreds of failure modes and a multitude of event codes. To assess the viability of the ontology for the targeted application, a qualitative usability evaluation study of the ontology is performed.}
}
@article{RUAN2025103538,
title = {Fine-tuning large language models with contrastive margin ranking loss for selective entity matching in product data integration},
journal = {Advanced Engineering Informatics},
volume = {67},
pages = {103538},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103538},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625004318},
author = {Qian Ruan and Dachuan Shi and Thomas Bauernhansl},
keywords = {Entity matching, Entity Resolution, Large language model, Contrastive learning, Data integration},
abstract = {Entity Matching (EM) concerns identifying entities from different data sources that correspond to the same real-world object. It is widely used for product data integration in e-commerce, product classification, and inventory management, enabling the matching of duplicate product records with heterogeneous descriptions across various platforms and software systems. The standard EM solution consists of two steps: a blocking step to retrieve a subset of candidates and a pairwise matching step to classify whether the query entity matches each candidate. However, a significant challenge arises when pairwise matching fails to account for similar distractors within the candidate subset, often leading to false positive matches. This issue has been largely overlooked in prior work and existing benchmark datasets. In this study, we address this gap through three key aspects. First, we revisit the standard pairwise EM setting by recompiling existing benchmark datasets to include more hard negative (HN) candidates, which are semantically similar to corresponding query entities. We then evaluate state-of-the-art (SOTA) pairwise matchers on these recompiled datasets, revealing the limitations of the conventional pairwise EM approach under more challenging and realistic conditions. Second, we propose a selective EM approach that formulates EM as a listwise selection task, where the query entity is compared directly with the entire candidate set rather than evaluated through independent pairwise classifications. Accordingly, a new evaluation framework is introduced, including recompiled benchmark datasets and a new evaluation metric. Third, we propose a selective EM method Mistral4SelectEM, which fine-tunes a large language model for selective EM by structuring it into a Siamese network and fine-tuning it with a novel contrastive margin ranking loss (CMRL). It aims to enhance the model’s ability to distinguish true positives from semantically similar HNs. Extensive experiments demonstrate that our method outperforms SOTA pairwise EM approaches in both efficiency and performance across multiple benchmark datasets. The code and the recompiled entity matching benchmark datasets are publicly available at: https://github.com/quickhdsdc/LLM4EntityMatching.}
}
@article{KRAVCHENKO2024101659,
title = {Language awareness: On the semiotics of talk and text},
journal = {Language Sciences},
volume = {105},
pages = {101659},
year = {2024},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2024.101659},
url = {https://www.sciencedirect.com/science/article/pii/S0388000124000482},
author = {Alexander V. Kravchenko},
keywords = {Natural language, Linguistic sign, Frege, Semiotic triangle, Two orders of semiotics, Human evolution},
abstract = {The article argues for a necessity to increase human awareness of language as functional biological behavior rather than simply a tool in the service of communication, by emphasizing the difference between talk and text as ontologically different semiotic phenomena characteristic of the human cognitive domain. The established tradition to view written words as linguistic signs leads the studies of natural language astray, effectively hiding its nature as biologically functional orientational behavior in a consensual domain that evolved with the evolution of our species and was not a cultural invention. Because of the identification of text with talk in linguistic semiotics, the empirical validity of the core semiotic concept of natural linguistic sign, based on the so-called semiotic triangle, is undermined. While talk is a dynamic fact of nature, text is a static artifact; it is argued, therefore, that the analytical approach to linguistic signs as objects in the world is inadequate, and the notions of first- and second-order semiotics are introduced. It is concluded that awareness of the cognitive-semiotic difference between talk and text and their respective roles in the evolution of humans may facilitate further research into the nature and origin of humanness.}
}
@article{WANG2018163,
title = {Systematic design space exploration using a template-based ontological method},
journal = {Advanced Engineering Informatics},
volume = {36},
pages = {163-177},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S1474034617304494},
author = {Ru Wang and Anand Balu Nellippallil and Guoxin Wang and Yan Yan and Janet K. Allen and Farrokh Mistree},
keywords = {Ontology, Decision-based design, Design space exploration, Process template, Reuse},
abstract = {The realization of complex engineered systems using models that are typically incomplete, inaccurate and not of equal fidelity requires the understanding and prediction of process behavior in design. This necessitates the need for extending designer’s abilities in making design decisions that are robust, flexible and modifiable particularly in the early stages of design. To address this requirement, we propose in this paper, an ontology for design space exploration and a template-based ontological method that supports systematic design space exploration ensuring the determination of the right combination of design information that meets the different goals and requirements set for a process chain. Using the proposed method, a designer is able to (1) systematically adjust the design space in due time to manage the risks of errors accumulating and propagating during the design of different stages of the process chain, (2) improve the ability to communicate and understand the interactions between design information in the process chain. We achieve the said through (1) procedure for design space exploration is identified to determine the sequence of activities needed for the systematic exploration of design space under uncertainty; (2) the decision-based design information flow is archived using the design space exploration process template and represented by utilizing frame-based ontology to facilitate the management of re-usable information. We demonstrate the efficacy of this template-based ontological method for design space exploration by carrying out the design of a multi-stage hot rod rolling system in steel manufacturing process chain.}
}
@article{GOETZ2020194,
title = {Ontology-based representation of tolerancing and design knowledge for an automated tolerance specification of product concepts},
journal = {Procedia CIRP},
volume = {92},
pages = {194-199},
year = {2020},
note = {16th CIRP Conference on Computer Aided Tolerancing (CIRP CAT 2020)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.03.128},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120309434},
author = {Stefan Goetz and Benjamin Schleich},
keywords = {Robust Design, Tolerance Specification, Ontology},
abstract = {Motivated by the high importance of the tolerance specification, numerous partially automated approaches have emerged. However, since these approaches require an extensive geometry definition, they are not applicable for an early definition of the tolerance concept and its evaluation by Robust Design aspects. Motivated by this lack, this paper presents a novel ontology-based approach combining knowledge from the product design and tolerancing domains to enable an automated tolerance specification of product concepts. This close linkage enables a more precise specification and bridges the gap to the subsequent final specification in accordance with current GPS standards. Finally, this novel approach is demonstrated by means of a crank drive mechanism.}
}
@article{TAVAKOLAN202075,
title = {Construction and resource short-term planning using a BIM-based ontological decision support system},
journal = {Canadian Journal of Civil Engineering},
volume = {48},
number = {1},
pages = {75-88},
year = {2020},
issn = {0315-1468},
doi = {https://doi.org/10.1139/cjce-2019-0439},
url = {https://www.sciencedirect.com/science/article/pii/S0315146820001364},
author = {Mehdi Tavakolan and Sina Mohammadi and Banafsheh Zahraie},
keywords = {collaborative construction planning, resource planning, ontology, semantic inference, building information modelling, decision support system, planification collaborative de la construction, planification des ressources, ontologie, inférence sémantique, modélisation des données du bâtiment, système d’aide à la décision},
abstract = {The dynamic nature and increasing complexity of the construction projects impose many challenges for project planning and control. As a project progresses, more information becomes available and the level of uncertainty decreases. It can be used to proactively check the validity of the previous decisions and develop revised and more detailed plans for the upcoming activities in construction planning meetings. For this purpose, this study implements ontological knowledge representation and semantic reasoning techniques to propose an intelligent information collection and decision support system framework for short-term collaborative construction and resource planning. Moreover, a new approach is suggested that allows for incorporating resource specifications and limitations, and complex multi-factor constraints in the ontological planning process. The framework was tested based on a real-world construction project and different application cases were discussed. The framework showed a promising performance for analyzing different scenarios and help the planners making informative decisions.
La nature dynamique et la complexité croissante des projets de construction posent de nombreux défis au niveau de la planification et du contrôle des projets. À mesure qu’un projet progresse, de plus amples données deviennent disponibles et le niveau d’incertitude diminue. Elles peuvent servir à vérifier de façon proactive la validité des décisions antérieures et à élaborer des plans révisés et plus détaillés pour les activités à venir dans les réunions de planification de la construction. À cette fin, cette étude met en œuvre des techniques de représentation des connaissances ontologiques et de raisonnement sémantique afin de proposer un cadre intelligent de collecte de données et de soutien à la décision pour la construction collaborative à court terme et la planification des ressources. De plus, on propose une nouvelle approche qui permet d’intégrer les spécifications et les limites des ressources, ainsi que les contraintes complexes multifactorielles dans le processus de planification ontologique. Le cadre a été mis à l’essai sur la base d’un projet de construction réel et différents cas d’application ont été discutés. Le cadre a montré une performance prometteuse pour analyser différents scénarios et aider les planificateurs à prendre des décisions en connaissance de cause. [Traduit par la Rédaction]}
}
@article{BUCHMANN2024102324,
title = {Large language models: Expectations for semantics-driven systems engineering},
journal = {Data & Knowledge Engineering},
volume = {152},
pages = {102324},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102324},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X2400048X},
author = {Robert Buchmann and Johann Eder and Hans-Georg Fill and Ulrich Frank and Dimitris Karagiannis and Emanuele Laurenzi and John Mylopoulos and Dimitris Plexousakis and Maribel Yasmina Santos},
keywords = {Large language models, Systems engineering, Conceptual modeling, Knowledge engineering},
abstract = {The hype of Large Language Models manifests in disruptions, expectations or concerns in scientific communities that have focused for a long time on design-oriented research. The current experiences with Large Language Models and associated products (e.g. ChatGPT) lead to diverse positions regarding the foreseeable evolution of such products from the point of view of scholars who have been working with designed abstractions for most of their careers - typically relying on deterministic design decisions to ensure systems and automation reliability. Such expectations are collected in this paper in relation to a flavor of systems engineering that relies on explicit knowledge structures, introduced here as “semantics-driven systems engineering”. The paper was motivated by the panel discussion that took place at CAiSE 2023 in Zaragoza, Spain, during the workshop on Knowledge Graphs for Semantics-driven Systems Engineering (KG4SDSE). The workshop brought together Conceptual Modeling researchers with an interest in specific applications of Knowledge Graphs and the semantic enrichment benefits they can bring to systems engineering. The panel context and consensus are summarized at the end of the paper, preceded by a proposed research agenda considering the expressed positions.}
}
@incollection{LOURDUSAMY202091,
title = {7 - Computational intelligence using ontology—A case study on the knowledge representation in a clinical decision support system},
editor = {Jitendra Kumar Verma and Sudip Paul and Prashant Johri},
booktitle = {Computational Intelligence and Its Applications in Healthcare},
publisher = {Academic Press},
pages = {91-104},
year = {2020},
isbn = {978-0-12-820604-1},
doi = {https://doi.org/10.1016/B978-0-12-820604-1.00007-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128206041000078},
author = {Ravi Lourdusamy and Xavierlal J. Mattam},
keywords = {Clinical decision support systems, Knowledge representation, Computational semantics, Ontology, Ontological engineering},
abstract = {Computational intelligence has been traditionally associated with neural networks, fuzzy systems, and genetic algorithms. Over the years there have been many developments in computational intelligence. At present, many other fields are part of the study and research in computational intelligence. With advances in cognitive sciences, more techniques of information processing by machines that show characteristics closely associated with human intelligence are being found. Some of these techniques have been studied for a long time, but in recent years there has been some maturity in the understanding and use of these techniques. One such technique is the use of semantics in computational intelligence. There has been a long-drawn-out philosophical debate between lingualism, which claims that there is no human thought without language, and “language of thought” theories, which believe that natural language is inessential to private thought. In an attempt to create intelligent machines, the use of semantics for knowledge representation and knowledge-based creation in a system follows the philosophy of lingualism. Different knowledge representations are used in a knowledge-based clinical decision support system. This chapter makes a study of various knowledge representations. The different theories behind the techniques used in the knowledge representations are discussed. The philosophy of lingualism and the use of semantics in computational intelligence are explained, while a study on semantic knowledge representation in clinical decision support systems is made. The conclusion is the explanation as to how ontological engineering can be used to create computational intelligence.}
}
@article{EISENBERG201846,
title = {Applying novel technologies and methods to inform the ontology of self-regulation},
journal = {Behaviour Research and Therapy},
volume = {101},
pages = {46-57},
year = {2018},
note = {An experimental medicine approach to behavior change: The NIH Science of Behavior Change (SOBC)},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2017.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S0005796717302048},
author = {Ian W. Eisenberg and Patrick G. Bissett and Jessica R. Canning and Jesse Dallery and A. Zeynep Enkavi and Susan Whitfield-Gabrieli and Oscar Gonzalez and Alan I. Green and Mary Ann Greene and Michaela Kiernan and Sunny Jung Kim and Jamie Li and Michael R. Lowe and Gina L. Mazza and Stephen A. Metcalf and Lisa Onken and Sadev S. Parikh and Ellen Peters and Judith J. Prochaska and Emily A. Scherer and Luke E. Stoeckel and Matthew J. Valente and Jialing Wu and Haiyi Xie and David P. MacKinnon and Lisa A. Marsch and Russell A. Poldrack},
keywords = {Self-regulation, Ontology, Neuroimaging, Intervention, Obesity, Smoking},
abstract = {Self-regulation is a broad construct representing the general ability to recruit cognitive, motivational and emotional resources to achieve long-term goals. This construct has been implicated in a host of health-risk behaviors, and is a promising target for fostering beneficial behavior change. Despite its clear importance, the behavioral, psychological and neural components of self-regulation remain poorly understood, which contributes to theoretical inconsistencies and hinders maximally effective intervention development. We outline a research program that seeks to define a neuropsychological ontology of self-regulation, articulating the cognitive components that compose self-regulation, their relationships, and their associated measurements. The ontology will be informed by two large-scale approaches to assessing individual differences: first purely behaviorally using data collected via Amazon's Mechanical Turk, then coupled with neuroimaging data collected from a separate population. To validate the ontology and demonstrate its utility, we will then use it to contextualize health risk behaviors in two exemplar behavioral groups: overweight/obese adults who binge eat and smokers. After identifying ontological targets that precipitate maladaptive behavior, we will craft interventions that engage these targets. If successful, this work will provide a structured, holistic account of self-regulation in the form of an explicit ontology, which will better clarify the pattern of deficits related to maladaptive health behavior, and provide direction for more effective behavior change interventions.}
}
@article{TACYILDIZ2020103554,
title = {A decision support system on the obesity management and consultation during childhood and adolescence using ontology and semantic rules},
journal = {Journal of Biomedical Informatics},
volume = {110},
pages = {103554},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103554},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420301829},
author = {Özgü Taçyıldız and Duygu {Çelik Ertuğrul}},
keywords = {Pediatric obesity, Ontology, Decision support systems, Personalized inference, Semantic rules},
abstract = {Background
Obesity is defined as abnormal or excessive fat accumulation that presents a risk to health according to the World Health Organization (WHO). Pediatric or childhood obesity is the most prevalent nutritional disorder among children and adolescents worldwide. In pediatric or childhood obesity, constant monitoring of the pediatric patients by health experts is required to provide efficient obesity management and treatment. Therefore, the patients are examined on a regular basis, the measurements are compared against predefined percentile values and the development of the pediatric patient is examined.
Results
This study discusses the design, implementation, and potential use of an ontology-based obesity management and consultation system which is a decision support system for health experts during treatments of the children and adolescent patients. The system does not only share instant gathered medical data to health experts but also examines the data as a smart medical assistant. The system includes an ontology-based inference engine module, which is a decision support module, and used to infer certain personalized suggestions for patients. Suggestions in four categories emerged as a result: (1) Development Feedback Suggestions, (2) Calorie Intake Suggestions and Physical Activities, (3) Mom Suggestions, and (4) Obesity Treatment Stage Suggestions. The methodologies applied and main technical contributions are discussed in three aspects: (1) Obesity Tracking Ontology, (2) Semantic Web Rule Knowledge base, and (3) Inference Engine Module. In this study, unlike other similar studies, ontology and rule based smart medical assistant which have different functionalities from adults’ obesity management is considered especially for obesity management of children and adolescents. The system also includes intensive pediatric health care expert involvement. Eighty case studies from real anonymous pediatric patients are analyzed and discussed in this experimental study.
Conclusions
The results retrieved from 80 case studies are promising in demonstrating the applicability, effectiveness and efficiency of the proposed approach. The inference engine module of the proposed system can be integrated semantically into intelligent and distributed decision support systems, and the system ontology can be used as a knowledge base in similar systems.}
}
@article{CIBRIAN2025104350,
title = {An agent-based approach for the automatic generation of valid SysMLv2 Models in industrial contexts},
journal = {Computers in Industry},
volume = {172},
pages = {104350},
year = {2025},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2025.104350},
url = {https://www.sciencedirect.com/science/article/pii/S0166361525001150},
author = {Eduardo Cibrián and Jose Olivert-Iserte and Juan Llorens and Jose María Álvarez-Rodríguez},
keywords = {Model-Based Systems Engineering (MBSE), SysML v2, Large Language Models (LLMs), Automated model generation, Agent-based systems, Retrieval-Augmented Generation (RAG)},
abstract = {Automating the generation of valid SysML v2 models from natural language specifications holds promise for advancing Model-Based Systems Engineering (MBSE) in industrial settings. However, current approaches based solely on Large Language Models (LLMs) often fail to meet the syntactic and semantic rigor required by formal modeling languages. This paper introduces a domain-informed, agent-based framework that combines LLMs with structured retrieval and iterative validation to synthesize correct SysML v2 models. The system integrates Retrieval-Augmented Generation (RAG) using a curated repository of SysML v2 examples and enforces compliance through a validation engine based on the official ANTLR grammar. Experimental results across diverse MBSE scenarios demonstrate that the integration of retrieval and validation mechanisms leads to a substantial improvement in model correctness and semantic alignment, beyond what each component achieves individually. This combined effect enables reliable, closed-loop generation of formal models from natural language, illustrating how domain-specific integration can transform general-purpose LLMs into reliable assistants for engineering design tasks.}
}
@article{SHEN201820,
title = {An ontology-driven clinical decision support system (IDDAP) for infectious disease diagnosis and antibiotic prescription},
journal = {Artificial Intelligence in Medicine},
volume = {86},
pages = {20-32},
year = {2018},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2018.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0933365717302348},
author = {Ying Shen and Kaiqi Yuan and Daoyuan Chen and Joël Colloc and Min Yang and Yaliang Li and Kai Lei},
keywords = {Ontology, Clinical decision support, Decision support system, Diagnosis classification, Antibiotics prescription},
abstract = {Background
The available antibiotic decision-making systems were developed from a physician’s perspective. However, because infectious diseases are common, many patients desire access to knowledge via a search engine. Although the use of antibiotics should, in principle, be subject to a doctor’s advice, many patients take them without authorization, and some people cannot easily or rapidly consult a doctor. In such cases, a reliable antibiotic prescription support system is needed.
Methods and results
This study describes the construction and optimization of the sensitivity and specificity of a decision support system named IDDAP, which is based on ontologies for infectious disease diagnosis and antibiotic therapy. The ontology for this system was constructed by collecting existing ontologies associated with infectious diseases, syndromes, bacteria and drugs into the ontology's hierarchical conceptual schema. First, IDDAP identifies a potential infectious disease based on a patient’s self-described disease state. Then, the system searches for and proposes an appropriate antibiotic therapy specifically adapted to the patient based on factors such as the patient’s body temperature, infection sites, symptoms/signs, complications, antibacterial spectrum, contraindications, drug–drug interactions between the proposed therapy and previously prescribed medication, and the route of therapy administration. The constructed domain ontology contains 1,267,004 classes, 7,608,725 axioms, and 1,266,993 members of “SubClassOf” that pertain to infectious diseases, bacteria, syndromes, anti-bacterial drugs and other relevant components. The system includes 507 infectious diseases and their therapy methods in combination with 332 different infection sites, 936 relevant symptoms of the digestive, reproductive, neurological and other systems, 371 types of complications, 838,407 types of bacteria, 341 types of antibiotics, 1504 pairs of reaction rates (antibacterial spectrum) between antibiotics and bacteria, 431 pairs of drug interaction relationships and 86 pairs of antibiotic-specific population contraindicated relationships. Compared with the existing infectious disease-relevant ontologies in the field of knowledge comprehension, this ontology is more complete. Analysis of IDDAP's performance in terms of classifiers based on receiver operating characteristic (ROC) curve results (89.91%) revealed IDDAP's advantages when combined with our ontology.
Conclusions and significance
This study attempted to bridge the patient/caregiver gap by building a sophisticated application that uses artificial intelligence and machine learning computational techniques to perform data-driven decision-making at the point of primary care. The first level of decision-making is conducted by the IDDAP and provides the patient with a first-line therapy. Patients can then make a subjective judgment, and if any questions arise, should consult a physician for subsequent decisions, particularly in complicated cases or in cases in which the necessary information is not yet available in the knowledge base.}
}
@article{XING201914,
title = {Ontology for safety risk identification in metro construction},
journal = {Computers in Industry},
volume = {109},
pages = {14-30},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S016636151830397X},
author = {Xuejiao Xing and Botao Zhong and Hanbin Luo and Heng Li and Haitao Wu},
keywords = {Domain ontology, Metro construction, Knowledge sharing and reuse, Safety risk identification},
abstract = {Safety risk identification of metro construction is a knowledge-intensive process involving various stakeholders and communities. Currently, safety risk information related to decision making in metro construction is ill-structurally stored in various disordered formats, which hinders knowledge sharing and reuse. This study develops a domain ontology (SRI-Onto) to formalize safety risk knowledge in metro construction to support safety risk identification. An ontology development method with five steps is adopted. The SRI-Onto organizes safety risk knowledge into seven unified classes (i.e. project, construction activity, risk factor, risk, risk grade, risk consequence, and risk prevention measure). Defined classes, together with corresponding properties and relations, are coded using the Protégé platform. Finally, the SRI-Onto is evaluated theoretically and practically, using criteria-based and application-based evaluations respectively. Results indicate that the SRI-Onto possesses the necessary and essential criteria to serve the purpose of knowledge sharing and reuse. Further, the SRI-Onto is predicted to be generally applicable to safety risk identification of metro construction.}
}
@article{SYN2019377,
title = {Megaprojects – symbolic and sublime: an ontological review},
journal = {International Journal of Managing Projects in Business},
volume = {12},
number = {2},
pages = {377-399},
year = {2019},
issn = {1753-8378},
doi = {https://doi.org/10.1108/IJMPB-03-2018-0054},
url = {https://www.sciencedirect.com/science/article/pii/S1753837819000056},
author = {Thant Syn and Arkalgud Ramaprasad},
keywords = {Megaprojects, Ontology, Research},
abstract = {Purpose
Megaprojects are symbolic milestones of human history. Most megaprojects are one-of-a-kind endeavors to which traditional project management principles are neither applicable nor suitable, rendering the holistic study of megaprojects especially difficult. There is no systemic framework that can help systematically assess and guide megaprojects and megaproject research. In the absence of such a framework there is a significant risk of bias in planning the projects and the topics researched. The purpose of this paper is to present an ontological framework of megaprojects and discuss how it can help analyze individual megaprojects and synthesize the corpus of megaproject research.
Design/methodology/approach
An ontology framework of megaproject is developed by deconstructing the symbolism and purpose of megaprojects into respective dimensions and their categories. The ontological framework is then used to map the extent literature on megaproject to identify the dominant themes and gaps in the state-of-the-research.
Findings
The megaproject research has predominantly focused on select stakeholders (builders, governments, and communities), translation stages (implementation and conceptualization), and sublime (mostly economic). Other aspects of megaprojects have received little or no attention.
Originality/value
The paper presents an ontological framework to holistically capture the symbolism and sublime of megaprojects. The framework is complete, expansive, and grounded, yet simple, parsimonious, and innovative. It is a tool for decision makers more than a formal ontology readable by machines.}
}
@article{MERIAH201985,
title = {Comparative Study of Ontologies Based ISO 27000 Series Security Standards},
journal = {Procedia Computer Science},
volume = {160},
pages = {85-92},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.447},
url = {https://www.sciencedirect.com/science/article/pii/S187705091931662X},
author = {Ines Meriah and Latifa Ben {Arfa Rabai}},
keywords = {ISO\IEC 27000 series, Security ontology, Ontology-based security standards, Security risk management, Security decision makers},
abstract = {Security management standards as ISO/IEC 27000 series provide guidelines, which enable to evaluate the security in the company on a continuous basis. Security ontology technology is the most recommended to make links between security concepts and related standards. This paper presents on a review of ontologies based ISO/IEC 27000 series security standards and provides recommendations for professionals and researchers who need to understand or incorporate one of ISO/IEC 27000 standards features to cover their business security needs. We select and examine in details six main ontologies focusing on the usage of ISO/IEC 27000 series security standards. For each security ontology, we review and then describe it in terms of aim, security concepts and ISO 27000 features. Based on this analysis, we propose a comparison between these ontologies considering several factors to pick out their benefits and limits in order to give a set of recommendations to security decision makers helping them to select an ontology regarding their security requirements.}
}
@article{ARAFEH2021769,
title = {Ontology based recommender system using social network data},
journal = {Future Generation Computer Systems},
volume = {115},
pages = {769-779},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.09.030},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20305963},
author = {Mohamad Arafeh and Paolo Ceravolo and Azzam Mourad and Ernesto Damiani and Emanuele Bellini},
keywords = {Social network, Data miner, Big data, Data analysis, Data sampling, Ontology, Recommender system},
abstract = {Online Social Network (OSN) is considered a key source of information for real-time decision making. However, several constraints lead to decreasing the amount of information that a researcher can have while increasing the time of social network mining procedures. In this context, this paper proposes a new framework for sampling Online Social Network (OSN). Domain knowledge is used to define tailored strategies that can decrease the budget and time required for mining while increasing the recall. An ontology supports our filtering layer in evaluating the relatedness of nodes. Our approach demonstrates that the same mechanism can be advanced to prompt recommendations to users. Our test cases and experimental results emphasize the importance of the strategy definition step in our social miner and the application of ontologies on the knowledge graph in the domain of recommendation analysis.}
}
@article{ZHENG2018135,
title = {Complex overlapping concepts: An effective auditing methodology for families of similarly structured BioPortal ontologies},
journal = {Journal of Biomedical Informatics},
volume = {83},
pages = {135-149},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2018.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S1532046418300984},
author = {Ling Zheng and Yan Chen and Gai Elhanan and Yehoshua Perl and James Geller and Christopher Ochs},
keywords = {National Cancer Institute thesaurus, SNOMED CT, Ontology auditing, Ontology quality assurance, Abstraction network, Family-based ontology quality assurance},
abstract = {In previous research, we have demonstrated for a number of ontologies that structurally complex concepts (for different definitions of “complex”) in an ontology are more likely to exhibit errors than other concepts. Thus, such complex concepts often become fertile ground for quality assurance (QA) in ontologies. They should be audited first. One example of complex concepts is given by “overlapping concepts” (to be defined below.) Historically, a different auditing methodology had to be developed for every single ontology. For better scalability and efficiency, it is desirable to identify family-wide QA methodologies. Each such methodology would be applicable to a whole family of similar ontologies. In past research, we had divided the 685 ontologies of BioPortal into families of structurally similar ontologies. We showed for four ontologies of the same large family in BioPortal that “overlapping concepts” are indeed statistically significantly more likely to exhibit errors. In order to make an authoritative statement concerning the success of “overlapping concepts” as a methodology for a whole family of similar ontologies (or of large subhierarchies of ontologies), it is necessary to show that “overlapping concepts” have a higher likelihood of errors for six out of six ontologies of the family. In this paper, we are demonstrating for two more ontologies that “overlapping concepts” can successfully predict groups of concepts with a higher error rate than concepts from a control group. The fifth ontology is the Neoplasm subhierarchy of the National Cancer Institute thesaurus (NCIt). The sixth ontology is the Infectious Disease subhierarchy of SNOMED CT. We demonstrate quality assurance results for both of them. Furthermore, in this paper we observe two novel, important, and useful phenomena during quality assurance of “overlapping concepts.” First, an erroneous “overlapping concept” can help with discovering other erroneous “non-overlapping concepts” in its vicinity. Secondly, correcting erroneous “overlapping concepts” may turn them into “non-overlapping concepts.” We demonstrate that this may reduce the complexity of parts of the ontology, which in turn makes the ontology more comprehensible, simplifying maintenance and use of the ontology.}
}
@article{WEGERIF201980,
title = {Exploring the ontological dimension of dialogic education through an evaluation of the impact of Internet mediated dialogue across cultural difference},
journal = {Learning, Culture and Social Interaction},
volume = {20},
pages = {80-89},
year = {2019},
note = {Dialogical approaches to learning: from theory to practice and back},
issn = {2210-6561},
doi = {https://doi.org/10.1016/j.lcsi.2017.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S2210656117301940},
author = {Rupert Wegerif and Jonathan Doney and Andrew Richards and Nasser Mansour and Shirley Larkin and Ian Jamison},
keywords = {Dialogic theory, CSCL, Blogging, Video-conferencing, Global education, Religious education, Dialogic research methodology},
abstract = {It has been claimed that dialogic education implies a direction of change upon an ontological dimension from monologic closed identities in the direction of more dialogic identifications characterised by greater openness to the other and greater identification with the process of dialogue. This paper recapitulates that theory and then provides an empirical illustration of what it looks like in practice. In order to do this a methodology for researching the impact of dialogic education is outlined and applied to the evaluation of the impact of a programme designed to promote greater dialogic open-mindedness: the Tony Blair Institute for Global Change's Generation Global Project (GG) supports schools in over twenty different countries to engage in dialogue with each other through videos and blogs. The methodology put forward argues that the understanding sought by educational research is dialogic in that it emerges from the dialogue between inside and outside perspectives. The findings offer some clear evidence of a shift in identifications resulting from dialogue through the analysis of changes in online language use supported by interview evidence. This study suggests that a pedagogical intervention can produce identity change in the direction of becoming more dialogic and shows that it is possible to evaluate this change.}
}
@article{ZAOUGA2019417,
title = {Towards an Ontology Based-Approach for Human Resource Management},
journal = {Procedia Computer Science},
volume = {151},
pages = {417-424},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.057},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919305198},
author = {Wiem Zaouga and Latifa Ben {Arfa Rabai} and Wafa Rashid Alalyani},
keywords = {Human Resource Ontology, Domain Ontology, Human Resource Management Processes, PMBOK},
abstract = {Human Resources (HRs) as one of the most valuable asset of any organizations play a crucial role in their success. Within the context of project management, HR Management (HRM) is perceived as an entire knowledge area with defined processes, Tools and Techniques (T&T) in PMBOK 5th Guide. Although this guide shows a strong focus on HRM, it does not illustrate, in a clear way, how to perform each process with the required competencies and the related T&T. Hence, by using only PMBOK processes, the project manager cannot be assisted to select the suitable team according to their skills, also the project team members are not able to interchange their competencies. To address these issues, we propose to use an ontological approach in order to build a common shared representation in the HRM domain. This approach fosters the interoperability among HRs as well as their efficient use of T&T; further it can provide whom using PMBOK a better understanding and guidance for managing HRs with evidence.}
}
@article{IQBAL201873,
title = {A mathematical evaluation for measuring correctness of domain ontologies using concept maps},
journal = {Measurement},
volume = {118},
pages = {73-82},
year = {2018},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2018.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0263224118300083},
author = {Rizwan Iqbal and Masrah Azrifah {Azmi Murad} and Layth Sliman and Clay Palmeira {da Silva}},
keywords = {Ontology engineering, Concept mapping, Ontology evaluation, Closeness index, Similarity index},
abstract = {There is a need for further research in the area of ontology evaluation specifically dealing with ontology development exploiting concept maps. The existing literature on ontology evaluation primarily emphasis on ontology formalisation as well as on performing logical inferences, which is usually not directly relevant for concept maps as they are commonly exploited as communication instruments for learning purposes. Commonly used techniques for evaluating concept maps for knowledge assessment may be adopted for a kind of criteria-based evaluation of a domain concept map with respect to a particular aspect. However, this makes its validity limited to a particular aspect or criteria. This paper presents a mathematical ontology evaluation technique to measure the correctness of domain ontologies engineered using concept maps. It is based on the notion of merging two different mathematical measures, namely closeness index and similarity index to come up with a combined index that takes different criteria or aspects into account while performing ontology evaluation. Therefore, the proposed technique makes the evaluation process more reliable and robust. Two case studies were conducted employing the proposed technique for evaluating two different domain ontologies that were engineered using concept maps. Calculations and results from the case studies showed that depending on the correctness of individual ontology, different values of combined Index was calculated manifesting the measure of correctness of each individual ontology in a quantifiable form. Moreover, the results depict that the technique provides in-depth evaluation, it is easy to adopt, requires no special skills, and is conveniently replicable.}
}
@article{NOVIKOVA2025127270,
title = {What are your privacy risks? Privacy risk assessment based on privacy policies analysis},
journal = {Expert Systems with Applications},
volume = {280},
pages = {127270},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127270},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425008929},
author = {Evgenia Novikova and Elena Doynikova and Igor Kotenko},
keywords = {Risk assessment, Semantic modeling, Ontology, User-centric risk score, Privacy policy},
abstract = {The paper proposes an intelligent methodology for user-centric privacy risk assessment that could be used in expert systems designed to assess and manage privacy risks. It is based on semantic modeling of the scenarios for using personal data and analysis of the privacy policies written in natural language. It also introduces a novel approach to calculating numerical privacy risk scores. This risk calculation allows users and organizations to understand the impact on privacy that may arise from implementation of the privacy policy specified in a text and enhance the privacy risk management procedures accordingly. Thus, the motivation of our research arises from the need for objective and clear privacy risk scores demonstrating possible impacts on the privacy of the users and potential financial losses of the organizations on the one hand, and the absence of the end-to-end methodology for assessment of privacy risks arising from the privacy policies on the other hand. The paper details the ontology construction process including the expert-based definition of the ontology competency questions and the privacy risk calculation algorithm. Examples of ontology fragments for the selected privacy policy are given. The experiments were conducted using the OPP-115 corpus of privacy policies comprising 115 privacy policies and demonstrated the applicability of the proposed approach to calculate and explain the risk score. The results of the experiments are validated by the experts.}
}
@article{ON2019,
title = {Sentiment Analysis of Social Media on Childhood Vaccination: Development of an Ontology},
journal = {Journal of Medical Internet Research},
volume = {21},
number = {6},
year = {2019},
issn = {1438-8871},
doi = {https://doi.org/10.2196/13456},
url = {https://www.sciencedirect.com/science/article/pii/S1438887119003017},
author = {Jeongah On and Hyeoun-Ae Park and Tae-Min Song},
keywords = {social media, vaccination, health information interoperability, semantics},
abstract = {Background
Although vaccination rates are above the threshold for herd immunity in South Korea, a growing number of parents have expressed concerns about the safety of vaccines. It is important to understand these concerns so that we can maintain high vaccination rates.
Objective
The aim of this study was to develop a childhood vaccination ontology to serve as a framework for collecting and analyzing social data on childhood vaccination and to use this ontology for identifying concerns about and sentiments toward childhood vaccination from social data.
Methods
The domain and scope of the ontology were determined by developing competency questions. We checked if existing ontologies and conceptual frameworks related to vaccination can be reused for the childhood vaccination ontology. Terms were collected from clinical practice guidelines, research papers, and posts on social media platforms. Class concepts were extracted from these terms. A class hierarchy was developed using a top-down approach. The ontology was evaluated in terms of description logics, face and content validity, and coverage. In total, 40,359 Korean posts on childhood vaccination were collected from 27 social media channels between January and December 2015. Vaccination issues were identified and classified using the second-level class concepts of the ontology. The sentiments were classified in 3 ways: positive, negative or neutral. Posts were analyzed using frequency, trend, logistic regression, and association rules.
Results
Our childhood vaccination ontology comprised 9 superclasses with 137 subclasses and 431 synonyms for class, attribute, and value concepts. Parent’s health belief appeared in 53.21% (15,709/29,521) of posts and positive sentiments appeared in 64.08% (17,454/27,236) of posts. Trends in sentiments toward vaccination were affected by news about vaccinations. Posts with parents’ health belief, vaccination availability, and vaccination policy were associated with positive sentiments, whereas posts with experience of vaccine adverse events were associated with negative sentiments.
Conclusions
The childhood vaccination ontology developed in this study was useful for collecting and analyzing social data on childhood vaccination. We expect that practitioners and researchers in the field of childhood vaccination could use our ontology to identify concerns about and sentiments toward childhood vaccination from social data.}
}
@article{OSMAN2021101460,
title = {An Alignment-Based Implementation of a Holistic Ontology Integration Method},
journal = {MethodsX},
volume = {8},
pages = {101460},
year = {2021},
issn = {2215-0161},
doi = {https://doi.org/10.1016/j.mex.2021.101460},
url = {https://www.sciencedirect.com/science/article/pii/S2215016121002533},
author = {Inès Osman and Salvatore Flavio Pileggi and Sadok {Ben Yahia} and Gayo Diallo},
keywords = {, , , , , , },
abstract = {Despite the intense research activity in the last two decades, ontology integration still presents a number of challenging issues. As ontologies are continuously growing in number, complexity and size and are adopted within open distributed systems such as the Semantic Web, integration becomes a central problem and has to be addressed in a context of increasing scale and heterogeneity. In this paper, we describe a holistic alignment-based method for customized ontology integration. The holistic approach proposes additional challenges as multiple ontologies are jointly integrated at once, in contrast to most common approaches that perform an incremental pairwise ontology integration. By applying consolidated techniques for ontology matching, we investigate the impact on the resulting ontology. The proposed method takes multiple ontologies as well as pairwise alignments and returns a refactored/non-refactored integrated ontology that faithfully preserves the original knowledge of the input ontologies and alignments. We have tested the method on large biomedical ontologies from the LargeBio OAEI track. Results show effectiveness, and overall, a decreased integration cost over multiple ontologies.•OIAR and AROM are two implementations of the proposed method.•OIAR creates a bridge ontology, and AROM creates a fully merged ontology.•The implementation includes the option of ontology refactoring.}
}
@article{ALSHDIFAT2020251,
title = {Ontology - based context resolution in internet of things enabled diagnostics},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {3},
pages = {251-256},
year = {2020},
note = {4th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies - AMEST 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.11.041},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320301877},
author = {Ali Al-Shdifat and Christos Emmanouilidis and Muhammad Khan and Andrew Starr},
keywords = {Context Management, Maintenance Ontology, Industrial Diagnostics},
abstract = {Internet of things (IoT)-generated data from industrial systems are often collected in non-actionable form, thus not directly aiding maintenance actions. Context information management is often seen as an enabler for interoperability and context-based service adaptation, acting as a mechanism for linking data with knowledge to adaptive data and services. Ontology-based approaches for semantic maintenance have been proposed in the past as a data and service mediation mechanism and are adopted here as the starting point employed to develop a context resolution service for industrial diagnostics. The underlying ontology of the context resolution mechanism is relevant to failure analysis of mechanical components. The terminology and relationship between concepts are structured on the basis of relevant standards with a reliability-oriented knowledge grounding. A reasoning mechanism is employed to deliver context resolution and the derived context can add a metadata layer on data or events generated by automated and human-driven means. The approach is applied on a gearbox test rig appropriate for emulating complex misalignment cases met in many manufacturing and aerospace applications.}
}
@article{ZHONG2022,
title = {An Ontology-Based Automation System:},
journal = {International Journal on Semantic Web and Information Systems},
volume = {18},
number = {1},
year = {2022},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.295946},
url = {https://www.sciencedirect.com/science/article/pii/S1552628322000047},
author = {Xiaofang Zhong and Yi Wang and Xiao Wen and Jianwei Liao},
keywords = {Automatic Fertilization, Bayesian Network Extension, Citrus Fertilization Ontology, Citrus Planting, Semantic Web},
abstract = {ABSTRACT
This paper presents an ontology-based approach to benefit automatic fertilization management for citrus orchards located in mountainous regions. The core of the fertilization approach is the citrus fertilization ontology that covers knowledge about citrus fertilizers and fertilization application. Specially, the approach can provide not only the yearly fertilization quantities of required pure nitrogen, phosphorus, and potassium according to their disease symptoms, but also the suitable fertilizing recommendations for the citrus orchards with different soil properties. The current version of the ontology (ver. 2.9.10) contains 103 classes, 34 properties, 800 instances, which are defined by 3056 RDF triples and is evaluated by using 90 competency questions. Furthermore, the authors run experiments with the proposal targeting at four citrus orchards in Chongqing and compare its outputs with the reference values advised by the agri-professionals of citrus planting.}
}
@article{BILGIN2018384,
title = {An ontology-based approach for delay analysis in construction},
journal = {KSCE Journal of Civil Engineering},
volume = {22},
number = {2},
pages = {384-398},
year = {2018},
issn = {1226-7988},
doi = {https://doi.org/10.1007/s12205-017-0651-5},
url = {https://www.sciencedirect.com/science/article/pii/S1226798824023481},
author = {Gozde Bilgin and Irem Dikmen and M. Talat Birgonul},
keywords = {construction sector, delay, delay analysis, ontology, ontology evaluation, taxonomy},
abstract = {Delay is a common problem of the construction sector and it is one of the major reasons of claims between project participants. Systematic and reliable delay analysis is critical for successful management of claims. In this study, a delay analysis ontology is proposed that may facilitate development of databases, information sharing as well as retrieval for delay analysis within construction companies. A detailed literature review on construction delays has been carried out during the development of the ontology and it is evaluated by using five case studies. The delay analysis ontology may be used for different purposes especially to support decision-making during risk and claim management processes. It may enable companies to create their own databases, corporate memories and develop decision support systems for better analysis of delays.}
}
@article{LEWIS2022247,
title = {Subject to labor: Racial capitalism and ontology in the post-emancipation Caribbean},
journal = {Geoforum},
volume = {132},
pages = {247-251},
year = {2022},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2020.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0016718520301573},
author = {Jovan Scott Lewis},
keywords = {Racial capitalism, Ontology, Caribbean, Labor, Indenture, Emancipation},
abstract = {Racial capitalism is often understood as a process that acts upon subjects that are already racialized, facilitating their exploitative incorporation into capitalist processes. In this article, I push past that definition to argue that racial capitalism is a fundamentally ontologizing process by which subjectivity is produced. I advance this point through an analysis of the central role of labor in the foundation of post-emancipation Caribbean subject formation . I argue that in crossing the Atlantic in the service of colonial capital, enslaved Africans, and later indentured Indians, underwent a process that undid previous senses of identity, thus demanding new ontological moorings rooted in plantation labor. The practice and conditions of labor experienced by both groups after emancipation provided a common basis of ontological orientation. Thus, racial capitalism through the instrument of labor, has fundamentally structured the terms and definition of Caribbean identity.}
}
@article{HOORN2022116,
title = {A robot’s sense-making of fallacies and rhetorical tropes. Creating ontologies of what humans try to say},
journal = {Cognitive Systems Research},
volume = {72},
pages = {116-130},
year = {2022},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2021.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S1389041721000772},
author = {Johan F. Hoorn and Denice J. Tuinhof},
keywords = {Social robots, Logical fallacies, Metaphors, Reference, Sense, Maxim of quality, Tableau reasoning, Epistemics of the virtual},
abstract = {In the design of user-friendly robots, human communication should be understood by the system beyond mere logics and literal meaning. Robot communication-design has long ignored the importance of communication and politeness rules that are ‘forgiving’ and ‘suspending disbelief’ and cannot handle the basically metaphorical way humans design their utterances. Through analysis of the psychological causes of illogical and non-literal statements, signal detection, fundamental attribution errors, and anthropomorphism, we developed a fail-safe protocol for fallacies and tropes that makes use of Frege’s distinction between reference and sense, Beth’s tableau analytics, Grice’s maxim of quality, and epistemic considerations to have the robot politely make sense of a user’s sometimes unintelligible demands.}
}
@article{WARNER2021102171,
title = {Enschede cries – Restoring ontological security after a fireworks disaster},
journal = {International Journal of Disaster Risk Reduction},
volume = {59},
pages = {102171},
year = {2021},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2021.102171},
url = {https://www.sciencedirect.com/science/article/pii/S2212420921001370},
author = {Jeroen Warner},
keywords = {Enschede, The Netherlands, Fireworks disaster, Cultural memory, Ontological security, Emotional turn in disaster studies},
abstract = {The article claims that to those affected, disaster is an existential experience. For them, it is an unexpected existential ‘event’ clearly separating a ‘before’ from an ‘after’. In the academic disaster domain however the ‘disaster as event’ is being eroded both by complexity approaches and critical approaches, which both, if for different reasons, consider disaster ‘normal’. By the example of the Enschede, the Netherlands, urban fireworks explosion of 2000 I argue that we should not only celebrate community resilience but take much more seriously how disasters may paralyse and traumatize individuals and communities. The application of Giddens' ‘ontological security’ to urban disaster foregrounds the importance for the disaster-affected population of regaining a sense of continuity and trust in the living and regulatory environment. Retaining (cultural) memory, also mediated through the arts, supports long-term rehabilitation. In the case under scrutiny, the municipal government indeed proved responsive to a desire to preserve disaster memory rather than just look ahead, yet unresolved forensic puzzles and lack of accountability may have slowed psychological closure.}
}
@article{PILEGGI2021115065,
title = {Knowledge interoperability and re-use in Empathy Mapping: an ontological approach},
journal = {Expert Systems with Applications},
volume = {180},
pages = {115065},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115065},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421005066},
author = {Salvatore F. Pileggi},
keywords = {Ontology, Data integration and re-use, Semantic interoperability, Semantic web, Data engineering, Knowledge engineering, Design thinking, Empathy mapping},
abstract = {Design Thinking is a human-centered approach extensively used across different domains that aims at problem solving, value creation for stakeholders and innovation by fostering creativity. The most characterising and critical step along the Design Thinking process is the empathy phase, in which stakeholder analysis is performed by looking at a given scenario from the perspective of different stakeholders. Such a methodology enables a systematic information gathering and organization that results in a deep understanding of actual problems, needs and expectations from the target stakeholders. The uniqueness of problems and the need for situation-specific data makes knowledge re-use not always practical, even within the most consolidated and experienced environments. In this paper we propose an ontological support to empathy mapping that aims to (i) establish an interoperable fine-grained data layer among the different data collected throughout the empathy mapping process, (ii) enable multi-scenario analysis underpinned by formal specifications and (iii) further empower the process through semantic enrichment and integration of insight from multiple sources and contexts. We believe this is the first step to design and properly integrate effective computational and AI-based functionalities along the creative design thinking process, as well as to enable in practice richer and more sophisticated approaches (e.g. through social networks).}
}
@article{GIANNAKOPOULOS2025127117,
title = {NAVMAT: An AI-supported naval failures knowledge management system},
journal = {Expert Systems with Applications},
volume = {277},
pages = {127117},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127117},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425007390},
author = {George Giannakopoulos and Andreas Sideras and Konstantinos Stamatakis and Nikolaos Melanitis},
keywords = {Knowledge management system, Ontology, Information retrieval, Human-expert knowledge instillation},
abstract = {We present “NAVMAT”, an intelligent, multilingual knowledge management platform designed to record and categorize material failure incidents reported in naval operations. This paper provides an overview of the platform, identifying its key software components and highlighting the information retrieval approach used to support user workflows. The platform primarily facilitates real-time, multilingual search and intelligent indexing, streamlining the incident management process while offering valuable insights from past incidents and knowledge resources. To achieve this, it employs a customized natural language processing pipeline integrated with a carefully engineered ontology. The ontology, regularly updated by domain experts, enriches the retrieval mechanism by instilling domain specific knowledge. This approach aims to reduce the significant variability in specialized terminology by promoting convergence towards a unified vocabulary.}
}
@article{DIMITROVA2020103450,
title = {An ontological approach for pathology assessment and diagnosis of tunnels},
journal = {Engineering Applications of Artificial Intelligence},
volume = {90},
pages = {103450},
year = {2020},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2019.103450},
url = {https://www.sciencedirect.com/science/article/pii/S0952197619303446},
author = {Vania Dimitrova and Muhammad Owais Mehmood and Dhavalkumar Thakker and Bastien Sage-Vallier and Joaquin Valdes and Anthony G. Cohn},
keywords = {Tunnel diagnosis, Ontology, Intelligent decision support systems, Linear transport structures},
abstract = {Tunnel maintenance requires complex decision making, which involves pathology diagnosis and risk assessment, to ensure full safety while optimising maintenance and repair costs. A Decision Support System (DSS) can play a key role in this process by supporting the decision makers in identifying pathologies based on disorders present in various tunnel portions and contextual factors affecting a tunnel. Another key aspect is to identify which spatial stretches within a tunnel contain pathologies of similar kinds within neighbouring tunnel segments. This paper presents PADTUN, a novel intelligent decision support system that assists with pathology diagnosis and assessment of tunnels with respect to their disorders and diagnosis influencing factors. It utilises semantic web technologies for knowledge capture, representation, and reasoning. The core of PADTUN is a family of ontologies which represent the main concepts and relations associated with pathology assessment, and capture the decision process concerning tunnel maintenance. Tunnel inspection data is linked to these ontologies to take advantage of inference capabilities offered by semantic technologies. In addition, an intelligent mechanism is presented which exploits abstraction and inference capabilities. Thus PADTUN provides the world’s first semantically based intelligent DSS for tunnel maintenance. PADTUN was developed by an interdisciplinary team of tunnel experts and knowledge engineers in real-world settings offered by the NeTTUN EU Project. An evaluation of the PADTUN system is performed using real-world tunnel data and diagnosis tasks. We show how the use of semantic technologies allows addressing the complex issues of tunnel pathology inferencing, aiding in, and matching transportation experts’ expectations of decision support. The methodology is applicable to any linear transport structures, offering intelligent ways to aid with complex decision processes related to diagnosis and maintenance.}
}
@article{CRISTANI2018527,
title = {ONTO-PLC: An ontology-driven methodology for converting PLC industrial plants to IoT},
journal = {Procedia Computer Science},
volume = {126},
pages = {527-536},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.07.287},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918312638},
author = {Matteo Cristani and Florenc Demrozi and Claudio Tomazzoli},
abstract = {We present the new methodology ONTO-PLC to deliver software programs on system-on-chip or single-board computers used to control industrial plants, as substitutes for programmable logic control technologies. The methodology is ontology-driven based on the abstract description of the plant at a level in which the plant itself is viewed as a set of instruments, each instrument being a set of machineries coordinated in functional terms by a control system, formed by sensors and actuators, under the control of an abstract model of behavior delivered by means of an extended finite state machine.}
}
@article{SHISHEHCHI2021100192,
title = {A rule based expert system based on ontology for diagnosis of ITP disease},
journal = {Smart Health},
volume = {21},
pages = {100192},
year = {2021},
issn = {2352-6483},
doi = {https://doi.org/10.1016/j.smhl.2021.100192},
url = {https://www.sciencedirect.com/science/article/pii/S2352648321000143},
author = {Saman Shishehchi and Seyed Yashar Banihashem},
keywords = {ITP disease, Medical expert system, Ontology, Semantic rules},
abstract = {This paper is aimed to implement the semantic rule based expert system for diagnosing a kind of blood immune thrombocytopenia disease. This paper presents an ontology to depict the knowledge domain of this disease, symptoms and its related treatments. The developed system uses the real data from laboratory of Iranian hospitals. In the system, the observed symptoms are taken from the patient via Java based graphical user interface. This system supports the patients suffering from this disease to get a type of their disease and recommends suitable treatments. Some semantic rules were defined and then, Jess as a reasoner inferenced the rules to do the diagnosis process. The diagnosis process is validated by blood specialists. Since this system can be used by patients, doctors or medical students anywhere, it helps to make the disease follow up easier and tries to save cost and time for patients. The questionnaire is developed to measure the usability of the system. The results of questionnaire were satisfactory after it was tested with 154 respondents. The reliability test was done and the Cronbach's Alpha was .865 which is higher than 0.7. The mean value of questionnaire is more than 4 and the total mean is 4.49 which is an acceptable value to show the high degree of user acceptance and accuracy of system.}
}
@article{WATROBSKI20191602,
title = {Ontology Supporting Green Supplier Selection Process},
journal = {Procedia Computer Science},
volume = {159},
pages = {1602-1613},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.331},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919315327},
author = {Jarosław Wątróbski},
keywords = {Green supplier selection, green supply chain, ontology-based model, supplier’ selection, evaluation criteria},
abstract = {At present, companies need to consider and include so-called ‘green strategies’ in order to retain competitive advantage. The common understanding of functionality of supply chain as well as collaborations among this between suppliers and partners means that companies need to do much more in order to ensure the greener effect. On the other hand, a scattered model of conducting businesses and global competition force companies to consider the necessity for development the activities of proper selection and evaluation of beneficial supply chain partnerships. Due to large amount of literature-based references of green supplier selection and evaluation, knowledge scattering of greener collaboration is noticeable. In addressing this research challenge, capturing knowledge in one place in the form of ontology for enabling selection and evaluation criteria of suppliers is proposed. Within this paper, an attempt to application of ontology-based model for supplier’ selection and evaluation criteria is developed. This model is implemented on base of distinctive set of criteria derived from revised literature, nevertheless the public and common availability of the proposed model suggests to other researches to collaborate in this field by adding, sharing and reusing knowledge of alternative criteria of green supplier’ selection.}
}
@article{SHARMA2022,
title = {Altering OWL Ontologies for Efficient Knowledge Organization on the Semantic Web},
journal = {International Journal of Information System Modeling and Design},
volume = {13},
number = {7},
year = {2022},
issn = {1947-8186},
doi = {https://doi.org/10.4018/IJISMD.313431},
url = {https://www.sciencedirect.com/science/article/pii/S1947818622000382},
author = {Abhisek Sharma and Sarika Jain},
keywords = {  ,   ,   ,   ,   },
abstract = {ABSTRACT
The increase in the number of users on the internet and the advancement in information technology have spiked the generation of information to an unprecedented level making information retrieval and web mining a difficult task. Semantic technologies can help improve the results of web mining by providing constructs that can help represent the web documents in a machine-understandable manner. To keep providing semantically rich services while keeping this surge in the amount of information in mind, we have to work towards ways to make the process of information management efficient while retaining its effectiveness. One of the ways to accomplish the above task is to improvise the knowledge organization in a manner that every piece of information is in its designated place. This paper discusses and addresses the problems with current knowledge organization methodologies and presents an algorithm to alter the available OWL ontologies. The authors were able to get a noticeable improvement in the amount of storage used by the ontology with fewer axioms without losing any information.}
}
@article{ZHANG2018140,
title = {Metallic materials ontology population from LOD based on conditional random field},
journal = {Computers in Industry},
volume = {99},
pages = {140-155},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.03.032},
url = {https://www.sciencedirect.com/science/article/pii/S0166361517302014},
author = {Xiaoming Zhang and Zhishen Zhang and Huiyong Wang and Mingming Meng and Dongyu Pan},
keywords = {Metallic materials ontology, Linked open data, Ontology population, Conditional random field (CRF)},
abstract = {In recent years, with the rapid development of ontology technology, many relatively perfect domain ontologies have emerged gradually and achieved favorable applications. However, for the existing metallic materials ontologies, such as the metallic materials ontology created by Ashino, MatonTO and ONTORULE, the knowledge of their instances is comparatively insufficient. Additionally, for the users, they hope that not only a large number of the materials instances are included in the ontology, but also the properties of the instances are desired. Linked Open Data (LOD) provides huge open knowledge bases which contain ample materials knowledge. Thus, we expect the knowledge of LOD can be inserted into a specific ontology. Obviously, it is not an easy work, since the LOD is very large, and its structure is inconsistent with ontology’s. Therefore, a method is proposed to populate a specific metallic materials ontology with the metallic materials information in the LOD. Firstly, in the LOD, we determine the information that can be filled into the existing metallic materials ontology. Then, we convert the LOD to Chain Triples (CHTs) according to the filling information. We use conditional random field (CRF) to achieve CHTs' filling positions in the specified metallic materials ontology. Finally, we insert the information into the ontology. The approach is evaluated in light of F-measure, and the experiment results demonstrate that the proposed approach can be effective to populate a specific ontology with the metallic materials data in LOD. This approach not only enriches the existing metallic materials ontology, but also greatly saves the manual efforts on the process of ontology population.}
}
@article{ZHANG202599,
title = {Osteosarcoma knowledge graph question answering system: deep learning-based knowledge graph and large language model fusion},
journal = {Intelligent Medicine},
volume = {5},
number = {2},
pages = {99-110},
year = {2025},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2024.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2667102625000269},
author = {Lulu Zhang and Weisong Zhao and Zhiwei Cheng and Yafei Jiang and Kai Tian and Jia Shi and Zhenyu Jiang and Yingqi Hua},
keywords = {Osteosarcoma, Knowledge graph, Large language model, Text mining},
abstract = {Objective
Osteosarcoma is a prevalent primary malignant bone tumor in children and adolescents, accounting for approximately 5 % of childhood malignancies. Because of its rarity and biological complexity, treatment breakthroughs for osteosarcoma have been limited. To advance research in this field, we aimed to construct the first comprehensive osteosarcoma knowledge graph (OSKG) using the PubMed database.
Methods
A systematic search of PubMed (2003–2023) using the keyword “osteosarcoma” yielded 25,415 abstracts. Leveraging BioBERT, pretrained on biomedical corpora and fine-tuned with osteosarcoma-specific manual annotations, we identified 16 entity types and 17 biological relationships. The extracted elements were synthesized to create the OSKG, resulting in a deep learning-based knowledge base to explore osteosarcoma pathogenesis and molecular mechanisms. We then developed a specialized question-answering system (knowledge graph question answering (KGQA)) powered by ChatGLM3. This system employs advanced natural language processing and incorporates the OSKG to ensure optimal response quality and accuracy.
Results
The pretrained BioBERT averaged > 92 % accuracy in entity and relationship training. Evaluation using 100 pairs of gold-standard quizzes showed that the final quiz system outperformed other large language models in accuracy and robustness.
Conclusion
The system is designed to provide accurate disease-related queries and answers, effectively facilitating knowledge acquisition and reasoning in medical research and clinical practice. This project offers a robust tool for osteosarcoma research and promotes the deep integration of knowledge graphs and artificial intelligence technologies in the medical field.}
}
@article{VIDAL201822,
title = {Ontology-based approach for the validation and conformance testing of xAPI events},
journal = {Knowledge-Based Systems},
volume = {155},
pages = {22-34},
year = {2018},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2018.04.035},
url = {https://www.sciencedirect.com/science/article/pii/S0950705118302065},
author = {Juan C. Vidal and Thomas Rabelo and Manuel Lama and Ricardo Amorim},
keywords = {Experience API, Ontologies, Ontology validation, Conformance testing},
abstract = {Learning analytics (LA) looks for a better understanding of learning and ways to optimize both the learning and the environments in which it occurs. One of its key research areas is focused on data interoperability, specifically on how to collect and store learning data. Proprietary systems usually store data in their own unique format and thus make it difficult to reuse LA solutions. Some approaches have appeared in the last years to overcome this issue and the Experience API (xAPI) has been the most successful in this area, primarily because of its generic approach not tied to any Learning Management System (LMS). However, the xAPI specification is informal, with some loose definitions, that may lead to unexpected mistakes. In order to avoid ambiguity, in this paper we present an xAPI ontology that captures the concepts and semantics of the specification, and has been validated with two datasets of the xAPI community. In addition, a web client has been developed to provide a validation tool that can check the correctness and conformance of individual xAPI files as well as complete xAPI datasets.}
}
@article{BENJIRA2025102405,
title = {Automated mapping between SDG indicators and open data: An LLM-augmented knowledge graph approach},
journal = {Data & Knowledge Engineering},
volume = {156},
pages = {102405},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102405},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24001290},
author = {Wissal Benjira and Faten Atigui and Bénédicte Bucher and Malika Grim-Yefsah and Nicolas Travers},
keywords = {Sustainable Development Goals (SDG), Large language model (LLM), Knowledge graph (KG), Open data, Schema mapping},
abstract = {Meeting the Sustainable Development Goals (SDGs) presents a large-scale challenge for all countries. SDGs established by the United Nations provide a comprehensive framework for addressing global issues. To monitor progress towards these goals, we need to develop key performance indicators and integrate and analyze heterogeneous datasets. The definition of these indicators requires the use of existing data and metadata. However, the diversity of data sources and formats raises major issues in terms of structuring and integration. Despite the abundance of open data and metadata, its exploitation remains limited, leaving untapped potential for guiding urban policies towards sustainability. Thus, this paper introduces a novel approach for SDG indicator computation, leveraging the capabilities of Large Language Models (LLMs) and Knowledge Graphs (KGs). We propose a method that combines rule-based filtering with LLM-powered schema mapping to establish semantic correspondences between diverse data sources and SDG indicators, including disaggregation. Our approach integrates these mappings into a KG, which enables indicator computation by querying graph’s topology. We evaluate our method through a case study focusing on the SDG Indicator 11.7.1 about accessibility of public open spaces. Our experimental results show significant improvements in accuracy, precision, recall, and F1-score compared to traditional schema mapping techniques.}
}
@article{CARDOSO2020105508,
title = {Construction and exploitation of an historical knowledge graph to deal with the evolution of ontologies},
journal = {Knowledge-Based Systems},
volume = {194},
pages = {105508},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.105508},
url = {https://www.sciencedirect.com/science/article/pii/S0950705120300241},
author = {Silvio Domingos Cardoso and Marcos {Da Silveira} and Cédric Pruski},
keywords = {Knowledge graphs, Ontology evolution, Biomedical ontology, Versioning},
abstract = {With the advances of Artificial Intelligence, the need for annotated data increases. However, the quality of these annotations can be impacted by the evolution of domain knowledge since the relations between successive versions of ontologies are rarely described and the history of concepts is not kept at the ontology level. As a consequence, using datasets annotated at different times becomes a real challenge for data- and knowledge-intensive systems. This work presents a way to address this problem. We introduce a Historical Knowledge Graph (HKG), where information from previous versions of an ontology can be found inside a single graph, reducing storage space (no need for versioning) and data treatment time (no need for laborious analysis of each version of the ontology). The HKG proposed in this work represents the evolutionary aspects of the knowledge in a structural way. Examples of the applicability of an HKG for information retrieval and the maintenance of semantic annotations show the capability of our approach for improving the quality of existing techniques.}
}
@article{TRIPPE2025102365,
title = {Special issue on applications of Generative AI and Large Language Models in the patent domain},
journal = {World Patent Information},
volume = {82},
pages = {102365},
year = {2025},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2025.102365},
url = {https://www.sciencedirect.com/science/article/pii/S0172219025000328},
author = {Tony Trippe and Jieh-Sheng Jason Lee}
}
@article{HELLMAN2021113416,
title = {Understanding addiction: The shift from epistemology to ontology},
journal = {Behavioural Brain Research},
volume = {412},
pages = {113416},
year = {2021},
issn = {0166-4328},
doi = {https://doi.org/10.1016/j.bbr.2021.113416},
url = {https://www.sciencedirect.com/science/article/pii/S0166432821003041},
author = {Matilda Hellman},
keywords = {Genealogy, Anthropocene-conscious approache (ACA), History of ideas, New materialism, Addiction},
abstract = {This essay contrasts a late modernist epistemological paradigm with an ontology-oriented Anthropocene-conscious Approach (ACA) as frameworks for understanding the coming into being and the making of addiction. Operationalizable theories and concepts of addiction have been crucial in an era with a great demand for compartmentalizing and systemically defining psychological struggles and social problems. In the modernistic progress story, the addiction phenomenon materializes through the conceptual division between capacity and non-capacity, with those capable of mastering their urges on one side and those incapable of doing so on the other. The ACA strives actively to move beyond artificial divides between agency/structure, culture/nature, mind/matter and instead explore phenomena ecologically across these continuums. This entails a conscious re-focus away from authoritative human-made assumptions towards new types of knowledge and knowing. In the ACA assemblage-like ontology, different elements are brought together in their capacities to affect each other into entities. Due to its claims of practical uses, I predict that the ACA will become as influential as Foucauldian genealogy in the field of addiction studies.}
}
@article{GUO2018455,
title = {SOR: An optimized semantic ontology retrieval algorithm for heterogeneous multimedia big data},
journal = {Journal of Computational Science},
volume = {28},
pages = {455-465},
year = {2018},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2017.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S187775031730176X},
author = {Kehua Guo and Zhonghe Liang and Yayuan Tang and Tao Chi},
keywords = {Ontology, Semantic-based retrieval, MapReduce, Multimedia big data, Big data retrieval, Retrieval algorithm},
abstract = {Semantic information can express the search intentions of users, and this approach has become an important tool in the field of information retrieval. To support semantic-based multimedia retrieval in big data environment, this paper presents an optimized algorithm called semantic ontology retrieval (SOR), which uses big data processing tools to store and retrieve ontologies from heterogeneous multimedia data. First, the background of semantic extraction and ontology representation for multimedia big data are addressed. Second, the methodology of SOR, including the model definition and retrieval algorithm, is proposed. Third, for parallel processing SOR in distributed nodes, a MapReduce-based retrieval framework is presented. Finally, to achieve high retrieval precision and good user experience, a user feedback scheme is designed. The experimental results illustrate that SOR is suitable for semantic-based retrieval for heterogeneous multimedia big data.}
}
@article{GOPFERT2024100383,
title = {Opportunities for large language models and discourse in engineering design},
journal = {Energy and AI},
volume = {17},
pages = {100383},
year = {2024},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2024.100383},
url = {https://www.sciencedirect.com/science/article/pii/S2666546824000491},
author = {Jan Göpfert and Jann M. Weinand and Patrick Kuckertz and Detlef Stolten},
keywords = {Product development process, Conceptual design, Design methodology, Design generation, Natural language processing, Foundation models, Multi-modal models},
abstract = {In recent years, large language models have achieved breakthroughs on a wide range of benchmarks in natural language processing and continue to increase in performance. Recently, the advances of large language models have raised interest outside the natural language processing community and could have a large impact on daily life. In this paper, we pose the question: How will large language models and other foundation models shape the future product development process? We provide the reader with an overview of the subject by summarizing both recent advances in natural language processing and the use of information technology in the engineering design process. We argue that discourse should be regarded as the core of engineering design processes, and therefore should be represented in a digital artifact. On this basis, we describe how foundation models such as large language models could contribute to the design discourse by automating parts thereof that involve creativity and reasoning, and were previously reserved for humans. We describe how simulations, experiments, topology optimizations, and other process steps can be integrated into a machine-actionable, discourse-centric design process. As an example, we present a design discourse on the optimization of wind turbine blades. Finally, we outline the future research that will be necessary for the implementation of the conceptualized framework.}
}
@article{CHAMARI2025116257,
title = {Towards portable model predictive control-based applications for demand side management in buildings},
journal = {Energy and Buildings},
volume = {347},
pages = {116257},
year = {2025},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2025.116257},
url = {https://www.sciencedirect.com/science/article/pii/S0378778825009879},
author = {Lasitha Chamari and Shalika Walker and Ekaterina Petrova and Pieter Pauwels},
keywords = {Brick ontology, Smart charging, Microservices, Resource description framework, Semantic web, Service-oriented architecture},
abstract = {Demand Side Management (DSM) applications in buildings rely on heterogeneous information systems, with data originating from different sources. Semantic Web technologies allow for connecting these disparate data sources by standardising metadata based on ontologies. Recent research focuses on designing portable control applications that can run across buildings. Although the first step towards realising portable control actions is standardising the metadata of the buildings, significant gaps still exist in the literature when it comes to creating portable Model Predictive Control (MPC)-based DSM applications. Many existing portable applications are simple rule-based programmes and the principles of semantic portability are not clear in the areas like with MPC systems for DSM. This paper proposes a combination of modular services and metadata standardisation towards making MPC systems more portable across buildings. The method consists of (1) decomposing the MPC system into modular and reusable services and exposing their data using already standardised web interfaces, (2) extending metadata schemes to formalise the information requirements of the modular services, and (3) devising a modular semantic-driven portability service to query, validate, and configure the MPC system. Although full portability remains a challenge due to heterogeneity in building systems and metadata modelling styles, our approach demonstrates the feasibility of using standardised ontologies, semantic validation, and modular services to partially automate configuration and integration, thereby a step towards portable MPC applications. The proposed workflow is implemented, tested, and validated in a MPC system as part of a DSM strategy for controlling Electric Vehicle (EV) charging behaviour in an office microgrid system.}
}
@article{BHARAMBE201892,
title = {Adaptive Pareto-based approach for geo-ontology matching},
journal = {Computers & Geosciences},
volume = {119},
pages = {92-108},
year = {2018},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2018.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0098300417310671},
author = {Ujwala Bharambe and Surya S. Durbha}
}
@article{HOCAOGLU201870,
title = {AdSiF: Agent driven simulation framework paradigm and ontological view},
journal = {Science of Computer Programming},
volume = {167},
pages = {70-90},
year = {2018},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2018.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167642318302648},
author = {Mehmet F. Hocaoğlu},
keywords = {Agent-driven simulation, Agent Programming, Logic programming, Ontology, State-oriented programming},
abstract = {AdSiF (Agent driven Simulation Framework) provides a programming environment for modeling, simulation, and programming agents, which fuses agent-based, object-oriented, aspect-oriented, and logic programming into a single paradigm. The power of this paradigm stems from its ontological background and the paradigms it embraces and integrates into a single paradigm called state-oriented programming. AdSiF commits to describe what exists and to model the agent reasoning abilities, which thereby drives model behaviors. Basically, AdSiF provides a knowledgebase and a depth first search mechanism for reasoning. It is possible to model different search mechanism for reasoning but depth first search is a default search mechanism for first order reasoning. The knowledge base consists of facts and predicates. The reasoning mechanism is combined with a dual-world representation, it is defined as an inner representation of a simulated environment, and it is constructed from time-stamped sensory data (or beliefs) obtained from that environment even when these data consist of errors. This mechanism allows the models to make decisions using the historical data of the models and its own states. The study provides a novel view to simulation and agent-modeling using a script-based graph programming structuring state-oriented programming with a multi-paradigm approach. The study also enhances simulation modeling and agent programming using logic programming and aspect orientation. It provides a solution framework for continuous and discrete event simulation and allows modelers to use their own simulation time management, event handling, distributed, and real time simulation algorithms.}
}
@article{HOARE202196,
title = {A contextual ontology for distributed urban data management},
journal = {Proceedings of the Institution of Civil Engineers - Smart Infrastructure and Construction},
volume = {172},
number = {3},
pages = {96-105},
year = {2021},
issn = {2397-8759},
doi = {https://doi.org/10.1680/jsmic.19.00015},
url = {https://www.sciencedirect.com/science/article/pii/S2397875921000053},
author = {Cathal Hoare and Sergio Pinheiro and Shushan Hu and James ODonnell},
keywords = {data, information technology, knowledge management},
abstract = {The evolution of information and communication technology in the construction domain has yielded a variety of heterogeneous data sources. While bespoke approaches have been developed to explore data merging for a variety of purposes, few have explored how to develop a multipurpose information organisation that can be reconfigured on a per-project basis. This paper describes an approach that, using a lightweight central server, is used to investigate the effectiveness of loose federations of information sources that together serve the information needs of a project. The central server provides both a common context through which relationships between the information sources can be expressed and a data register to enable information discovery. The paper describes the creation of an ontology to capture this context and a software architecture to support its use. The efficacy of the approach is illustrated through describing the use of the server for marshalling data used in a renovation project.}
}
@article{IOANAALEXANDRA2021883,
title = {Towards Accessibility in Education through Smart Speakers. An ontology based approach},
journal = {Procedia Computer Science},
volume = {192},
pages = {883-892},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.091},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921015799},
author = {Todericiu Ioana-Alexandra and Şerban Camelia and Dioşan Laura},
keywords = {Ontology, Smart Speaker, Cloud, Education, University, Accessibility},
abstract = {As the world changes, so does the future of our students. In this respect, the evolution of the technology comes up with specific environments for educational purpose. Building smart learning environments supported by e-learning platforms is an important area of research in education domain within our days. The evolution of these smart learning environments is justified by some events (Covid19) that force students to learn remotely. The paper proposes a formalisation using ontology for providing an inclusive approach of universities’ websites, having as instance a software application component using Alexa smart speaker, that currently remains at a design level, which integrates different services (Amazon Web Services, Microsoft Services) for a proper virtual environment platform, for both students and teachers. It addresses the main concerns of the current educational system and provides a smart solution through the use of Artificial Intelligence based tools. The proposed approach not only achieves unifying data and knowledge-share mechanisms in a remotely mode, but it brings also a good learning experience, increasing the effectiveness and the efficiency of the learning process.}
}
@article{MOCANU2023100036,
title = {Knowledge representation and acquisition in the era of large language models: Reflections on learning to reason via PAC-Semantics},
journal = {Natural Language Processing Journal},
volume = {5},
pages = {100036},
year = {2023},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2023.100036},
url = {https://www.sciencedirect.com/science/article/pii/S294971912300033X},
author = {Ionela G. Mocanu and Vaishak Belle},
keywords = {Pac-semantics, Logical knowledge bases, Knowledge acquisition},
abstract = {Human beings are known for their remarkable ability to comprehend, analyse, and interpret common sense knowledge. This ability is critical for exhibiting intelligent behaviour, often defined as a mapping from beliefs to actions, which has led to attempts to formalize and capture explicit representations in the form of databases, knowledge bases, and ontologies in AI agents. But in the era of large language models (LLMs), this emphasis might seem unnecessary. After all, these models already capture the extent of human knowledge and can infer appropriate things from it (presumably) as per some innate logical rules. The question then is whether they can also be trained to perform mathematical computations. Although the consensus on the reliability of such models is still being studied, early results do seem to suggest they do not offer logically and mathematically consistent results. In this short summary article, we articulate the motivations for still caring about logical/symbolic artefacts and representations, and report on recent progress in learning to reason via the so-called probably approximately correct (PAC)-semantics.}
}
@article{ANNANE201851,
title = {Building an effective and efficient background knowledge resource to enhance ontology matching},
journal = {Journal of Web Semantics},
volume = {51},
pages = {51-68},
year = {2018},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300179},
author = {Amina Annane and Zohra Bellahsene and Faiçal Azouaou and Clement Jonquet},
keywords = {Ontology matching, Ontology alignment, Background knowledge, Indirect matching, External resource, Anchoring, Derivation, Background knowledge selection, Supervised machine learning},
abstract = {Ontology matching is critical for data integration and interoperability. Original ontology matching approaches relied solely on the content of the ontologies to align. However, these approaches are less effective when equivalent concepts have dissimilar labels and are structured with different modeling views. To overcome this semantic heterogeneity, the community has turned to the use of external background knowledge resources. Several methods have been proposed to select ontologies, other than the ones to align, as background knowledge to enhance a given ontology-matching task. However, these methods return a set of complete ontologies, while, in most cases, only fragments of the returned ontologies are effective for discovering new mappings. In this article, we propose an approach to select and build a background knowledge resource with just the right concepts chosen from a set of ontologies, which improves efficiency without loss of effectiveness. The use of background knowledge in ontology matching is a double-edged sword: while it may increase recall (i.e., retrieve more correct mappings), it may lower precision (i.e., produce more incorrect mappings). Therefore, we propose two methods to select the most relevant mappings from the candidate ones: (1) a selection based on a set of rules and (2) a selection based on supervised machine learning. Our experiments, conducted on two Ontology Alignment Evaluation Initiative (OAEI) datasets, confirm the effectiveness and efficiency of our approach. Moreover, the F-measure values obtained with our approach are very competitive to those of the state-of-the-art matchers exploiting background knowledge resources.}
}
@article{FATHALLA2018151,
title = {SemSur: A Core Ontology for the Semantic Representation of Research Findings},
journal = {Procedia Computer Science},
volume = {137},
pages = {151-162},
year = {2018},
note = {Proceedings of the 14th International Conference on Semantic Systems 10th – 13th of September 2018 Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S187705091831620X},
author = {Said Fathalla and Sahar Vahdati and Sören Auer and Christoph Lange},
keywords = {SemSur Ontology, Semantic Metadata Enrichment, SWRL rules, Scholarly Communication, Semantic Publishing},
abstract = {The way how research is communicated using text publications has not changed much over the past decades. We have the vision that ultimately researchers will work on a common structured knowledge base comprising comprehensive semantic and machine-comprehensible descriptions of their research, thus making research contributions more transparent and comparable. We present the SemSur ontology for semantically capturing the information commonly found in survey and review articles. SemSur is able to represent scientific results and to publish them in a comprehensive knowledge graph, which provides an efficient overview of a research field, and to compare research findings with related works in a structured way, thus saving researchers a significant amount of time and effort. The new release of SemSur covers more domains, defines better alignment with external ontologies and rules for eliciting implicit knowledge. We discuss possible applications and present an evaluation of our approach with the retrospective, exemplary semantification of a survey. We demonstrate the utility of the SemSur ontology to answer queries about the different research contributions covered by the survey. SemSur is currently used and maintained at OpenResearch.org.}
}
@article{XIAO2021100177,
title = {Ontology-Mediated SPARQL Query Answering over Knowledge Graphs},
journal = {Big Data Research},
volume = {23},
pages = {100177},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2020.100177},
url = {https://www.sciencedirect.com/science/article/pii/S2214579620300459},
author = {Guohui Xiao and Julien Corman},
abstract = {Ontology-Mediated Query Answering (OMQA) is a well-established framework to answer queries over Knowledge Graphs (KGs), enriched with rdfs or owl ontologies. OMQA was originally designed for Unions of Conjunctive Queries (UCQs), and based on certain answers. More recently, OMQA has been extended to sparql queries, but to our knowledge, none of the efforts made in this direction (either in the literature, or the so-called W3C sparql entailment regimes) is able to capture both certain answers for UCQs and the standard interpretation of sparql over a plain graph. We formalize these as requirements to be met by any semantics that aims at conciliating certain answers and sparql answers, and extend these with three additional requirements. Then we define two semantics that satisfies all requirements for sparql queries with select, union, join, and optional. Finally, we investigate the combined complexity of query answering under these semantics over a KG enriched with a DL-LiteR ontology, showing that for several fragments of sparql, known upper-bounds for query answering over a plain KG are matched.}
}
@article{KONJENGBAM201862,
title = {Aspect ontology based review exploration},
journal = {Electronic Commerce Research and Applications},
volume = {30},
pages = {62-71},
year = {2018},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2018.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1567422318300541},
author = {Anand Konjengbam and Neelesh Dewangan and Nagendra Kumar and Manish Singh},
keywords = {Electronic commerce, Review exploration, Opinion mining, Aspect ontology},
abstract = {User feedback in the form of customer reviews, blogs, and forum posts is an essential feature of e-commerce. Users often read online product reviews to get an insight into the quality of various aspects of a product. Besides, users have different aspect preferences, and they look for reviews that contain relevant information regarding their preferred aspect(s). However, as reviews are unstructured and voluminous, it becomes exhaustive and laborious for users to find relevant reviews. Lack of domain knowledge about various aspects and sub-aspects of a product, and how they are related to each other, also add to the problem. Although this information could be there in product reviews, it is not easy for users to spot it instantly from the reviews. This paper seeks to address the above problems and presents two novel algorithms that summarize product reviews, and provides an interactive search interface, similar to popular faceted navigation. We solve the problem by creating an aspect ontology tree with high aspect extraction precision.}
}
@article{CEYLAN2025104294,
title = {Explanations for query answers under existential rules},
journal = {Artificial Intelligence},
volume = {341},
pages = {104294},
year = {2025},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2025.104294},
url = {https://www.sciencedirect.com/science/article/pii/S000437022500013X},
author = {İsmail İlkan Ceylan and Thomas Lukasiewicz and Enrico Malizia and Andrius Vaicenavičius},
keywords = {Artificial intelligence, Computational complexity, Ontologies, Existential rules, Datalog+/–, Query answering, Ontology mediated query answering, Explanations},
abstract = {Ontology-based data access is an extensively studied paradigm aiming at improving query answers with the use of an “ontology”. An ontology is a specification of a domain of interest, which, in this context, is described via a logical theory. As a form of logical entailment, ontology-mediated query answering is fully interpretable, which makes it possible to derive explanations for ontological query answers. This is a quite important aspect, as the fact that many recent AI systems mostly operating as black boxes has led to some serious concerns. In the literature, various works on explanations in the context of description logics (DLs) have appeared, mostly focusing on explaining concept subsumption and concept unsatisfiability in the ontologies. Some works on explaining query entailment in DLs have appeared as well, however, mainly dealing with inconsistency-tolerant semantics and, actually, non-entailment of the queries. Surprisingly, explaining ontological query entailment has received little attention for ontology languages based on existential rules. In fact, although DLs are popular formalisms to model ontologies, it is generally agreed that rule-based ontologies are well-suited for data-intensive applications, as they allow us to conveniently deal with higher-arity relations, which naturally occur in standard relational databases. The goal of this work is to close this gap, and study the problem of explaining query entailment in the context of existential rules ontologies in terms of minimal subsets of database facts. We provide a thorough complexity analysis for several decision problems associated with minimal explanations for various classes of existential rules, and for different complexity measures.}
}
@article{TENGKUIZHAR2018310,
title = {Utilising ontology for “heteregeneous data analysis in organizational goals”},
journal = {International Journal of Web Information Systems},
volume = {15},
number = {3},
pages = {310-323},
year = {2018},
issn = {1744-0084},
doi = {https://doi.org/10.1108/IJWIS-05-2018-0046},
url = {https://www.sciencedirect.com/science/article/pii/S1744008418000150},
author = {Tengku Adil {Tengku Izhar} and Bernady O. Apduhan and Torab Torabi},
keywords = {Ontology, Data linkage, Dataset, Organizational goals ontology, Organizational goals},
abstract = {Purpose
The purpose of this paper is to assess the level of the organizational goal accomplishment by assessing the reliance relationship between organizational data and organizational goals.
Design/methodology/approach
The evaluation of the organizational goals is based on design and operational level, which can serve in ranking of the organizational goals achievement and hence assist the decision-making process in achieving the organizational goals. To achieve this aim, the authors propose an ontology to develop the relationship between organizational data and organizational goals.
Findings
Data goals dependency shows the dependency relationship between organizational data and organizational goals. At the same time, data goals dependency assists the process of identifying data attributes, where the authors suggest that these data attributes are relevant in relation to the organizational goals.
Originality/value
The contribution of this paper will serve as the first step to evaluate the relevance of organizational data to assist decision-making in relation to the organizational goals.}
}
@article{RIZZO2018340,
title = {Approximate classification with web ontologies through evidential terminological trees and forests},
journal = {International Journal of Approximate Reasoning},
volume = {92},
pages = {340-362},
year = {2018},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2017.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X17301019},
author = {Giuseppe Rizzo and Nicola Fanizzi and Claudia d'Amato and Floriana Esposito},
keywords = {Ontologies, Logic decision trees, Dempster–Shafer theory, Instance classification},
abstract = {In the context of the Semantic Web, assigning individuals to their respective classes is a fundamental reasoning service. It has been shown that, when purely deductive reasoning falls short, this problem can be solved as a prediction task to be accomplished through inductive classification models built upon the statistical evidence elicited from ontological knowledge bases. However also these data-driven alternative classification models may turn out to be inadequate when instances are unevenly distributed over the various targeted classes To cope with this issue, a framework based on logic decision trees and ensemble learning is proposed. The new models integrate the Dempster–Shafer theory with learning methods for terminological decision trees and forests. These enhanced classification models allow to explicitly take into account the underlying uncertainty due to the variety of branches to be followed up to classification leaves (in the context of a single tree) and/or to the different trees within the ensemble model (the forest). In this extended paper, we propose revised versions of the algorithms for learning Evidential Terminological Decision Trees and Random Forests considering alternative heuristics and additional evidence combination rules with respect to our former preliminary works. A comprehensive and comparative empirical evaluation proves the effectiveness and stability of the classification models, especially in the form of ensembles.}
}
@article{LASTRADIAZ2021101636,
title = {A large reproducible benchmark of ontology-based methods and word embeddings for word similarity},
journal = {Information Systems},
volume = {96},
pages = {101636},
year = {2021},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2020.101636},
url = {https://www.sciencedirect.com/science/article/pii/S0306437920301058},
author = {Juan J. Lastra-Díaz and Josu Goikoetxea and Mohamed Ali {Hadj Taieb} and Ana Garcia-Serrano and Mohamed {Ben Aouicha} and Eneko Agirre and David Sánchez},
keywords = {Ontology-based semantic similarity measures, Word embeddings, Information Content models, Reproducible benchmark, HESML, Reprozip},
abstract = {This work is a companion reproducibility paper of the experiments and results reported in Lastra-Diaz et al. (2019a), which is based on the evaluation of a companion reproducibility dataset with the HESML V1R4 library and the long-term reproducibility tool called Reprozip. Human similarity and relatedness judgements between concepts underlie most of cognitive capabilities, such as categorization, memory, decision-making and reasoning. For this reason, the research on methods for the estimation of the degree of similarity and relatedness between words and concepts has received a lot of attention in the fields of artificial intelligence and cognitive sciences. However, despite the huge research effort done, there is a lack of a self-contained, reproducible and extensible collection of benchmarks which being amenable to become a de facto standard for large scale experimentation in this line of research. In order to bridge this reproducibility gap, this work introduces a set of reproducible experiments on word similarity and relatedness by providing a detailed reproducibility protocol together with a set of software tools and a self-contained reproducibility dataset, which allow that all experiments and results in our aforementioned work to be reproduced exactly. Our aforementioned primary work introduces the largest, most detailed and reproducible experimental survey on word similarity and relatedness reported in the literature, which is based on the implementation of all evaluated methods into the same software platform. Our reproducible experiments evaluate most of methods in the families of ontology-based semantic similarity measures and word embedding models. We also detail how to extend our experiments to evaluate other unconsidered experimental setups. Finally, we provide a corrigendum for a mismatch in the MC28 similarity scores used in our original experiments.}
}
@article{FRASER2020102610,
title = {Doing ontopolitically-oriented research: Synthesising concepts from the ontological turn for alcohol and other drug research and other social sciences},
journal = {International Journal of Drug Policy},
volume = {82},
pages = {102610},
year = {2020},
issn = {0955-3959},
doi = {https://doi.org/10.1016/j.drugpo.2019.102610},
url = {https://www.sciencedirect.com/science/article/pii/S0955395919303172},
author = {Suzanne Fraser},
keywords = {Research methods, Stengers, Latour, Alcohol and other drug use, Ontological turn},
abstract = {The ontological turn has had a significant impact on the social sciences, including the social sciences of alcohol and other drug use. Work questioning the materiality of drugs, and the discourses of compulsion and dependence that co-constitute public understandings of drugs and their effects, is now relatively common in the field. In this article I discuss the new assumptions and methods informing the ontological turn, linking them together to produce an approach that synthesises what has become a fertile but rather piecemeal domain of critical drugs studies. In doing so, I identify and define to what I will term, following these intellectual trajectories, ‘ontopolitically-oriented research’ for the alcohol and other drug social sciences. This article will discuss two research projects: one that set out to generate new knowledge on lived experiences of addiction, and one that set out to rethink the standard illicit drug use safe injecting fitpack to better serve couples who inject together. The aim of this article will not be to report on project findings however. Instead it will provide a synthesis of research methods inspired by, and interpreted through, the ontological turn, using the projects as examples by considering them from the point of view of their ontological politics. As I will argue, the projects and their outcomes were fundamentally inspired by the insight that research not only explores and describes realities, it actively constitutes the realities it explores, playing a direct role in reconstituting realities through its conduct, outcomes and communications. I adopt the term ‘ontopolitically-oriented research’ to describe this approach. The analysis in this article will focus on the projects’ methods, describing the ways these methods were interpreted and implemented in ways best able to articulate and fulfil project aims. In concluding, the article will propose a set of features of ontopolitically-oriented research, as well as some observations on the steps, obstacles, priorities and pitfalls ontopolitically-oriented research may encounter in pursuing its aims.}
}
@article{HIPPOLYTE2021100263,
title = {A domain-agnostic ontology for unified metrology data management},
journal = {Measurement: Sensors},
volume = {18},
pages = {100263},
year = {2021},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2021.100263},
url = {https://www.sciencedirect.com/science/article/pii/S2665917421002269},
author = {Jean-Laurent Hippolyte and Michael Chrubasik and Frédéric Brochu and Maurizio Bevilacqua},
keywords = {Ontology, Semantic web, Metadata},
abstract = {Τhe National Physical Laboratory's (NPL) impact on science, economy and well-being largely relies on proper dissemination and exploitation of scientific evidence from data. Measurement data and software that generates it must therefore implement the Findable, Accessible, Interoperable and Reusable principles (FAIR), whilst enabling data traceability, uncertainty, intellectual property protection and compliance with legislation. Semantic web technologies are ideally suited to support such a unified measurement description through deeply ingrained knowledge sharing and data curation capabilities. This paper presents a semantic-based research approach to data and metadata management which aims to unify the organisation of scientific and engineering digital resources across NPL laboratories and engineering services through the development of a domain-agnostic measurement ontology (DAMO). The ontology development is elucidated using a tailored ontology engineering methodology (OEM) applying an agile and iterative approach based on previously published OEMs. Finally, the use of DAMO is demonstrated through three laboratory-based case studies.}
}
@article{JUCKETT201922,
title = {Concept detection using text exemplars aligned with a specialized ontology},
journal = {Data & Knowledge Engineering},
volume = {119},
pages = {22-35},
year = {2019},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X18301642},
author = {David A. Juckett and Eric P. Kasten and Fred N. Davis and Mark Gostine},
keywords = {Concept extraction, Exemplar matching, NLP, Ontology, Taxonomy, Progress notes, Pain medicine},
abstract = {Knowledge extraction from text documents requires identifying and classifying semantic content. Utilizing an appropriate domain ontology can facilitate this process if words and phrases can be linked to the classes and relationships within the ontology. This paper presents an exemplar-based algorithm to link text to semantically similar classes within an ontology constructed for the chronic pain medicine domain. Human annotators linked classes to text segments within a random document set for construction of an exemplar dictionary, which we examined for completeness using Zipf plot analysis. An algorithm was created to use this dictionary on previously unseen text to form a map between sentence text and probable class assignments. We performed a 5×5 cross-validation between human and algorithm annotations and examined both ROC and precision versus recall curves to show that the algorithm can identify the many medical and biopsychosocial components from the texts. We briefly describe a use case for detecting pain relief from various interventions utilizing the word-by-class maps. We conclude that an exemplar-based method can be a valuable tool in knowledge extraction from texts that share similar construction, such as medical progress notes.}
}
@article{ROSENBERG2021113734,
title = {“I don't know what home feels like anymore”: Residential spaces and the absence of ontological security for people returning from incarceration},
journal = {Social Science & Medicine},
volume = {272},
pages = {113734},
year = {2021},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2021.113734},
url = {https://www.sciencedirect.com/science/article/pii/S0277953621000666},
author = {Alana Rosenberg and Danya E. Keene and Penelope Schlesinger and Allison K. Groves and Kim M. Blankenship},
keywords = {Housing, Mass incarceration, Reentry, Ontological security, Surveillance, Residential space, Social determinants of health},
abstract = {Housing is central to health equity, and mass incarceration is an important but understudied aspect of housing vulnerability and health inequity. One way in which housing can be linked to health and health inequity is through ontological security. Ontological security, or a sense of feeling at home, is comprised of constancy, daily routines, privacy, and a basic security that enables the development of one's identity. It has been theorized as a mechanism by which people reap the health benefits of housing. Based on two waves of interviews in 2017–2018 with a sample of 27 people returning from incarceration in a northeast U.S. city, we describe participants' residential experiences during the first two years after release. Participants lived in residential group settings, with friends, partners and family, or were homeless. They experienced impermanence, punitive place rules, surveillance, and a lack of control. In contrast, participants spoke about their idea of home, imagined from the past or for the future, as a place of privacy, control, and wellbeing. This analysis expands the study of ontological security by detailing its absence among people returning from incarceration. The concept of ontological security holds promise in delineating the ways in which housing provides health benefits, and is particularly useful for understanding the needs and experiences of those returning from prison and seeking to restart their lives in the community. Relatedly, participant narratives point to the expansion of the carceral state beyond prison, including into residential space, with implications for the intersection of housing and health equity.}
}
@article{BUTT2018338,
title = {Transdisciplinary Engineering Design Education: Ontology for a Generic Product Design Process},
journal = {Procedia CIRP},
volume = {70},
pages = {338-343},
year = {2018},
note = {28th CIRP Design Conference 2018, 23-25 May 2018, Nantes, France},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.02.019},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118300842},
author = {Mehwish Butt and Alyona Sharunova and Mario Storga and Yasir Imtiaz Khan and Ahmed Jawad Qureshi},
keywords = {Design, Process, Transdisciplinary Engineering Design, Transdisciplinary Engineering Design Education Ontology, Taxonomy},
abstract = {Today’s highly integrated product development practices emphasize the need to transform the engineering education from disciplinary to transdisciplinary. This paper is based on the results of an empirical study designed to introduce a common transdisciplinary design process in engineering education. It aims to validate the hypothesis that engineering disciplines in education share a common engineering design process. It describes the methodology for the development of a Transdisciplinary Engineering Design Education Ontology (TEDEO) for eight major engineering disciplines. It proposes a high-level transdisciplinary engineering design process that consolidates a diverse array of engineering terms and concepts into a generalized model.}
}
@article{RECTOR2019100002,
title = {On beyond Gruber: “Ontologies” in today’s biomedical information systems and the limits of OWL},
journal = {Journal of Biomedical Informatics},
volume = {100},
pages = {100002},
year = {2019},
note = {Articles initially published in Journal of Biomedical Informatics: X 1-4, 2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.yjbinx.2019.100002},
url = {https://www.sciencedirect.com/science/article/pii/S2590177X19300010},
author = {Alan Rector and Stefan Schulz and Jean Marie Rodrigues and Christopher G Chute and Harold Solbrig},
keywords = {Knowledge representation, Ontology, Terminology, ICD, OWL, Description logics},
abstract = {The word “ontology” was introduced to information systems when only closed-world reasoning systems were available. It was “borrowed” from philosophy, but literal links to its philosophical meaning were explicitly disavowed. Since then, open-world reasoning systems based on description logics have been developed, OWL has become a standard, and philosophical issues have been raised. The result has too often been confusion. The question “What statements are ontological” receives a variety of answers. A clearer vocabulary that is better suited to today’s information systems is needed. The project to base ICD-11 on a “Common Ontology” required addressing this confusion. This paper sets out to systematise the lessons of that experience and subsequent discussions. We explore the semantics of open-world and closed-world systems. For specifying knowledge bases and software, we propose “invariants” or, more fully, “the first order invariant part of the background domain knowledge base” as an alternative to the words “ontology” and “ontological.” We discuss the role and limitations of OWL and description logics and how they are complementary to closed world systems such as frames and to less formal “knowledge organisation systems”. We illustrate why the conventions of classifications such as ICD cannot be formulated directly in OWL, but can be linked to OWL knowledge bases by queries. We contend that while OWL and description logics are major advances for representing invariants and terminologies, they must be combined with other technologies to represent broader background knowledge faithfully. The ICD-11 architecture is one approach. We argue that such hybrid architectures can and should be developed further.}
}
@article{KIM2020110695,
title = {Understanding and recommending security requirements from problem domain ontology: A cognitive three-layered approach},
journal = {Journal of Systems and Software},
volume = {169},
pages = {110695},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110695},
url = {https://www.sciencedirect.com/science/article/pii/S016412122030145X},
author = {Bong-Jae Kim and Seok-Won Lee},
keywords = {Security, Requirements engineering, Ontology},
abstract = {Socio-technical systems (STS) are inherently complex due to the heterogeneity of its intertwined components. Therefore, ensuring STS security continues to pose significant challenges. Persistent security issues in STS are extremely critical to address as threats to security can affect entire enterprises, resulting in significant recovery costs. A profound understanding of the problems across multiple dimensions of STS is the key in addressing such security issues. However, we lack a systematic acquisition of the scattered knowledge related to design, development, and execution of STS. In this work, we methodologically analyze security issues from a requirements engineering perspective. We propose a cognitive three-layered framework integrating various modeling methodologies and knowledge sources related to security. This framework helps in understanding essential components of security and making recommendations of security requirements regarding threat analyses and risk assessments using Problem Domain Ontology (PDO) knowledge base. We also provide tool support for our framework. With the goal-oriented security reference model, we demonstrate how security requirements are recommended based on PDO, with the help of the tool. The organized acquisition of knowledge from SME groups and the domain working group provides rich context of security requirements, and also enhances the re-usability of the knowledge set.}
}
@article{CASTANE2018373,
title = {An ontology for heterogeneous resources management interoperability and HPC in the cloud},
journal = {Future Generation Computer Systems},
volume = {88},
pages = {373-384},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.05.086},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17330467},
author = {Gabriel G. Castañé and Huanhuan Xiong and Dapeng Dong and John P. Morrison},
keywords = {Cloud interoperability, HPC in cloud, Resource management, Ontology, Self-management clouds},
abstract = {The ever-increasing number of customers that have been using cloud computing environments is driving heterogeneity in the cloud infrastructures. The incorporation of heterogeneous resources to traditional homogeneous infrastructures is supported by specific resource managers cohabiting with traditional resource managers. This blend of resource managers raises interoperability issues in the Cloud management domain as customer services are exposed to disjoint mechanisms and incompatibilities between APIs and interfaces. In addition, deploying and configuring HPC workloads in such environments makes porting HPC applications, from traditional cluster environments to the Cloud, complex and ineffectual. Many efforts have been taken to create solutions and standards for ameliorating interoperability issues in inter-cloud and multi-cloud environments and parallels exist between these efforts and the current drive for the adoption of heterogeneity in the Cloud. The work described in this paper attempts to exploit these parallels; managing interoperability issues in Cloud from a unified perspective. In this paper the mOSAIC ontology, pillar of the IEEE 2302 — Standard for Intercloud Interoperability and Federation, is extended towards creating the CloudLightning Ontology (CL-Ontology), in which the incorporation of heterogeneous resources and HPC environments in the Cloud are considered. To support the CL-Ontology, a generic architecture is presented as a driver to manage heterogeneity in the Cloud and, as a use case example of the proposed architecture, the internal architecture of the CloudLightning system is redesigned and presented to show the feasibility of incorporating a semantic engine to alleviate interoperability issues to facilitate the incorporation of HPC in Cloud.}
}
@article{FENZ2018551,
title = {Ontology-based information security compliance determination and control selection on the example of ISO 27002},
journal = {Information and Computer Security},
volume = {26},
number = {5},
pages = {551-567},
year = {2018},
issn = {2056-4961},
doi = {https://doi.org/10.1108/ICS-02-2018-0020},
url = {https://www.sciencedirect.com/science/article/pii/S2056496118000259},
author = {Stefan Fenz and Thomas Neubauer},
keywords = {Decision support systems, Compliance, Organizations, Risk management, security, Ontology},
abstract = {Purpose
The purpose of this paper is to provide a method to formalize information security control descriptions and a decision support system increasing the automation level and, therefore, the cost efficiency of the information security compliance checking process. The authors advanced the state-of-the-art by developing and applying the method to ISO 27002 information security controls and by developing a semantic decision support system.
Design/methodology/approach
The research has been conducted under design science principles. The formalized information security controls were used in a compliance/risk management decision support system which has been evaluated with experts and end-users in real-world environments.
Findings
There are different ways of obtaining compliance to information security standards. For example, by implementing countermeasures of different quality depending on the protection needs of the organization. The authors developed decision support mechanisms which use the formal control descriptions as input to support the decision-maker at identifying the most appropriate countermeasure strategy based on cost and risk reduction potential.
Originality/value
Formalizing and mapping the ISO 27002 controls to the security ontology enabled the authors to automatically determine the compliance status and organization-wide risk-level based on the formal control descriptions and the modelled environment, including organizational structures, IT infrastructure, available countermeasures, etc. Furthermore, it allowed them to automatically determine which countermeasures are missing to ensure compliance and to decrease the risk to an acceptable level.}
}
@article{VELOUDIS2019373,
title = {Achieving security-by-design through ontology-driven attribute-based access control in cloud environments},
journal = {Future Generation Computer Systems},
volume = {93},
pages = {373-391},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.08.042},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17320782},
author = {Simeon Veloudis and Iraklis Paraskakis and Christos Petsos and Yannis Verginadis and Ioannis Patiniotakis and Panagiotis Gouvas and Gregoris Mentzas},
keywords = {Context-aware security, Ontologies, Access control policies, Data privacy, Security-by-design, Semantic reasoning},
abstract = {The constantly increasing number of cyberattacks worldwide raise significant security concerns that generally deter small, medium and large enterprises from adopting the cloud paradigm and benefitting from the numerous advantages that it offers. One way to alleviate these concerns is to devise suitable policies that infuse adequate access controls into cloud services. However, the dynamicity inherent in cloud environments, coupled with the heterogeneous nature of cloud services, hinders the formulation of effective and interoperable access control policies that are suitable for the underlying domain of application. To this end, this work proposes an approach to the semantic representation of access control policies and, in particular, to the semantic representation of the context expressions incorporated in such policies. More specifically, the proposed approach enables stakeholders to accurately define the structure of their policies, in terms of relevant knowledge artefacts, and thus infuse into these policies their particular security and business requirements. This clearly leads to more effective policies, whilst it enables semantic reasoning about the abidance of policies by the prescribed structure. In order to alleviate the scalability concerns associated with semantic reasoning, the proposed approach introduces a reference implementation that extends XACML 3.0 with an expert system fused with reasoning capabilities through the incorporation of suitable meta-rules.}
}
@article{SHI201845,
title = {Evaluating an optimized backward chaining ontology reasoning system with innovative custom rules},
journal = {Information Discovery and Delivery},
volume = {46},
number = {1},
pages = {45-56},
year = {2018},
issn = {2398-6247},
doi = {https://doi.org/10.1108/IDD-10-2017-0070},
url = {https://www.sciencedirect.com/science/article/pii/S2398624718000158},
author = {Hui Shi and Dazhi Chong and Gongjun Yan},
keywords = {Semantic web, Benchmark, Ontology, Backward chaining reasoner, Innovative custom rules, Ontology reasoning system},
abstract = {Purpose
Semantic Web is an extension of the World Wide Web by tagging content with “meaning”. In general, question answering systems based on semantic Web face a number of difficult issues. This paper aims to design an experimental environment with custom rules and scalable data sets and evaluate the performance of a proposed optimized backward chaining ontology reasoning system. This study also compares the experimental results with other ontology reasoning systems to show the performance and scalability of this ontology reasoning system.
Design/methodology/approach
The authors proposed a semantic question answering system. This system has been built using ontological knowledge base including optimized backward chaining ontology reasoning system and custom rules. With custom rules, the proposed semantic question answering system will be able to answer questions that contain qualitative descriptors such as “groundbreaking” resesarch and “tenurable at university x”. Scalability has been one of the difficult issues faced by an optimized backward chaining ontology reasoning system and semantic question answering system. To evaluate the proposed ontology reasoning system, first, the authors design a number of innovative custom rule sets and corresponding query sets. The innovative custom rule sets and query sets will contribute to the future research on evaluating ontology reasoning systems as well. Then they design an experimental environment including ontologies and scalable data sets and metrics. Furthermore, they evaluate the performance of the proposed optimized backward chaining reasoning system on supporting custom rules. The evaluation results have been compared with other ontology reasoning systems as well.
Findings
The proposed innovative custom rules and query sets can be effectively employed for evaluating ontology reasoning systems. The evaluation results show that the scalability of the proposed backward chaining ontology reasoning system is better than in-memory reasoning systems. The proposed semantic question answering system can be integrated in sematic Web applications to solve scalability issues. For light weight applications, such as mobile applications, in-memory reasoning systems will be a better choice.
Originality/value
This paper fulfils an identified need for a study on evaluating an ontology reasoning system on supporting custom rules with and without external storage.}
}
@article{NAVARRO2018429,
title = {Leveraging ontologies and machine-learning techniques for malware analysis into Android permissions ecosystems},
journal = {Computers & Security},
volume = {78},
pages = {429-453},
year = {2018},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2018.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167404818302311},
author = {Luiz C. Navarro and Alexandre K.W. Navarro and André Grégio and Anderson Rocha and Ricardo Dahab},
keywords = {Malware, Android permissions, Ontology, Bags of graphs, Machine learning, Discriminant features},
abstract = {Smartphones form a complex application ecosystem with a myriad of components, properties, and interfaces that produce an intricate relationship network. Given the intrinsic complexity of this system, we hereby propose two main contributions. First, we devise a methodology to systematically determine and analyze the complex relationship network among components, properties, and interfaces associated with the permission mechanism in Android ecosystems. Second, we investigate whether it is possible to identify characteristics shared by malware samples at this high level of abstraction that could be leveraged to unveil their presence. We propose an ontology-based framework to model the relationships between application and system elements, together with a machine-learning approach to analyze the complex network that arises therefrom. We represent the ontological model for the considered Android ecosystem with 4570 apps through a graph with some 55,000 nodes and 120,000 edges. Experiments have shown that a classifier operating on top of this complex representation can achieve an accuracy of 88% and precision of 91% and is capable of identifying and determining 24 features that correspond to 70 important graph nodes related to malware activity, which is a remarkable feat for security.}
}
@incollection{BERNASCONI2025168,
title = {Conceptual Modeling for Bioinformatics},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {168-178},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00003-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027000038},
author = {Anna Bernasconi and Alberto {García S.}},
keywords = {Bioinformatics, Conceptual modeling, Data modeling, Database modeling, Database schema, Entity-Relationship diagram, Explanation, Ontological Models, Ontologies, Unified modeling language.},
abstract = {Conceptual Modeling is a standard practice in Database and Information Systems research community. It has also been applied to Bioinformatics-related problems, representing models as Entity-Relationship diagrams or with the Unified Modeling Language. Conceptual models serve two main purposes: explaining complex domains such as the genome and its biology, and supporting the effective design of data management systems for biological and genomics data. Several examples are available in the literature for both human genomics and viral genomics. Conceptual models have been enriched with ontological information when additional semantics was needed.}
}
@article{HABCHI2025105495,
title = {Advanced deep learning and large language models: Comprehensive insights for cancer detection},
journal = {Image and Vision Computing},
volume = {157},
pages = {105495},
year = {2025},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2025.105495},
url = {https://www.sciencedirect.com/science/article/pii/S0262885625000836},
author = {Yassine Habchi and Hamza Kheddar and Yassine Himeur and Adel Belouchrani and Erchin Serpedin and Fouad Khelifi and Muhammad E.H. Chowdhury},
keywords = {Cancer diagnosis, Federated learning, Transfer learning, Reinforcement learning, Transformer-based learning, Large language models},
abstract = {In recent years, the rapid advancement of machine learning (ML), particularly deep learning (DL), has revolutionized various fields, with healthcare being one of the most notable beneficiaries. DL has demonstrated exceptional capabilities in addressing complex medical challenges, including the early detection and diagnosis of cancer. Its superior performance, surpassing both traditional ML methods and human accuracy, has made it a critical tool in identifying and diagnosing diseases such as cancer. Despite the availability of numerous reviews on DL applications in healthcare, a comprehensive and detailed understanding of DL’s role in cancer detection remains lacking. Most existing studies focus on specific aspects of DL, leaving significant gaps in the broader knowledge base. This paper aims to bridge these gaps by offering a thorough review of advanced DL techniques, namely transfer learning (TL), reinforcement learning (RL), federated learning (FL), Transformers, and large language models (LLMs). These cutting-edge approaches are pushing the boundaries of cancer detection by enhancing model accuracy, addressing data scarcity, and enabling decentralized learning across institutions while maintaining data privacy. TL enables the adaptation of pre-trained models to new cancer datasets, significantly improving performance with limited labeled data. RL is emerging as a promising method for optimizing diagnostic pathways and treatment strategies, while FL ensures collaborative model development without sharing sensitive patient data. Furthermore, Transformers and LLMs, traditionally utilized in natural language processing (NLP), are now being applied to medical data for enhanced interpretability and context-based predictions. In addition, this review explores the efficiency of the aforementioned techniques in cancer diagnosis, it addresses key challenges such as data imbalance, and proposes potential solutions. It aims to be a valuable resource for researchers and practitioners, offering insights into current trends and guiding future research in the application of advanced DL techniques for cancer detection.}
}
@article{LE2024100052,
title = {The performance of large language models on fictional consult queries indicates favorable potential for AI-assisted vascular surgery consult handling},
journal = {JVS-Vascular Insights},
volume = {2},
pages = {100052},
year = {2024},
issn = {2949-9127},
doi = {https://doi.org/10.1016/j.jvsvi.2023.100052},
url = {https://www.sciencedirect.com/science/article/pii/S2949912723000491},
author = {Quang Le and Kedar S. Lavingia and Michael Amendola},
keywords = {Artificial intelligence, Consult, Delivery of care, Large language model, Vascular emergencies},
abstract = {Objective
Recently, the use of large language models (LLMs) in medicine has become a prominent topic of discussion due to the rapid improvement of these tools in understanding and responding to natural language. Several models are widely available to the public, both proprietary and open-sourced. We aim to evaluate the possible use of such LLMs in vascular surgery by understanding their abilities to process common consult requests.
Methods
The senior author created 25 fictional vascular surgery consultation queries based on common consultation requests. Five attending surgeons and four LLMs (GPT 3.5, GPT 4, Bard, and Falcon 40B) were asked to answer whether each consult was an emergency that needed immediate attention within an hour. Responders were also asked whether the next best step was an examination, additional imaging, or an urgent operation. GPT 3.5 and 4 also provided free-response answers on the next best step, graded by attending surgeons based on scientific accuracy, possible harm, and content completeness.
Results
The rates of accurate emergency identification were 88%, 100%, 76%, and 88% for GPT 3.5, GPT 4, Falcon 40B, and Bard, respectively. Although they have similar overall accuracy, GPT 3.5 has a high sensitivity at 100%, whereas Bard has a high specificity at 90%. GPT 4.0 had 100% sensitivity and specificity. LLMs agreed with the majority surgeon opinion on the next best step in 64% (GPT 3.5), 32% (GPT 4), 68% (Falcon 40B), and 36% (Bard) of cases. GPT 3.5 and 4 had a collective ratio of 89.5% of answers adhering to the scientific consensus. Only 5% of responses were highly likely to cause clinically significant harm. Although only 4% included incorrect content, 17.5% of answers missed important content. There was no significant difference between GPT 3.5 and 4 regarding the free-response grade.
Conclusions
Existing, widely available LLMs exhibited a solid ability to identify vascular emergencies, with GPT 4.0 agreeing with surgeon attendings in 100% of cases. However, these models continue to have identifiable deficiencies in treatment recommendations, a higher-level task. Future models might help triage incoming consults and provide preliminary management suggestions. The utility of such tools in clinical practice remains to be explored.}
}
@article{MICHEL2025104175,
title = {Seeing economic development like a large language model. A methodological approach to the exploration of geographical imaginaries in generative AI},
journal = {Geoforum},
volume = {158},
pages = {104175},
year = {2025},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2024.104175},
url = {https://www.sciencedirect.com/science/article/pii/S0016718524002367},
author = {Boris Michel and Yannick Ecker},
abstract = {The recent hype surrounding the disruptive potential of AI technologies in the form of large language models or text to image generators also raises questions for geographical research and practice. These questions include the power relations and inequalities inscribed in these systems, their significance for work and labor relations, their ecological and economic impact, but also the geographical and spatial imaginaries they reproduce. This article focuses on the latter and formulates a series of theoretical and methodological considerations for dealing with the output of these systems. As we assume that outputs generated by large language models will play an increasing role in the future, both in public and media discourses as well as in the discourses and practices of spatial planning and economic policy making, we consider it important to gain a critical understanding of these socio-technical systems. The empirical object of investigation of this paper is generated output that deals with questions of regional development and economic challenges in three European regions that are currently particularly affected by the transition to a climate-neutral economy and are designated by the European Union as Just Transition Fund Territories. We are particularly interested in how geographical imaginaries about these regions are formulated, how economic and social problems of these regions are presented and how this is translated into planning advice and development plans.}
}
@article{ALRUQIMI201915,
title = {Bridging the Gap between the Social and Semantic Web: Extracting domain-specific ontology from folksonomy},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {31},
number = {1},
pages = {15-21},
year = {2019},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2017.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S131915781730229X},
author = {Mohammed Alruqimi and Noura Aknin},
abstract = {Folksonomies have become very popular as means to organize large sets of resources shared over the Social Web. The bottom-up nature of folksonomies has proved to be an interesting alternative to the current effort at semantic web ontologies since folksonomies provide a rich terminology generated by large user-communities. Besides, ontologies extracted from folksonomies can represent the intelligence collective of social communities. Such ontologies also represent a core element of a new feature of the Web, the Internet of Things. Many research studies have captured semantics in folksonomies, some of which have developed ontologies from folksonomy. However, the formal specific-domain ontology consisting of domain-dependent relations has not been researched yet. This paper introduces an algorithm for deriving a domain-specific ontology from folksonomy tags. The proposed algorithm starts by collecting a domain-specific terminology; next, discovering a pre-defined set of conceptual relationships among the domain terminologies. The evaluation of the algorithm, using a dataset extracted from BibSonomy, demonstrated that the algorithm could effectively learn domain ontologies consisting of domain concepts linked by meaningful and high accurate relationships. Furthermore, the proposed algorithm can help reduce common issues related to tag ambiguity and synonymous tags.}
}
@article{PAREDESVALVERDE201890,
title = {An ontology-based approach with which to assign human resources to software projects},
journal = {Science of Computer Programming},
volume = {156},
pages = {90-103},
year = {2018},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2018.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167642318300042},
author = {Mario Andrés Paredes-Valverde and María del Pilar Salas-Zárate and Ricardo Colomo-Palacios and Juan Miguel Gómez-Berbís and Rafael Valencia-García},
keywords = {Software project, Ontologies, Semantic indexing, Assigning human resources},
abstract = {Human resources play a critical role in the success of software projects. Ensuring the correct assignment of them to a specific project is, therefore, an immediate requirement for Software development organizations. Within this context, this work explores the use of ontologies in the building of a decision support system that will help human resources managers or project leaders to select those employees who are best suited to participating in a new software development project. Ontologies allow the system to discover semantic relatedness among new and previous software projects by means of its requirements specification. The system can, therefore, suggest those people who have participated on similar projects. We have proved the effectiveness of our approach by conducting an evaluation in a software development organization. Our findings confirm the success of our approach and reveal that it may bring considerable benefits to the software development process.}
}
@article{SIVARAJKUMAR2024,
title = {An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/55318},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424000383},
author = {Sonish Sivarajkumar and Mark Kelley and Alyssa Samolyk-Mazzanti and Shyam Visweswaran and Yanshan Wang},
keywords = {large language model, LLM, LLMs, natural language processing, NLP, in-context learning, prompt engineering, evaluation, zero-shot, few shot, prompting, GPT, language model, language, models, machine learning, clinical data, clinical information, extraction, BARD, Gemini, LLaMA-2, heuristic, prompt, prompts, ensemble},
abstract = {Background
Large language models (LLMs) have shown remarkable capabilities in natural language processing (NLP), especially in domains where labeled data are scarce or expensive, such as the clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches.
Objective
The objective of this study is to assess the effectiveness of various prompt engineering techniques, including 2 newly introduced types—heuristic and ensemble prompts, for zero-shot and few-shot clinical information extraction using pretrained language models.
Methods
This comprehensive experimental study evaluated different prompt types (simple prefix, simple cloze, chain of thought, anticipatory, heuristic, and ensemble) across 5 clinical NLP tasks: clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, and medication attribute extraction. The performance of these prompts was assessed using 3 state-of-the-art language models: GPT-3.5 (OpenAI), Gemini (Google), and LLaMA-2 (Meta). The study contrasted zero-shot with few-shot prompting and explored the effectiveness of ensemble approaches.
Results
The study revealed that task-specific prompt tailoring is vital for the high performance of LLMs for zero-shot clinical NLP. In clinical sense disambiguation, GPT-3.5 achieved an accuracy of 0.96 with heuristic prompts and 0.94 in biomedical evidence extraction. Heuristic prompts, alongside chain of thought prompts, were highly effective across tasks. Few-shot prompting improved performance in complex scenarios, and ensemble approaches capitalized on multiple prompt strengths. GPT-3.5 consistently outperformed Gemini and LLaMA-2 across tasks and prompt types.
Conclusions
This study provides a rigorous evaluation of prompt engineering methodologies and introduces innovative techniques for clinical information extraction, demonstrating the potential of in-context learning in the clinical domain. These findings offer clear guidelines for future prompt-based clinical NLP research, facilitating engagement by non-NLP experts in clinical NLP advancements. To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative artificial intelligence, and we hope that it will inspire and inform future research in this area.}
}
@article{MONSEN2018e210,
title = {Use of the Omaha System for ontology-based text mining to discover meaning within CaringBridge social media journals},
journal = {Kontakt},
volume = {20},
number = {3},
pages = {e210-e216},
year = {2018},
issn = {1212-4117},
doi = {https://doi.org/10.1016/j.kontakt.2018.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S1212411718300072},
author = {Karen A. Monsen and Sasank Maganti and Robert A. Giaquinto and Michelle A. Mathiason and Ragnhildur I. Bjarnadottir and Mary Jo Kreitzer},
keywords = {Text mining, Social media, Ontology, Omaha System, CaringBridge, Terminology},
abstract = {Objectives
The goals of this study were to examine the feasibility of using ontology-based text mining with CaringBridge social media journal entries in order to understand journal content from a whole-person perspective. Specific aims were to describe Omaha System problem concept frequencies in the journal entries over a four-step process overall, and relative to Omaha System Domains; and to examine the four step method including the use of standardized terms and related words.
Design
Ontology-based retrospective observational feasibility study using text mining methods.
Sample
A corpus of social media text consisting of 13,757,900 CaringBridge journal entries from June 2006 to June 2016.
Measures
The Omaha System terms, including problems and signs/symptoms, were used as the foundational lexicon for this study. Development of an extended lexicon with related words for each problem concept expanded the semantics-powered data analytics approach to reflect consumer word choices.
Results
All Omaha System problem concepts were identified in the journal entries, with consistent representation across domains. The approach was most successful when common words were used to represent clinical terms. Preliminary validation of journal examples showed appropriate representation of the problem concepts.
Conclusions
This is the first study to evaluate the feasibility of using an interface terminology and ontology (the Omaha System) as a text mining information model. Further research is needed to systematically validate these findings, refine the process as needed to advance the study of CaringBridge content, and extend the use of this method to other consumer-generated journal entries and terminologies.}
}
@article{KOSONOCKY20241150,
title = {Mining patents with large language models elucidates the chemical function landscape††Electronic supplementary information (ESI) available. See DOI: https://doi.org/10.1039/d4dd00011k},
journal = {Digital Discovery},
volume = {3},
number = {6},
pages = {1150-1159},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d4dd00011k},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24000925},
author = {Clayton W. Kosonocky and Claus O. Wilke and Edward M. Marcotte and Andrew D. Ellington},
abstract = {The fundamental goal of small molecule discovery is to generate chemicals with target functionality. While this often proceeds through structure-based methods, we set out to investigate the practicality of methods that leverage the extensive corpus of chemical literature. We hypothesize that a sufficiently large text-derived chemical function dataset would mirror the actual landscape of chemical functionality. Such a landscape would implicitly capture complex physical and biological interactions given that chemical function arises from both a molecule's structure and its interacting partners. To evaluate this hypothesis, we built a Chemical Function (CheF) dataset of patent-derived functional labels. This dataset, comprising 631 K molecule–function pairs, was created using an LLM- and embedding-based method to obtain 1.5 K unique functional labels for approximately 100 K randomly selected molecules from their corresponding 188 K unique patents. We carry out a series of analyses demonstrating that the CheF dataset contains a semantically coherent textual representation of the functional landscape congruent with chemical structural relationships, thus approximating the actual chemical function landscape. We then demonstrate through several examples that this text-based functional landscape can be leveraged to identify drugs with target functionality using a model able to predict functional profiles from structure alone. We believe that functional label-guided molecular discovery may serve as an alternative approach to traditional structure-based methods in the pursuit of designing novel functional molecules.}
}
@article{UMBRICO20201097,
title = {An Ontology for Human-Robot Collaboration},
journal = {Procedia CIRP},
volume = {93},
pages = {1097-1102},
year = {2020},
note = {53rd CIRP Conference on Manufacturing Systems 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.04.045},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120306107},
author = {Alessandro Umbrico and Andrea Orlandini and Amedeo Cesta},
keywords = {Human-Robot Collaboration, Artificial Intelligence, Formal Ontology},
abstract = {The diffusion of Human-Robot Collaborative cells is prevented by some barriers. Classical control approaches seem not yet fully suitable for facing variability conveyed by the presence of human operators beside robots. Heterogeneous knowledge representation capabilities and abstract reasoning are crucial to enhance flexibility of control solutions. This work presents SOHO (Sharework Ontology for Human Robot Collaboration), a novel ontology specifically designed for Human-Robot Collaboration. The paper describes the pursued context-based approach, the novelty of the designed ontology with respect to the state of the art and shows its validity in a realistic Human-Robot Collaboration scenario.}
}
@article{ALMASRI2025101630,
title = {SOLAR: Illuminating LLM performance in API discovery and service ranking for edge AI and IoT},
journal = {Internet of Things},
volume = {32},
pages = {101630},
year = {2025},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2025.101630},
url = {https://www.sciencedirect.com/science/article/pii/S2542660525001441},
author = {Eyhab Al-Masri and Ishwarya Narayana Subramanian},
keywords = {Large language models, Web service discovery, API ranking, Service-orinted computing, API selection, AI for Web Services, LLM benchmarking, Ranking consistency, AI models},
abstract = {The growing complexity of web service and API discovery calls for robust methods to evaluate how well Large Language Models (LLMs) retrieve, rank, and assess APIs. However, current LLMs often produce inconsistent results, highlighting the need for structured, multi-dimensional evaluation. This paper introduces SOLAR (Systematic Observability of LLM API Retrieval), a framework that assesses LLM performance across three key dimensions: functional capability, implementation feasibility, and service sustainability. We evaluate four leading LLMs—GPT-4 Turbo (OpenAI), Claude 3.5 Sonnet (Anthropic), LLaMA 3.2 (Meta), and Gemini 2.0 Flash (Google)—on their ability to identify, prioritize, and evaluate APIs across varying query complexities. Results show GPT-4 Turbo and Claude 3.5 Sonnet achieve high functional alignment (FCA ≥ 0.75 for simple queries) and strong ranking consistency (Spearman’s ρ ≈ 0.95). However, all models struggle with implementation feasibility and long-term sustainability, with feasibility scores declining as complexity increases and sustainability scores remaining low (SSI ≈ 0.40), limiting deployment potential. Despite retrieving overlapping APIs, models often rank them inconsistently, raising concerns for AI-driven service selection. SOLAR identifies strong correlations between functional accuracy and ranking stability but weaker links to real-world feasibility and longevity. These findings are particularly relevant for Edge AI environments, where real-time processing, distributed intelligence, and reliable API integration are critical. SOLAR offers a comprehensive lens for evaluating LLM effectiveness in service discovery, providing actionable insights to advance robust, intelligent API integration across IoT and AI-driven systems. Our work aims to inform both future model development and deployment practices in high-stakes computing environments.}
}
@article{LUO2024100488,
title = {Large language model-based code generation for the control of construction assembly robots: A hierarchical generation approach},
journal = {Developments in the Built Environment},
volume = {19},
pages = {100488},
year = {2024},
issn = {2666-1659},
doi = {https://doi.org/10.1016/j.dibe.2024.100488},
url = {https://www.sciencedirect.com/science/article/pii/S2666165924001698},
author = {Hanbin Luo and Jianxin Wu and Jiajing Liu and Maxwell Fordjour Antwi-Afari},
keywords = {Construction assembly robot, Large language model, Code generation, , Human–robot collaboration},
abstract = {Offline programming (OLP) is a mainstream approach for controlling assembly robots at construction sites. However, existing methods are tailored to specific assembly tasks and workflows, and thus lack flexibility. Additionally, the emerging large language model (LLM)-based OLP cannot effectively handle the code logic of robot programming. Thus, this paper addresses the question: How can robot control programs be generated effectively and accurately for diverse construction assembly tasks using LLM techniques? This paper describes a closed user-on-the-loop control framework for construction assembly robots based on LLM techniques. A hierarchical strategy to generate robot control programs is proposed to logically integrate code generation at high and low levels. Additionally, customized application programming interfaces and a chain of action are combined to enhance the LLM's understanding of assembly action logic. An assembly task set was designed to evaluate the feasibility and reliability of the proposed approach. The results show that the proposed approach (1) is widely applicable to diverse assembly tasks, and (2) can improve the quality of the generated code by decreasing the number of errors. Our approach facilitates the automation of construction assembly tasks by simplifying the robot control process.}
}
@article{CHIZHIKOVA2025108515,
title = {Automatic TNM staging of colorectal cancer radiology reports using pre-trained language models},
journal = {Computer Methods and Programs in Biomedicine},
volume = {259},
pages = {108515},
year = {2025},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108515},
url = {https://www.sciencedirect.com/science/article/pii/S016926072400508X},
author = {Mariia Chizhikova and Pilar López-Úbeda and Teodoro Martín-Noguerol and Manuel C. Díaz-Galiano and L. Alfonso Ureña-López and Antonio Luna and M. Teresa Martín-Valdivia},
keywords = {Colorectal cancer staging, NLP, TNM, Text classification, Transformer, Language models},
abstract = {Background and Objective:
Colorectal cancer is one of the major causes of cancer death worldwide. Essential for prognosis and treatment planning, TNM staging offers critical insights into the advancement of colorectal cancer. However, manual TNM staging from colon magnetic resonance imaging (MRI) is a laborious and error prone process. This study introduces an automated text classification system for TNM staging of colon MRI images in Spanish.
Methods:
A dataset of 1319 Spanish colon MRI reports was collected and manually labeled with TNM staging. In order to automate the task of TNM staging, a multimodal system was proposed. The system is based on RoBERTa language model pre-trained on a combination of biomedical and clinical Spanish language corpora and uses Natural Language Processing (NLP) techniques to extract relevant categorical and numerical features from MRI reports.
Results:
The performance of the system was evaluated using different metrics and the results obtained are very promising: the best performance among the proposed systems reached 0.7464, 0.8792 and 0.6776 of macro F1-score for T, N and M respectively.
Conclusions:
This study demonstrates the feasibility of using a language model for automatic TNM staging based on Spanish clinical reports of colorectal cancer patients. The proposed system can be a useful tool to improve the efficiency and accuracy of colorectal cancer diagnosis.}
}
@article{QAZI201875,
title = {An Ontology-based Term Weighting Technique for Web Document Categorization},
journal = {Procedia Computer Science},
volume = {133},
pages = {75-81},
year = {2018},
note = {International Conference on Robotics and Smart Manufacturing (RoSMa2018)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918309542},
author = {Aijazahamed Qazi and R.H. Goudar},
keywords = {Ontology, Term occurrence, Classification, Confusion matrix},
abstract = {Web document classification has become crucial as there has been a massive increase in the magnitude of web pages across the web. In the research community, an efficient approach to this problem is based on machine learning techniques. Ontology forms the heart of knowledge representation for any domain. This paper proposes an ontology-based term weighting technique which is novel and efficient for the classification of web pages. The proposed approach builds domain ontology and selects the features that significantly improve the prediction performance. Experiments were conducted on domain based web pages and classification performance was calculated with state of the art classification algorithms. The experimental analysis demonstrates that the proposed approach produces significantly better results compared to the traditional keyword-based approaches.}
}
@article{VALENTINE2020102879,
title = {Commentary on Alex Stevens (2020) Critical realism and the ‘ontological politics of drug policy’},
journal = {International Journal of Drug Policy},
volume = {84},
pages = {102879},
year = {2020},
issn = {0955-3959},
doi = {https://doi.org/10.1016/j.drugpo.2020.102879},
url = {https://www.sciencedirect.com/science/article/pii/S0955395920302115},
author = {kylie valentine and Kate Seear}
}
@article{TRAPPEY201819,
title = {Construction and validation of an ontology-based technology function matrix: Technology mining of cyber physical system patent portfolios},
journal = {World Patent Information},
volume = {55},
pages = {19-24},
year = {2018},
note = {Advanced Analytics of Intellectual Property Information for TechMining},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2018.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0172219018300139},
author = {Amy J.C. Trappey and Charles V. Trappey and Usharani Hareesh Govindarajan and Allen C.C. Jhuang},
keywords = {Technology function matrix (TFM), Cyber Physical System (CPS), Patent analysis, Patent portfolio},
abstract = {This research develops a computer-supported ontology-based Technology Function Matrix (TFM) construction method, called eTFM, as an approach to reduce technology mining man-power and enhance the accuracy and consistency of patent analysis results. The paper addresses a rarely discussed issue of the TFM validation. The proposed validation approach compares the TFMs construction based on both on the domain ontology and the International Patent Classification (IPC) classes. The research demonstrates the methodology's practical applications using the patent analysis case of cyber physical system (CPS), an essential core technology enabling advanced manufacturing and Industry 4.0.}
}
@article{KOUTSOMITROPOULOS2019157,
title = {Semantic annotation and harvesting of federated scholarly data using ontologies},
journal = {Digital Library Perspectives},
volume = {35},
number = {34},
pages = {157-171},
year = {2019},
issn = {2059-5816},
doi = {https://doi.org/10.1108/DLP-12-2018-0038},
url = {https://www.sciencedirect.com/science/article/pii/S2059581619000084},
author = {Dimitrios A. Koutsomitropoulos},
keywords = {Thesauri, Open educational repositories, Ontologies, Learning objects, Subject classification, Keywords expansion},
abstract = {Purpose
Effective synthesis of learning material is a multidimensional problem, which often relies on handpicking approaches and human expertise. Sources of educational content exist in a variety of forms, each offering proprietary metadata information and search facilities. This paper aims to show that it is possible to harvest scholarly resources from various repositories of open educational resources (OERs) in a federated manner. In addition, their subject can be automatically annotated using ontology inference and standard thematic terminologies.
Design/methodology/approach
Based on a semantic interpretation of their metadata, authors can align external collections and maintain them in a shared knowledge pool known as the Learning Object Ontology Repository (LOOR). The author leverages the LOOR and show that it is possible to search through various educational repositories’ metadata and amalgamate their semantics into a common learning object (LO) ontology. The author then proceeds with automatic subject classification of LOs using keyword expansion and referencing standard taxonomic vocabularies for thematic classification, expressed in SKOS.
Findings
The approach for automatic subject classification simply takes advantage of the implicit information in the searching and selection process and combines them with expert knowledge in the domain of reference (SKOS thesauri). This is shown to improve recall by a considerable factor, while precision remains unaffected.
Originality/value
To the best of the author’s knowledge, the idea of subject classification of LOs through the reuse of search query terms combined with SKOS-based matching and expansion has not been investigated before in a federated scholarly setting.}
}
@article{BENEDIKT201852,
title = {Logical foundations of information disclosure in ontology-based data integration},
journal = {Artificial Intelligence},
volume = {262},
pages = {52-95},
year = {2018},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370218303060},
author = {Michael Benedikt and Bernardo {Cuenca Grau} and Egor V. Kostylev},
keywords = {Knowledge representation and reasoning, Ontologies, Ontology-based data access, Data integration, Query answering, Data privacy},
abstract = {Ontology-based data integration systems allow users to effectively access data sitting in multiple sources by means of queries over a global schema described by an ontology. In practice, data sources often contain sensitive information that the data owners want to keep inaccessible to users. Our aim in this paper is to lay the logical foundations of information disclosure in ontology-based data integration. Our focus is on the semantic requirements that a data integration system should satisfy before it is made available to users for querying, as well as on the computational complexity of checking whether such requirements are fulfilled. In particular, we formalise and study the problem of determining whether a given data integration system discloses a source query to an attacker. We consider disclosure on a particular dataset, and also whether a schema admits a dataset on which disclosure occurs. We provide matching lower and upper complexity bounds on disclosure analysis, in the process introducing a number of techniques for analysing logical privacy issues in ontology-based data integration.}
}
@article{MAKSIMOV202070,
title = {Ontology of Properties and its Methods of Use: Properties and Unit extraction from texts},
journal = {Procedia Computer Science},
volume = {169},
pages = {70-75},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.116},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920302398},
author = {Nikolay Maksimov and Anastasia Gavrilkina and Victoriy Kuzmina and Ekaterina Borodina},
keywords = {Ontology, properties identification, text processing, fact extraction},
abstract = {Methods of automatic extraction and identification of properties, quantities and units of measurement from texts are considered. The developed ontology of properties and units of measurement includes both fundamental connections of properties and units of measurement and connections that show the formation of concepts. The technology of property identification is based on fact that semantically significant text elements extracted within the boundaries of one or several sentences forming the semantic neighborhood of the property are correlated with the corresponding components of the ontology, which allows to restore the missing semantic fragments or identify discrepancies in the designations. The results of experimental studies of the effectiveness of the developed tools are presented.}
}
@article{CHAKIRI2020171,
title = {A data warehouse hybrid design framework using domain ontologies for local good-governance assessment},
journal = {Transforming Government: People, Process and Policy},
volume = {14},
number = {2},
pages = {171-203},
year = {2020},
issn = {1750-6166},
doi = {https://doi.org/10.1108/TG-04-2019-0025},
url = {https://www.sciencedirect.com/science/article/pii/S1750616620000074},
author = {Houda Chakiri and Mohammed {El Mohajir} and Nasser Assem},
keywords = {e-Government, Decision making, Public administration, OWL, Ontology, Municipality, Domain ontology, Data warehouse, Good-governance},
abstract = {Purpose
Most local governance assessment tools are entirely or partially based on stakeholders’ surveys, focus groups and benchmarks of different local governments in the world. These tools remain a subjective way of local governance evaluation. To measure the performance of local good-governance using an unbiased assessment technique, the authors have developed a framework to help automate the design process of a data warehouse (DW), which provides local and central decision-makers with factual, measurable and accurate local government data to help assess the performance of local government. The purpose of this paper is to propose the extraction of the DW schema based on a mixed approach that adopts both i* framework for requirements-based representation and domain ontologies for data source representation, to extract the multi-dimensional (MD) elements. The data was collected from various sources and information systems (ISs) deployed in different municipalities.
Design/methodology/approach
The authors present a framework for the design and implementation of a DW for local good-governance assessment. The extraction of facts and dimensions of the DW’s MD schema is done using a hybrid approach, where the extraction of requirement-based DW schema and source-based DW schema are done in parallel followed by the reconciliation of the obtained schemas to obtain the good-governance assessment DW final design.
Findings
The authors developed a novel framework to design and implement a DW for local good-governance assessment. The framework enables the extraction of the DW MD schema by using domain ontologies to help capture semantic artifacts and minimize misconceptions and misunderstandings between different stakeholders. The introduction and use of domain ontologies during the design process serves the generalization and automation purpose of the framework.
Research limitations/implications
The presently conducted research faced two main limitations as follows: the first is the full automation of the design process of the DW and the second, and most important, is access to local government data as it remains limited because of the lack of digitally stored data in municipalities, especially in developing countries in addition to the difficulty of accessing the data because of regulatory aspects and bureaucracy.
Practical implications
The local government environment is among the public administrations most subject to change-adverse cultures and where the authors can face high levels of resistance and significant difficulties during the implementation of decision support systems, despite the commitment/engagement of decision-makers. Access to data sources stored by different ISs might be challenging. While approaching the municipalities for data access, it was done in the framework of a research project within one of the most notorious universities in the country, which gave more credibility and trust to the research team. There is also a need for further testing of the framework to reveal its scalability and performance characteristics.
Originality/value
Compared to other local government assessment ad hoc tools that are partially or entirely based on subjectively collected data, the framework provides a basis for automated design of a comprehensive local government DW using e-government domain ontologies for data source representation coupled with the goal, rationale and business process diagrams for user requirements representations, thus enabling the extraction of the final DW MD schema.}
}
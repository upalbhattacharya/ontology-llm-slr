@article{LUONG2025104160,
title = {ConceptUML: Multiphase unsupervised threat detection via latent concept learning, Hidden Markov Models and topic modelling},
journal = {Journal of Information Security and Applications},
volume = {93},
pages = {104160},
year = {2025},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2025.104160},
url = {https://www.sciencedirect.com/science/article/pii/S2214212625001978},
author = {Khanh Luong and Arash Mahboubi and Geoff Jarrad and Seyit Camtepe and Michael Bewong and Mohammed Bahutair and Hamed Aboutorab and Hang Thanh Bui},
keywords = {Threat hunting, Lateral movement, MITRE, CAPEC, Concept extraction, Hidden Markov Model, Topic modelling},
abstract = {Detecting lateral movement threats in large-scale system logs is a critical challenge due to the scarcity of labelled attack data, the presence of imbalanced datasets, and the sophisticated nature of modern adversaries. To address these issues, we propose ConceptUML, a semantic-driven, fully unsupervised threat detection framework designed to automatically identify anomalies related to lateral movement in heterogeneous log data. ConceptUML is structured around a three-phase architecture. In Phase 1 (Latent Semantic Learning), contextualized embeddings generated by Sentence-BERT are combined with Non-negative Matrix Factorization to extract abstract concepts from system logs and external threat intelligence sources such as MITRE ATT&CK and CAPEC. In Phase 2 (Unsupervised Threat Detection), a Hidden Markov Model is applied to cluster logs based on learned concepts, and each cluster is scored according to its semantic similarity to known adversarial techniques. Phase 3 (Decision Refinement) uses topic modelling to further isolate malicious event log subsets from within suspicious clusters, enabling high-precision triage. We evaluate ConceptUML using four real-world event log datasets, including Windows Event Logs and multiple subsets of the LMD-23 dataset, encompassing attacks such as exploitation of hashing techniques and remote services. The enhanced model with topic modelling achieves up to 92.54% detection quality and reduces detection error to as low as 8.14%, outperforming several baseline approaches including AutoEncoder, LogAnomaly, LOF, and DBScan. Our results confirm that ConceptUML delivers interpretable, scalable, and highly effective detection of lateral movement threats without requiring labelled training data or extensive manual feature engineering.}
}
@article{ZACK2025100246,
title = {Artificial Intelligence and Multi-Omics in Pharmacogenomics: A New Era of Precision Medicine},
journal = {Mayo Clinic Proceedings: Digital Health},
volume = {3},
number = {3},
pages = {100246},
year = {2025},
issn = {2949-7612},
doi = {https://doi.org/10.1016/j.mcpdig.2025.100246},
url = {https://www.sciencedirect.com/science/article/pii/S2949761225000537},
author = {Mike Zack and Danil N. Stupichev and Alex J. Moore and Ioan D. Slobodchikov and David G. Sokolov and Igor F. Trifonov and Allan Gobbs},
abstract = {Pharmacogenomics is entering a transformative phase as high-throughput “omics” techniques become increasingly integrated with state-of-the-art artificial intelligence (AI) methods. Although early successes in single-gene pharmacogenetics reported clear clinical benefits, many drug response phenotypes are governed by intricate networks of genomic variants, epigenetic modifications, and metabolic pathways. Multi-omics approaches address this complexity by capturing genomic, transcriptomic, proteomic, and metabolomic data layers, offering a comprehensive view of patient-specific biology. Advanced AI models, including deep neural networks, graph neural networks, and representation learning techniques, further enhance this landscape by detecting hidden patterns, filling gaps in incomplete data sets, and enabling in silico simulations of treatment responses. Such capabilities not only improve predictive accuracy but also deepen mechanistic insights, revealing how gene–gene and gene–environment interactions shape therapeutic outcomes. At the same time, real-world data from diverse patient populations is broadening the evidence base, underscoring the importance of inclusive datasets and population-specific algorithms to reduce health disparities. Despite challenges related to data harmonization, interpretability, and regulatory oversight, the synergy between multi-omics integration and AI-driven analytics holds relevant promise for revolutionizing clinical decision-making. In this review, we highlighted key technological advances, discussed current limitations, and outlined future directions for translating multi-omics plus AI innovations into routine personalized medicine.}
}
@article{GERANI2019302,
title = {Modeling content and structure for abstractive review summarization},
journal = {Computer Speech & Language},
volume = {53},
pages = {302-331},
year = {2019},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2016.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0885230816301917},
author = {Shima Gerani and Giuseppe Carenini and Raymond T. Ng},
keywords = {Sentiment, Summarization, Rhetorical Structure, ConceptNet},
abstract = {Reviews are valuable sources of information for many important decision making tasks. Summarizing the massive amount of reviews, which are available these days on many entities and services, is critical to help users better digest the sentiment about an entity or a service and its aspects (i.e. features of the entity or the service). This article presents a novel aspect-based summarization framework that generates an abstract from multiple reviews of an entity without the need for a handcrafted feature taxonomy or any training data. We generate summaries using Natural Language Generation (NLG) by taking into account the importance of aspects, as well as the association between them. We model these information in the form of a tree, called Aspect Hierarchy Tree (AHT), in which nodes indicate the important aspects and edges indicate the relationship between them. We propose and investigate three alternative content selection and structuring models for the automatic construction of an AHT in our summarization framework: 1) Rhetorical model, which captures the aspects' importance and relationship by looking at the way people discuss and relate the aspects when expressing opinion in their reviews. 2) Conceptual model, which exploits a common-sense knowledge base (e.g. ConceptNet) to find the conceptual association between aspects. 3) Hybrid model, which exploits both the rhetorical and conceptual information. Our abstractive summarization framework has the potential to implement one of the proposed models dependingon the application or apply all three models and let a user choose the output, depending on his/her desire to use the conceptual, rhetorical or both sources of information. Quantitative and qualitative analysis on the resulting AHTs of the three content selection and structuring models for seven entities in three domains shows that the three models generate AHTs that differ in interesting ways in terms of both content (i.e. selected aspects to be included in the summary) and structure (i.e. the relation between aspects).}
}
@incollection{HOWITT202099,
title = {Land Rights},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {99-104},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10477-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955104779},
author = {Richard Howitt},
keywords = {Dispossession, Indigenous peoples, Indigenous rights, Land, Racism, Self-determination, Settler colonialism, Terra nullius, United nations declaration on the rights of indigenous peoples},
abstract = {Land rights has been a focal point of the struggles of Indigenous peoples in many jurisdictions. These struggles have deeply marked intercultural relations between Indigenous and settler populations globally. This article considers the interplay of legal, social, and political discourses in delivering and/or denying Indigenous peoples' rights to their traditional territories. Drawing largely on the experiences of former British colonies, this article considers the wider significance of Indigenous ontologies and recognizes that legal argument focused on securing legal property titles is, at best, simply a first step in rethinking the implications of Indigenous ontologies for geographical scholarship and application more broadly. It concludes that limited focus on questions of land rights in isolation from broader issues of human rights, social justice, and sustainable human–environmental relations is unhelpful.}
}
@article{WANG2023101569,
title = {Detecting interdisciplinary semantic drift for knowledge organization based on normal cloud model},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {6},
pages = {101569},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101569},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823001234},
author = {Zhongyi Wang and Siyuan Peng and Jiangping Chen and Amoni G. Kapasule and Haihua Chen},
keywords = {Interdisciplinary semantic drift, Knowledge representation, Normal cloud model, Knowledge potential energy, Knowledge potential difference},
abstract = {To reduce the conceptual ambiguity in interdisciplinary knowledge organization systems (KOSs) and enhance interdisciplinary KOS management, this paper proposes a framework for interdisciplinary semantic drift (ISD) detection based on the normal cloud model (NCM). In this framework, we first analyze the features of interdisciplinary concepts and propose a novel interdisciplinary concept extraction method based on cross-discipline statistical information. Secondly, the high-performance knowledge representation model NCM is adopted to represent each interdisciplinary concept with uncertainty, and then a new ISD degree calculation method is proposed based on the similarity cloud algorithm. Thirdly, to identify the direction of ISD after the degree calculation, we propose an ISD direction identification method according to the theory of knowledge potential energy (KPE). Fourthly, based on the above procedure, we propose an ISD detection algorithm to identify and visualize the ISD process. Finally, we evaluate the proposed framework on the concept of “information entropy” and compare the performance with three baselines. Experimental results demonstrate that our framework outperforms[ all the baselines, and the result is comparable to experts’ judgments (0.808 on Spearman correlation, p<0.001). The research indicates the meaning of an interdisciplinary concept will drift from the high KPE discipline to the low KPE discipline as long as interdisciplinary knowledge potential differences (KPD) exist between these two related disciplines. We further identify three key factors that affect the degree of ISD: the length of the discipline chain of an interdisciplinary concept transfer, the number of source disciplines that an interdisciplinary concept comes from, and the knowledge distance between the source discipline and the target discipline.}
}
@article{KANG2020,
title = {Building a Pharmacogenomics Knowledge Model Toward Precision Medicine: Case Study in Melanoma},
journal = {JMIR Medical Informatics},
volume = {8},
number = {10},
year = {2020},
issn = {2291-9694},
doi = {https://doi.org/10.2196/20291},
url = {https://www.sciencedirect.com/science/article/pii/S2291969420000265},
author = {Hongyu Kang and Jiao Li and Meng Wu and Liu Shen and Li Hou},
keywords = {pharmacogenomics, knowledge model, BERT–CRF model, named entity recognition, melanoma},
abstract = {Background
Many drugs do not work the same way for everyone owing to distinctions in their genes. Pharmacogenomics (PGx) aims to understand how genetic variants influence drug efficacy and toxicity. It is often considered one of the most actionable areas of the personalized medicine paradigm. However, little prior work has included in-depth explorations and descriptions of drug usage, dosage adjustment, and so on.
Objective
We present a pharmacogenomics knowledge model to discover the hidden relationships between PGx entities such as drugs, genes, and diseases, especially details in precise medication.
Methods
PGx open data such as DrugBank and RxNorm were integrated in this study, as well as drug labels published by the US Food and Drug Administration. We annotated 190 drug labels manually for entities and relationships. Based on the annotation results, we trained 3 different natural language processing models to complete entity recognition. Finally, the pharmacogenomics knowledge model was described in detail.
Results
In entity recognition tasks, the Bidirectional Encoder Representations from Transformers–conditional random field model achieved better performance with micro-F1 score of 85.12%. The pharmacogenomics knowledge model in our study included 5 semantic types: drug, gene, disease, precise medication (population, daily dose, dose form, frequency, etc), and adverse reaction. Meanwhile, 26 semantic relationships were defined in detail. Taking melanoma caused by a BRAF gene mutation into consideration, the pharmacogenomics knowledge model covered 7 related drugs and 4846 triples were established in this case. All the corpora, relationship definitions, and triples were made publically available.
Conclusions
We highlighted the pharmacogenomics knowledge model as a scalable framework for clinicians and clinical pharmacists to adjust drug dosage according to patient-specific genetic variation, and for pharmaceutical researchers to develop new drugs. In the future, a series of other antitumor drugs and automatic relation extractions will be taken into consideration to further enhance our framework with more PGx linked data.}
}
@article{TRAPPEY2020101980,
title = {Identify trademark legal case precedents - Using machine learning to enable semantic analysis of judgments},
journal = {World Patent Information},
volume = {62},
pages = {101980},
year = {2020},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2020.101980},
url = {https://www.sciencedirect.com/science/article/pii/S0172219019300638},
author = {Charles V. Trappey and Amy J.C. Trappey and Bo-Hung Liu},
keywords = {Trademark infringement, Clustering, Latent dirichlet allocation, Precedence analysis, Recommendation platform},
abstract = {Legal case precedents have a considerable impact on the development of litigation strategies. This research uses the neural network language modeling (NNLM) approach to analyze and identify judgment documents of US trademark (TM) litigation cases as precedents of a given target case. In this research, the NNLM has been trained using 4835 TM litigation documents. There are more than 800,000 words in the entire training text set including more than 150,000 vocabularies. The words in TM legal documents are vectorized to train the NN model for e-discovery of semantically correlated precedents and their features. Specifically, non-supervised machine learning (ML) methods, including clustering and Latent Dirichlet Allocation (LDA), are applied to form the TM legal document clusters, topics, and key terminologies used to characterize the TM case descriptions and precedents. The definition of the clusters, topics and corresponding key terms enhance the ability of the system to recommend and explain similar case judgments for any given TM case of interest or a cease and desist letter with detailed claims of infringement. Further, the intelligent approach provides macro and micro views for companies to research TM litigation trends as a means to better protect their brand equity.}
}
@article{VARMA2021e08035,
title = {Graph NLU enabled question answering system},
journal = {Heliyon},
volume = {7},
number = {9},
pages = {e08035},
year = {2021},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2021.e08035},
url = {https://www.sciencedirect.com/science/article/pii/S2405844021021381},
author = {Sandeep Varma and Shivam Shivam and Snigdha Biswas and Pritam Saha and Khushi Jalan},
keywords = {Conversational analytics, Graph traversal, Knowledge graph, Natural language query, Question answering, Structured data, Tabular data},
abstract = {With a huge amount of information being stored as structured data, there is an increasing need for retrieving exact answers to questions from tables. Answering natural language questions on structured data usually involves semantic parsing of query to a machine understandable format which is then used to retrieve information from the database. Training semantic parsers for domain specific tasks is a tedious job and does not guarantee accurate results. In this paper, we used conversational analytics tool to create the user interface and to get the required entities and intents from the query thus avoiding the traditional semantic parsing approach. We then make use of Knowledge Graph for querying in structured data domain. Knowledge graphs can be easily leveraged for question answering systems, to use them as the database. We extract appropriate answers for different types of queries which have been illustrated in the Results section.}
}
@article{FLATER2018144,
title = {Architecture for software-assisted quantity calculus},
journal = {Computer Standards & Interfaces},
volume = {56},
pages = {144-147},
year = {2018},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2017.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0920548917303069},
author = {David Flater},
keywords = {SI, Quantity, Unit, Uncertainty, Value, Unit 1},
abstract = {A quantity value, such as 5 kg, consists of a number and a reference (often an International System of Units (SI) unit) that together express the magnitude of a quantity. Many software libraries, packages, and ontologies that implement “quantities and units” functions are available. Although all of them begin with SI and associated practices, they differ in how they address issues such as ad hoc counting units, ratios of two quantities of the same kind, and uncertainty. This short article describes an architecture that addresses the complete set of functions in a simple and consistent fashion. Its goal is to encourage more convergent thinking about the functions and the underlying concepts so that the many disparate implementations, present and future, will become more consistent with one another.}
}
@article{KARMAKAR2025106369,
title = {Semantic BIM enrichment using a hybrid ML and rule-based framework for automated tenement compliance checking},
journal = {Automation in Construction},
volume = {177},
pages = {106369},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106369},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525004091},
author = {Ankan Karmakar and Venkata Santosh Kumar Delhi},
keywords = {Building information modeling, Semantic enrichment, Machine learning, Industry foundation classes, Automated code compliance checking},
abstract = {Semantic enrichment enhances BIM models by extracting structured information, improving their applicability for Automated Code Compliance Checking. Rule-based methods rely on well-defined conditions but struggle with tasks like space classification, where explicit checking rules are unavailable. Meanwhile, ML-based classification introduces adaptability but faces liability challenges due to misclassifications. This paper proposes a hybrid framework integrating ML for space classification and rule-based inferencing for tenement identification. The approach ensures that ML automates preprocessing, improving classification through meticulous feature engineering, while rule-based reasoning guarantees logical consistency during verification. Validated using real-world datasets from residential projects in Mumbai, India as a case, the ML-based space classification component achieves an F1-score of 0.85 and accuracy of 0.86, demonstrating its effectiveness. The deterministic tenement identification process delivers error-free results for various dwelling configurations, making it highly suitable for verification workflows. This study advances scalable BIM-based compliance systems by refining semantic enrichment methodologies for future applications.}
}
@article{REMAKI2025,
title = {Improving Phenotyping of Patients With Immune-Mediated Inflammatory Diseases Through Automated Processing of Discharge Summaries: Multicenter Cohort Study},
journal = {JMIR Medical Informatics},
volume = {13},
year = {2025},
issn = {2291-9694},
doi = {https://doi.org/10.2196/68704},
url = {https://www.sciencedirect.com/science/article/pii/S2291969425000687},
author = {Adam Remaki and Jacques Ung and Pierre Pages and Perceval Wajsburt and Elise Liu and Guillaume Faure and Thomas Petit-Jean and Xavier Tannier and Christel Gérardin},
keywords = {secondary use of clinical data for research and surveillance, clinical informatics, clinical data warehouse, electronic health record, data science, artificial intelligence, AI, natural language processing, ontologies, classifications, coding, tools, programs and algorithms, immune-mediated inflammatory diseases},
abstract = {Background
Valuable insights gathered by clinicians during their inquiries and documented in textual reports are often unavailable in the structured data recorded in electronic health records (EHRs).
Objective
This study aimed to highlight that mining unstructured textual data with natural language processing techniques complements the available structured data and enables more comprehensive patient phenotyping. A proof-of-concept for patients diagnosed with specific autoimmune diseases is presented, in which the extraction of information on laboratory tests and drug treatments is performed.
Methods
We collected EHRs available in the clinical data warehouse of the Greater Paris University Hospitals from 2012 to 2021 for patients hospitalized and diagnosed with 1 of 4 immune-mediated inflammatory diseases: systemic lupus erythematosus, systemic sclerosis, antiphospholipid syndrome, and Takayasu arteritis. Then, we built, trained, and validated natural language processing algorithms on 103 discharge summaries selected from the cohort and annotated by a clinician. Finally, all discharge summaries in the cohort were processed with the algorithms, and the extracted data on laboratory tests and drug treatments were compared with the structured data.
Results
Named entity recognition followed by normalization yielded F1-scores of 71.1 (95% CI 63.6-77.8) for the laboratory tests and 89.3 (95% CI 85.9-91.6) for the drugs. Application of the algorithms to 18,604 EHRs increased the detection of antibody results and drug treatments. For instance, among patients in the systemic lupus erythematosus cohort with positive antinuclear antibodies, the rate increased from 18.34% (752/4102) to 71.87% (2949/4102), making the results more consistent with the literature.
Conclusions
While challenges remain in standardizing laboratory tests, particularly with abbreviations, this work, based on secondary use of clinical data, demonstrates that automated processing of discharge summaries enriched the information available in structured data and facilitated more comprehensive patient profiling.}
}
@article{BAHGAT20183259,
title = {Efficient email classification approach based on semantic methods},
journal = {Ain Shams Engineering Journal},
volume = {9},
number = {4},
pages = {3259-3269},
year = {2018},
issn = {2090-4479},
doi = {https://doi.org/10.1016/j.asej.2018.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S2090447918300455},
author = {Eman M. Bahgat and Sherine Rady and Walaa Gad and Ibrahim F. Moawad},
keywords = {Email classification, Spam, WordNet ontology, Semantic similarity, Features reduction},
abstract = {Emails have become one of the major applications in daily life. The continuous growth in the number of email users has led to a massive increase of unsolicited emails, which are also known as spam emails. Managing and classifying this huge number of emails is an important challenge. Most of the approaches introduced to solve this problem handled the high dimensionality of emails by using syntactic feature selection. In this paper, an efficient email filtering approach based on semantic methods is addressed. The proposed approach employs the WordNet ontology and applies different semantic based methods and similarity measures for reducing the huge number of extracted textual features, and hence the space and time complexities are reduced. Moreover, to get the minimal optimal features’ set, feature dimensionality reduction has been integrated using feature selection techniques such as the Principal Component Analysis (PCA) and the Correlation Feature Selection (CFS). Experimental results on the standard benchmark Enron Dataset showed that the proposed semantic filtering approach combined with the feature selection achieves high computational performance at high space and time reduction rates. A comparative study for several classification algorithms indicated that the Logistic Regression achieves the highest accuracy compared to Naïve Bayes, Support Vector Machine, J48, Random Forest, and radial basis function networks. By integrating the CFS feature selection technique, the average recorded accuracy for the all used algorithms is above 90%, with more than 90% feature reduction. Besides, the conducted experiments showed that the proposed work has a highly significant performance with higher accuracy and less time compared to other related works.}
}
@article{WALIA201989,
title = {An efficient automated answer scoring system for Punjabi language},
journal = {Egyptian Informatics Journal},
volume = {20},
number = {2},
pages = {89-96},
year = {2019},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2018.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S1110866518301439},
author = {Tarandeep Singh Walia and Gurpreet Singh Josan and Amarpal Singh},
keywords = {Natural Language Processing, Automated scoring system, Ambiguity},
abstract = {Automated Scoring is a developing technology. The accuracy and reliability of these systems have been proven to be much higher. Besides being a time-and money-saver, there are a number of studies which are being conducted to provide variety in feedback, not only on grammatical issues, but also on semantic related issues. This will reduce not only the paper load of the teachers, but as well as teachers’ assessment related issues. In this paper, we remove those dummy note bins, thereby having 183 dimensions in total. We augmented the input by concatenating it with the first-order difference of the semitone filtered spectrogram. We observed a significant increase of the transcription performance with this addition. Compared to feedforward neural networks, recurrent neural network (RNN) are capable of learning temporal dependency of sequential data, which is the property found in music answer scoring.Also, the Long Short-Term Memory (LSTM) unit has a memory block updated only when an input or forget gate is open, and the gradients can propagate through memory cells without being multiplied each time step. Throughout backward and forward layers together, the networks can access to both history and future of the given time frame. Comparative analyses show that the proposed technique outperforms existing techniques.}
}
@article{LEE2025111163,
title = {Developing Tech2Vec: A new embedding approach of technology information using a triple layer},
journal = {Computers & Industrial Engineering},
volume = {205},
pages = {111163},
year = {2025},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2025.111163},
url = {https://www.sciencedirect.com/science/article/pii/S0360835225003092},
author = {Suyeong Lee and Sunhye Kim and Daye Lee and Byungun Yoon},
keywords = {Patent analysis, Patent retrieval, Technology embedding, Natural language processing},
abstract = {The recent increase in the number of patent applications highlights the urgent need for an effective embedding technique to automatically analyze enormous patent datasets. Extensive research is being conducted on the application of high-performance artificial intelligence (AI) technology to enhance patent analysis tasks. However, these studies do not consider various types of data. Instead, they examine technological information from a single perspective, such as technological terminology, patent functions, and goods. To cover all aspects, namely, technological system, function, and technology, a technological information analysis model that exploits both structured and unstructured data from previous patent filings is required. Therefore, this study proposes a new embedding approach called Tech2Vec to conduct function-oriented patent searches that can use the function and technological information of patent documents. More precisely, various types of technological information included in patent applications are organized into a triple layer, that is, the system, function, and component layers; vectorized layer by layer; and concatenated into a single technology vector. For example, by leveraging the patents and papers of three sectors, namely electric vehicles, displays and industrial robots, Tech2Vec is effectively applied and mapped to the technological latent space. Additionally, a function-oriented patent search is performed by comparing the query vectors entered by a user in natural language rather than the search query format. This study may be used as a reference for a range of technology management activities, such as document categorization, technological opportunity identification, and technology evolution analysis.}
}
@article{RATHNASIRI2023102085,
title = {Data-driven approaches to built environment flood resilience: A scientometric and critical review},
journal = {Advanced Engineering Informatics},
volume = {57},
pages = {102085},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102085},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623002136},
author = {Pavithra Rathnasiri and Onaopepo Adeniyi and Niraj Thurairajah},
keywords = {Built assets, Data-driven, Computational methods, Community, Environment, Flood, Resilience, Society},
abstract = {Environmental hazards such as floods significantly frustrate the functionality of built assets. In addressing flood-induced challenges, data usage has become important. Despite existing vast flood-related research, no research has presented a comprehensive insight into global studies on data-driven built environment flood resilience. Hence, this study conducted a comprehensive review of data-driven approaches to flood resilience. Scientometric analysis revealed emerging countries, authorships, keywords, and research hotspots. The critical review revealed data-centric approaches such as Machine Learning (ML), Artificial Intelligence (AI), Flood Simulations, Bayesian Modelling, Building Information Modelling (BIM) and Geographic Information Systems (GIS). However, they were mainly deployed in hydraulic flood simulations for prediction, monitoring, risk, and damage assessments. Further, the potentials of computational methods in tackling built environment resilience challenges were identified. Deploying the approaches in the future requires a better understanding of the status quo. These methods include hybrid data-driven approaches, ontology-based knowledge representation, multiscale modelling, knowledge graphs, blockchain technology, convolutional neural networks, automated approaches integrated with social media data, data assimilation, BIM models linked with sensors and satellite imagery and ML and AI-based digital twin models. Nevertheless, reference to data-informed built-asset resilience decisions and clear-cut implications on built-asset resilience improvement remain indistinct in many studies. This suggests that more opportunities exist to contextualise data for built environment flood resilience. This study concluded with a conceptual map of flood context, methodologies, data types engaged, and future computational methods with directions for future research.}
}
@article{BEKAMIRI2024123536,
title = {PatentSBERTa: A deep NLP based hybrid model for patent distance and classification using augmented SBERT},
journal = {Technological Forecasting and Social Change},
volume = {206},
pages = {123536},
year = {2024},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2024.123536},
url = {https://www.sciencedirect.com/science/article/pii/S0040162524003329},
author = {Hamid Bekamiri and Daniel S. Hain and Roman Jurowetzki},
keywords = {Technological distance, Patent classification, Deep NLP, Augmented SBERT, Hybrid model, Model explainability},
abstract = {This study presents an efficient approach for utilizing text data to calculate patent-to-patent (p2p) technological similarity and proposes a hybrid framework for leveraging the resulting p2p similarity in applications such as semantic search and automated patent classification. To achieve this, we create embeddings using Sentence-BERT (SBERT) on patent claims. For domain adaptation of the general SBERT model, we implement an augmented approach to fine-tune SBERT using in-domain supervised patent claims data. The study utilizes SBERT's efficiency in creating embedding distance measures to map p2p similarity in large sets of patent data. We demonstrate applications of the framework for the use case of automated patent classification with a simple K Nearest Neighbors (KNN) model that predicts assigned Cooperative Patent Classification (CPC) based on the class assignment of the K patents with the highest p2p similarity. The results show that p2p similarity captures technological features in terms of CPC overlap, and the approach is useful for automatic patent classification based on text data. Moreover, the presented classification framework is simple, and the results are easy to interpret and evaluate by end-users via instance-based explanations. The study performs an out-of-sample model validation, predicting all assigned CPC classes on the subclass (663) level with an F1 score of 66 %, outperforming the current state-of-the-art in text-based multi-label patent classification. The study also discusses the applicability of the presented framework for semantic intellectual property (IP) search, patent landscaping, and technology mapping. Finally, the study outlines a future research agenda to leverage multi-source patent embeddings, evaluate their appropriateness across applications, and improve and validate patent embeddings by creating domain-expert curated Semantic Textual Similarity (STS) benchmark datasets.}
}
@article{JIN2023114038,
title = {Building a deep learning-based QA system from a CQA dataset},
journal = {Decision Support Systems},
volume = {175},
pages = {114038},
year = {2023},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2023.114038},
url = {https://www.sciencedirect.com/science/article/pii/S0167923623001136},
author = {Sol Jin and Xu Lian and Hanearl Jung and Jinsoo Park and Jihae Suh},
keywords = {Question answering (QA) system, Community question answering (CQA), BERT, T5},
abstract = {A man-made machine-reading comprehension (MRC) dataset is necessary to train the answer extraction part of existing Question Answering (QA) systems. However, a high-quality and well-structured dataset with question-paragraph-answer pairs is not usually found in the real world. Furthermore, updating or building an MRC dataset is a challenging and costly affair. To address these shortcomings, we propose a QA system that uses a large-scale English Community Question Answering (CQA) dataset (i.e., Stack Exchange) composed of 3,081,834 question-answer pairs. The QA system adopts a classifier-retriever-summarizer structure design. The question classifier and the answer retriever part are based on a Bidirectional Encoder Representations from Transformers (BERT) Natural Language Processing (NLP) model by Google, and the summarizer part introduces a deep learning-based Text-to-Text Transfer Transformer (T5) model to summarize the long answers. We instantiated the proposed QA system with 140 topics from the CQA dataset (including topics such as biology, law, politics, etc.) and conducted human and automatic evaluations. Our system presented encouraging results, considering that it provides high-quality answers to the questions in the test set and satisfied the requirements to develop a QA system without MRC datasets. Our results show the potential of building automatic and high-performance QA systems without being limited by man-made datasets, a significant step forward in the research of open-domain or specific-domain QA systems.}
}
@article{FABIANO2021100107,
title = {Wetland spirits and indigenous knowledge: Implications for the conservation of wetlands in the Peruvian Amazon},
journal = {Current Research in Environmental Sustainability},
volume = {3},
pages = {100107},
year = {2021},
issn = {2666-0490},
doi = {https://doi.org/10.1016/j.crsust.2021.100107},
url = {https://www.sciencedirect.com/science/article/pii/S2666049021000839},
author = {Emanuele Fabiano and Christopher Schulz and Manuel {Martín Brañas}},
keywords = {Amazon, Cosmovision, Indigenous knowledge, Spirits, Urarina, Wetland conservation},
abstract = {Globally, the importance of indigenous and local knowledge systems for science, policy, environmental conservation and the cultural heritage of indigenous peoples is increasingly being recognised. The Amazon region in particular is home to many indigenous peoples who have conserved their cultural traditions and knowledge, despite growing threats to the environment and traditional lifestyles and cultures. Based on insights from ethnographic research in three indigenous communities, here we present a case study on the indigenous knowledge of the Urarina people of the Chambira Basin in the Peruvian Amazon and its implications for conservation. We describe, for the first time, a series of anthropomorphic and territorial “wetland spirits”, who are associated with particular wetland ecosystems and range in character from the benign to outright aggressive. Their presence may indirectly benefit conservation of wetlands, as humans fear or respect these wetland spirits and adapt their behaviour accordingly. While benign spirits may be seen as positive models to follow, aggressive spirits may deter unsustainable harvesting of resources through fear of disease or death. However, their cultural status is not adequately captured by such rational-scientific explanations. Wetland spirits are important characters within the indigenous cosmos of humans and non-humans, which is built on a relational, rather than extractive model of connecting humans and nature. We discuss our findings in the context of wider conceptual debates on recognising relational ontologies in environmental policy and conservation, the paradigm of biocultural conservation, as well as their implications for land titling, and incorporating indigenous perspectives in local education.}
}
@article{WANG2025110842,
title = {A module partition method for complex product based on the knowledge hypergraph},
journal = {Engineering Applications of Artificial Intelligence},
volume = {152},
pages = {110842},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110842},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625008425},
author = {Pengchao Wang and Jianjie Chu and Suihuai Yu and Fangmin Cheng and Ning Ding and Yangfan Cong},
keywords = {Product design, Product module partition, Knowledge hypergraph, Hypergraph neural network, Clustering algorithm},
abstract = {The modular design can effectively control the development cycle and cost of complex product, with product module partition (PMP) serving as the foundation of modularization. However, models constructed based on expert knowledge are inadequate in effectively capturing the relationships within complex product, which undermines the efficiency and accuracy of PMP. To solve this problem, this paper introduces hypergraph theory into the field of PMP, specifically, proposes a PMP method based on the knowledge hypergraph (KHG). First, the multiple coupling relationships between complex product are defined from the perspective of "Function-Behavior-Structure-Constraint", to form the pattern layer of the KHG. Then, the joint learning algorithm, which contains the pretraining model, Bi-directional Long Short-Term Memory network, Conditional Random Field and Attention layer, is proposed to automatically extract design knowledge from large-scale text data to form the data layer of the KHG. Furthermore, considering that the PMP model needs to learn nonlinear relationship features, achieve end-to-end optimization, and have strong anti-noise ability, hypergraph neural networks are used to partition the complex product modules, which contains the importance calculation, hypergraph convolution, modularity maximum and self-supervised module. Finally, a case study is conducted using a snow removal equipment as an example, the knowledge extraction accuracy reaches 91.67 %, and the PMP modularity is 0.68, thus validating the feasibility of the proposed method. Additionally, the comparison is made with other knowledge extraction and hypergraph clustering algorithms using public datasets, which further confirms the feasibility and superiority of the proposed method.}
}
@article{PUTNIK2022678,
title = {Engineering is Design and only Design - Part I: The value of making a distinctive sign},
journal = {Procedia CIRP},
volume = {109},
pages = {678-683},
year = {2022},
note = {32nd CIRP Design Conference (CIRP Design 2022) - Design in a changing world},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.313},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122007636},
author = {Goran D. Putnik and Zlata Putnik and Pedro Pinheiro and Cátia Alves},
keywords = {Design, engineering, sign, semiotics},
abstract = {The paper addresses the question: what is engineering? We intuitively know engineering applications such as manufacturing, production, industry, management, business. The answer is not consensual because it is not easy. Furthermore, the ontological question brings us to a second question. What distinguishes engineering from other areas? It is the creative ability that distinguishes engineering. And this artificial faculty only exists in Design. Epistemology in science promotes the existence of herds, increasingly specialized groups of knowledge production. Nevertheless, engineers assume themselves as makers, and in the growing diversity promoted by specialization, they will certainly give different answers when asked about their work. We aggregate all of them as sign-makers. Therefore, engineering is Design and only Design. We reject other views. The argument presented on the phenomenological level considers them false. This paper demonstrates that it is mandatory to create a distinctive sign, which places engineering as relevant in organizations. Without the sign described in semiotics, engineering, which could pretend to be everything, becomes trivial.}
}
@article{THIKE2019300,
title = {Materials failure analysis utilizing rule-case based hybrid reasoning method},
journal = {Engineering Failure Analysis},
volume = {95},
pages = {300-311},
year = {2019},
issn = {1350-6307},
doi = {https://doi.org/10.1016/j.engfailanal.2018.09.033},
url = {https://www.sciencedirect.com/science/article/pii/S1350630718301493},
author = {Phyu Hnin Thike and Zhou Xu and Yuan Cheng and Ying Jin and Peng Shi},
keywords = {Failure analysis, Failure mode, Rule-based reasoning, Case-based reasoning, Hybrid reasoning},
abstract = {Materials failure problems are becoming a serious concern because there would be a variety of consequences possibly affecting public safety. Therefore, materials failure analysis is needed to find out the reason of failure and avoid similar failures' reoccurring. However, materials failure cases mostly rely on manual analysis by experts with sufficient domain knowledge, leading to the situation of time-consuming, low efficiency and difficult evaluation. This paper proposes a rule-case based hybrid reasoning approach for materials failure analysis with the aid of ontology. Hundreds of materials failure cases from different industries were collected and analyzed by rule-based reasoning, case-based reasoning and our hybrid method. It is demonstrated that the rule-case based hybrid reasoning method can obviously provide better analysis results in comparison with rule-based reasoning and case-based reasoning alone.}
}
@article{SOMAN2020103369,
title = {Linked-Data based Constraint-Checking (LDCC) to support look-ahead planning in construction},
journal = {Automation in Construction},
volume = {120},
pages = {103369},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103369},
url = {https://www.sciencedirect.com/science/article/pii/S0926580520309493},
author = {Ranjith K. Soman and Miguel Molina-Solana and Jennifer K. Whyte},
keywords = {Linked-data, Shapes constraint language (SHACL), Constraints, Dynamic constraint-checking, Scheduling constraints, Linked-data based Constraint-Checking (LDCC)},
abstract = {In the construction sector, complex constraints are not usually modeled in conventional scheduling and 4D building information modeling software, as they are highly dynamic and span multiple domains. The lack of embedded constraint relationships in such software means that, as Automated Data Collection (ADC) technologies become used, it cannot automatically deduce the effect of deviations to schedule. This paper presents a novel method, using semantic web technologies, to model and validate complex scheduling constraints. It presents a Linked-Data based Constraint-Checking (LDCC) approach, using the Shapes Constraint Language (SHACL). A prototype web application is developed using this approach and evaluated using an OpenBIM dataset. Results demonstrate the potential of LDCC to check for constraint violation in distributed construction data. This novel method (LDCC) and its first prototype is a contribution that can be extended in future research in linked-data, BIM based rule-checking, lean construction and ADC.}
}
@article{CUCCIO2019157,
title = {Rethinking the abstract/concrete concepts dichotomy: Comment on “Words as social tools: Language, sociality and inner grounding in abstract concepts” by Anna M. Borghi et al.},
journal = {Physics of Life Reviews},
volume = {29},
pages = {157-160},
year = {2019},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2019.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S1571064519300703},
author = {V. Cuccio and F. Caruana}
}
@article{ABUSALIH2021103076,
title = {Domain-specific knowledge graphs: A survey},
journal = {Journal of Network and Computer Applications},
volume = {185},
pages = {103076},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103076},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521000990},
author = {Bilal Abu-Salih},
keywords = {Knowledge graph, Domain-specific knowledge graph, Knowledge graph construction, Knowledge graph embeddings, Knowledge graph evaluation, Domain ontology, Survey},
abstract = {Knowledge Graphs (KGs) have made a qualitative leap and effected a real revolution in knowledge representation. This is leveraged by the underlying structure of the KG which underpins a better comprehension, reasoning and interpretation of knowledge for both human and machine. Therefore, KGs continue to be used as the main means of tackling a plethora of real-life problems in various domains. However, there is no consensus in regard to a plausible and inclusive definition of a domain-specific KG. Further, in conjunction with several limitations and deficiencies, various domain-specific KG construction approaches are far from perfect. This survey is the first to offer a comprehensive definition of a domain-specific KG. Also, the paper presents a thorough review of the state-of-the-art approaches drawn from academic works relevant to seven domains of knowledge. An examination of current approaches reveals a range of limitations and deficiencies. At the same time, uncharted territories on the research map are highlighted to tackle extant issues in the literature and point to directions for future research.}
}
@article{KEAN2025109125,
title = {Intuitive physical reasoning is not mediated by linguistic nor exclusively domain-general abstract representations},
journal = {Neuropsychologia},
volume = {213},
pages = {109125},
year = {2025},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2025.109125},
url = {https://www.sciencedirect.com/science/article/pii/S0028393225000600},
author = {Hope H. Kean and Alexander Fung and R.T. Pramod and Jessica Chomik-Morales and Nancy Kanwisher and Evelina Fedorenko},
abstract = {The ability to reason about the physical world is a critical tool in the human cognitive toolbox, but the nature of the representations that mediate physical reasoning remains debated. Here, we use fMRI to illuminate this question by investigating the relationship between the physical-reasoning system and two well-characterized systems: a) the domain-general Multiple Demand (MD) system, which supports abstract reasoning, including mathematical and logical reasoning, and b) the language system, which supports linguistic computations and has been hypothesized to mediate some forms of thought. We replicate prior findings of a network of frontal and parietal areas that are robustly engaged by physical reasoning and identify an additional physical-reasoning area in the left frontal cortex, which also houses components of the MD and language systems. Critically, direct comparisons with tasks that target the MD and the language systems reveal that the physical-reasoning system overlaps with the MD system, but is dissociable from it in fine-grained activation patterns, which replicates prior work. Moreover, the physical-reasoning system does not overlap with the language system. These results suggest that physical reasoning does not rely on linguistic representations, nor exclusively on the domain-general abstract reasoning that the MD system supports.}
}
@article{TRIANTAFYLLIDIS2025145783,
title = {Accelerating circular cities with semi-automatic building information modeling for existing buildings},
journal = {Journal of Cleaner Production},
volume = {514},
pages = {145783},
year = {2025},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2025.145783},
url = {https://www.sciencedirect.com/science/article/pii/S0959652625011333},
author = {Georgios Triantafyllidis and Daniel Beat Müller and Steffen Wellinger and Lizhen Huang},
keywords = {Parametric BIM modeling, Existing buildings, Circular economy, Material reuse, Material remanufacturing, Sustainable construction, High-granularity data},
abstract = {The lack of high-granularity information on existing building stock impedes efficient material reuse and recycling, which is essential for promoting the circular economy in the building industry. Building Information Modeling (BIM) can fill this gap by offering detailed information about the material composition of buildings. However, creating BIM models for existing buildings remains costly, time-consuming, and labor-intensive. This study proposes a novel method for accelerating the creation of georeferenced BIM models by integrating heterogeneous mass data and domain knowledge about building construction. The method achieves 95 % accuracy in estimating material intensities for external load-bearing timber frame walls and roof components. This approach provides valuable insights for stakeholders, facilitating the transition to circular cities by supporting material stock analysis at both macro and micro levels. By expediting the BIM modeling process, this method enhances the granularity of material stock analysis, supports efficient material reuse and recycling, and promotes urban sustainability.}
}
@incollection{PENCE2022165,
title = {Chapter 7 - Conclusions, historiographical and philosophical},
editor = {Charles H. Pence},
booktitle = {The Rise of Chance in Evolutionary Theory},
publisher = {Academic Press},
pages = {165-179},
year = {2022},
isbn = {978-0-323-91291-4},
doi = {https://doi.org/10.1016/B978-0-323-91291-4.00001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323912914000017},
author = {Charles H. Pence},
keywords = {Natural selection, Chance, Probability, Ontology, Biometry, Mendelism},
abstract = {In this concluding chapter, I step back and consider both some broader themes drawn throughout the book, and make comparisons with two works (one by Gayon and one by Depew and Weber) that have presented similar approaches. Subjects explored include a more sustained argument for a continuous reading of the history of the period (and against the metaphor of the “eclipse of Darwinism” or a “Mendelian Revolution”), discussion of the ontology underlying mathematical theories of biology, and the historiography of the era now often known as the “biometry-Mendelism controversy.”}
}
@article{METWALLY2025178136,
title = {Integrated network pharmacology and in vivo experimental approaches unveil the modulatory effect of telmisartan on autophagy in a rat model of nonalcoholic steatohepatitis.},
journal = {European Journal of Pharmacology},
pages = {178136},
year = {2025},
issn = {0014-2999},
doi = {https://doi.org/10.1016/j.ejphar.2025.178136},
url = {https://www.sciencedirect.com/science/article/pii/S0014299925008908},
author = {Sami S. Metwally and Rasha H. Abdel-Ghany and Atef S. Elgharbawy and Mohamed Mohsen and Amira Ebrahim Alsemeh and Esraa M. Zakaria},
keywords = {Telmisartan, pioglitazone, network pharmacology, NASH, autophagy},
abstract = {Abstract:
Defective autophagy contributes to the progression of various diseases, including nonalcoholic steatohepatitis (NASH). While telmisartan’s hepatoprotective effects are well established, its impact on hepatic autophagy in NASH remains unclear. This study employed network pharmacology combined with in vivo experiments to predict and validate telmisartan’s effects on hepatic autophagy in a rat model of NASH. Additionally, we investigated whether telmisartan could augment pioglitazone’s protective effects. Network pharmacology identified 97 common molecular targets shared by telmisartan and NASH. Key targets included Signal Transducer and Activator of Transcription 3 (STAT3), B-cell lymphoma 2 (Bcl2), and Nuclear Factor kappa-light-chain-enhancer of activated B cells (NF-κB), regulators of cellular autophagy, suggesting telmisartan’s potential interaction with autophagy-related pathways. To validate these findings, NASH was induced in adult male rats via an 18-week high fructose, fat, and salt diet, with telmisartan, pioglitazone, or their combination administered orally during the final 10 weeks. Blood pressure, glycemic, lipid, and liver function parameters were quantified in serum. Besides, hepatic markers of autophagy (Beclin1, LC3, and p62), inflammation, oxidative stress, fibrosis, and apoptosis were measured. Moreover, liver histology and collagen deposition were examined. Telmisartan significantly enhanced hepatic autophagy more than pioglitazone. Furthermore, combination therapy synergistically increased autophagy beyond the effects of either drug alone. Both drugs similarly ameliorated hepatic inflammation and oxidative stress markers. These results demonstrate that telmisartan’s hepatoprotective effects are partly mediated by restoration of hepatic autophagy. Moreover, the data suggest that combined telmisartan and pioglitazone therapy may provide enhanced benefit in NASH treatment, warranting further clinical investigation.}
}
@article{MATHUR2025108642,
title = {Current advancement in AI-integrated drug discovery: Methods and applications},
journal = {Biotechnology Advances},
volume = {83},
pages = {108642},
year = {2025},
issn = {0734-9750},
doi = {https://doi.org/10.1016/j.biotechadv.2025.108642},
url = {https://www.sciencedirect.com/science/article/pii/S0734975025001284},
author = {Yash Mathur and Arunabh Choudhury and Sneh Prabha and Mohammad Umar Saeed and Md Nayab Sulaimani and Taj Mohammad and Md. Imtaiyaz Hassan},
keywords = {Artificial intelligence, Drug discovery, Proteomics, AI-assisted drug discovery, Computer-aided drug discovery},
abstract = {Artificial intelligence (AI) has grown in prominence over the decade and continues to advance frighteningly. With additional research in the computer hardware field, the accuracy and precision of AI models will increase exponentially. The interdisciplinary nature of AI expands the possibility of application in every field of study. The use of AI in human healthcare has also been on the rise, with the involvement of interactive models. Since drug development is a prominent part of the field, there are bound to be AI models capable of improving parameters and predictions for various techniques. This review explores the recent developments in the applications of AI in the scope of drug discovery. Focusing on the workflow of a standard interdisciplinary drug discovery approach, this review aims to provide information about various AI-enabled tools in the field. We begin with an in-depth overview of the different AI models and architectures frequently employed in the field. Next, we reviewed the applications of AI in drug discovery, discussing the state-of-the-art models and tools employed for topics such as data analysis, functional annotation, virtual screening, clinical trial optimization, and much more. Discussing the prospects, challenges, and limitations that the field faces, this review attempts to encompass the essence of AI-based drug discovery. We anticipate this review will aid the innovation of more brilliant AI tools for various subtopics of the drug discovery and development field.}
}
@article{KHATUN2025113756,
title = {Classification of triple extraction and RDF generation using boosted BERT fused attention convolutional Bi-directional LSTM with optimization},
journal = {Knowledge-Based Systems},
volume = {324},
pages = {113756},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113756},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125008020},
author = {Rubaya Khatun and Arup Sarkar},
keywords = {Resource description frameworks, Triples, Bidirectional encoder representations from transformers, Bi-LSTM, Bald eagle search optimization},
abstract = {Knowledge and expertise are the two most frequent ways of sharing information in a structured manner. A depth of knowledge is typically made up of triples of Resource Description Frameworks (RDF) that characterize the entities and their relationships. Some of the current limitations include higher computational costs, longer execution times, and more complexity in achieving better results. The machine learning based techniques generate lower performance compared to deep learning approaches. Initially, the data are collected using BBC News, Kaggle, and Lonely Planet datasets. After the collection of data, pre-processing steps like data cleaning, Parts of Speech (PoS) tagging, tokenization, stop word removal, and stemming are applied to the data to enhance further performance. The entity and attribute features are extracted by Boosted Bidirectional Encoder Representations from Transformers (B-BERT). The classification can be performed using fused attention with a convolutional Bidirectional LSTM (AcBL) approach. An improved Bald Eagle Search Optimization (EBesO) algorithm is used to optimize the loss derived from the classification phase to improve model accuracy. The overall performance outcomes of the proposed method attain 99.1 % accuracy on BBC, 99.7 % accuracy on Kaggle, and 99 % accuracy on Lonely Planet. The proposed technique acquires 97.9 % precision on BBC, 98.7 % precision on Kaggle, and 97.9 % precision on Lonely Planet. The F1 score of the proposed methodology accomplishes 97.9 % on BBC, 98.6 % on Kaggle, and 97.8 % on Lonely Planet. Recall that the proposed model achieves 98 % on BBC, 98.5 % on Kaggle, and 97.6 % on Lonely Planet.}
}
@article{BARNETT2020112977,
title = {Neural imaginaries at work: Exploring Australian addiction treatment providers’ selective representations of the brain in clinical practice},
journal = {Social Science & Medicine},
volume = {255},
pages = {112977},
year = {2020},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2020.112977},
url = {https://www.sciencedirect.com/science/article/pii/S0277953620301969},
author = {Anthony I. Barnett and Martyn Pickersgill and Ella Dilkes-Frayne and Adrian Carter},
keywords = {Australia, Addiction, Clinical practice, Drug treatment, Neuroscience, Neuroimaging, Translation, Qualitative},
abstract = {Although addiction neuroscience hopes to uncover the neural basis of addiction and deliver a wide range of novel neuro-interventions to improve the treatment of addiction, the translation of addiction neuroscience to practice has been widely viewed as a ‘bench to bedside’ failure. Importantly, though, this linear ‘bench to bedside’ conceptualisation of knowledge translation has not been attentive to the role addiction treatment providers play in reproducing, translating, or resisting neuroscientific knowledge. This study explores how, to what extent, and for what purpose addiction treatment providers deploy neuroscientific representations and discuss the brain in practice. It draws upon interviews with 20 Australian treatment providers, ranging from addiction psychiatrists in clinics to case-workers in therapeutic communities. Our findings elucidate how different treatment providers: (1) invoke the authority and make use of neuroscience in practice (2) make reference to neuroscientific concepts (e.g., neuroplasticity); and sometimes represent the brain using vivid neurobiological language, metaphors, and stories; and, (3) question the therapeutic benefits of discussing neuroscience and the use of neuroimages with clients. We argue that neurological ontologies of addiction, whilst shown to be selectively and strategically invoked in certain circumstances, may also at times be positioned as lacking centrality and salience within clinical work. In doing so, we render problematic any straightforward assumption about the universal import of neuroscience to practice that underpins narratives of ‘bench to bedside’ translation.}
}
@article{ZHANG2023101245,
title = {Tracing textual silences and ideological tensions in adopted inclusive education legislation in China},
journal = {Linguistics and Education},
volume = {78},
pages = {101245},
year = {2023},
issn = {0898-5898},
doi = {https://doi.org/10.1016/j.linged.2023.101245},
url = {https://www.sciencedirect.com/science/article/pii/S0898589823001043},
author = {Hui Zhang and Diana Arya},
keywords = {Critical realism, Legalization, Textual silence, Critical discourse analysis, Inclusive education},
abstract = {A substantial body of literature exists on inclusive education practices in large countries like China, ranging from the introduction of laws and regulations to cross-country comparisons of the implementation of policies. Yet, little is known about the potential role that legal documents play in shaping ideological assumptions and actions among stakeholders, including parents. The authors used critical discourse analysis (van Dijk, 1993) as an analytic guide for exploring explicated and implied meanings within both legal texts associated with China's “learning in the regular classroom” (LRC) model as well as reported data from parents of children diagnosed with autism spectrum disorder (ASD) about their experiences advocating for inclusive education in China through interpretative phenomenological analysis (Eatough & Smith, 2017). By examining the connections between discursive practice and social structures embedded in an institutional use of legal discourse and delving into the unique experiences of parents as insiders, the authors discovered not only that textual silence exists in LRC model legal texts regarding obligation, text precision, and dispute resolution but that it also influenced parents of children with ASD as they navigated and advocated for their children under the LRC model. This study is an extension of an earlier investigation of parental perspectives (Zhang, Qian & Singer, 2022). By revealing the typically obscured assumptions and implications of educational policies, our study demonstrates the potential significance and benefits of conducting similar research in different national contexts.}
}
@article{TIAN2021102325,
title = {Employment discrimination analysis of Library and Information Science based on entity recognition},
journal = {The Journal of Academic Librarianship},
volume = {47},
number = {2},
pages = {102325},
year = {2021},
issn = {0099-1333},
doi = {https://doi.org/10.1016/j.acalib.2021.102325},
url = {https://www.sciencedirect.com/science/article/pii/S0099133321000161},
author = {Ye Tian and Jingbei Zhang},
keywords = {Employment discrimination, Named entity recognition, Mainland China, Library and Information Sciences},
abstract = {The main purpose of this research is to provide an aggregated description of current employment discrimination in Library and Information Science (LIS) job market in Mainland China utilizing entity recognition approach. Specifically, a recruitment corpus with ontology-driven rules is firstly built. Then, the Bi-LSTM-CRF model on an annotated subset of the corpus is trained and verified by the rest of the corpus. Further, a quantitative statistics of the discrimination (via entities annotations) and an aggregation of the prediction of annual demand for jobs were conducted. Finally, we evaluate our approach by collecting 5297 LIS job advertisements in the public sector from 2015 to 2019 and conclude the result that average F1 of the entity recognition on 520 posts with 5411 entities is up to 91.06%. We statistically find that there exists serious institutional and employer discrimination ranging from political status (22.5%), age (15.4%), household registration (14.0%), to educational background (13.8%), etc. To our best knowledge, this is the first study investigating employment discrimination in the field of LIS in mainland China.}
}
@article{NORENA2025112529,
title = {Continuous learning of event-based systems by using pre-conceptual schemas},
journal = {Journal of Systems and Software},
volume = {230},
pages = {112529},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2025.112529},
url = {https://www.sciencedirect.com/science/article/pii/S0164121225001979},
author = {Paola Noreña and Elizabeth Suescún and Carlos M. Zapata and Bogdan Scaunasu and Geoffrey Elliot and Hans-Arno Jacobsen and David Mosquera},
keywords = {Continuous learning, Complex event processing, Event-driven systems, Pre-conceptual schemas, Software systems},
abstract = {An event-based system (EBS) and its core function of complex event processing (CEP) are highly used to react to the aggregation/combination of events translated into signals, messages, alerts, and data. Some modeling approaches have been used to represent elements of the domain of CEP and EBS. Understanding and knowledge about consistent elements of the domain and architecture are fundamental for continuous learning in the engineering software processes of a system. Continuous learning is a human sustainability practice used for the training of development teams in DevOps environments—a set of practices integrating software development (Dev) and IT operations (Ops). However, the modeling approaches often present such elements of the domain and CEP elements separated into two or more models. Therefore, they lack a consistent and complete view of the functionality of EBS. In this paper, we aim to integrate elements of domain and CEP in the same model by using a pre-conceptual schema (PCS) for continuous learning of EBS. PCS are conceptual models for representing and acquiring domain-specific knowledge containing events. We first propose a base model for representing the EBS, integrating elements of both the domain and the CEP based on PCS. We represent a lab study for a warehouse monitoring system, applying the base model, and after translating it into code. In this way, we evaluate the resulting model.}
}
@article{MENG20181,
title = {Privacy-aware cloud service selection approach based on P-Spec policy models and privacy sensitivities},
journal = {Future Generation Computer Systems},
volume = {86},
pages = {1-11},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17321271},
author = {Yunfei Meng and Zhiqiu Huang and Yu Zhou and Changbo Ke},
keywords = {Policy matching, Service selection, Privacy sensitivities},
abstract = {In cloud computing era, massive free and function-powerful cloud services can be selected by consumers at any time. However, owing to lacking of privacy policy transparency mechanism and privacy policy comparison mechanism, it is difficult for service consumers to distinguish what is a trusted service and what is a malicious service. To solve these problems, we conceive a comprehensive framework whose primary goal is to set up a policy matching engine as a service mediator to assist service consumers to select out a group privacy trusted services whose privacy policy can comply with consumer’s privacy preferences. Accordingly, we propose a formal policy specification language named P-Spec, which can be utilized to describe the service’s privacy policies and consumer’s privacy preferences explicitly. We further propose a privacy-aware service selection approach, which consists of a group of P-Spec policy models, introduced privacy metrics and a specific policy matching algorithm based on privacy sensitivities. To verify the effectiveness and feasibility of our approach, we implement a proof-of-concept prototype to carry out the relevant experimental studies. The experimental results illustrate our approach can still work well with increasing the scale of policy models. We further utilize the relevant linear fit theories to predict the execution performance of our approach in real cloud, the final predicted results illustrate its performance is permitted and can be improved in real. Lastly, we compare our P-Spec language with some other policy languages and evaluate the comparative results.}
}
@article{WANG2024101637,
title = {KddRES: A Multi-level Knowledge-driven Dialogue Dataset for Restaurant Towards Customized Dialogue System},
journal = {Computer Speech & Language},
volume = {87},
pages = {101637},
year = {2024},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101637},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824000202},
author = {Hongru Wang and Wai-Chung Kwan and Min Li and Zimo Zhou and Kam-Fai Wong},
keywords = {Task-oriented Dialogue System, Cantonese, Hierarchical Slots, Low-resource language, Customized Dialogue System},
abstract = {To alleviate the shortage of dialogue datasets for Cantonese, one of the low-resource languages, and facilitate the development of customized task-oriented dialogue systems, we propose KddRES, the first Cantonese Knowledge-driven dialogue dataset for REStaurants. It contains 834 multi-turn dialogues, 8000 utterances, and 26 distinct slots. The slots are hierarchical, and beneath the 26 coarse-grained slots are the additional 16 fine-grained slots. Annotations of dialogue states and dialogue actions at both the user and system sides are provided to suit multiple downstream tasks such as natural language understanding and dialogue state tracking. To effectively detect hierarchical slots, we propose a framework HierBERT by modelling label semantics and relationships between different slots. Experimental results demonstrate that KddRES is more challenging compared with existing datasets due to the introduction of hierarchical slots and our framework is particularly effective in detecting secondary slots and achieving a new state-of-the-art. Given the rich annotation and hierarchical slot structure of KddRES, we hope it will promote research on the development of customized dialogue systems in Cantonese and other conversational AI tasks, such as dialogue state tracking and policy learning.}
}
@article{KALITA2020190,
title = {Searching the great metadata timeline},
journal = {Library Hi Tech},
volume = {39},
number = {1},
pages = {190-204},
year = {2020},
issn = {0737-8831},
doi = {https://doi.org/10.1108/LHT-08-2019-0168},
url = {https://www.sciencedirect.com/science/article/pii/S0737883120000287},
author = {Deepjyoti Kalita and Dipen Deka},
keywords = {Metadata, Ontology, Metadata history, Library cataloguing, Linked data},
abstract = {Purpose
The purpose of this paper is to make a systematic review of the library metadata development history listing out the most significant landmarks and influencing events from Thomas Bodley's rules to the latest BIBFRAME architecture, compare their significance and suitability in the modern-day Web environment.
Design/methodology/approach
Four time divisions were identified, namely pre-1900 era, 1900–1950, post-1950 to pre-Web era and post-Web era based on pre-set information available to the authors regarding catalogue rules. Under these four divisions, relevant information sources regarding the purpose of the study were identified; various metadata standards released at different times were consulted.
Findings
Library catalogue standards have undergone transitive changes from one form to another primarily influenced by the changing work environment and different forms of resource availability in libraries. Modern-day metadata standards are influenced by the opportunities provided by the World Wide Web towards libraries and work as a suitable base for data organisation at par with Semantic Web standards.
Research limitations/implications
Information organisation processes have gone towards a more data-centric approach than earlier document-centric nature in current Semantic Web environment. Libraries had to make a move in this process, and modern-day guidelines in this regard bring the possibility of large-scale discovery services through curated information resources.
Originality/value
The study discovers relationships between key events in the course of development of metadata standards and provides suggestions and predictions regarding it's future developments.}
}
@article{TANG2019127,
title = {A review of building information modeling (BIM) and the internet of things (IoT) devices integration: Present status and future trends},
journal = {Automation in Construction},
volume = {101},
pages = {127-139},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.01.020},
url = {https://www.sciencedirect.com/science/article/pii/S0926580518305764},
author = {Shu Tang and Dennis R. Shelden and Charles M. Eastman and Pardis Pishdad-Bozorgi and Xinghua Gao},
keywords = {Building Information Modeling (BIM), Internet of Things (IoT) Device, Sensors, Smart building, Smart City, Smart built environment, Integration},
abstract = {The integration of Building Information Modeling (BIM) with real-time data from the Internet of Things (IoT) devices presents a powerful paradigm for applications to improve construction and operational efficiencies. Connecting real-time data streams from the rapidly expanding set of IoT sensor networks to the high-fidelity BIM models provides numerous applications. However, BIM and IoT integration research are still in nascent stages, there is a need to understand the current situation of BIM and IoT device integration. This paper conducts a comprehensive review with the intent to identify common emerging areas of application and common design patterns in the approach to tackling BIM-IoT device integration along with an examination of current limitations and predictions of future research directions. Altogether, 97 papers from 14 AEC related journals and databases in other industry over the last decade were reviewed. Several prevalent domains of application namely Construction Operation and Monitoring, Health & Safety Management, Construction Logistic & Management, and Facility Management were identified. The authors summarized 5 integration methods with description, examples, and discussion. These integration methods are utilizing BIM tools' APIs and relational database, transform BIM data into a relational database using new data schema, create new query language, using semantic web technologies and hybrid approach. Based on the observed limitations, prominent future research directions are suggested, focusing on service-oriented architecture (SOA) patterns and web services-based strategies for BIM and IoT integration, establishing information integration & management standards, solving interoperability issue, and cloud computing.}
}
@article{DENG2024108947,
title = {Improving semantic similarity computation via subgraph feature fusion based on semantic awareness},
journal = {Engineering Applications of Artificial Intelligence},
volume = {136},
pages = {108947},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108947},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624011059},
author = {Yuanfei Deng and Wen Bai and Jiawei Li and Shun Mao and Yuncheng Jiang},
keywords = {Semantic similarity, Semantic awareness, Knowledge graph, -core, -truss},
abstract = {Semantic similarity is a critical aspect of natural language processing, as it evaluates the degree of similarity within a knowledge graph. Various computational methods, including distance-based and feature-based approaches, have been proposed to accurately measure this similarity. While existing methods can leverage diverse features within heterogeneous knowledge graphs, representing the overall structure, which encompasses a wide array of heterogeneous elements such as abstract descriptions and hidden relationships, remains challenging. To address the aforementioned challenges, our approach begins by employing the same text embedding method to map both abstract and category features into a unified vector space. We then extract features from DBpedia to construct concept and category graphs. Subsequently, we introduce a k-truss method based on semantic awareness within the DBpedia Concept Graph. This method identifies the significance of neighbouring concept nodes and assigns varying weights to enhance the representation of abstract features. Additionally, we propose a k-core method based on semantic awareness within the DBpedia Category Graph. This method identifies the importance of neighbouring category nodes and assigns different weights to enhance the representation of category features. Finally, we employ a hybrid weighting approach based on a feature fusion model to calculate semantic similarity. Experimental results demonstrate that our methods achieve a 5.33% improvement compared to existing approaches.}
}
@article{XIANG2024108181,
title = {Novel biomarkers associated with oxidative stress and immune infiltration in intervertebral disc degeneration based on bioinformatics approaches},
journal = {Computational Biology and Chemistry},
volume = {112},
pages = {108181},
year = {2024},
issn = {1476-9271},
doi = {https://doi.org/10.1016/j.compbiolchem.2024.108181},
url = {https://www.sciencedirect.com/science/article/pii/S1476927124001695},
author = {Min Xiang and Yue Lai and Jianlin Shen and Bo Wei and Huan Liu and Wenhua Huang},
keywords = {Intervertebral disc degeneration, Oxidative stress, Immune infiltration, PPIA, PXN},
abstract = {Background
The etiology of intervertebral disc degeneration (IVDD), a prevalent degenerative disease in the elderly, remains to be fully elucidated. The objective of this study was to identify immune infiltration and oxidative stress (OS) biomarkers in IVDD, aiming to provide further insights into the intricate pathogenesis of IVDD.
Methods
The Gene Expression microarrays were obtained from the Gene Expression Omnibus (GEO) database. We conducted enrichment analysis of Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) terms. Subsequently, the R language packages CIBERSORT, MCPcounter, and WGCNA were employed to compare immune infiltration levels between IVDD samples and control samples. A protein-protein interaction (PPI) network was constructed using the Search Tools for the Retrieval of Interacting Genes (STRING) database to identify significant gene clusters. To identify hub genes, we employed Cytoscape's Molecular Complex Detection (MCODE) plug-in. The mRNA levels of hub genes in the cell model were validated by qPCR, while Western blotting was used to validate their protein levels.
Results
The GSE70362 dataset from the GEO database identified a total of 1799 genes that were differentially expressed. Among these, 43 genes were found to be differentially expressed and also associated with OS. The differentially expressed genes associated with OS and the immune-related module genes identified through WGCNA were further intersected, resulting in the identification of 10 key genes that were differentially expressed and played crucial roles in both immune response and OS. Subsequently, we validated four diagnostic markers (PPIA, MAP3K5, PXN, and JAK2) using the GSE122429 external dataset. In a cellular model of OS in NP cells, we have identified the upregulation of PPIA and PXN genes, which could serve as novel markers for IVDD.
Conclusion
The study successfully identified and validated differentially expressed genes associated with oxidative stress and immune infiltration in IVDD samples compared to normal ones. Notably, the newly discovered biomarkers PPIA and PXN have not been previously reported in IVDD-related research.}
}
@incollection{KALEMI2018495,
title = {Semantic Networking Facility for the Biorefining Community},
editor = {Anton Friedl and Jiří J. Klemeš and Stefan Radl and Petar S. Varbanov and Thomas Wallek},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {43},
pages = {495-500},
year = {2018},
booktitle = {28th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-64235-6.50088-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780444642356500887},
author = {Edlira Kalemi and Linsey Koo and Franjo Cecelja},
keywords = {wiki, semantic technologies, biorefining community, knowledge sharing},
abstract = {Biorefining is a dynamic field with ever growing number of computer models developed, heterogeneous data acquired and generally new knowledge generated in large volumes, all to serve functions at different scales and for different purposes. Sharing and reusing of these resources, especially models and data, inherently saves developing time, but also enables solving ever more complex problems and addressing ever more complex functions. This, in turn, necessitates efficient networking tool to allow for publishing, discovery, but also inferring new knowledge and integration of resources, dominantly computer models. This paper introduces a semantic networking facility developed to enable an efficient reuse and sharing of knowledge and resources in the domain of biorefining. This networking facility, which is built around model integration platform, allows for knowledge and other resources in the domain of biorefining to be accessible by humans, but also by agents and services. It is organised as a collaborative, WiKi like platform, aiming to facilitate collaboration and to bring biorefining community together by sharing knowledge and resources. The operation of the proposed facility is supported by a biorefining domain ontology, but also by Semantic Media Wiki extension. The features of the Biorefining Semantic Wiki include semantic repository of the biorefining models, model discovery and integration tool, supply chain networking and demonstration, documentation, including published article repository and discovery, forum for the community and with active social media of the biorefining community. The operation of Biorefining Semantic Wiki has been implemented as a semantic web service and its operation verified in practice.}
}
@article{GARCIANUNES20191,
title = {Using a conceptual system for weak signals classification to detect threats and opportunities from web},
journal = {Futures},
volume = {107},
pages = {1-16},
year = {2019},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2018.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0016328717303270},
author = {Pedro Ivo Garcia-Nunes and Ana Estela Antunes {da Silva}},
keywords = {Competitive intelligence, Knowledge representation, Conceptual system, Strategic discontinuities, Weak signal},
abstract = {The concept of weak signal (WS) was proposed by Igor Ansoff in the 1970s to describe early information about an emerging phenomenon in organizational environments. Surprising events can be avoided by methods for monitoring and analyzing these signals, continually. These methods enable the detection process of potential threats and opportunities that may arise from those surprising events. While this strategic process is very useful, conventional methods for WS monitoring and analysis do not avoid cognitive and motivational problems that can harm the competitive intelligence capacity of organizations. The solution of these problems can be related to methods that allow conducting the detection process collectively, quickly and with the least effort, through the eventual support of automatic tools. Ontologies, for example, are computational artifacts that represent organizational knowledge whose features can be used in a conceptual system for classification of WS. The purpose of this work is to present such system, which also uses Web crawling, competitive forces and PESTLE analysis in the detection of threats and opportunities. A case example also is discussed showing that this approach can be used as a viable alternative to support surveillance of strategic discontinuities that permeate the organizational environment.}
}
@article{SPREAFICO2021103525,
title = {Using text mining to retrieve information about circular economy},
journal = {Computers in Industry},
volume = {132},
pages = {103525},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103525},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521001329},
author = {Christian Spreafico and Matteo Spreafico},
keywords = {Circular economy, Text mining, Dependency patterns, Patents},
abstract = {This paper proposes a method of text mining to automatically retrieve knowledge from patents on how to recycle and reuse a waste. The main novelties are the introduction of a set of specific dependency patterns and the introduction of a partially revised TRIZ (Russian acronym for "Theory of Inventive Problem Solving") ontology to classify the retrieved information. The proposed dependency patterns were manually extracted from a sample patents pool about waste recycling and reuse. The classification of the information is based on different classes: (1) what transformations can be carried out on the waste, (2) what technologies can be used to carry out these transformations, (3) what products can be obtained by transforming the waste, (4) what functions can be carried out by the waste, (5) with which technologies, and (6) on which entities. An automatic implementation of the proposed method, involving the manual check of the retrieved results, was tested through a case study about wood chip recycling and reuse. Compared to the dependency patterns from the literature, the proposed ones allowed to retrieve 28 % more pertinent information. This results mainly depends by better ability of the proposed patterns to better discriminate the relevant sentences from which to extract information, compared to the other patterns (i.e. + 40 %). The automatic classification of the information was also correctly performed: in almost each class, precision and recall were higher than 60 % and on average equal to 90 %.}
}
@article{WEN2024100197,
title = {A practical guide to apply AI in childhood cancer: Data collection and AI model implementation},
journal = {EJC Paediatric Oncology},
volume = {4},
pages = {100197},
year = {2024},
issn = {2772-610X},
doi = {https://doi.org/10.1016/j.ejcped.2024.100197},
url = {https://www.sciencedirect.com/science/article/pii/S2772610X24000576},
author = {Shuping Wen and Stefan Theobald and Pilar Gangas and Karina C. {Borja Jiménez} and Johannes H.M. Merks and Reineke A. Schoot and Marcel Meyerheim and Norbert Graf},
keywords = {Childhood cancer, Paediatric cancer, Artificial intelligence, Machine learning, Healthcare data, Regulatory Framework},
abstract = {Childhood cancer is a leading cause of death in children, and the increasing availability of digital healthcare data, coupled with rapid progress in artificial intelligence (AI), brings a transformative opportunity to revolutionise its diagnosis, treatment and ultimately improve patient outcomes by leveraging diverse data resources. However, the effective application of AI in childhood cancer requires strict adherence to regulatory and best practice guidelines for patient data preparation and AI model development. Currently, there is a lack of such regulatory and methodological guidance specifically tailored for the paediatric community. This review seeks to address this gap. Beginning with an overview of existing regulatory frameworks, it examines the types of data currently in use or with potential use in developing AI applications for childhood cancer. This encompasses data from traditional sources, such as patient data and electronic health records (EHRs), as well as emerging sources like social media data and social determinants of health. This review also outlines the rules and criteria for collecting, processing, and sharing these data. Informed consent and re-consent are required for data collection and re-use, and data quality, privacy, and security as well as data standardisation, harmonisation and interoperability are important for data processing. Additionally, this review clarifies the essential requirements and methodologies for developing AI models in childhood cancer and healthcare. It also emphasises the importance of AI being trustworthy, protecting privacy, and being accountable and validated in clinical settings. By systematically addressing these key components, this review aims to provide comprehensive knowledge and practical tools for the reliable application and implementation of AI in paediatric cancer to enhance AI acceptance and promote its widespread integration within the childhood cancer community. This, in turn, will lead to improved diagnosis, treatment and outcomes for children with cancer.}
}
@article{GE2025103768,
title = {Resilient cognitive twin: A generative approach},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103768},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103768},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625006615},
author = {Chenyu Ge and Shengfeng Qin},
keywords = {Human-Cyber-Physical System, Digital twin, Cognitive twin, Engineered system, Resilience, Decision assistance, Smart city},
abstract = {Improving the resilience of an engineering system can be addressed from both a physical engineered system (PES) and its virtual replica--digital twin (DT) system. On the one hand, a DT can enhance the resilience of its PES by enabling real-time monitoring/proactive intervention and predictive maintenance. On the other hand, its deep integration with PES also increases the complexity of the overall system--both virtual and physical. This interdependence can transform single-sided risks into overall system-wide vulnerabilities, potentially undermining--even compromising--the resilience of the PES itself. To address this challenge, this paper proposes a new concept and framework of the Resilient Cognitive Twin (RCT), that supports generative reconfiguration and bidirectional feedback across the human–cyber–physical continuum. The framework is supported by a generative approach with three new enabling mechanisms: (1) a requirement decomposition and recompositing mechanism with edge-cloud-centre collaboration for forming a unified yet loosely coupled and low-level foundation of RCT; (2) a dynamic evolution mechanism enabling right-time co-evolution of data-model-service relationships for a changing environment and stakeholder needs; and (3) a generative cognitive mechanism for situational awareness and decision-making, responding to changing situations with proper and resilient services and their coordination/scheduling. The proposed framework and enabling approach are validated through an urban flood resilient DT system, demonstrating its capability to enhance resilience in complex, distributed environments, paving the way for Human-Cyber-Physical Systems and future industrial and societal applications.}
}
@article{MAZZETTI2024106002,
title = {The model-as-a-resource paradigm for geoscience digital ecosystems},
journal = {Environmental Modelling & Software},
volume = {176},
pages = {106002},
year = {2024},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2024.106002},
url = {https://www.sciencedirect.com/science/article/pii/S136481522400063X},
author = {Paolo Mazzetti and Stefano Nativi},
keywords = {Scientific models, Interoperability, Digital ecosystems, MaaR, Earth observation},
abstract = {A long-term goal of environmental science and Earth observation is to enable the creation of a “Model Web” of semantically interconnected data and models. Geospatial models are usually exposed on the Web as services accessible through heterogeneous interfaces. However, such services, which represent instances of the paradigm called Model-as-a-Service (MaaS), cannot be easily exploited beyond their original use as defined by the service provider. To overcome this important limitation and better support transparency, reproducibility, replicability and reusability of the model (following the Open Science paradigm), we investigated the adoption of a Model-as-a-Resource (MaaR) approach, in which a model is considered a generic digital resource that, as such, can play different roles in different potential use cases. The proposed MaaR framework can play an important enabling role in the realization of those digital ecosystems that generate environmental knowledge. The main challenges and opportunities are discussed in the manuscript.}
}
@article{BAKIR2024100315,
title = {Comprehensive expression profiling analysis to investigate salt and drought responding genes in wild barley (Hordeum spontaneum L.)},
journal = {Plant Stress},
volume = {11},
pages = {100315},
year = {2024},
issn = {2667-064X},
doi = {https://doi.org/10.1016/j.stress.2023.100315},
url = {https://www.sciencedirect.com/science/article/pii/S2667064X23001823},
author = {Melike Bakır and Ahu {Altınkut Uncuoğlu} and Canan {Yüksel Özmen} and Funda {Yılmaz Baydu} and Kemal Kazan and Umut Kibar and Karen Schlauch and John C. Cushman and Ali Ergül},
keywords = {Wild barley ( L.), Drought and salinity, Transcriptome analysis},
abstract = {Drought and salinity are among the most affecting abiotic stress factors that limit the productivity in economically important plants. Understanding drought and salt tolerance mechanism is pivotal for developing drought and salt tolerant crop cultivars. Here, we monitored the changes in gene expression profiles of leaf and root tissues of two wild barley (Hordeum spontaneum L.) accessions after treatment with salt and drought stresses. We identified a total of 641 DEGs across 24 conditions (2 accessions, 2 stress conditions, 2 tissues and 3 time-points). Interestingly, there was a relatively little overlap between the leaf and the root stress responsive gene expression patterns, suggesting that different stress associated processes might be operating in these tissues during stress adaptation. In hierarchical clustering, the genes with distinct expression patterns were nearly half found in Cluster 1 and Cluster 9 and DEGs found in Cluster 9 were significantly enriched for metabolic processes and organic substance metabolic process. Gene ontology (GO) annotations of biological process of salt and drought analysis identified genes in categories including, catabolic processes, oxidation–reduction and organic substance metabolic process. Molecular function analyses significantly detected transcripts associated with oxidoreductase activity and nucleotide binding. Overall, our results revealed a number of candidate genes and plant processes associated with stress tolerance in wild barley. Determination of responsive genes will shed light on other barley studies aiming to elucidate molecular mechanism of drought or salt tolerance.}
}
@article{CHOUDHURY2019315,
title = {Islamic finance instruments for promoting long-run investment in the light of the well-being criterion (maslaha)},
journal = {Journal of Islamic Accounting and Business Research},
volume = {10},
number = {2},
pages = {315-339},
year = {2019},
issn = {1759-0817},
doi = {https://doi.org/10.1108/JIABR-11-2016-0133},
url = {https://www.sciencedirect.com/science/article/pii/S1759081719000027},
author = {Masudul Alam Choudhury and Mohammad Shahadat Hossain and Mohammad Taqiuddin Mohammad},
keywords = {Abstract-empirical methodology, Equity-based Islamic financing instruments, Long-run investment, Well-being analysis of poverty alleviation},
abstract = {Purpose
The purpose of this study of this methodological abstraction is erected the nature of the well-being function as evaluative criterion. The well-being function (maslaha) evaluates the interrelationships between long-run investment (real sector), the corresponding financial instruments (financial sector) and the embedded socioeconomic variables and ethical values conveyed by extensive complementarities and participation in a systemic approach of unity of knowledge. Among the financing variables to be selected will be the transformation of debt-instruments into equity instruments. All financial instruments are to be transformed into a holistic participatory pooled portfolio.
Design/methodology/approach
The paper establishes the point that, the idea of long-run is appropriately that of a juncture of Islamic change during which the objective of well-being (maslaha) is evaluated (estimation leading to simulation) with long-run investment and Islamic financing instruments on the basis of the Islamic methodological worldview. This methodological worldview is premised on the ontological foundation of the episteme of organic unity of knowledge and the resulting world-system. The Qur’an refers to this foundation of knowledge as Tawhid. Tawhid is used in this paper to mean the Primal Ontological Law of Unity of Knowledge.
Findings
The most critical long-run investment program focused on is poverty alleviation and its equity-based financing instruments that reduce debt progressively to attain sustainable grassroots development with the ability to own, and the social capability to distribute resources and enable the grassroots. The corresponding interaction, integration and evolutionary dynamics of learning that emanate from the interrelationship of poverty alleviation as the focus of long-run investments and their attenuating financing instruments, along with the implications of inter-causal socioeconomic variables and the embedded episteme of unity of knowledge in the well-being function (maslaha). This paper is thus an abstracto-empirical contribution to the literature of Islamic finance, long-run investment and socioeconomic development with global significance.
Research limitations/implications
The choice of long-run investment for poverty alleviation and the corresponding Islamic financing instruments are summarized by the following Tawhidi epistemic schema (an extractive picture). Upon this epistemic methodological worldview, the entire structure of well-being and sustainability of socioeconomic development lies.
Practical implications
The paper brings out many of the properties that ought to be the truly moral/ethical and thereby the conformable analytical nature of the model of financing and investment in a combination of short-, medium- and long-term mobilization of resources to attain levels of social well-being as the objective criterion. Empirical work is done to bring the objective criterion to an applied level and to critically examine the work in the same field being carried out by many other ones, including authors and institutions. The empirical work done here can be widely extended to the case of estimating of the maslaha function (well-being).
Social implications
This paper carries an essentially moral and social perspective in its methodological orientation that is derived from the Islamic epistemological foundations of unity of knowledge (Tawhid) and applied to Islamic finance and investment theory with the well-being objective criterion.
Originality/value
This is an original paper that combines methodological abstraction with applied financing and investment perspectives. Such an abstracto-empirical approach has not been done in Islamic research writings.}
}
@article{MARCHESIN2022100139,
title = {Empowering digital pathology applications through explainable knowledge extraction tools},
journal = {Journal of Pathology Informatics},
volume = {13},
pages = {100139},
year = {2022},
issn = {2153-3539},
doi = {https://doi.org/10.1016/j.jpi.2022.100139},
url = {https://www.sciencedirect.com/science/article/pii/S2153353922007337},
author = {Stefano Marchesin and Fabio Giachelle and Niccolò Marini and Manfredo Atzori and Svetla Boytcheva and Genziana Buttafuoco and Francesco Ciompi and Giorgio Maria {Di Nunzio} and Filippo Fraggetta and Ornella Irrera and Henning Müller and Todor Primov and Simona Vatrano and Gianmaria Silvello},
keywords = {Clinical practice, Digital pathology, Expert systems, Explainable AI, Knowledge extraction, Machine learning},
abstract = {Exa-scale volumes of medical data have been produced for decades. In most cases, the diagnosis is reported in free text, encoding medical knowledge that is still largely unexploited. In order to allow decoding medical knowledge included in reports, we propose an unsupervised knowledge extraction system combining a rule-based expert system with pre-trained Machine Learning (ML) models, namely the Semantic Knowledge Extractor Tool (SKET). Combining rule-based techniques and pre-trained ML models provides high accuracy results for knowledge extraction. This work demonstrates the viability of unsupervised Natural Language Processing (NLP) techniques to extract critical information from cancer reports, opening opportunities such as data mining for knowledge extraction purposes, precision medicine applications, structured report creation, and multimodal learning. SKET is a practical and unsupervised approach to extracting knowledge from pathology reports, which opens up unprecedented opportunities to exploit textual and multimodal medical information in clinical practice. We also propose SKET eXplained (SKET X), a web-based system providing visual explanations about the algorithmic decisions taken by SKET. SKET X is designed/developed to support pathologists and domain experts in understanding SKET predictions, possibly driving further improvements to the system.}
}
@article{YOUNES2021320,
title = {Building Bi-script Language Resources for the Tunisian Dialect’s NLP},
journal = {Procedia Computer Science},
volume = {189},
pages = {320-327},
year = {2021},
note = {AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.05.101},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921012254},
author = {Jihene Younes and Hadhemi Achour and Emna Souissi and Ahmed Ferchichi},
keywords = {Tunisian dialect, Romanized, Arabic, Language resources, Corpus, Lexicon, Dictionary, Identification, Transliteration, Deep learning, Social web},
abstract = {Language resources like corpora, lexicons and dictionaries are the key element to automatically process any natural language. In this paper, we focus on the written Tunisian dialect (TD) which is abundantly present on social media and yet still qualified as a low-resource language. We automatically construct sizable bi-script TD language resources from the social web using two deep learning based-NLP components for the TD identification and the TD transliteration. We target the Latin transcription of the language and we use it to generate TD language resources written in the Arabic script. The presented work resulted in creating a Romanized TD corpus composed of 284,894 TD messages extracted from YouTube, an Arabic TD corpus generated through an automatic Latin to Arabic transliteration of the extracted Romanized TD corpus, and two bi-script TD dictionaries (Latin-to-Arabic and Arabic-to-Latin), respectively composed of 293,570 entries and 155,954 entries. The creation process and the constructed TD resources are described and evaluated.}
}
@article{MA2024105889,
title = {Multi-granularity retrieval of mineral resource geological reports based on multi-feature association},
journal = {Ore Geology Reviews},
volume = {165},
pages = {105889},
year = {2024},
issn = {0169-1368},
doi = {https://doi.org/10.1016/j.oregeorev.2024.105889},
url = {https://www.sciencedirect.com/science/article/pii/S0169136824000222},
author = {Kai Ma and Junyuan Deng and Miao Tian and Liufeng Tao and Junjie Liu and Zhong Xie and Hua Huang and Qinjun Qiu},
keywords = {Multi-granularity association, Mineral resource report, Geological text mining, Natural language processing, Semantic retrieval},
abstract = {Massive geologic report contains all kinds of multimodal geologic data information (geologic text, geologic maps, geologic tables, etc.), which contain a lot of rich geologic basic knowledge and expert experience knowledge about rocks and minerals, stratigraphic structure, geologic age, geographic location, and so on. Accurate retrieval of specific information from massive geologic data has become an important need for geologic information retrieval. However, the majority of existing research primarily revolves around extracting and associating information at a single granularity to facilitate geological semantic retrieval, which ignores many potential semantic associations, leading to ambiguity and fuzziness in semantic retrieval. To solve these problems, this paper proposes a multi-granularity (document-chapter-paragraph) geological information retrieval framework for accurate semantic retrieval. The framework firstly extracts topic feature information, spatiotemporal feature information, figure and table feature information based on the multi-granularity of geological reports. Then, an improved apriori algorithm is used to mine and visualize the associations among the feature information to discover the semantic associations of the geological reports at multiple levels of granularity. Finally, experiments are designed to validate the application of the proposed multi-granularity information retrieval framework on the accurate retrieval of geological reports. The experimental results show that the proposed multi-granularity information retrieval framework in this paper can dig deeper into underlying geo-semantic information and realize accurate retrieval.}
}
@article{BLANKESTIJN2024807,
title = {Whole blood transcriptome in long-COVID patients reveals association with lung function and immune response},
journal = {Journal of Allergy and Clinical Immunology},
volume = {154},
number = {3},
pages = {807-818},
year = {2024},
issn = {0091-6749},
doi = {https://doi.org/10.1016/j.jaci.2024.04.032},
url = {https://www.sciencedirect.com/science/article/pii/S0091674924005669},
author = {Jelle M. Blankestijn and Nadia Baalbaki and Somayeh Bazdar and Inés Beekers and Rosanne J.H.C.G. Beijers and Joop P. {van den Bergh} and Lizan D. Bloemsma and Merel E.B. Cornelissen and Tamara Dekker and Jan Willem Duitman and Laura Houweling and John J.L. Jacobs and Ivo {van der Lee} and Paulien M.A. Linders and Lieke C.E. Noij and Esther J. Nossent and Marianne A. {van de Pol} and Brigitte M. Sondermeijer and J.J. Miranda Geelhoed and Els J.M. Weersink and Korneliusz Golebski and Mahmoud I. Abdel-Aziz and Anke H. {Maitland-van der Zee}},
keywords = {Long-COVID, phenotyping, lung function, DLCO, transcriptome},
abstract = {Background
Months after infection with severe acute respiratory syndrome coronavirus 2, at least 10% of patients still experience complaints. Long-COVID (coronavirus disease 2019) is a heterogeneous disease, and clustering efforts revealed multiple phenotypes on a clinical level. However, the molecular pathways underlying long-COVID phenotypes are still poorly understood.
Objectives
We sought to cluster patients according to their blood transcriptomes and uncover the pathways underlying their disease.
Methods
Blood was collected from 77 patients with long-COVID from the Precision Medicine for more Oxygen (P4O2) COVID-19 study. Unsupervised hierarchical clustering was performed on the whole blood transcriptome. These clusters were analyzed for differences in clinical features, pulmonary function tests, and gene ontology term enrichment.
Results
Clustering revealed 2 distinct clusters on a transcriptome level. Compared with cluster 2 (n = 65), patients in cluster 1 (n = 12) showed a higher rate of preexisting cardiovascular disease (58% vs 22%), higher prevalence of gastrointestinal symptoms (58% vs 29%), shorter hospital duration during severe acute respiratory syndrome coronavirus 2 infection (median, 3 vs 8 days), lower FEV1/forced vital capacity (72% vs 81%), and lower diffusion capacity of the lung for carbon monoxide (68% vs 85% predicted). Gene ontology term enrichment analysis revealed upregulation of genes involved in the antiviral innate immune response in cluster 1, whereas genes involved with the adaptive immune response were upregulated in cluster 2.
Conclusions
This study provides a start in uncovering the pathophysiological mechanisms underlying long-COVID. Further research is required to unravel why the immune response is different in these clusters, and to identify potential therapeutic targets to create an optimized treatment or monitoring strategy for the individual long-COVID patient.}
}
@article{TEBBE2022101808,
title = {Is natural language processing the cheap charlie of analyzing cheap talk? A horse race between classifiers on experimental communication data},
journal = {Journal of Behavioral and Experimental Economics},
volume = {96},
pages = {101808},
year = {2022},
issn = {2214-8043},
doi = {https://doi.org/10.1016/j.socec.2021.101808},
url = {https://www.sciencedirect.com/science/article/pii/S2214804321001488},
author = {Eva Tebbe and Benjamin Wegener},
keywords = {Laboratory experiments, Communication, Cheap talk, Classification of natural language messages, Machine learning},
abstract = {We conduct a horse race between various classification algorithms in order to assess whether some algorithms might be more appropriate for classifying experimental communication data than others. We use data reported by various experimental studies involving digital written communication. The effectiveness criterion for comparing the algorithms is based on the agreement between human message-codings and classifications generated by the respective text classification approaches. Our results show that Gradient Boosting Machines are a good choice for separating empty talk from relevant talk messages. This holds (1) independent of the training set size, (2) when the ratio of empty talk to relevant talk is low, and (3) when there are not only two, but three message classes.}
}
@article{JONEK2024139,
title = {Manual assembly planning with AI Image Generators},
journal = {Procedia CIRP},
volume = {130},
pages = {139-144},
year = {2024},
note = {57th CIRP Conference on Manufacturing Systems 2024 (CMS 2024)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.10.068},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124012216},
author = {Michael Jonek and Malte Bast and Martin Manns},
keywords = {Production planning, AI-assisted planning, AI art, Assembly instructions, Manual assembly},
abstract = {For small and medium-sized enterprises (SMEs), the planning of manual assembly activities represents a significant cost and resource factor, requiring precision and meticulous organization. To ensure a stable competitive and economical production, the steps involved in manual assembly must be optimized. In today’s digital era, Artificial Intelligences (AI) offer innovative approaches and opportunities to streamline processes. In addition to LLM AIs, AI image generators are currently attracting a lot of attention as they generate very realistic and detailed images based on a given description. Images or visualisations of the situation are often used to make work instructions in manual assembly easier to understand. AI image generators can be used to visualise assembly process steps for automatically generated work instructions. In this research, a quantitative measure is proposed that can be used to rate how correctly the actual situation is depicted by highlighting incorrect or false objects and operations and assessing the accuracy of the context. The measure is validated by qualitatively evaluating images created with DALL-E 3 in an user study by both workers and planning experts from industry and comparing them with the quantitative measure. This will enable further research in the field of automated work planning and the comparison of different AI image generation tools for use in assembly planning.}
}
@article{LOBBEN2020102929,
title = {Classifiers in non-European languages and semantic impairments in western neurological patients have a common cognitive structure},
journal = {Lingua},
volume = {245},
pages = {102929},
year = {2020},
issn = {0024-3841},
doi = {https://doi.org/10.1016/j.lingua.2020.102929},
url = {https://www.sciencedirect.com/science/article/pii/S0024384120301376},
author = {Marit Lobben and Agata Bochynska and Stine Tanggaard and Bruno Laeng},
keywords = {Category-specific semantic impairments, Classifier languages, Semantic deficits, Brain's semantic system, Semantic universals},
abstract = {We compared two non-overlapping data sets: (a) the semantic categories in representative samples of nominal classification languages (N=334); and in a non-overlapping population (b) semantic category-specific impairments of European neurological patients (N=121). Each of these appears to organize world objects in natural kinds (e.g., animal, body part, plant, fruit and vegetable, liquid) or manmade kinds (e.g., food, clothing, tool, vehicle, furniture). We show that whenever a specific semantic category is found as a cognitive impairment, this category also exists in some language as a semantic classifier. Since all of the existing semantic impairments reports are with speakers whose languages lack semantic nominal classification systems, the present regularities between grammar and cognitive deficit suggests the idea that cognitive universals may constrain how the neural substrate of knowledge is organized and can break apart but also how they can become expressed in different grammars.}
}
@article{PATERSONSHALLARD2020250,
title = {Holistic approaches to river restoration in Aotearoa New Zealand},
journal = {Environmental Science & Policy},
volume = {106},
pages = {250-259},
year = {2020},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2019.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S1462901118313285},
author = {Heather Paterson-Shallard and Karen Fisher and Meg Parsons and Leane Makey}
}
@article{SCHOONDERWOERD2022102831,
title = {Design patterns for human-AI co-learning: A wizard-of-Oz evaluation in an urban-search-and-rescue task},
journal = {International Journal of Human-Computer Studies},
volume = {164},
pages = {102831},
year = {2022},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2022.102831},
url = {https://www.sciencedirect.com/science/article/pii/S107158192200060X},
author = {Tjeerd A.J. Schoonderwoerd and Emma M. van Zoelen and Karel van den Bosch and Mark A. Neerincx},
keywords = {Human-AI co-learning, Human-AI collaboration, Design patterns, Learning design patterns, Urban-search-and-rescue, Wizard-of-Oz study},
abstract = {The rapid advancement of technology empowered by artificial intelligence is believed to intensify the collaboration between humans and AI as team partners. Successful collaboration requires partners to learn about each other and about the task. This human-AI co-learning can be achieved by presenting situations that enable partners to share knowledge and experiences. In this paper we describe the development and implementation of a task context and procedures for studying co-learning. More specifically, we designed specific sequences of interactions that aim to initiate and facilitate the co-learning process. The effects of these interventions on learning were evaluated in an experiment, using a simplified virtual urban-search-and-rescue task for a human-robot team. The human participants performed a victim rescue- and evacuation mission in collaboration with a wizard-of-Oz (i.e., a confederate of the experimenter who executed the robot-behavior consistent with an ontology-based AI-model). The designed interaction sequences, formulated as Learning Design Patterns (LDPs), were intended to bring about co-learning. Results show that LDPs support the humans understanding and awareness of their robot partner and of the teamwork. No effects were found on collaboration fluency, nor on team performance. Results are used to discuss the importance of co-learning, the challenges of designing human-AI team tasks for research into this phenomenon, and the conditions under which co-learning is likely to be successful. The study contributes to our understanding of how humans learn with and from AI-partners, and our propositions for designing intentional learning (LDPs) provide directions for applications in future human-AI teams.}
}
@article{ETTAHIRI2024534,
title = {Extended ArchiMate Metamodel with a context-awareness layer for a dynamic Enterprise architecture model},
journal = {Procedia Computer Science},
volume = {239},
pages = {534-540},
year = {2024},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2023},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.203},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924014467},
author = {Imane Ettahiri and Karim Doumi},
keywords = {Enterprise Architecture, dynamic, context-awareness, immunology systems},
abstract = {Nowadays, in a turbulent environment, enterprise must be proactive and even predictive to survive in front of the plenty natures of dynamics. Achieving the desired proactivity and predictability, supposes that the enterprise works in advance on its internal transformation plans, but also develops an awareness to its context, to intercept, analyze and simulate any potential trigger of change that could affect its stability or more precisely its state of equilibrium. In this work, the concept of enterprise architecture is considered as a tool for establishing alignment between business, strategy, and IT. This paper is a continuation to previous works seeking for a dynamic model of enterprise architecture, able to respond in an autonomic manner to the turbulent environment. The self-adaptiveness was highlighted basically using the well-known loop, monitor, analyze, plan, execute and Knowledge. The use of Case-Based Reasoning was suggested to store a collection of problems and their resolutions, then using algorithms of similarity and adaptation to suggest the problem’s best fit solution. To make the reasoning mechanism and similarity process as reliable as possible, it was important to maximize experiences and expand the knowledge base. For thus, the proposition was to share knowledge bases. In this paper, the context awareness is considered as second source to enrich the knowledge base and to stay vigilant and sensitive to triggers of change.}
}
@incollection{SVITEK202187,
title = {9 - Complex adaptive systems},
editor = {Miroslav Svítek},
booktitle = {Information Physics},
publisher = {Academic Press},
pages = {87-98},
year = {2021},
isbn = {978-0-323-91011-8},
doi = {https://doi.org/10.1016/B978-0-323-91011-8.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323910118000056},
author = {Miroslav Svítek},
keywords = {Basic Smart City agent, Smart City concept, City Information Management, Smart Resilient City (SRC), intelligent transport systems, telematics, ITS architecture, ontology, multiagent technologies (MATs), multiagent systems (MAS), Key Performance Indicators (KPIs)},
abstract = {The chapter deals with practical examples of providing smart services using software agents. Each software agent negotiates the future steps with other agents in the virtual information environment. For the effective agents’ negotiations, the so-called KPIs (Key Performance Indicators) and ontological rules (semantic interoperability) must be set so that the software agents understand each other. The presented approach is shown on the specific examples of smart resilient cities or intelligent transport systems, the two being the longtime interest areas of the author.}
}
@article{ROKEM2025101316,
title = {Open-source models for development of data and metadata standards},
journal = {Patterns},
volume = {6},
number = {7},
pages = {101316},
year = {2025},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2025.101316},
url = {https://www.sciencedirect.com/science/article/pii/S2666389925001643},
author = {Ariel Rokem and Vani Mandava and Nicoleta Cristea and Anshul Tambay and Kristofer Bouchard and Carolina Berys-Gonzalez and Andy Connolly},
keywords = {data, metadata, standards, open-source software},
abstract = {Summary
Machine learning and artificial intelligence promise to accelerate research and understanding across many scientific disciplines. Harnessing the power of these techniques requires aggregating scientific data. In tandem, the importance of open data for reproducibility and scientific transparency is gaining recognition, and data are increasingly available through digital repositories. Leveraging efforts from disparate data collection sources, however, requires interoperable and adaptable standards for data description and storage. Through the synthesis of experiences in astronomy, high-energy physics, earth science, and neuroscience, we contend that the open-source software (OSS) model provides significant benefits for standard creation and adaptation. We highlight resultant issues, such as balancing flexibility vs. stability and utilizing new computing paradigms and technologies, that must be considered from both the user and developer perspectives to ensure pathways for recognition and sustainability. We recommend supporting and recognizing the development and maintenance of OSS data standards and software consistent with widely adopted scientific tools.}
}
@article{GALER2024101211,
title = {Clinical signatures of genetic epilepsies precede diagnosis in electronic medical records of 32,000 individuals},
journal = {Genetics in Medicine},
volume = {26},
number = {11},
pages = {101211},
year = {2024},
issn = {1098-3600},
doi = {https://doi.org/10.1016/j.gim.2024.101211},
url = {https://www.sciencedirect.com/science/article/pii/S109836002400145X},
author = {Peter D. Galer and Shridhar Parthasarathy and Julie Xian and Jillian L. McKee and Sarah M. Ruggiero and Shiva Ganesan and Michael C. Kaufman and Stacey R. Cohen and Scott Haag and Chen Chen and William K.S. Ojemann and Dan Kim and Olivia Wilmarth and Priya Vaidiswaran and Casey Sederman and Colin A. Ellis and Alexander K. Gonzalez and Christian M. Boßelmann and Dennis Lal and Rob Sederman and David Lewis-Smith and Brian Litt and Ingo Helbig},
keywords = {Developmental epileptic encephalopathy, Electronic medical record, Epilepsy, Natural language processing, Precision medicine},
abstract = {Purpose
An early genetic diagnosis can guide the time-sensitive treatment of individuals with genetic epilepsies. However, most genetic diagnoses occur long after disease onset. We aimed to identify early clinical features suggestive of genetic diagnoses in individuals with epilepsy through large-scale analysis of full-text electronic medical records.
Methods
We extracted 89 million time-stamped standardized clinical annotations using Natural Language Processing from 4,572,783 clinical notes from 32,112 individuals with childhood epilepsy, including 1925 individuals with known or presumed genetic epilepsies. We applied these features to train random forest models to predict SCN1A-related disorders and any genetic diagnosis.
Results
We identified 47,774 age-dependent associations of clinical features with genetic etiologies a median of 3.6 years before molecular diagnosis. Across all 710 genetic etiologies identified in our cohort, neurodevelopmental differences between 6 to 9 months increased the likelihood of a later molecular diagnosis 5-fold (P < .0001, 95% CI = 3.55-7.42). A later diagnosis of SCN1A-related disorders (area under the curve [AUC] = 0.91) or an overall positive genetic diagnosis (AUC = 0.82) could be reliably predicted using random forest models.
Conclusion
Clinical features predictive of genetic epilepsies precede molecular diagnoses by up to several years in conditions with known precision treatments. An earlier diagnosis facilitated by automated electronic medical records analysis has the potential for earlier targeted therapeutic strategies in the genetic epilepsies.}
}
@article{WU2025103258,
title = {Digital thread in engineering: Concept, state of art, and enabling framework},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103258},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103258},
url = {https://www.sciencedirect.com/science/article/pii/S147403462500151X},
author = {Shouxuan Wu and Guoxin Wang and Jinzhi Lu and Yan Yan and Yihui Gong and Mengru Dong and Dimitris Kiritsis},
keywords = {Digital thread, Systematic analysis, Bibliometric analysis, Engineering},
abstract = {The digital thread (DT), a new paradigm for the development of complex engineering systems, has garnered widespread attention in recent years. The DT serves as a key driver and game-changer for the digital transformation of engineering systems due to its advantages in promoting the collaboration of stakeholders to aid decision-making. This study provides a systematic and quantitative survey of publications about the DT. By using bibliometric analysis tools, current progress on the DT in the engineering domain is visually identified. We analyzed the distribution of research teams, professional publishing sources, current research topics, and future research trends, revealing that current research of DT emphasizes the features of lifecycle, integration, interoperability, and decision-making inspired by the data-information-knowledge-wisdom (DIKW) model. Through a systematic review of relevant publications from the perspective of these key features, we propose an enabling framework of the DT, including the concept, reference architecture, and “3I” enabling technologies (integration; interoperability; and intelligent decision technologies). Finally, we outline the current challenges faced by the DT in the engineering domain, including issues about integration challenges, professional workforce, and cost analysis. This study aims to facilitate a more comprehensive understanding of the current states and research directions of the DT in the engineering domain.}
}
@article{DASCAL2020310,
title = {What's left for the neo-Copenhagen theorist},
journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
volume = {72},
pages = {310-321},
year = {2020},
issn = {1355-2198},
doi = {https://doi.org/10.1016/j.shpsb.2019.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S135521981830220X},
author = {Michael Dascal},
abstract = {Frauchiger and Renner (2018) argue that no ‘single-world’ theory can consistently maintain quantum mechanical predictions for all systems. Following Bub (2017, 2018, 2019), I argue here that this is overstated, and use their result to develop a framework for neo-Copenhagen theories that avoid the problem. To describe the framework I introduce two concepts, ontological information deficits, and information frames, and explore how these may ultimately be fleshed out by the theorist. I then consider some immediate worries that may be raised against the framework, and conclude by looking at how some existing theories may be seen to fit into it.}
}
@incollection{LUSSIER20241,
title = {1.01 - Enhancing Precision Medicine and Wellness with Computing and AI across Clinical, Imaging, Environmental, Multi-Omics, Wearable Sensors, and Socio-Cognitive Data},
editor = {Kenneth S. Ramos},
booktitle = {Comprehensive Precision Medicine (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {1-8},
year = {2024},
isbn = {978-0-12-824256-8},
doi = {https://doi.org/10.1016/B978-0-12-824010-6.00082-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128240106000824},
author = {Yves A. Lussier},
keywords = {Artificial intelligence and medicine, Biomedical informatics, Biomedical ontologies and terminologies, Biomedical semantic interoperability, Biomedical syntactic interoperability, Clinical data science, Clinical decision support, Clinical explainability and interpretability, Clinical reasoning with certainty and uncertainty, Electronic health record, Genomic medicine, ML and medicine, Multi-omics medicine, N-of-1 trials, Precision diagnostic, Precision medicine, Precision therapeutic, Precision wellness, Predictive clinical analytics, Single-subject studies},
abstract = {The convergence of computational capabilities and data-driven methodologies, spurred by the onset of the Third Industrial Revolution (Digital Age) as well as their integration into robotic tools and medical implants, often referred to as the Fourth Industrial Revolution (Park, 2016), is instigating a substantive transformation in clinical decision support that is rapidly changing the economics and practice of healthcare (Sutton et al., 2020). Here we introduce the strengths, challenges, and future trajectories of computational medicine, informatics, and machine learning (ML) methods as applied in the realm of precision healthcare and wellness (Lee et al., 2018). Precision medicine and precision wellness, situated at the intersection of technological advancements in clinical decision-making methods that impact the use, reuse, transformations, and analysis of data ranging from the nanoscale to clinical and societal dimensions of measurements, is the focal point of our discussion. We approach this subject through a dual perspective: one being a clinical data-focused approach that incorporates biomedical informatics with syntactic and semantic methods of interoperability (Garde et al., 2007; Strasberg et al., 2021), as well as human-interpretable decision algorithm and the other being a bottom-up approach rooted in genomics, biophysical and multiscale methods that increasingly employ robust yet human-opaque ML analytics (Lussier and Li, 2012). We conclude with an exploration of the remaining challenges, prospective opportunities, and future directions that arise at the confluence of these multifaceted methodologies inclusive of artificial intelligence and large language models in medicine (Shehab et al., 2022).}
}
@article{LU2023119920,
title = {A service composition evolution method that combines deep clustering and a service requirement context model},
journal = {Expert Systems with Applications},
volume = {224},
pages = {119920},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.119920},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423004219},
author = {Jiawei Lu and Jiahong Zheng and Zhenbo Chen and Qibing Wang and Duanni Li and Gang Xiao},
keywords = {Service composition, Service evolution, Service clustering, User requirement, Service matching},
abstract = {Service composition can quickly build new value-added composite services by combining existing Web services. However, with the complex and changeable Internet environment, composite services need to effectively understand and manage requirements in a flexible and adaptive manner, and also be able to quickly and proactively provide high-quality services through a series of dynamic evolution models. In response to these challenges, in this study, we propose a service composition evolution method that can adaptively select the appropriate service to improve user satisfaction and service quality. First, we introduce a deep clustering method based on the topic model and auto-encoder, which considers the function description documents and parameter information of the services to reduce the search space of the candidate services. Furthermore, we describe a requirement-oriented context-sensitive task model to integrate functional and non-functional user requirements by connecting the context of Web subtasks. Then, we present a dynamic service matching method with QoS threshold judgement to filter and rank the services. We applied the proposed evolution method to the datasets of real-world Web services and measured the performance using standard measurement metrics. The prototype implementation and results of simulation experiments verified the effectiveness of each part of our method.}
}
@article{ZHANG2025106692,
title = {Cross-platform EPC collaboration framework for construction management of pumped storage hydropower utilizing industry foundation classes and Semantic Web},
journal = {Tunnelling and Underground Space Technology},
volume = {163},
pages = {106692},
year = {2025},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2025.106692},
url = {https://www.sciencedirect.com/science/article/pii/S088677982500330X},
author = {Sherong Zhang and Shihang Zhang and Zhengqiao Wu and Chao Wang and Zhiyong Jiang and Xiaohua Wang},
keywords = {Tunneling, Underground powerhouse, Building Information Modeling (BIM), Industry Foundation Classes (IFC), Artificial Intelligence (AI), Pumped hydro energy storage},
abstract = {Engineering, Procurement, and Construction (EPC) is a widely adopted project delivery method that centralizes responsibility but frequently faces challenges related to fragmented documentation in pumped storage hydropower projects. Although Building Information Modeling (BIM) integrates multidisciplinary data to create digital representations, interoperability issues continue to hinder seamless collaboration between design and construction teams. To overcome these challenges, we propose a cross-platform EPC collaboration framework that integrates Industry Foundation Classes (IFC) with Semantic Web technologies. We validated the feasibility of the prototype system through a case study, providing technical guidance for applying recursive reasoning to query sequential data within the IFC-graph model. This proposed method improves the benefits of BIM by improving accessibility and management of engineering information. The findings of this study advance the development of automated workflows in construction management through the adoption of open standards.}
}
@article{FU2025,
title = {Semantic-Aware Framework for Backdoor Detection in AI Models},
journal = {International Journal on Semantic Web and Information Systems},
volume = {21},
number = {1},
year = {2025},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.378675},
url = {https://www.sciencedirect.com/science/article/pii/S1552628325000031},
author = {Kang Fu and Jianhua Dai},
keywords = {Backdoor Attack Detection, AI-Based Cybersecurity Services, Artificial Intelligence Security, Privacy Protection, Deep Learning},
abstract = {ABSTRACT
In untrusted environments, deep learning models face a serious risk of backdoor attacks, where hidden triggers can lead to malicious behavior. Existing methods are costly and struggle to handle complex, semantically hidden triggers, limiting their use in healthcare, security, and edge computing. This study introduces a semantic awareness framework that uses lightweight trusted models and Semantic Web techniques to effectively detect abnormal behavior. It identifies backdoor triggers through semantic perturbation and uses deep learning algorithms to remove them, thus maintaining performance on clean data. The experiment shows that the detection accuracy is increased by 12%, and the error deletion rate is reduced by 20%. The framework effectively handles subtle attacks. By combining semantic analysis with efficient model alignment, it provides a robust, interpretable defense against resource-constrained Settings. Contributions include a new semantic-driven detection strategy that advances practical AI security in high-risk applications such as healthcare and autonomous systems.}
}
@article{WETTER2022121501,
title = {OpenBuildingControl: Digitizing the control delivery from building energy modeling to specification, implementation and formal verification},
journal = {Energy},
volume = {238},
pages = {121501},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.121501},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221017497},
author = {Michael Wetter and Paul Ehrlich and Antoine Gautier and Milica Grahovac and Philip Haves and Jianjun Hu and Anand Prakash and Dave Robin and Kun Zhang},
keywords = {Control, Building, HVAC, Simulation},
abstract = {The current process for specifying, installing and commissioning building control sequences is largely manual and based on ambiguous natural language specifications. It lacks a formal end-to-end quality control and it has been shown not to deliver high performance sequences at scale. While high-performance HVAC control sequences enable significant reductions in energy consumption, errors in implementing the control logic are common even for less advanced sequences. To improve this situation, we present a digitized building control delivery workflow with formal end-to-end verification, a Control Description Language for the digital specification of building control sequences within this workflow, and software tools that enable digitization of this process. Using the process and tools introduced here, mechanical designers can customize, test and improve these sequences within annual energy simulation, store them in a library for use in other projects, and export them for bidding. Control providers can implement the sequences on existing control product lines through code generation. Commissioning providers can formally verify whether as-installed sequences conform to the digital design specification that was exported by the mechanical designer. Moreover, control product development teams can use the reference implementations of these libraries within their product testing to ensure that their products reproduce the behavior of the reference implementations. This paper presents this process, the language and the supporting software, together with examples of all of the above steps. The presented work has given rise to a new proposed standard, ASHRAE 231P, that will allow digitizing the building control delivery process through the standardization of a control-vendor independent format for exchanging control logic that we pioneered through the here presented work.}
}
@article{BISCHL2025101317,
title = {OpenML: Insights from 10 years and more than a thousand papers},
journal = {Patterns},
volume = {6},
number = {7},
pages = {101317},
year = {2025},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2025.101317},
url = {https://www.sciencedirect.com/science/article/pii/S2666389925001655},
author = {Bernd Bischl and Giuseppe Casalicchio and Taniya Das and Matthias Feurer and Sebastian Fischer and Pieter Gijsbers and Subhaditya Mukherjee and Andreas C. Müller and László Németh and Luis Oala and Lennart Purucker and Sahithya Ravi and Jan N. {van Rijn} and Prabhant Singh and Joaquin Vanschoren and Jos {van der Velde} and Marcel Wever},
keywords = {machine learning, artificial intelligence, open science, networked science, benchmarking, FAIR data, reproducibility, meta-learning, automated machine learning, OpenML},
abstract = {Summary
OpenML is an open-source platform that democratizes machine-learning evaluation by enabling anyone to share datasets in uniform standards, define precise machine-learning tasks, and automatically share detailed workflows and model evaluations. More than just a platform, OpenML fosters a collaborative ecosystem where scientists create new tools, launch initiatives, and establish standards to advance machine learning. Over the past decade, OpenML has inspired over 1,500 publications across diverse fields, from scientists releasing new datasets and benchmarking new models to educators teaching reproducible science. Looking back, we detail and describe the platform’s impact by looking at usage and citations. We share lessons from a decade of building, maintaining, and expanding OpenML, highlighting how rich metadata, collaborative benchmarking, and open interfaces have enhanced research and interoperability. Looking ahead, we cover ongoing efforts to expand OpenML’s capabilities and integrate with other platforms, informing a broader vision for open-science infrastructure for machine learning.}
}
@article{YAZDANI201980,
title = {Query-based integration of heterogeneous knowledge bases for search and rescue tasks},
journal = {Robotics and Autonomous Systems},
volume = {117},
pages = {80-91},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018303488},
author = {Fereshta Yazdani and Sebastian Blumenthal and Nico Huebel and Asil Kaan Bozcuoğlu and Michael Beetz and Herman Bruyninckx},
keywords = {Querying big data, Knowledge sharing, Knowledge management, Knowledge maintenance},
abstract = {Recently, advances in robotics’ technology and research focus on complex scenarios. In these scenarios, robots have to act and respond fast to situational demands. First, they require heterogeneous knowledge from various sources. Then, they need to integrate this knowledge with their reasoning methodologies. These reasoning methodologies are typically different for every domain. This paper introduces an integrated knowledge processing methodology. This methodology uses query mechanisms and model-to-model transformations. Combining these two mechanisms enables processing of heterogeneous knowledge bases. The methodology is demonstrated for an outdoor scenario with diverse systems. In this scenario knowledge and reasoning methods from various sources are integrated. This includes static knowledge from. Open Sreet Map and Digital Elevation Models. The Robot Scene Graph tracks changes in the world and provides geometric reasoning. KnowRob with its Sherpa ontology and openEASE provide further reasoning capabilities.}
}
@article{FATIMA2023106718,
title = {Improving news headline text generation quality through frequent POS-Tag patterns analysis},
journal = {Engineering Applications of Artificial Intelligence},
volume = {125},
pages = {106718},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106718},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623009028},
author = {Noureen Fatima and Sher Muhammad Daudpota and Zenun Kastrati and Ali Shariq Imran and Saif Hassan and Nouh Sabri Elmitwally},
keywords = {POS tagging, Text generation, Low resource language, Generative pre-trained transformer, Attention mechanism},
abstract = {Original synthetic content writing is one of the human abilities that algorithms aspire to emulate. The advent of sophisticated algorithms, especially based on neural networks has shown promising results in recent times. A watershed moment was witnessed when the attention mechanism was introduced which paved the way for transformers, a new exciting architecture in natural language processing. Recent sensations like GPT and BERT for synthetic text generation rely on NLP transformers. Although, GPT and BERT-based models are capable of generating creative text given they are properly trained on abundant data, however, the generated text suffers the quality aspect when limited data is available. This is especially an issue for low-resource languages where labeled data is still scarce. In such cases, the generated text, more often than not, lacks the proper sentence structure, thus unreadable. This study proposes a post-processing step in text generation that improves the quality of generated text through the GPT model. The proposed post-processing step is based on the analysis of POS tagging patterns in the original text and accepts only those generated sentences from GPT which satisfy POS patterns that are originally learned from the data. We exploit the GPT model to generate English headlines by utilizing Australian Broadcasting Corporation (ABC) news dataset. Furthermore, for assessing the applicability of the model in low-resource languages, we also train the model on the Urdu news dataset for Urdu news headlines generation. The experiments presented in this paper on these datasets from high- and low-resource languages show that the performance of generated headlines has a significant improvement by using the proposed headline POS pattern extraction. We evaluate the performance through subjective evaluation as well as using text generation quality metrics like BLEU and ROUGE.}
}
@article{BAGHAPOUR20185,
title = {A computer-based approach for data analyzing in hospital’s health-care waste management sector by developing an index using consensus-based fuzzy multi-criteria group decision-making models},
journal = {International Journal of Medical Informatics},
volume = {118},
pages = {5-15},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618305343},
author = {Mohammad Ali Baghapour and Mohammad Reza Shooshtarian and Mohammad Reza Javaheri and Sina Dehghanifard and Razieh Sefidkar and Amir Fadaei Nobandegani},
keywords = {Process mining, Data mining, Hospital, Health-care waste, Index, Decision-making},
abstract = {Background
Proper Health-Care Waste Management (HCWM) and integrated documentation in this sector of hospitals require analyzing massive data collected by hospital’s health experts. This study presented a quantitative software-based index to assess the HCWM process performance by integrating ontology-based Multi-Criteria Group Decision-Making techniques and fuzzy modeling that were coupled with data mining. This framework represented the Complex Event Processing (CEP) and Corporate Performance Management (CPM) types of Process Mining in which a user-friendly software namely Group Fuzzy Decision-Making (GFDM) was employed for index calculation.
Findings
Assessing the governmental hospitals of Shiraz, Iran in 2016 showed that the proposed index was able to determine the waste management condition and clarify the blind spots of HCWM in the hospitals. The index values under 50 were found in some of the hospitals showing poor process performance that should be at the priority of optimization and improvement.
Conclusion
The proposed framework has distinctive features such as modeling the uncertainties (risks) in hospitals’ process assessment and flexibility enabling users to define the intended criteria, stakeholders, and number of hospitals. Having computer-aided approach for decision process also accelerates the index calculation as well as its accuracy which would contribute to more willingness of hospitals’ experts and other end-users to use the index in practice. The methodology could efficiently be employed as a tool for managing hospitals’ event logs and digital documentation in big data environment not only for the health-care waste management, but also in other administrative wards of hospitals.}
}
@article{LARREAGALLEGOS2024641,
title = {A computational framework for modeling socio-technical agents in the life-cycle sustainability assessment of supply networks},
journal = {Sustainable Production and Consumption},
volume = {46},
pages = {641-654},
year = {2024},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2024.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S2352550924000691},
author = {Gustavo Larrea-Gallegos and Antonino Marvuglia and Tomás {Navarrete Gutiérrez} and Enrico Benetto},
keywords = {Agent-based modeling, Complex system, Supply chain, Sustainable production, Sustainable behaviors, Green consciousness, ABM},
abstract = {Supply networks are now more complex and intertwined than ever, and it is necessary to have the capacity of proposing effective policies that consider aspects of complexity and human behavior. To address this complexity, practitioners commonly rely on Agent-Based Modeling (ABM) as modeling paradigm since it permits to analyze unpredictable system properties that arise from the interaction of agents. Nevertheless, current ABM implementations in life-cycle sustainability studies lack of modularity, meaning that practitioners repeat efforts when modeling socio-technical agents. To deal with this issue, a common framework for modeling production and consumption entities is necessary, for which we propose an algebraic framework for representing computational agents. Our framework provides notions and instructions to set an operational configuration that describes socio-technological agents in a systematic manner to then be used in an ABM model. The framework is rooted on principles of Life Cycle Assessment, and it is a bottom-up generalization of the Stochastic Technology-of-Choice Model. The framework's components are conceived to depict the technical and non-technical relationships of an agent's decision space, while also being suited to handle the logic of different decision problems, such as the sourcing problem. To demonstrate the capabilities of our framework, we provide a proof of concept in which we study the effects of introducing agents with sustainable behaviors (Agents of Change) in a system dominated by profit-driven agents. We evaluated two criteria to introduce Agents of Change: strategic and random. The results indicate that a strategic introduction of Agents of Change can be beneficial from a financial and environmental perspective. We show that for a particular strategy, Agents of Change contribute to the reduction of the financial risk of other Agents of Change, while also decreasing the impact of the system. Finally, this proof of concept shows that our framework can be used to address fundamental sustainability inquiries while still being compatible with current life-oriented methods.}
}
@article{TSOPRA201824,
title = {Using preference learning for detecting inconsistencies in clinical practice guidelines: Methods and application to antibiotherapy},
journal = {Artificial Intelligence in Medicine},
volume = {89},
pages = {24-33},
year = {2018},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2018.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0933365718300873},
author = {Rosy Tsopra and Jean-Baptiste Lamy and Karima Sedki},
keywords = {Preference learning, Antibiotherapy, Clinical practice guidelines, Inconsistencies in guidelines},
abstract = {Clinical practice guidelines provide evidence-based recommendations. However, many problems are reported, such as contradictions and inconsistencies. For example, guidelines recommend sulfamethoxazole/trimethoprim in child sinusitis, but they also state that there is a high bacteria resistance in this context. In this paper, we propose a method for the semi-automatic detection of inconsistencies in guidelines using preference learning, and we apply this method to antibiotherapy in primary care. The preference model was learned from the recommendations and from a knowledge base describing the domain. We successfully built a generic model suitable for all infectious diseases and patient profiles. This model includes both preferences and necessary features. It allowed the detection of 106 candidate inconsistencies which were analyzed by a medical expert. 55 inconsistencies were validated. We showed that therapeutic strategies of guidelines in antibiotherapy can be formalized by a preference model. In conclusion, we proposed an original approach, based on preferences, for modeling clinical guidelines. This model could be used in future clinical decision support systems for helping physicians to prescribe antibiotics.}
}
@article{TABBICHE2023,
title = {Applying Machine Learning and Model-Driven Approach for the Identification and Diagnosis Of Covid-19},
journal = {International Journal of Distributed Systems and Technologies},
volume = {14},
number = {1},
year = {2023},
issn = {1947-3532},
doi = {https://doi.org/10.4018/IJDST.321648},
url = {https://www.sciencedirect.com/science/article/pii/S1947353223000065},
author = {Mohammed Nadjib Tabbiche and Mohammed Fethi Khalfi and Reda Adjoudj},
keywords = {Concrete Syntax, COVID-19, Graphics Editors, Internet of Things (IoT), Machine Learning, MDE, SIRIUS, Smart Healthcare System, Supervised Learning, Ubiquitous Systems},
abstract = {ABSTRACT
Ubiquitous environments are not fixed in time. Entities are constantly evolving; they are dynamic. Ubiquitous applications therefore have a strong need to adapt during their execution and react to the context changes, and developing ubiquitous applications is still complex. The use of the separation of needs and model-driven engineering present the promising solutions adopted in this approach to resolve this complexity. The authors thought that the best way to improve efficiency was to make these models intelligent. That's why they decided to propose an architecture combining machine learning with the domain of modeling. In this article, a novel tool is proposed for the design of ubiquitous applications, associated with a graphical modeling editor with a drag-drop palette, which will allow to instantiate in a graphical way in order to obtain platform independent model, which will be transformed into platform specific model using Acceleo language. The validity of the proposed framework has been demonstrated via a case study of COVID-19.}
}
@article{SON201858,
title = {Deep Phenotyping on Electronic Health Records Facilitates Genetic Diagnosis by Clinical Exomes},
journal = {The American Journal of Human Genetics},
volume = {103},
number = {1},
pages = {58-73},
year = {2018},
issn = {0002-9297},
doi = {https://doi.org/10.1016/j.ajhg.2018.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S000292971830171X},
author = {Jung Hoon Son and Gangcai Xie and Chi Yuan and Lyudmila Ena and Ziran Li and Andrew Goldstein and Lulin Huang and Liwei Wang and Feichen Shen and Hongfang Liu and Karla Mehl and Emily E. Groopman and Maddalena Marasa and Krzysztof Kiryluk and Ali G. Gharavi and Wendy K. Chung and George Hripcsak and Carol Friedman and Chunhua Weng and Kai Wang},
keywords = {electronic health records, phenotyping, biomedical informatics, natural language processing, knowledge engineering, precision medicine, diagnosis, next-generation sequencing, exome, genome},
abstract = {Integration of detailed phenotype information with genetic data is well established to facilitate accurate diagnosis of hereditary disorders. As a rich source of phenotype information, electronic health records (EHRs) promise to empower diagnostic variant interpretation. However, how to accurately and efficiently extract phenotypes from heterogeneous EHR narratives remains a challenge. Here, we present EHR-Phenolyzer, a high-throughput EHR framework for extracting and analyzing phenotypes. EHR-Phenolyzer extracts and normalizes Human Phenotype Ontology (HPO) concepts from EHR narratives and then prioritizes genes with causal variants on the basis of the HPO-coded phenotype manifestations. We assessed EHR-Phenolyzer on 28 pediatric individuals with confirmed diagnoses of monogenic diseases and found that the genes with causal variants were ranked among the top 100 genes selected by EHR-Phenolyzer for 16/28 individuals (p < 2.2 × 10−16), supporting the value of phenotype-driven gene prioritization in diagnostic sequence interpretation. To assess the generalizability, we replicated this finding on an independent EHR dataset of ten individuals with a positive diagnosis from a different institution. We then assessed the broader utility by examining two additional EHR datasets, including 31 individuals who were suspected of having a Mendelian disease and underwent different types of genetic testing and 20 individuals with positive diagnoses of specific Mendelian etiologies of chronic kidney disease from exome sequencing. Finally, through several retrospective case studies, we demonstrated how combined analyses of genotype data and deep phenotype data from EHRs can expedite genetic diagnoses. In summary, EHR-Phenolyzer leverages EHR narratives to automate phenotype-driven analysis of clinical exomes or genomes, facilitating the broader implementation of genomic medicine.}
}
@article{SETH2025,
title = {Technologies for Interoperable Internet of Medical Things Platforms to Manage Medical Emergencies in Home and Prehospital Care: Scoping Review},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/54470},
url = {https://www.sciencedirect.com/science/article/pii/S1438887125001153},
author = {Mattias Seth and Hoor Jalo and Åsa Högstedt and Otto Medin and Bengt Arne Sjöqvist and Stefan Candefjord},
keywords = {Internet of Medical Things, enabling technologies, standards, cross-domain interoperability, scoping review, technology, medical emergency, internet, prehospital care, gerontology, global population, chronic disease, multimorbidity, health care system, home-based care, innovation, digital health, health informatics, telehealth, artificial intelligence},
abstract = {Background
The aging global population and the rising prevalence of chronic disease and multimorbidity have strained health care systems, driving the need for expanded health care resources. Transitioning to home-based care (HBC) may offer a sustainable solution, supported by technological innovations such as Internet of Medical Things (IoMT) platforms. However, the full potential of IoMT platforms to streamline health care delivery is often limited by interoperability challenges that hinder communication and pose risks to patient safety. Gaining more knowledge about addressing higher levels of interoperability issues is essential to unlock the full potential of IoMT platforms.
Objective
This scoping review aims to summarize best practices and technologies to overcome interoperability issues in IoMT platform development for prehospital care and HBC.
Methods
This review adheres to a protocol published in 2022. Our literature search followed a dual search strategy and was conducted up to August 2023 across 6 electronic databases: IEEE Xplore, PubMed, Scopus, ACM Digital Library, Sage Journals, and ScienceDirect. After the title, abstract, and full-text screening performed by 2 reviewers, 158 articles were selected for inclusion. To answer our 2 research questions, we used 2 models defined in the protocol: a 6-level interoperability model and a 5-level IoMT reference model. Data extraction and synthesis were conducted through thematic analysis using Dedoose. The findings, including commonly used technologies and standards, are presented through narrative descriptions and graphical representations.
Results
The primary technologies and standards reported for interoperable IoMT platforms in prehospital care and HBC included cloud computing (19/30, 63%), representational state transfer application programming interfaces (REST APIs; 17/30, 57%), Wi-Fi (17/30, 57%), gateways (15/30, 50%), and JSON (14/30, 47%). Message queuing telemetry transport (MQTT; 7/30, 23%) and WebSocket (7/30, 23%) were commonly used for real-time emergency alerts, while fog and edge computing were often combined with cloud computing for enhanced processing power and reduced latencies. By contrast, technologies associated with higher interoperability levels, such as blockchain (2/30, 7%), Kubernetes (3/30, 10%), and openEHR (2/30, 7%), were less frequently reported, indicating a focus on lower level of interoperability in most of the included studies (17/30, 57%).
Conclusions
IoMT platforms that support higher levels of interoperability have the potential to deliver personalized patient care, enhance overall patient experience, enable early disease detection, and minimize time delays. However, our findings highlight a prevailing emphasis on lower levels of interoperability within the IoMT research community. While blockchain, microservices, Docker, and openEHR are described as suitable solutions in the literature, these technologies seem to be seldom used in IoMT platforms for prehospital care and HBC. Recognizing the evident benefit of cross-domain interoperability, we advocate a stronger focus on collaborative initiatives and technologies to achieve higher levels of interoperability.
International Registered Report Identifier (IRRID)
RR2-10.2196/40243}
}
@incollection{ALEMU2025532,
title = {Metadata Standards and Models},
editor = {David Baker and Lucy Ellis},
booktitle = {Encyclopedia of Libraries, Librarianship, and Information Science (First Edition)},
publisher = {Academic Press},
edition = {First Edition},
address = {Oxford},
pages = {532-545},
year = {2025},
isbn = {978-0-323-95690-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.00035-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895000353},
author = {Getaneh Alemu},
keywords = {Authority headings, BIBFRAME, Controlled Vocabularies, FRBR, Linked Data, LRM, Dublin Core, MARC21, Metadata, Metadata models, Metadata standards, RDA},
abstract = {This entry on metadata standards and models provides a succinct overview of metadata, its essential role in information organization. It defines the concept of metadata and its various functions, types, models and standards, underscoring its importance in enhancing description, search, retrieval, discovery, access and usage of information resources across different organizations including libraries, archives, e-commerce and research data management. The paper delves into the roles of different stakeholders in metadata creation, examining notable standards such as Dublin Core, MARC21, IFLA LRM, FRBR, RDA and BIBFRAME. It also emphasizes the importance of controlled vocabularies and authority headings in maintaining consistency and improving information retrieval. Finally, the paper offers insights into future perspectives and emerging trends in metadata standards and models, including the potential for enrichment, linking, openness, and filtering, driven by collaboration, Semantic Web, open science, big data and AI. The entry highlights that modern metadata standards and models are pivotal in driving resource usage and contributing to an efficient and interconnected information landscape.}
}
@article{SHIDIK2024100358,
title = {Indonesian disaster named entity recognition from multi source information using bidirectional LSTM (BiLSTM)},
journal = {Journal of Open Innovation: Technology, Market, and Complexity},
volume = {10},
number = {3},
pages = {100358},
year = {2024},
issn = {2199-8531},
doi = {https://doi.org/10.1016/j.joitmc.2024.100358},
url = {https://www.sciencedirect.com/science/article/pii/S2199853124001525},
author = {Guruh Fajar Shidik and Filmada Ocky Saputra and Galuh Wilujeng Saraswati and Nurul Anisa Sri Winarsih and Muhammad Syaifur Rohman and Ricardus Anggi Pramunendar and Edi Jaya Kusuma and Danny Oka Ratmana and Valentijn Venus and Pulung Nurtantio Andono and Zainal Arifin Hasibuan},
keywords = {Disaster emergency information, Named entity recognition, Deep learning, BiLSTM networks, Random oversampling},
abstract = {Precise logistic support is essential after a disaster occurs. It must be timely, accurate, targeted, and based on existing needs. However, obtaining sufficient and accurate information related to logistic distribution locations remains a key problem. Therefore, implementing Named Entity Recognition (NER) can address this issue. In recent years, news coverage through Indonesian digital news media and social media accounts has emerged as a promising source for building a disaster data corpus. This study implemented NER to extract and identify named entities from text-based information, particularly from Indonesian digital news media. In addition to using regular entities from the NER standard, this study introduced new entities specialized for disaster-related information, including DISASTER, SCALE, SUPPLIES, CASUALTIES, and OUTSIDE. The new disaster corpus in the Indonesian language for the NER model was obtained with an imbalanced dataset composition. To overcome this problem, random oversampling was applied. This study also utilized the BiLSTM model to recognize each entity in new textual information, evaluating its performance when the proposed Indonesian disaster corpus was used as a training reference in the deep learning model. Several optimization algorithms applied in BiLSTM were evaluated. The results showed improved BiLSTM performance using Adam optimization and a balanced corpus. Performance indicators achieved were 93.4 %, 82.4 %, and 87.5 % for precision, recall, and F1-score, respectively. The BiLSTM network captured long-range dependencies in sequential data provided by NER. Oversampling ensured that the proposed NER model could precisely recognize all entities and reduce biased results. Thus, the BiLSTM method can better identify entities in the textual corpus of Indonesian disaster-related online news.}
}
@article{LIU2024113192,
title = {Deciphering the mechanisms of the IL-6/CXCL1 and NOD-like receptor signaling pathways in otitis media with effusion in rodents},
journal = {International Immunopharmacology},
volume = {142},
pages = {113192},
year = {2024},
issn = {1567-5769},
doi = {https://doi.org/10.1016/j.intimp.2024.113192},
url = {https://www.sciencedirect.com/science/article/pii/S1567576924017144},
author = {Yixuan Liu and Tingting Qian and Nanfeng Zhang and Jiazhen Cao and Xiaoling Lu and Qiling Tong and Xinyuan Wang and Huawei Li and Shan Sun and Huiqian Yu},
keywords = {Otitis media with effusion, NLRP3 activation, IL-6, NF-κB},
abstract = {Background
Otitis media with effusion (OME) often leads to pediatric hearing loss and is influenced by innate and adaptive immune responses. Innate immunity serves as the non-specific first line of defense against OME.
Methods
We induced OME in rats using ovalbumin. We administered IL-6 monoclonal antibodies intranasally to inhibit IL-6, and we injected an NF-κB inhibitor intraperitoneally to explore the role of IL-6 in innate immunity and its interaction with the NOD-like receptor signaling pathway. We analyzed RNA-sequencing data with Gene Ontology and Kyoto Encyclopedia of Genes and Genomes pathways to assess signaling pathways involved in OME. We also utilized Western blot, quantitative real-time PCR, and immunohistochemistry on middle ear samples and used microscopy to identify immune cells in ear wash fluids.
Results
Our study suggests a pivotal role for IL-6 in the immune pathways of rats with OME via the regulation of CXCL1-mediated pathways. Increased levels of IL-6 and CXCL1 were observed in the middle ear tissues, and activation of the NLRP3 inflammasome in OME rats led to an immune response via NF-κB, thus promoting IL-6 and CXCL1 production, which was reduced by IL-6 antibody treatment.
Conclusions
Our findings confirm that IL-6 and CXCL1 play significant roles in the innate immune response in OME in rodents, predominantly via the NOD-like receptor signaling pathway and NLRP3 inflammasome activation. This research sheds light on OME pathogenesis and its immune-related mechanisms.}
}
@article{GAMINI202260,
title = {The principle of simplicity for Quṭb al-Dīn Shīrāzī},
journal = {Studies in History and Philosophy of Science},
volume = {91},
pages = {60-65},
year = {2022},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2021.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S0039368121001916},
author = {Amir-Mohammad Gamini and Mohammad-Mahdi Sadrforati},
keywords = {Quṭb al-Dīn Shīrāzī, Principle of simplicity, Parsimony, Elegance, Concept of orb},
abstract = {This paper seeks to show how Quṭb al-Dīn Shīrāzī, a prominent astronomer of the Islamic age of science, employed the principle of simplicity in choosing among rival planetary models. Following some previous studies, we distinguish syntactic and ontological accounts of simplicity. Shīrāzī explicitly refers to these methodological principles and utilizes them to reduce the number of orbs in the planetary models. We provide cases where Shīrāzī applies the former account of simplicity, and others where he uses the latter. In some complicated cases, however, he appeals to a third kind of simplicity in favor of conceptual consistency. We argue that retaining the uniform motion of orbs plays a critical role in his favoring a particular account of simplicity.}
}
@article{AGARONNIK20201739,
title = {Challenges of Developing a Natural Language Processing Method With Electronic Health Records to Identify Persons With Chronic Mobility Disability},
journal = {Archives of Physical Medicine and Rehabilitation},
volume = {101},
number = {10},
pages = {1739-1746},
year = {2020},
issn = {0003-9993},
doi = {https://doi.org/10.1016/j.apmr.2020.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S0003999320302914},
author = {Nicole D. Agaronnik and Charlotta Lindvall and Areej El-Jawahri and Wei He and Lisa I. Iezzoni},
keywords = {Electronic health records, Machine learning, Natural language processing, Rehabilitation},
abstract = {Objective
To assess the utility of applying natural language processing (NLP) to electronic health records (EHRs) to identify individuals with chronic mobility disability.
Design
We used EHRs from the Research Patient Data Repository, which contains EHRs from a large Massachusetts health care delivery system. This analysis was part of a larger study assessing the effects of disability on diagnosis of colorectal cancer. We applied NLP text extraction software to longitudinal EHRs of colorectal cancer patients to identify persons who use a wheelchair (our indicator of mobility disability for this analysis). We manually reviewed the clinical notes identified by NLP using directed content analysis to identify true cases using wheelchairs, duration or chronicity of use, and documentation quality.
Setting
EHRs from large health care delivery system
Participants
Patients (N=14,877) 21-75 years old who were newly diagnosed with colorectal cancer between 2005 and 2017.
Interventions
Not applicable.
Main Outcome Measures
Confirmation of patients’ chronic wheelchair use in NLP-flagged notes; quality of disability documentation.
Results
We identified 14,877 patients with colorectal cancer with 303,182 associated clinical notes. NLP screening identified 1482 (0.5%) notes that contained 1+ wheelchair-associated keyword. These notes were associated with 420 patients (2.8% of colorectal cancer population). Of the 1482 notes, 286 (19.3%, representing 105 patients, 0.7% of the total) contained documentation of reason for wheelchair use and duration. Directed content analysis identified 3 themes concerning disability documentation: (1) wheelchair keywords used in specific EHR contexts; (2) reason for wheelchair not clearly stated; and (3) duration of wheelchair use not consistently documented.
Conclusions
NLP offers an option to screen for patients with chronic mobility disability in much less time than required by manual chart review. Nonetheless, manual chart review must confirm that flagged patients have chronic mobility disability (are not false positives). Notes, however, often have inadequate disability documentation.}
}
@article{LENG2024158,
title = {Review of manufacturing system design in the interplay of Industry 4.0 and Industry 5.0 (Part I): Design thinking and modeling methods},
journal = {Journal of Manufacturing Systems},
volume = {76},
pages = {158-187},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524001596},
author = {Jiewu Leng and Jiwei Guo and Junxing Xie and Xueliang Zhou and Ang Liu and Xi Gu and Dimitris Mourtzis and Qinglin Qi and Qiang Liu and Weiming Shen and Lihui Wang},
keywords = {Manufacturing system design, Production system design, Smart manufacturing, Industry 5.0, Design methods},
abstract = {The concurrent societal goals and environmental challenges force industries to become more human-centric, sustainable, and resilient, which is envisioned as a value-oriented Industry 5.0 paradigm. The technology-driven Industry 4.0 paradigm is in a prosperous stage. Advanced information and manufacturing technologies have shown promising benefits, but challenges persist in the integration and implementation of smart manufacturing systems. Manufacturing system design (MSD) not only affects production efficiency and product quality but also is related to the strategic planning and sustainable development of manufacturers. New requirements for MSD appear and MSD has revealed new trends. MSD in the interplay of Industry 4.0 and Industry 5.0 is discussed in this two-part review, based on literature exploration within the Web of Science database. To obtain a systematic understanding of methodological, procedural, and technological exploration in a balanced way, this two-part review builds a Thinking-Modeling-Process-Enabler (TMPE) framework for reviewing MSD. In this paper (Part I of the two-part review), MSD methods are summarized and categorized from the design thinking viewpoint. Then, the manufacturing system modeling methods are categorized and discussed. Challenges and future research directions are identified in the MSD evolution towards Industry 5.0. Part II will detail the design processes and enablers (i.e., the P and E dimensions of the TMPE framework). This two-part review is anticipated to offer novel insights for advancing MSD research and engineering in the interplay of Industry 4.0 and Industry 5.0.}
}
@article{HADNI2024416,
title = {A novel perspective using Chaotic-Grey Wolf Optimization Algorithm for Arabic Feature Selection Problem},
journal = {Procedia Computer Science},
volume = {244},
pages = {416-424},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.216},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924030175},
author = {Meryeme HADNI and Hassane HJIAJ and Mounir GOUIOUEZ and Meryeme AMANE},
keywords = {Grey Wolf Optimization, Chaotic method, Text classification, Firefly Algorithm, Feature Selection},
abstract = {In the field of neural language processing, current research projects that apply meta-heuristic approaches to Arabic text are extremely limited given the complexity of this language in terms of structure, grammatical rules, morphology, syntactic analysis and derivation rules. In this paper, we propose a new approach for processing Arabic documents using the chaotic grey wolf optimization technique, which is employed as a supervised learning method by simulating the behavior of grey wolves in searching, circling and pursuing their prey. The proposed method is divided into four phases. The term weighting phase uses TF- IDFC-RF, which is based on TF-IDF and the relevance frequency of terms in documents and classes. The feature selection phase combines the chaotic method of the grey wolf optimization algorithm to both reduce and identify relevant features for model building. In the third phase, we use three classifiers: firefly algorithm, SVM and NB to classify documents into several classes. In the experimental process, we used two collections of reference documents to present a comparative study of the different term weighting systems. The experimental results show that the new architecture, based on the chaotic optimization of the grey wolf algorithm with the Firefly algorithm, obtained the best results in terms of accuracy.}
}
@article{NAIR2024102,
title = {Evaluating the Impact of Text Data Augmentation on Text Classification Tasks using DistilBERT},
journal = {Procedia Computer Science},
volume = {235},
pages = {102-111},
year = {2024},
note = {International Conference on Machine Learning and Data Engineering (ICMLDE 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924006896},
author = {Aarathi Rajagopalan Nair and Rimjhim Padam Singh and Deepa Gupta and Priyanka Kumar},
keywords = {Natural Language Processing, DistilBERT, Text Data Augmentation, Synonym Augmentation, Contextual Word Embeddings, Black Translation},
abstract = {Data augmentation entails artificially expanding the dataset's size by applying various transformations to the existing raw data. Enhancing the quality and quantity of the datasets with varying sizes by employing varieddata augmentation techniques has immense importance in the field on Natural Language Processing. Several notable applications for instance text classification, sentiment analysis, text summarization, etc. have proven to be benefitted immensely with the employment of text augmentation techniques. Hence, the paper focuses on efficient text classification using varied datasets of different sizes; small- 500 instances, medium-5564 instances and large-43934 instances.The work considers the standard DistilBERT model, a popular transformer-based language model and presents the impact on the performance of the modelafter employing different text augmentation techniques. The study specifically focuses on three augmentation methods: (a) Synonym augmentation:that involves replacing words with their synonyms to enhance vocabulary diversity and generalization, (b) Contextual word embeddings that enriches semantic understanding by leveraging pre-trained language models, and (c) Black translation that entails translating the text into a another different language and then translating it back, introducing variations in the data and capturing different linguistic patterns.Additionally,the work also discusses the combined effect of employing all three augmentation techniques simultaneously. Moreover, the study also aims compares the relation between the dataset sizes and the performance of the augmentation techniques. The study considers three standard datasets for the analysis and presents a comprehensive analysis using accuracy and F1 score as evaluation metrics. The results highlight the efficacy of each technique across small, medium, and large datasets, enabling a nuanced understanding of their benefits in different data scenarios. The findings indicate the varying degrees of improvement achieved through each augmentation technique.The enhancement achieved by applying text augmentation varied from around 2% on large datasets to 20% on smaller datasets.}
}
@article{BJERREGAARD2025103004,
title = {Foundation models of protein sequences: A brief overview},
journal = {Current Opinion in Structural Biology},
volume = {91},
pages = {103004},
year = {2025},
issn = {0959-440X},
doi = {https://doi.org/10.1016/j.sbi.2025.103004},
url = {https://www.sciencedirect.com/science/article/pii/S0959440X25000223},
author = {Andreas Bjerregaard and Peter Mørch Groth and Søren Hauberg and Anders Krogh and Wouter Boomsma},
abstract = {Protein sequence models have evolved from simple statistics of aligned families to versatile foundation models of evolutionary scale. Enabled by self-supervised learning and an abundance of protein sequence data, such foundation models now play a central role in protein science. They facilitate rich representations, powerful generative design, and fine-tuning across diverse domains. In this review, we trace modeling developments and categorize them into methodological trends over the modalities they describe and the contexts they condition upon. Following a brief historical overview, we focus our attention on the most recent trends and outline future perspectives.}
}
@article{BINAMUNGU2023111749,
title = {Behaviour driven development: A systematic mapping study},
journal = {Journal of Systems and Software},
volume = {203},
pages = {111749},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111749},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223001449},
author = {Leonard Peter Binamungu and Salome Maro},
keywords = {Behaviour Driven Development, Systematic mapping study, Systematic mapping studies in software engineering},
abstract = {Context:
Behaviour Driven Development (BDD) uses scenarios written in semi-structured natural language to express software requirements in a way that can be understood by all stakeholders. The resulting natural language specifications can also be executed to reveal correct and problematic parts of a software. Although BDD was introduced about two decades ago, there is a lack of secondary studies in peer-reviewed scientific literature, making it difficult to understand the state of BDD research and existing gaps.
Objective:
To understand the current state of BDD research by conducting a systematic mapping study that covers studies published from 2006 (when BDD was introduced) to 2021.
Method:
By following the guidelines for conducting systematic mapping studies in software engineering, we sought to answer research questions on types of venues in which BDD papers have been published, research types, contribution types, studied topics and their evolution, as well as evaluation methods used in published BDD research.
Results:
The study identified 166 papers which were mapped. Key results include the following: the dominance of conference papers; scarcity of research with insights from the industry; shortage of philosophical papers on BDD; acute shortage of metrics for measuring various aspects of BDD specifications and the processes for producing BDD specifications; the dominance of studies on using BDD for facilitating various software development endeavours, improving the BDD process and associated artefacts, and applying BDD in different contexts; scarcity of studies on using BDD alongside other software techniques and technologies; increase in diversity of studied BDD topics; and notable use of case studies and experiments to study different BDD aspects.
Conclusion:
The paper improves our understanding of the state of the art of BDD, and highlights important areas of focus for future BDD research.}
}
@article{NAIR20232058,
title = {Building an Explainable Diagnostic Classification Model for Brain Tumor using Discharge Summaries},
journal = {Procedia Computer Science},
volume = {218},
pages = {2058-2070},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.182},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923001825},
author = {Priyanka C. Nair and Deepa Gupta and Bhagavatula Indira Devi and Vani Kanjirangat},
keywords = {Brain tumor, discharge summary, Natural Language Processing, clinical concept extraction, classification;XGboost, ELI5, LCE, LightGBM},
abstract = {A brain tumor is a mass of cells growing abnormally in the brain. The lesions formed in the suprasellar region of the brain, called suprasellar lesions, affect common anatomical locations causing an array of symptoms, including headache and blurred or low vision. These symptoms lead to a misdiagnosis of the tumor as common issues like refractive index problems, and the tumor gets diagnosed very late. This study focuses on these suprasellar lesions (namely Pituitary adenoma, Craniopharyngioma, and Meningioma), which have not been explored much using machine learning. We have collected 422 discharge summaries of patients admitted to the neurosurgery department of the National Institute of Mental Health and Neuroscience (NIMHANS), Bangalore, India, during 2014-2019. This work aims to build a model for classifying lesions into three categories. Features are the clinical concepts identified from the discharge summary using Natural Language Processing (NLP) and regular expression-based rules. The features and corresponding values thus extracted are represented as Analytical Base Table and fed to the classification model after the processing. The model utilizes XGBoost, Local Cascade Ensemble, Histogram-based gradient boosting, LightGBM, and CatBoost classifiers, which have the ability to inherently handle the missing data. Though the machine learning models perform well in classification, the interpretability and generalizability is often questioned especially in critical domains such as medical and healthcare. Hence model performance has been analyzed using the ELI5 tool, a python package for explainable AI. This tool identifies the critical features in the data on a patient basis, providing a more interpretable model for clinicians.}
}
@article{MOSTAFA2025100364,
title = {A structural topic modeling of communication research: insights from over a century of journals' abstracts},
journal = {International Journal of Information Management Data Insights},
volume = {5},
number = {2},
pages = {100364},
year = {2025},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2025.100364},
url = {https://www.sciencedirect.com/science/article/pii/S2667096825000461},
author = {Mohamed M. Mostafa and Mohammad Alhur and Ahmed M. Moustafa},
keywords = {Communication research, Topic modeling, STM, Text mining, Machine learning},
abstract = {Communication research is a broad and interdisciplinary field that is strongly influenced by several other disciplines, including behavioral and human sciences. This study uses structural topic modeling (STM) to analyze and trace the intellectual structure of the field over the past century based on a twenty-nine communication journals’ corpus encompassing 24,983 abstracts, totaling more than two million words. Results show a wide range of important research themes in the field, including communication theory, media analysis, health communication, rhetorical theory, interpersonal interactions, small group communications, political debate, and speech education. Diachronically, our results reveal that some topics, such as “speech education” and “political debate” have waned over time, whereas other topics, such as “narrative/discourse analysis” and “global policy change” have gained recently more attention from communication scholars. These findings underscore not only the intellectual breadth and historical evolution of communication research but also highlight key paradigm shifts in the field. The study demonstrates how computational text analysis can inform meta-theoretical understanding and strategic planning within academic disciplines.}
}
@article{AHMED20242529,
title = {Smart Contract Generation through NLP and Blockchain for Legal Documents},
journal = {Procedia Computer Science},
volume = {235},
pages = {2529-2537},
year = {2024},
note = {International Conference on Machine Learning and Data Engineering (ICMLDE 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.04.238},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924009141},
author = {Sayyed Usman Ahmed and Abutalha Danish and Nesar Ahmad and Tameem Ahmad},
keywords = {Smart Contract, Legal NLP, Natural Language Processing, Blockchain},
abstract = {The rise in legislation and the need for task automation systems has resulted in an amplified requirement for software development. This is aimed at improving the accuracy and efficiency of reading and interpreting laws in legal activities, allowing for enhanced precision and swiftness. Currently, there is a need for analysts to interpret written legislation and then encode it into computer programs, which often leads to errors. As cryptocurrencies gain popularity, there is growing interest in utilizing Blockchain technology in the legal field. Specifically, smart contracts can be used to integrate business rules into laws and automate blockchain management. However, the process of writing fast and high-quality code can be improved by leveraging artificial intelligence based techniques such as natural language processing (NLP) to help practitioners. Despite reviewing the current state-of-the-art, there is a lack of existing work that combines smart contracts and NLP in the context of legislation analysis. This study work generates intelligent code from legislation analysis, utilizing NLP and Blockchain for this purpose. In this research work a pilot prototype for smart contract generation is developed and initial code samples are presented. This work demonstrates a promising results with 96% accuracy in Name Entity Recognition (NER) and also highlights the importance of smart contract generation through NLP.}
}
@article{IRIHAMYE2025100261,
title = {Sustainable by Design: Digital Health Business Models for Equitable Global Health Impact in LMICs},
journal = {Mayo Clinic Proceedings: Digital Health},
pages = {100261},
year = {2025},
issn = {2949-7612},
doi = {https://doi.org/10.1016/j.mcpdig.2025.100261},
url = {https://www.sciencedirect.com/science/article/pii/S2949761225000689},
author = {Elvin Irihamye and Justin Hadad and Natasha Ali and Bruno Holthof and Francis Wafula and Chris Paton and Mike English and Shobhana Nagraj},
keywords = {Digital Health, Global Health, Business Model, Sustainability, LMIC},
abstract = {This study explores challenges and potential strategies related to sustaining digital health business models and markets in low-and-low-middle income countries (LMICs) using a critical interpretive synthesis (CIS) approach. We extracted 21 articles from a database search that yielded over 1,300 hits and used insights from seven expert reviewers with experience operating or funding digital health companies in LMICs. Findings reveal four key challenges: (1) internal challenges related to managing value creation for complex stakeholder networks and external challenges related to (2) infrastructure, (3) financing, and (4) regulation. Entrepreneurs must address these through iterative business strategies, but broader market-shaping interventions remain essential. Such interventions could include facilitating strategic partnerships, fit-for-purpose regulation, enhancing public procurement, and innovative financing instruments. Health-systems can tailor interventions around their unique contexts by prioritising technologies, recruiting local market participants, analysing shared barriers in the business environment, focusing on feasible interventions and iterating to sustain a competitive environment.}
}
@article{BROOKS2025102518,
title = {The Fortitude Framework: A psychological model for sustained leadership presence in nursing},
journal = {Nursing Outlook},
volume = {73},
number = {5},
pages = {102518},
year = {2025},
issn = {0029-6554},
doi = {https://doi.org/10.1016/j.outlook.2025.102518},
url = {https://www.sciencedirect.com/science/article/pii/S002965542500171X},
author = {Aisha K. Brooks},
keywords = {Fortitude Framework, Psychological appraisal theory, Nursing leadership development, Ethical presence in nursing, Public health nursing, Sustained leadership under pressure},
abstract = {ABSTRACT
Background
Despite growing recognition of the emotional strain and ethical complexity in nursing leadership, most leadership models continue to emphasize postcrisis recovery, role-based performance, or external outputs. This narrow framing leaves a gap in understanding how leaders can sustain their presence and judgment before, during, and after prolonged challenges.
Purpose
This conceptual article introduces the Fortitude Framework, a leadership model grounded in psychological appraisal theory and adapted from Pretorius’ theory of fortitude.
Methods
A concept analysis anchored in Pretorius’ theory of fortitude, combined with a theoretical synthesis of related constructs (resilience, grit, hardiness, and self-efficacy), was conducted to clarify fortitude’s conceptual boundaries. This process generated three interdependent internal capacities—Unapologetically Grounded (intrapersonal appraisal), Strategically Aligned (transpersonal appraisal), and Ethically Present (interpersonal appraisal)—that support sustained leadership presence under pressure.
Discussion
The Fortitude Framework translates these capacities into observable leadership patterns that can be actively developed through reflective practice, coaching, and context-sensitive leadership development. Public health nurses are offered as exemplars of this model in practice.
Conclusion
The Fortitude Framework is situated within existing literature on resilience, self-efficacy, and moral presence, distinguishing fortitude as a foundational capacity rather than a reactive trait. It offers a new lens for leadership development and mentoring while guiding system-level strategies to support psychological sustainability in nursing leadership.}
}
@article{WU2023101939,
title = {MEGACare: Knowledge-guided multi-view hypergraph predictive framework for healthcare},
journal = {Information Fusion},
volume = {100},
pages = {101939},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.101939},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523002555},
author = {Jialun Wu and Kai He and Rui Mao and Chen Li and Erik Cambria},
keywords = {Electronic health record, Healthcare, Hypergraph, Multi-view learning, Information bottleneck},
abstract = {Predicting a patient’s future health condition by analyzing their Electronic Health Records (EHRs) is a trending subject in the intelligent medical field, which can help clinicians prescribe safely and effectively, and also make more accurate diagnoses. Benefiting from powerful feature extraction capabilities, graph representation learning can capture complex relationships and achieve promising performance in many clinical prediction tasks. However, existing works either exclusively consider single domain knowledge with an independent task or do not fully capitalize on domain knowledge that can provide more predictive signals in the code encoding stage. Moreover, the heterogeneous and high-dimensional nature of EHR data leads to a deficiency of hardly encoding implicit high-order correlations. To address these limitations, we proposed a knowledge-guided Multi-viEw hyperGrAph predictive framework (MEGACare) for diagnosis prediction and medication recommendation. Our MEGACare leveraged multi-faceted medical knowledge, including ontology structure, code description, and molecular information to enhance medical code presentations. Furthermore, we constructed an EHR hypergraph and a multi-view learning framework to capture the high-order correlation between patient visits and medical codes. Specifically, we propose three perspectives around the pairwise relationship between patient visits and medical codes to comprehensively learn patient representation and enhance the robustness of our framework. We evaluated our MEGACare framework against a set of state-of-the-art methods for two clinical outcome prediction tasks in the public MIMIC-III dataset, and the results showed that our proposed framework was superior to the baseline methods.11Our code and data are released at https://github.com/senticnet/MEGACare.}
}
@article{LIU2020105390,
title = {A cross-region transfer learning method for classification of community service cases with small datasets},
journal = {Knowledge-Based Systems},
volume = {193},
pages = {105390},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.105390},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119306331},
author = {Zhao-ge Liu and Xiang-yang Li and Li-min Qiao and Dilawar Khan Durrani},
keywords = {Community service, Case classification, Small datasets, Transfer learning, Domain adaptation},
abstract = {The precise classification of community service cases is the most fundamental aspect of intelligent community service systems. However, data imbalance makes it challenging to achieve the desired level of precise classification. Existing transfer learning methods use open Internet knowledge for identifying case features and mining potential feature relations. However, community service cases have the characteristics such as short text length and non-public content, which restrict the transfer learning modes. In this paper, a cross-region transfer learning method is proposed to solve the classification problem of cases with small datasets in data imbalance situation while considering the perspective of regional cooperation. First, an ontology modeling method is applied to standardize the case features, reducing the effect of semantic ambiguity on transfer results. Secondly, to improve the effectiveness of source domain classification, this paper utilizes an extended marginal fisher analysis where the distance is measured by the inner product between data. Next, the mapping matrix from target domain to source domain is learned through domain adaptation. Finally, the method is verified based on the empirical data from Lanzhou and Beidaihe in China. Experimental results on classifiers show the proposed method helps regions to improve case classification rates significantly through knowledge complementation. The proposed approach can be followed to build case-based community service systems of reasonable accuracy and limited sample sizes.}
}
@article{KARLOS202162,
title = {Instance-Based Zero-Shot learning for semi-Automatic MeSH indexing},
journal = {Pattern Recognition Letters},
volume = {151},
pages = {62-68},
year = {2021},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2021.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167865521002865},
author = {Stamatis Karlos and Nikolaos Mylonas and Grigorios Tsoumakas},
keywords = {Zero-shot learning, Multi-label text classification, Sentence-embeddings, MeSH terms, Label’s space dependencies, Weakly-supervised methods},
abstract = {Zero-shot learning constitutes a variant of the broader category of weakly supervised learning algorithms. Its main asset is the possibility of identifying entities for which no training data are provided in advance. Under this extreme scenario, conventional supervised learning methods cannot operate properly, while consumption of human resources for obtaining even limited instances may be highly restricted, especially when the label space is quite complex because of its cardinality and the underlying semantic dependencies. However, removing the human factor from the learning loop under complicated tasks cannot guarantee robust performance. Thus, semi-automated solutions are widely accepted by both the research and industrial communities, favoring cooperation of human and machine, mainly for alleviating the spent effort of the former, and for acquiring safer predictions. In contrast with the majority of existing Zero-shot learning approaches, we propose a generalized instance-based method oriented towards tackling the Multi-label classification task without performing any transductive operations over the test instances. Instead, we aim to provide a label ranking of the unseen classes exploiting sentence-based semantic embeddings and label similarities, through a dedicated fine-tuned language representational model. We also use a pattern matching rule to further boost the ranking of our method. Some realistic assumptions are made in order for our approach to work correctly and provide said ranking. Results on a biomedical database with a semantically rich fine-grained label space are really promising, rendering its utilization as a helpful and computationally inexpensive tool for facilitating semi-automated indexing.}
}
@article{GIOMBONI2025102544,
title = {The media literacy divide: Ideological framing of labor in public relations agency internship postings},
journal = {Public Relations Review},
volume = {51},
number = {1},
pages = {102544},
year = {2025},
issn = {0363-8111},
doi = {https://doi.org/10.1016/j.pubrev.2025.102544},
url = {https://www.sciencedirect.com/science/article/pii/S0363811125000062},
author = {Joseph R. Giomboni},
keywords = {Public relations, Internships, Precarious labor, Employability, Social media, Media literacy, Invisible labor, Career management},
abstract = {This study examines the institutional practices that shape and inform internships within the public relations industry. The investigation is pursued through a textual analysis of recruitment postings at 18 leading PR agencies and marketing communications firms to provide insights on how the industries solicit student workers, illicit emotional responses to the media text, and exploit the ontological rewards of future employment. This study aims to uncover internship postings role as discursive objects that articulate not only expectations between organizations and students, but also code a certain kind of ideological vision for what intern labor should entail. By examining how agencies solicit intern labor, researchers, educators, and practitioners can gain a better understanding of the defined role and responsibilities of prospective interns. The findings suggest PR and communication agencies recruit labor on two professional tracks: technicians and industry exposure for future management. Future technicians perform emotional labor when solicited by agencies through work-as-fun mantras to join creative cultures rooted in networking and professional development workshops. These individuals are required to be ambitious, personable, self-starters who can work on multiple projects on tight deadlines. Other agency positions feature industry exposure for future managers who are partnered with award-winning professionals. In addition to administrative research and media monitoring skills, I argue media literacy is required for ideal management track candidates who are storytellers, consume and evaluate news coverage, and identify strategic opportunities for the agency or clients within a changing media landscape.}
}
@article{JI2023493,
title = {Construction and application of knowledge graph for grid dispatch fault handling based on pre-trained model},
journal = {Global Energy Interconnection},
volume = {6},
number = {4},
pages = {493-504},
year = {2023},
issn = {2096-5117},
doi = {https://doi.org/10.1016/j.gloei.2023.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S2096511723000683},
author = {Zhixiang Ji and Xiaohui Wang and Jie Zhang and Di Wu},
keywords = {Power-grid dispatch fault handling, Knowledge graph, Pre-trained model, Auxiliary decision-making},
abstract = {With the construction of new power systems, the power grid has become extremely large, with an increasing proportion of new energy and AC/DC hybrid connections. The dynamic characteristics and fault patterns of the power grid are complex; additionally, power grid control is difficult, operation risks are high, and the task of fault handling is arduous. Traditional power-grid fault handling relies primarily on human experience. The difference in and lack of knowledge reserve of control personnel restrict the accuracy and timeliness of fault handling. Therefore, this mode of operation is no longer suitable for the requirements of new systems. Based on the multi-source heterogeneous data of power grid dispatch, this paper proposes a joint entity–relationship extraction method for power-grid dispatch fault processing based on a pre-trained model, constructs a knowledge graph of power-grid dispatch fault processing and designs, and develops a fault-processing auxiliary decision-making system based on the knowledge graph. It was applied to study a provincial dispatch control center, and it effectively improved the accident processing ability and intelligent level of accident management and control of the power grid.}
}
@article{DELREAL2024105933,
title = {Shielding software systems: A comparison of security by design and privacy by design based on a systematic literature review},
journal = {Computer Law & Security Review},
volume = {52},
pages = {105933},
year = {2024},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2023.105933},
url = {https://www.sciencedirect.com/science/article/pii/S0267364923001437},
author = {Cristina Del-Real and Els {De Busser} and Bibi {van den Berg}},
keywords = {Software development life cycle, Software security, Systems development, Interdisciplinary research, Software design},
abstract = {Background
The design of software systems plays a crucial role in mitigating cybersecurity incidents. Security by Design (SbD) aims to ensure foundational security throughout the design process. However, it lacks a precise interdisciplinary definition. Comparing it with Privacy by Design (PbD), which has seen more conceptual development, highlights the need for a comprehensive understanding of SbD.
Objectives
This study systematically searches and reviews relevant definitions of SbD in comparison with PbD.
Method
Following PRISMA guidelines, we conducted a systematic review of SbD and PbD definitions, searching ACM Digital Library, EBSCO Library, IEEE Xplore, ProQuest, Scopus, and Web of Science. A total of 46 studies were included, identifying 86 definitions. Thirteen themes were identified, including ontology, object of protection, outcome to avoid, means of implementation, added value, and focus of the definition.
Results
Definitions varied in their descriptions of SbD and PbD, the objects of protection, outcomes to avoid, means of implementation, and lifecycle focus. PbD definitions adopted a rights-based approach, anchored in Ann Cavoukian's principles and an interdisciplinary perspective.
Discussion
SbD and PbD definitions lack clarity and uniformity. PbD is better defined, while SbD lacks anchorage and has varied approaches. Both should protect individuals and organizations, address cyber-attacks, and be implemented early in the development process. PbD is more comprehensive, involving technology and organization, while SbD focuses mainly on the technical product. PbD is associated with recognized rights, but the connection between SbD and human rights is unclear. Future research should clarify the specific value protected by SbD, adopt principles from PbD, and take an interdisciplinary approach.}
}
@article{WINTER2020102989,
title = {A review of research into animal ethics in tourism: Launching the annals of tourism research curated collection on animal ethics in tourism},
journal = {Annals of Tourism Research},
volume = {84},
pages = {102989},
year = {2020},
issn = {0160-7383},
doi = {https://doi.org/10.1016/j.annals.2020.102989},
url = {https://www.sciencedirect.com/science/article/pii/S016073832030133X},
author = {Caroline Winter},
keywords = {Animal rights, Ecofeminism, Ecocentrism, Welfare, Utilitarianism, Instrumentalism},
abstract = {A review of research on animal ethics and tourism, based on a sample of 74 articles in ten tourism journals is presented. A range of ethics positions was identified including rights, ecofeminist, ecocentric, welfare, utilitarian, and instrumental. Some studies challenge the ontological bases, and therefore the moral considerability of animals used in tourism: speciesism, native/introduced, a wild-captive continuum and domestic animals. Other themes include the harm caused to animals, and an ‘animal gaze’ which commodifies animals as objects. The ethical positions of the tourism industry, regulatory groups and tourists were also identified. Overall, the articles challenge the use of animals for entertainment, and confirm the imperative for a developing body of research in the field of animal ethics.}
}
@article{PETCH2019,
title = {Extracting Clinical Features From Dictated Ambulatory Consult Notes Using a Commercially Available Natural Language Processing Tool: Pilot, Retrospective, Cross-Sectional Validation Study},
journal = {JMIR Medical Informatics},
volume = {7},
number = {4},
year = {2019},
issn = {2291-9694},
doi = {https://doi.org/10.2196/12575},
url = {https://www.sciencedirect.com/science/article/pii/S2291969419000942},
author = {Jeremy Petch and Jane Batt and Joshua Murray and Muhammad Mamdani},
keywords = {natural language processing, electronic health record, tuberculosis},
abstract = {Background
The increasing adoption of electronic health records (EHRs) in clinical practice holds the promise of improving care and advancing research by serving as a rich source of data, but most EHRs allow clinicians to enter data in a text format without much structure. Natural language processing (NLP) may reduce reliance on manual abstraction of these text data by extracting clinical features directly from unstructured clinical digital text data and converting them into structured data.
Objective
This study aimed to assess the performance of a commercially available NLP tool for extracting clinical features from free-text consult notes.
Methods
We conducted a pilot, retrospective, cross-sectional study of the accuracy of NLP from dictated consult notes from our tuberculosis clinic with manual chart abstraction as the reference standard. Consult notes for 130 patients were extracted and processed using NLP. We extracted 15 clinical features from these consult notes and grouped them a priori into categories of simple, moderate, and complex for analysis.
Results
For the primary outcome of overall accuracy, NLP performed best for features classified as simple, achieving an overall accuracy of 96% (95% CI 94.3-97.6). Performance was slightly lower for features of moderate clinical and linguistic complexity at 93% (95% CI 91.1-94.4), and lowest for complex features at 91% (95% CI 87.3-93.1).
Conclusions
The findings of this study support the use of NLP for extracting clinical features from dictated consult notes in the setting of a tuberculosis clinic. Further research is needed to fully establish the validity of NLP for this and other purposes.}
}
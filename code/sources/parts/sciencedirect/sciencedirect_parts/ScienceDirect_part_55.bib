@article{LIU2025115886,
title = {A systematic review of modeling method of multi-energy coupling and conversion for urban buildings},
journal = {Energy and Buildings},
volume = {342},
pages = {115886},
year = {2025},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2025.115886},
url = {https://www.sciencedirect.com/science/article/pii/S0378778825006164},
author = {Shuo Liu and Yi Dai and Xiaohua Liu and Tao Zhang and Chaoliang Wang and Wei Liu},
keywords = {Multi-energy coupling and conversion, Urban building energy modelling, Multi-energy systems, Energy efficiency, Sustainable urban planning},
abstract = {With the continuous growth of global energy demand and accelerating urbanization, urban building energy systems are becoming increasingly complex and diverse. To achieve energy efficiency and carbon reduction goals, modelling the coupling and conversion of multiple energy forms—such as cooling, heating, and electricity—has emerged as a key research focus. This review summarizes the prevailing modelling approaches, including physics-based models, data-driven models, and hybrid models, and systematically analyses their strengths and limitations in multi-energy collaborative modelling. Particular attention is given to the current state of integration between urban building energy modelling (UBEM) and multi-energy systems (MES), highlighting a significant disconnection between demand-side load simulation and supply-side energy flow optimization. Through analysis of representative case studies, this paper demonstrates the practical effectiveness of coupled cooling-heating-electricity modelling in enhancing energy efficiency and reducing carbon emissions in urban buildings. Finally, it is proposed that future research should focus on strengthening the coupling mechanisms among different models and promoting integrated modelling and co-optimization of UBEM and MES, thereby providing a theoretical foundation and technical support for the design of intelligent and low-carbon urban energy systems.}
}
@article{SHE2025128207,
title = {Multi-view syntax-semantics information bottleneck for dependency-driven relation extraction},
journal = {Expert Systems with Applications},
volume = {288},
pages = {128207},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.128207},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425018275},
author = {Wei She and Xiwang Li and Linpu Lv and Youwei Wang and Honghui Dong and Zhao Tian},
keywords = {Relation extraction, Dependency tree, Information bottleneck, Syntax information, Semantics information},
abstract = {Dependency-driven relation extraction (DDRE) intends to improve the classification performance by exploring the syntax information contained in dependency tree. Although recent works have made impressive advances, they usually suffer severe challenges of losing sequential semantics information due to the pruning strategy. In addition, task-irrelevant information in representations such as noise or entity appositive information might be dominant in the procedure of representation learning, which interferes with the final performance of relation classification. To address these challenges, we propose a novel multi-view syntax-semantics information bottleneck (MS2IB) model, which aims at exploring supplementary information from a newly incorporated view and captures minimum sufficient information for the DDRE task. Specifically, MS2IB treats the DDRE task as a multi-view information learning procedure, where the sequential semantics information is supplemented under the guidance of representation learning with multiple views. Meanwhile, the minimum sufficient task-relevant information is extracted from the aspect of information compression and information preservation simultaneously. Finally, we formulate the objective of MS2IB as an information loss function based on the measurement of mutual information, where a new variational approach is presented to ensure its local optimum. Experiments on benchmark datasets and self-constructed dataset are conducted to show the superiority of MS2IB over the state-of-the-art models. For example, the proposed MS2IB achieves 91.42 % Precision, 89.21 % Recall and 90.28 % F1-score on the SemEval-2010 dataset. The MS2IB model further demonstrates strong generalization on domain datasets and robust performance on large-scale and noisy dataset. With the promising performance on both universal and domain datasets, the proposed model can be applied into various practical applications, such as information extraction and inference of transportation incidents/accidents.}
}
@article{BERGLUND2023e00411,
title = {Visions of futures and futures of visions: Entrepreneurs, artifacts, and worlds},
journal = {Journal of Business Venturing Insights},
volume = {20},
pages = {e00411},
year = {2023},
issn = {2352-6734},
doi = {https://doi.org/10.1016/j.jbvi.2023.e00411},
url = {https://www.sciencedirect.com/science/article/pii/S2352673423000409},
author = {Henrik Berglund and Dimo Dimov},
keywords = {Entrepreneurship, Opportunities, Possibilities, Artifacts, Design},
abstract = {In a recent effort to develop the individual-opportunity nexus, Ramoglou and McMullen (2022) argue that extant conceptualizations of opportunities fail because they reify opportunities by engaging in “thing-talk”. Their proposed alternative ignores concrete things by reinterpreting the nexus in terms of confident entrepreneurs (who imagine world-states) and world-states (that are possible or not). But, regardless of formulation, the dualistic nexus framework fails to account for the creative aspects of entrepreneurship and places impossible demands on the concept of opportunity. A triadic view of entrepreneurs, artifacts, and worlds transcends the distinction between “thing-talk” and “confidence-talk” as central to an unambiguous scholarly use of opportunity language. Acknowledging artifacts as tangible interfaces between entrepreneurial confidence and real-world conditions also prompts a reevaluation of what Ramoglou and McMullen (2022) term “entrepreneurial work”, calling for an approach that duly acknowledges its creative, artifact-centered, and indeed world-making character.}
}
@article{JOULLIE2023113565,
title = {One truth and one standard for its telling: Reporting on and about scientific business research},
journal = {Journal of Business Research},
volume = {157},
pages = {113565},
year = {2023},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.113565},
url = {https://www.sciencedirect.com/science/article/pii/S014829632201030X},
author = {Jean-Etienne Joullié and Anthony M. Gould},
keywords = {Transparency, Positivism, Analytic-synthetic distinction, Syntheticity, Analycity},
abstract = {There is consensus in literature that transparency (accurate and honest reporting) in management and business research is desirable. To improve transparency, commentators have stressed that research articles contain detail about procedural replicability, thus allowing for results reproducibility, at least (in the case of samples) across multiple trials. For all its merit, such advice neglects the particularised role of language in conveying scientific findings. This article argues that embracing positivist epistemology in management and business research entails adhering to linguistic standards. It is through such adherence that transparency is realised.}
}
@article{TSENG2024103714,
title = {What do Chinese bidirectional Life–Xì (‘Drama’) similes/metaphors tell us about metaphorical bidirectionality?},
journal = {Lingua},
volume = {303},
pages = {103714},
year = {2024},
issn = {0024-3841},
doi = {https://doi.org/10.1016/j.lingua.2024.103714},
url = {https://www.sciencedirect.com/science/article/pii/S0024384124000433},
author = {Ming-Yu Tseng},
keywords = {Bidirectionality, Chinese language, Drama metaphor, Life metaphor},
abstract = {This study investigates bidirectionality in figurative expressions by focusing on a pair of explicit bidirectional similes/metaphors in Chinese. This pair of similes/metaphors is also a saying: rénshēng rú xì, xì rú rénshēng (‘life is like a drama, a drama is like life’). The juxtaposition of bidirectional life–xì (‘drama’) similes – and the bidirectional pattern of metaphorical thought that motivates them – is rarely addressed in metaphor studies. By drawing on discourse examples in which the pair of metaphors is used, this paper examines how the bidirectional figurative expressions are interpreted in discourse and what light the pair of metaphors can shed on metaphorical bidirectionality. Building upon the recent discussion of metaphorical bidirectionality and extended conceptual metaphor theory, this paper develops an enriched understanding of bidirectionality, which is conceptualised as a symmetrical perspective that regards two subjects as interacting with one another on a more or less equal footing, or the principal subject and subsidiary subject as being reversible, and which exhibits unsettling metaphoricity in the literal–metaphorical continuum.}
}
@article{XIONG2025100017,
title = {Towards the next generation of digital terrain modelling and analysis: New value-added perspective for geoscience},
journal = {Information Geography},
volume = {1},
number = {1},
pages = {100017},
year = {2025},
issn = {3050-5208},
doi = {https://doi.org/10.1016/j.infgeo.2025.100017},
url = {https://www.sciencedirect.com/science/article/pii/S305052082500017X},
author = {Liyang Xiong and Sijin Li and Hongen Wang and Jun Chen and Fengyize Yu and Nuozhou Shen and Guoan Tang},
keywords = {Digital terrain analysis, DEM, Value-added framework},
abstract = {Digital terrain modelling and analysis play critical roles in geoscientific research, which underpins the understanding of Earth's surface dynamics and processes. Traditional digital elevation models (DEMs) capture the geometric characteristics of the Earth's surface at a specific time, providing limited insights into the underlying geomorphological processes. To address these limitations, this study proposes the concept of value-added digital terrain, which extends elevation-only DEMs by integrating multidimensional information such as temporal dynamics, spatial relationships, and geomorphological attributes. The proposed framework encompasses quality enhancement methods, multidimensional information integration, and process-based dynamic modelling. Quality enhancement is achieved through a comprehensive framework involving positional accuracy improvement, terrain feature enhancement, spatial relationship optimization, and regional morphology reconstruction. Enrichment with multidimensional information, including temporal and spatial dimension extensions, object and morphological feature integrations, and attribute augmentation, allows for a deeper exploration of terrain processes and mechanisms. Finally, the paper highlights the significance of transitioning from morphological representation to process-oriented analysis, emphasizing the necessity for future digital terrain analysis to evolve towards dynamic, multidimensional, and globally applicable frameworks.}
}
@article{ZHAI2024213204,
title = {Multifactor quantitative evaluation of tight sandstone reservoirs based on Geodetector model and spatial statistics: First member of the Paleogene Shahejie Formation, southern Raoyang Sag, Bohai Bay Basin},
journal = {Geoenergy Science and Engineering},
volume = {242},
pages = {213204},
year = {2024},
issn = {2949-8910},
doi = {https://doi.org/10.1016/j.geoen.2024.213204},
url = {https://www.sciencedirect.com/science/article/pii/S2949891024005748},
author = {Chenyu Zhai and Qingchun Meng and Jinyong Li and Fajun Guo and Jun Xie and Xiuwei Wang and Hongmei Wang and Ping Li and Jie Cui and Li Wang},
keywords = {Reservoir evaluation, Spatial heterogeneity, Kriging model, K-means clustering, GD model},
abstract = {Identifying high-quality reservoirs in tight sandstone is challenging, and their scientific and accurate evaluation is crucial. Research on the spatial heterogeneity of reservoir-influencing factors remains limited. In this paper, the first member of the Shahejie Formation in Raoyang Sag is used as an example for quantitative evaluation of tight sandstone reservoirs based on spatial statistics. The spatial distribution of eight reservoir factors such as sand thickness, porosity, permeability, etc. was considered. Using the Kriging model and k-means clustering, spatial interpolation and determination of reservoir types and classification intervals were performed. The Geodetector model can analyze the spatial heterogeneity of reservoir factors, and then establish the interpretation force q value and interaction relationships for each factor. Finally, the reservoir evaluation map is generated based on cluster analysis. The results indicate that reservoirs can be divided into four types based on quality. The q values of the eight reservoir factors are 0.293, 0.205, 0.115, 0.275, 0.216, 0.189, 0.238 and 0.149. The influence of each factor on the reservoir increases linearly after the interaction. High quality reservoirs are mainly distributed in the northeast and northwest of the study area. The Geodetector model-driven reservoir evaluation process realizes accurate identification of reservoir properties by deeply analyzing and quantifying the spatial distribution patterns and internal correlation of reservoir factors. It is characterized by the discovery of dominant factors, reliable evaluation effect and good adaptability. The method expands the boundary of reservoir evaluation, which is more accurate and refined, and provides a reliable decision-making basis for reservoir development.}
}
@article{CAO2022208,
title = {CAILIE 1.0: A dataset for Challenge of AI in Law - Information Extraction V1.0},
journal = {AI Open},
volume = {3},
pages = {208-212},
year = {2022},
issn = {2666-6510},
doi = {https://doi.org/10.1016/j.aiopen.2022.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666651022000237},
author = {Yu Cao and Yuanyuan Sun and Ce Xu and Chunnan Li and Jinming Du and Hongfei Lin},
keywords = {Legal information extraction, Legal artificial intelligence},
abstract = {Legal information extraction requires identifying and classifying legal elements from specific legal documents. Considering that information extraction is mainly regarded as the first step in natural language understanding, the quality of legal information extraction results certainly has an immense impact on the performance of various legal artificial intelligence (AI) downstream tasks. However, Chinese judicial information extraction datasets are very scarce due to the particularity of legal documents. In response to this situation, we constructed a dataset for Challenge of AI in Law - Information Extraction V1.0 (CAILIE 1.0). The following two features of CAILIE are worth highlighting: 1) the entity definition focuses on more fine-grained theft document information, providing more interpretability for downstream legal AI; and 2) we define entity labels with judicial attributes based on natural attribute labels to meet the needs of Chinese judicial practice. We implement some classic models on this dataset. The experimental results show that legal information extraction is still challenging and additional research is required for this task to be solved.}
}
@article{LE2020,
title = {Prediction of Medical Concepts in Electronic Health Records: Similar Patient Analysis},
journal = {JMIR Medical Informatics},
volume = {8},
number = {7},
year = {2020},
issn = {2291-9694},
doi = {https://doi.org/10.2196/16008},
url = {https://www.sciencedirect.com/science/article/pii/S2291969420002045},
author = {Nhat Le and Matthew Wiley and Antonio Loza and Vagelis Hristidis and Robert El-Kareh},
keywords = {consumer health information, decision support techniques, electronic health record},
abstract = {Background
Medicine 2.0—the adoption of Web 2.0 technologies such as social networks in health care—creates the need for apps that can find other patients with similar experiences and health conditions based on a patient’s electronic health record (EHR). Concurrently, there is an increasing number of longitudinal EHR data sets with rich information, which are essential to fulfill this need.
Objective
This study aimed to evaluate the hypothesis that we can leverage similar EHRs to predict possible future medical concepts (eg, disorders) from a patient’s EHR.
Methods
We represented patients’ EHRs using time-based prefixes and suffixes, where each prefix or suffix is a set of medical concepts from a medical ontology. We compared the prefixes of other patients in the collection with the state of the current patient using various interpatient distance measures. The set of similar prefixes yields a set of suffixes, which we used to determine probable future concepts for the current patient’s EHR.
Results
We evaluated our methods on the Multiparameter Intelligent Monitoring in Intensive Care II data set of patients, where we achieved precision up to 56.1% and recall up to 69.5%. For a limited set of clinically interesting concepts, specifically a set of procedures, we found that 86.9% (353/406) of the true-positives are clinically useful, that is, these procedures were actually performed later on the patient, and only 4.7% (19/406) of true-positives were completely irrelevant.
Conclusions
These initial results indicate that predicting patients’ future medical concepts is feasible. Effectively predicting medical concepts can have several applications, such as managing resources in a hospital.}
}
@article{BESHARATIFOUMANI2020998,
title = {Fundamentals and new achievements in feature-based modeling, a review},
journal = {Procedia Manufacturing},
volume = {51},
pages = {998-1004},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.140},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920319971},
author = {Hossein Besharati-Foumani and Mika Lohtander and Juha Varis},
keywords = {Feature-based modeling, Feature interoperability, Knowledge representation},
abstract = {Feature-based modeling approach has been widely utilized in various applications of product and process engineering. Initially, it was introduced for geometry representation in computer-aided design (CAD) utilities. With further developments in computer-aided engineering, process planning, and manufacturing, feature-based modeling gained higher importance and role for the integration of these systems. Despite considerable efforts to gain domain-neutral definition for features, they have been mostly defined for specific applications that restrict their reusability for different systems. Recent progress in the Internet of Things (IoT), Artificial Intelligence (AI), Big Data, in addition to additive and hybrid manufacturing techniques, has realized a new manufacturing paradigm called Social Manufacturing which needs significant cross-domain interactions and intensifies the demand for feature interoperability. In this paper, after presenting a brief history of the feature-based modeling approach and its development challenges, we focus on finding the approaches to tackle feature interoperability issues and investigate the role of features in Model-Based Definition (MBD) and the Social Manufacturing paradigm. Also, future research pathways to develop the feature-based modeling approach is presented.}
}
@article{MRHAR2025100616,
title = {A deep learning framework for optimizing personalized online course recommendation and selection},
journal = {Decision Analytics Journal},
volume = {16},
pages = {100616},
year = {2025},
issn = {2772-6622},
doi = {https://doi.org/10.1016/j.dajour.2025.100616},
url = {https://www.sciencedirect.com/science/article/pii/S2772662225000724},
author = {Khaoula Mrhar and Mounia Abik},
keywords = {Deep learning, Optimization, Predictive modeling, Recommender system, Sentiment analysis, Cognitive assessment},
abstract = {Massive Open Online Courses (MOOCs) and the broad adoption of distance learning over the past few years have caused a remarkable shift in the educational landscape. However, the vast majority of available MOOCs often challenge learners in selecting courses that align with their academic goals, leading to high dropout rates. This paper presents an unexploited opportunity for formal educational institutions to integrate MOOCs into their curricula by guiding and supporting learners on these platforms. It introduces a Deep Semantic MOOC Recommender System (DSMRS) designed to help learners choose MOOCs aligned with their formal curriculum. The system utilizes advanced Natural Language Processing (NLP) techniques to deliver personalized recommendations to learners. It employs a top-N recommender algorithm and leverages three key strategies: (a) an optimized Explicit Semantic Analysis (ESA) to measure semantic similarity between course descriptions and learning objectives; (b) Sentiment Analysis, using a Bayesian Convolutional Neural Network-Long Short-Term Memory (CNN-LSTM) model to analyze learner reviews of MOOCs; and (c) Classification of Recommended MOOCs based on Bloom’s Taxonomy, categorizing MOOCs according to cognitive complexity. The results highlight that experimentation conducted with the system demonstrates promising performance.}
}
@article{HWANG2025129548,
title = {GPT-Empowered Question–Answer Dataset for Informative and Empathetic Support for Korean Childhood Cancer Survivors},
journal = {Expert Systems with Applications},
pages = {129548},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.129548},
url = {https://www.sciencedirect.com/science/article/pii/S095741742503163X},
author = {Kyunbum Hwang and Mirae Kim and Min Ah Kim and Chaerim Park and Yehwi Park and Chungyeon Lee and Jooyoung Lim and Hayoung Oh},
keywords = {childhood cancer, cancer survivorship, alleviating hallucination, pseudo-scoring, relational knowledge graph,},
abstract = {Despite improvements in survival rates, childhood cancer survivors in South Korea still face significant challenges in accessing the psychological and informational support they need. To address these challenges, we developed the Korean Childhood Cancer Survivor Question-Answer (KCCSQA) dataset in which contains 3,876 question–answer pairs. The questions were sourced from websites, academic articles, and an online survey, where 119 childhood cancer survivors contributed 1,283 questions. We used GPT-4 Turbo to generate the responses, followed by an expert evaluation by 11 specialists to ensure factual accuracy, complementarity, comprehensibility, and empathy. The overall quality of the GPT-generated responses was rated 4.98 out of 6, indicating a high level of quality. To enhance the dataset, we integrated a relational knowledge graph to mitigate hallucinations in the AI-generated answers, achieving a performance of 0.979 in hallucination detection. Additionally, a pseudo-scoring system was implemented for continuous quality assessment. The dataset’s effectiveness was evaluated through a pilot study involving 14 childhood cancer survivors, who interacted with a retrieval-based QA system using a single-turn chatbot format. The mean satisfaction rating was 4.36 on a 6-point Likert scale, and all participants expressed a willingness to use the system again.}
}
@article{XU2024503,
title = {A Three-Stage Dynamic Risk Model for Metro Shield Tunnel Construction},
journal = {KSCE Journal of Civil Engineering},
volume = {28},
number = {2},
pages = {503-516},
year = {2024},
issn = {1226-7988},
doi = {https://doi.org/10.1007/s12205-023-0655-2},
url = {https://www.sciencedirect.com/science/article/pii/S1226798824004070},
author = {Na Xu and Chaoran Guo and Li Wang and Xueqing Zhou and Ying Xie},
keywords = {Risk assessment and analysis, Construction safety, Dynamic bayesian network, Shield tunnel construction, Excavation},
abstract = {The complex construction process of the metro shield method often leads to safety accidents. The various construction stages of shield tunnel construction comprise different construction activities and are accompanied by different safety risk factors. However, traditional risk assessment often evaluates the risk factors as a whole before shield tunnel construction and does not evaluate the risk factors dynamically by construction stages and by construction activities. To fill this gap, this paper aims to construct a dynamic Bayesian-based safety risk assessment model for shield tunnel construction from the perspective of changing construction stages and activities. First, safety risk factors were identified using the work breakdown structure-risk breakdown structure (WBS-RBS) method. Then, a three-stage dynamic assessment model of safety risks was constructed to depict the shield launch, shield tunnel, and shield reach. The dynamic Bayesian network (DBN) was improved to address the model with triangular fuzzy numbers and the leaky noisy-or-gate extension model. Finally, a case study was conducted. The model proposed in this paper is able to reveal the dynamic evolution of safety risks triggered by different construction activities. It offers a new simulated model for the prevention of safety accidents in the construction of metro shields.}
}
@article{ALBORE2023104318,
title = {Skill-based design of dependable robotic architectures},
journal = {Robotics and Autonomous Systems},
volume = {160},
pages = {104318},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104318},
url = {https://www.sciencedirect.com/science/article/pii/S092188902200207X},
author = {Alexandre Albore and David Doose and Christophe Grand and Jérémie Guiochet and Charles Lesire and Augustin Manecy},
keywords = {Skill-based architecture, Dependability, Development process, Model-checking, Fault-Tree Analysis},
abstract = {Software architectures for autonomous systems are generally structured with 3 layers: a decisional layer managing autonomous reasoning, a functional layer managing reactive tasks and processing, and an executive layer bridging the gap between both. The executive layer plays a central role, as it links high-level tasks with low-level processing, and is generally responsible for the robustness or the fault-tolerance of the overall system. In this paper, we propose a development process for such an executive layer that emphasizes on the dependability of this layer. To do so, we structure the executive layer using skills, that are formally defined using a specific language, and we then provide some tools to verify these models, generate some code, and a methodology to assess the fault-tolerance of the resulting architecture.}
}
@article{ALJAMEL2021,
title = {A Semantic Knowledge-Based Framework for Information Extraction and Exploration},
journal = {International Journal of Decision Support System Technology},
volume = {13},
number = {2},
year = {2021},
issn = {1941-6296},
doi = {https://doi.org/10.4018/IJDSST.2021040105},
url = {https://www.sciencedirect.com/science/article/pii/S1941629621000033},
author = {Abduladem Aljamel and Taha Osman and Dhavalkumar Thakker},
keywords = {Information Extraction, Knowledge Representation, Knowledge-Based Approach, Machine Learning, Natural Language Processing, Non-Binary Relations, Open Linked Data, Semantic Web Technologies},
abstract = {ABSTRACT
The availability of online documents that describe domain-specific information provides an opportunity in employing a knowledge-based approach in extracting information from web data. This research proposes a novel comprehensive semantic knowledge-based framework that helps to transform unstructured data to be easily exploited by data scientists. The resultant sematic knowledgebase is reasoned to infer new facts and classify events that might be of importance to end users. The target use case for the framework implementation was the financial domain, which represents an important class of dynamic applications that require the modelling of non-binary relations. Such complex relations are becoming increasingly common in the era of linked open data. This research in modelling and reasoning upon such relations is a further contribution of the proposed semantic framework, where non-binary relations are semantically modelled by adapting the semantic reasoning axioms to fit the intermediate resources in the N-ary relations requirements.}
}
@article{BOECK2025107877,
title = {Combined dietary omega-3 and omega-6 fatty acids protect against hyperglycemia-associated retinopathy in neonatal mice},
journal = {Pharmacological Research},
volume = {219},
pages = {107877},
year = {2025},
issn = {1043-6618},
doi = {https://doi.org/10.1016/j.phrs.2025.107877},
url = {https://www.sciencedirect.com/science/article/pii/S1043661825003020},
author = {Myriam Boeck and Hitomi Yagi and Pia Lundgren and Aldina Pivodic and Anders K. Nilsson and Yan Zeng and Chuck T. Chen and Taku Kasai and Deokho Lee and Shen Nian and Victoria Hirst and Katherine Neilsen and Chaomei Wang and Jeff Lee and Mathew Yu and Andrew McCutcheon and Sasha A. Singh and Masanori Aikawa and Richard P. Bazinet and Zhongjie Fu and Ann Hellström and Lois EH Smith},
keywords = {Postnatal hyperglycemia, Retinopathy of prematurity, DHA, ARA, Retinal vasculature},
abstract = {Retinopathy of prematurity (ROP) with early vessel loss (Phase I) followed by uncontrolled vessel growth (Phase II) causes visual impairment in premature infants. Although supplementation with omega-3 (n-3) docosahexaenoic acid (DHA) alone shows mixed results in preventing ROP, supplementation with both n-3 DHA and n-6 arachidonic acid (ARA) in early postnatal life reduces severe ROP by 50 % (Mega Donna Mega study). In the Mega Donna Mega study, 146 (72.6 %) of 201 included infants had at least one hyperglycemic episode during the first 14 days of life, which is a strong ROP risk factor. We therefore evaluated the protective effects and mechanisms of combined dietary n-3 DHA and n-6 ARA in a neonatal mouse model of hyperglycemia-induced suppression of retinal vascular development (Phase I ROP). At postnatal day (P) 10, retinal vessel growth was improved in pups from mothers on diets enriched with 1 % DHA + 2 % ARA vs. 3 % DHA. Lipid changes in pup plasma and RPE complex (retinal pigment epithelium with choroid and sclera) were in accordance with maternal diets' DHA and ARA levels, indicating that milk lipids reflected maternal diets. Proteomic retinal analysis revealed increased abundances of proteins related to mitochondrial respiration and glucose metabolism with the combined diet. Inhibition of mitochondrial ATP synthase negated the protective effects of the combined diet. In conclusion, combined DHA+ARA oral maternal supplementation protects against hyperglycemia-induced retinopathy in mouse neonates (Phase I ROP model) through enhanced retinal metabolism, suggesting the potential of balanced lipid supplementation for ROP prevention.}
}
@article{WOSIAK20212422,
title = {Using semantic enrichment methods in expert search system for recruitment process in IT corporation},
journal = {Procedia Computer Science},
volume = {192},
pages = {2422-2431},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921017488},
author = {Agnieszka Wosiak},
keywords = {expert search system, semantic enrichment, text data analysis, preprocessing and outlier detection;},
abstract = {The problem of intelligent information retrieval and semantic enrichment becomes more and more popular due to the difficulty of searching and analyzing large text datasets. The common approach assumes user manual queries in natural language. Various semantic enrichment methods and intelligent text searching allow obtaining more accurate results leading to broader knowledge and user satisfaction. This research presents state-of-the-art methods of searching with enrichment and building rankings of results for the expert recruitment process in IT industry. The proposed model implements full-text search, semantic enrichment, and machine learning to match experts with job offers. Different data sources on expert competencies were used, including curricula vitae, historical data, and Internet resources. The testing results confirm an improvement in the search quality compared to the existing systems in the recruitment company.}
}
@article{DIBUDUO20252192,
title = {In vitro studies of human erythropoiesis using a 3D silk-based bone marrow model that generates erythroblastic islands},
journal = {Blood Advances},
volume = {9},
number = {9},
pages = {2192-2206},
year = {2025},
issn = {2473-9529},
doi = {https://doi.org/10.1182/bloodadvances.2024014905},
url = {https://www.sciencedirect.com/science/article/pii/S2473952925001077},
author = {Christian A. {Di Buduo} and Francesca Careddu and Samuele Metti and Marco Lunghi and Santo Diprima and Virginia Camilotto and Giovanna Bruni and Umberto Gianelli and Delfina Tosi and Cesare Perotti and Claudia {Del Fante} and Mario Cazzola and Paola Braghetta and David L. Kaplan and Giampaolo Minetti and Luca Malcovati and Alessandra Balduini},
abstract = {Abstract
The pursuit of ex vivo erythrocyte generation has led to the development of various culture systems that simulate the bone marrow microenvironment. However, these models often fail to fully replicate the hematopoietic niche's complex dynamics. In our research, we use a comprehensive strategy that emphasizes physiological red blood cell (RBC) differentiation using a minimal cytokine regimen. A key innovation in our approach is the integration of a 3-dimensional (3D) silk-based scaffold engineered to mimic both the physical and chemical properties of human bone marrow. This scaffold facilitates critical macrophage-RBC interactions and incorporates fibronectin functionalization to support the formation of erythroblastic island (EBI)–like niches. We observed diverse stages of erythroblast maturation within these niches, driven by the activation of autophagy, which promotes organelle clearance and membrane remodeling. This process leads to reduced surface integrin expression and significantly enhances RBC enucleation. Using a specialized bioreactor chamber, millions of RBCs can be detached from the EBIs and collected in transfusion bags via dynamic perfusion. Inhibition of autophagy through pharmacological agents or α4 integrin blockade disrupted EBI formation, preventing cells from completing their final morphological transformations, having them trapped in the erythroblast stage. Our findings underscore the importance of the bone marrow niche in maintaining the structural integrity of EBIs and highlight the critical role of autophagy in facilitating organelle clearance during RBC maturation. RNA sequencing analysis further confirmed that these processes are uniquely supported by the 3D silk scaffold, which is essential for enhancing RBC production ex vivo.}
}
@article{RAIS2021,
title = {An Efficient Method for Biomedical Word Sense Disambiguation Based on Web-Kernel Similarity},
journal = {International Journal of Healthcare Information Systems and Informatics},
volume = {16},
number = {4},
year = {2021},
issn = {1555-3396},
doi = {https://doi.org/10.4018/IJHISI.20211001.oa9},
url = {https://www.sciencedirect.com/science/article/pii/S1555339621000300},
author = {Mohammed Rais and Mohammed Bekkali and Abdelmonaime Lachkar},
keywords = {Biomedical Word Sense Disambiguation, Conceptualization, Context Concept, MSH-WSD, Rough Set Theory, Short Text Similarity},
abstract = {ABSTRACT
Searching for the best sense for a polysemous word remains one of the greatest challenges in the representation of biomedical text. To this end, word sense disambiguation (WSD) algorithms mostly rely on an external source of knowledge, like a thesaurus or ontology, for automatically selecting the proper concept of an ambiguous term in a given window of context using semantic similarity and relatedness measures. In this paper, the authors propose a web-based kernel function for measuring the semantic relatedness between concepts to disambiguate an expression versus multiple possible concepts. This measure uses the large volume of documents returned by PubMed search engine to determine the greater context for a biomedical short text through a new term weighting scheme based on rough set theory (RST). To illustrate the efficiency of our proposed method, they evaluate a WSD algorithm based on this measure on a biomedical dataset (MSH-WSD) that contains 203 ambiguous terms and acronyms. The obtained results demonstrate promising improvements.}
}
@article{PIPER2022,
title = {Longitudinal Study of a Website for Assessing American Presidential Candidates and Decision Making of Potential Election Irregularities Detection},
journal = {International Journal on Semantic Web and Information Systems},
volume = {18},
number = {1},
year = {2022},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.305802},
url = {https://www.sciencedirect.com/science/article/pii/S1552628322000199},
author = {Justin Piper and James A. Rodger},
keywords = {Artificial Intelligence, Benford’s Law, Data Envelope Analysis, Decision Making, Engineering, Ontological, Word-Sense Disambiguation (WSD)},
abstract = {ABSTRACT
The authors employ the concept of word sense disambiguation to determine the inherent meaning of voter intentions regarding possible political candidates from the 2016 Presidential election. They present the findings based on a website (www.presidentselect.com) that they developed, where candidates can be examined and their true assets and competencies in three major areas of eligibility, education, and experience can be deciphered. Data envelope analysis is used to determine underlying word instances for elected and successful outputs. They also utilize the web site results to longitudinally extend these findings for decision making of potential election fraud detection in the 2020 Presidential election, utilizing Benford’s Law. The results shed light on these phenomena and provide new insights into the word sense disambiguation literature.}
}
@article{MARMO2020103275,
title = {Building performance and maintenance information model based on IFC schema},
journal = {Automation in Construction},
volume = {118},
pages = {103275},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103275},
url = {https://www.sciencedirect.com/science/article/pii/S0926580520302181},
author = {Rossella Marmo and Francesco Polverino and Maurizio Nicolella and Andrej Tibaut},
keywords = {Building Information Modelling (BIM), Building Performance Assessment (BPA), Facility Management (FM), Operation and Maintenance (O&M), Industry Foundation Classes (IFC), Model View Definition (MVD), Visual Programming Language (VPL), Entity-Relationship model (ER model), Healthcare facilities, Operation theatre},
abstract = {The integration of operational information in building life-cycle management is a relevant topic within the facilities management domain. A previous study proposed a methodology based on Building Information Modelling (BIM) and Facility Management (FM) systems integration, underpinned by a Performance Information Model, to support the performance assessment and the maintenance management. The purpose of this paper is to extend and validate that methodology with an open standard approach. A subset of the Industry Foundation Classes (IFC) schema related to performance assessment and maintenance procedures is mapped into a relational database. The database supports the integration between BIM models and facilities information systems. Case studies including healthcare buildings test the database and a new application. The results contribute to the BIM adoption in the FM domain. Also, the studied and validated IFC schema subset can be considered as a contribution to the Model View Definition for FM.}
}
@article{PANKAEW2024100497,
title = {Proteomic profiling of peanut hairy root culture unveils distinctive adaptive responses induced by elicitor treatment across diverse time intervals},
journal = {Plant Stress},
volume = {12},
pages = {100497},
year = {2024},
issn = {2667-064X},
doi = {https://doi.org/10.1016/j.stress.2024.100497},
url = {https://www.sciencedirect.com/science/article/pii/S2667064X24001519},
author = {Chanyanut Pankaew and Phadtraphorn Chayjarung and Chonnikan Tothong and Sompop Pinit and Wannapa Khanthit and Sirinan Temwong and Arpassara Maliprom and Sittiruk Roytrakul and Apinun Limmongkon},
keywords = {Proteomic profile, Adaptive response, Plant defense, Signaling pathway, Hairy root},
abstract = {Plants employ a diverse array of responses to counteract and defend against stressors. Utilizing quantitative proteomic analysis allows for the exploration of global proteomic dynamics in cells and tissues at specific time points. In order to comprehend the intricate mechanisms underlying plant responses, we conducted a proteomic investigation of peanut hairy root cultures induced with a combination of chitosan (CHT), methyl jasmonate (MeJA), and cyclodextrin (CD) at multiple time points. The study identified 292, 317, and 327 differentially expressed proteins (DEPs) in hairy root tissues at 24-, 48-, and 72 h intervals post-elicitation, respectively. Principal component analysis (PCA) distinctly clustered each timepoint of treatment and the control group. Gene ontology (GO) enrichment analysis highlighted terms such as "responding to biotic stimuli" and "defense response" during the early response, with the upregulation of Heat shock cognate 70 kDa protein, Wound-induced protein 1, and Disease resistance-like protein. The 72 h time point specifically showed an upregulation of superoxide dismutase (SOD) and auxin response factor, indicating a delayed response and illustrating how hairy roots adapt to stress in later stages. Furthermore, GO enrichment at later stages implicated alterations in protein production, involving both synthesis and degradation processes. Proteolytic enzymes, including those in the ubiquitin/proteasome pathway, were identified as facilitating protein degradation. The detection of small molecular weight (MW) peptides in the culture medium of elicited hairy roots suggested their origin from the proteolytic cleavage of precursor proteins. The GO enrichment of culture medium peptides pointed towards involvement in signaling pathways. The suggested signaling cascade acts as a potential link that connects the recognition of stress to the initiation of appropriate response to external stressors. This approach delves into dynamic changes in protein expression and secretion patterns over various intervals, providing insights into the plant's adaptive response to external stress.}
}
@incollection{2025395,
title = {Index},
editor = {Rajesh Kumar Dhanaraj and M. Nalini and Malathy Sathyamoorthy and Manar Mohaisen},
booktitle = {Knowledge Graph-Based Methods for Automated Driving},
publisher = {Elsevier},
pages = {395-411},
year = {2025},
isbn = {978-0-443-30040-0},
doi = {https://doi.org/10.1016/B978-0-443-30040-0.09993-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780443300400099938}
}
@article{CARETTATEIXEIRA20211097,
title = {Proposal for a health information management model based on Lean thinking},
journal = {Procedia Computer Science},
volume = {181},
pages = {1097-1104},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.306},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921003550},
author = {Jéssica Cristina {Caretta Teixeira} and Filipe Andrade Bernardi and Rui Pedro Charters {Lopes Rijo} and Domingos Alves},
keywords = {Lean Healthcare, Health Information Systems, Quality Improvement},
abstract = {Although assessing quality in the health field is a prominent challenge, there is unanimity among managers that it is necessary to select appropriate assessment systems and methods to assist the administration of services and provide decision-making with the least degree of uncertainty possible. Lean, also known as lean philosophy, is a management model that has been used in the area of Health. The management of data, knowledge, and health services must be carefully performed, so that quality care can be offered. at all levels of care. In this way, when implementing Lean strategies in Information Technology, it is necessary to evaluate all its processes within the institution to eliminate waste, structure functions within the applied methodology and measure improvement at all levels of the organization. Thus, the general objective of this article is that of a study that leads to a health information management model based on Lean thinking in the municipality of Ituverava. The highly heterogeneous, and sometimes ambiguous, nature of the medical language and its constant evolution, the high amount of data generated constantly by the automation of processes and the emergence of new technologies constitute the foundation for the inevitable computerization of health to promote the production and management of knowledge. Adopting Lean thinking in health may seem a challenge initially for managers and team members, but as the first results begin to appear, profound and concrete changes are visible for positive transformation for improvements in the quality of the service provided, until the culture can be learned completely in order to have the perfect care.}
}
@article{LEE2020101117,
title = {Customer requirement-driven design method and computer-aided design system for supporting service innovation conceptualization handling},
journal = {Advanced Engineering Informatics},
volume = {45},
pages = {101117},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101117},
url = {https://www.sciencedirect.com/science/article/pii/S1474034620300860},
author = {Ching-Hung Lee and Chun-Hsien Chen and Yu-Chi Lee},
keywords = {Requirement-driven design, Computer-aided service design, Abductive logic, Service innovation},
abstract = {How to create value to meet customers’ requirements by effectively using advanced digital technology (DT) for digital transformation is an implementation challenge for new and future scenarios of current contexts. Valid digital transformation issues need to be considered to deliver novel values with sustainability concerns. Attempting to effectively achieve a successful service system design in an industrial context, a holistic customer requirement-driven service design method with abductive logic was proposed to drive self-service productivity enhancement. Laddering theory, ontology-based design knowledge hierarchy (DKH), theory of inventive problem solving (TRIZ) and quality function deployment (QFD) technique are adopted in terms of effective design knowledge handling in innovation conceptualization with a traceable path and abductive logic linking from customer concerns, business context and technology digitization. Then, a comprehensive case study of an empirical smart meal-ordering service system for the digital transformation of canteen processes was illustrated to verify this approach.}
}
@article{BRAUN20201963251,
title = {Computing on Phenotypic Descriptions for Candidate Gene Discovery and Crop Improvement},
journal = {Plant Phenomics},
volume = {2020},
pages = {1963251},
year = {2020},
issn = {2643-6515},
doi = {https://doi.org/10.34133/2020/1963251},
url = {https://www.sciencedirect.com/science/article/pii/S264365152400044X},
author = {Ian R. Braun and Colleen F. Yanarella and Carolyn J. Lawrence-Dill},
abstract = {Many newly observed phenotypes are first described, then experimentally manipulated. These language-based descriptions appear in both the literature and in community datastores. To standardize phenotypic descriptions and enable simple data aggregation and analysis, controlled vocabularies and specific data architectures have been developed. Such simplified descriptions have several advantages over natural language: they can be rigorously defined for a particular context or problem, they can be assigned and interpreted programmatically, and they can be organized in a way that allows for semantic reasoning (inference of implicit facts). Because researchers generally report phenotypes in the literature using natural language, curators have been translating phenotypic descriptions into controlled vocabularies for decades to make the information computable. Unfortunately, this methodology is highly dependent on human curation, which does not scale to the scope of all publications available across all of plant biology. Simultaneously, researchers in other domains have been working to enable computation on natural language. This has resulted in new, automated methods for computing on language that are now available, with early analyses showing great promise. Natural language processing (NLP) coupled with machine learning (ML) allows for the use of unstructured language for direct analysis of phenotypic descriptions. Indeed, we have found that these automated methods can be used to create data structures that perform as well or better than those generated by human curators on tasks such as predicting gene function and biochemical pathway membership. Here, we describe current and ongoing efforts to provide tools for the plant phenomics community to explore novel predictions that can be generated using these techniques. We also describe how these methods could be used along with mobile speech-to-text tools to collect and analyze in-field spoken phenotypic descriptions for association genetics and breeding applications.}
}
@article{ATMAKURU2025102673,
title = {Artificial intelligence-based suicide prevention and prediction: A systematic review (2019–2023)},
journal = {Information Fusion},
volume = {114},
pages = {102673},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102673},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524004512},
author = {Anirudh Atmakuru and Alen Shahini and Subrata Chakraborty and Silvia Seoni and Massimo Salvi and Abdul Hafeez-Baig and Sadaf Rashid and Ru San Tan and Prabal Datta Barua and Filippo Molinari and U Rajendra Acharya},
keywords = {Artificial intelligence, Natural language processing, Machine learning, Mental health, Suicide prevention, Suicide prediction},
abstract = {Suicide is a major global public health concern, and the application of artificial intelligence (AI) methods, such as natural language processing (NLP), machine learning (ML), and deep learning (DL), has shown promise in advancing suicide prediction and prevention efforts. Recent advancements in AI – particularly NLP and DL have opened up new avenues of research in suicide prediction and prevention. While several papers have reviewed specific detection techniques like NLP or DL, there has been no recent study that acts as a one-stop-shop, providing a comprehensive overview of all AI-based studies in this field. In this work, we conduct a systematic literature review to identify relevant studies published between 2019 and 2023, resulting in the inclusion of 156 studies. We provide a comprehensive overview of the current state of research conducted on AI-driven suicide prevention and prediction, focusing on different data types and AI techniques employed. We discuss the benefits and challenges of these approaches and propose future research directions to improve the practical application of AI in suicide research. AI is highly capable of improving the accuracy and efficiency of risk assessment, enabling personalized interventions, and enhancing our understanding of risk and protective factors. Multidisciplinary approaches combining diverse data sources and AI methods can help identify individuals at risk by analyzing social media content, patient histories, and data from mobile devices, enabling timely intervention. However, challenges related to data privacy, algorithmic bias, model interpretability, and real-world implementation must be addressed to realize the full potential of these technologies. Future research should focus on integrating prediction and prevention strategies, harnessing multimodal data, and expanding the scope to include diverse populations. Collaboration across disciplines and stakeholders is essential to ensure that AI-driven suicide prevention and prediction efforts are ethical, culturally sensitive, and person-centered.}
}
@article{QU20181002,
title = {Computing semantic similarity based on novel models of semantic representation using Wikipedia},
journal = {Information Processing & Management},
volume = {54},
number = {6},
pages = {1002-1021},
year = {2018},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2018.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0306457317309226},
author = {Rong Qu and Yongyi Fang and Wen Bai and Yuncheng Jiang},
keywords = {Semantic similarity, Concept similarity, Information content, Feature-based methods, Wikipedia},
abstract = {Computing Semantic Similarity (SS) between concepts is one of the most critical issues in many domains such as Natural Language Processing and Artificial Intelligence. Over the years, several SS measurement methods have been proposed by exploiting different knowledge resources. Wikipedia provides a large domain-independent encyclopedic repository and a semantic network for computing SS between concepts. Traditional feature-based measures rely on linear combinations of different properties with two main limitations, the insufficient information and the loss of semantic information. In this paper, we propose several hybrid SS measurement approaches by using the Information Content (IC) and features of concepts, which avoid the limitations introduced above. Considering integrating discrete properties into one component, we present two models of semantic representation, called CORM and CARM. Then, we compute SS based on these models and take the IC of categories as a supplement of SS measurement. The evaluation, based on several widely used benchmarks and a benchmark developed by ourselves, sustains the intuitions with respect to human judgments. In summary, our approaches are more efficient in determining SS between concepts and have a better human correlation than previous methods such as Word2Vec and NASARI.}
}
@article{KONYS20202297,
title = {How to support digital sustainability assessment? An attempt to knowledge systematization},
journal = {Procedia Computer Science},
volume = {176},
pages = {2297-2311},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.288},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920321943},
author = {Agnieszka Konys},
keywords = {Digital sustainability, sustainable development, sustainability, knowledge base, digital development, ontology},
abstract = {Digital sustainability has great potential for use in many areas. It refers to technological ecosystems, including various mobile payment platforms, crowdfunding, peer-to-peer loans, large financial-related datasets, artificial intelligence, machine learning, blockchain, and digital tokens internet of things. This creates relatively new opportunities for sustainable development. As a consequence, the digital sustainability gains the more attention of the research community. Therefore, a knowledge base containing selected documents in the field of digital sustainability is the main aim of this article. An analysis of available resources is supported by theoretical foundations including both comparative analysis and bibliometric analysis of selected approaches. Based on the retrieved 22 documents and their in-depth analysis by manual screening and by using bibliometric analysis, the set of criteria encompassed 12 main criteria and 276 sub-criteria covering various aspects of digital sustainability and providing relevant knowledge about analyzed documents and their content. To test the functionality of the proposed knowledge base, a number of sample competency queries were provided. Hopefully, this approach will be a starting point to complete missing knowledge to complete the puzzle of digital sustainability.}
}
@article{GORRIZ2020237,
title = {Artificial intelligence within the interplay between natural and artificial computation: Advances in data science, trends and applications},
journal = {Neurocomputing},
volume = {410},
pages = {237-270},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.05.078},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220309292},
author = {Juan M. Górriz and Javier Ramírez and Andrés Ortíz and Francisco J. Martínez-Murcia and Fermin Segovia and John Suckling and Matthew Leming and Yu-Dong Zhang and Jose Ramón Álvarez-Sánchez and Guido Bologna and Paula Bonomini and Fernando E. Casado and David Charte and Francisco Charte and Ricardo Contreras and Alfredo Cuesta-Infante and Richard J. Duro and Antonio Fernández-Caballero and Eduardo Fernández-Jover and Pedro Gómez-Vilda and Manuel Graña and Francisco Herrera and Roberto Iglesias and Anna Lekova and Javier {de Lope} and Ezequiel López-Rubio and Rafael Martínez-Tomás and Miguel A. Molina-Cabello and Antonio S. Montemayor and Paulo Novais and Daniel Palacios-Alonso and Juan J. Pantrigo and Bryson R. Payne and Félix {de la Paz López} and María Angélica Pinninghoff and Mariano Rincón and José Santos and Karl Thurnhofer-Hemsi and Athanasios Tsanas and Ramiro Varela and Jose M. Ferrández},
keywords = {Artificial intelligence (AI), Machine learning, Deep learning, Reinforcement learning, Evolutionary computation, Ontologies, Artificial neural networks (ANNs), Big data, Robotics, Neuroscience, Human–machine interaction, Virtual reality, Emotion recognition, Computational neuroethology, Autism, Dyslexia, Alzheimer, Parkinson, Glaucoma, AI for social well-being},
abstract = {Artificial intelligence and all its supporting tools, e.g. machine and deep learning in computational intelligence-based systems, are rebuilding our society (economy, education, life-style, etc.) and promising a new era for the social welfare state. In this paper we summarize recent advances in data science and artificial intelligence within the interplay between natural and artificial computation. A review of recent works published in the latter field and the state the art are summarized in a comprehensive and self-contained way to provide a baseline framework for the international community in artificial intelligence. Moreover, this paper aims to provide a complete analysis and some relevant discussions of the current trends and insights within several theoretical and application fields covered in the essay, from theoretical models in artificial intelligence and machine learning to the most prospective applications in robotics, neuroscience, brain computer interfaces, medicine and society, in general.}
}
@article{HU20241049,
title = {English translation evaluation method based on BP neural network},
journal = {Procedia Computer Science},
volume = {243},
pages = {1049-1058},
year = {2024},
note = {The 4th International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.125},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924021331},
author = {Yang Hu},
keywords = {BP neural network, Machine translation, Information fusion, English translation},
abstract = {In order to solve the problems of machine translation efficiency and translation quality, this paper proposes an English translation evaluation system based on BP neural network algorithm. This method provides users with a more intelligent machine translation service experience. With the help of BP neural network algorithm, taking English online translation as the research object, Google's translation quality is the best, with an error frequency of only 167, while Baidu translation and iFLYTEK translation in China have a high error rate of 266 and 301 respectively, which is much higher than Google translation. A model of machine translation evaluation based on neural network algorithm is proposed to better solve the disadvantages of traditional English machine translation. The results show that the machine translation system based on neural network algorithm can further optimize the problems existing in machine translation, such as insufficient use of information and large scale of model parameters, and further improve the performance of neural network machine translation.}
}
@article{BUYLE201986,
title = {Raising interoperability among base registries: The evolution of the Linked Base Registry for addresses in Flanders},
journal = {Journal of Web Semantics},
volume = {55},
pages = {86-101},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300519},
author = {Raf Buyle and Ziggy Vanlishout and Serena Coetzee and Dieter {De Paepe} and Mathias {Van Compernolle} and Geert Thijs and Bert {Van Nuffelen} and Laurens {De Vocht} and Peter Mechant and Björn {De Vidts} and Erik Mannens},
keywords = {Semantic web, Ontology, e-government, Address registry, Linked Open Data, Semantic interoperability},
abstract = {The transformation of society towards a digital economy and government austerity creates a new context leading to changing roles for both government and private sector. Boundaries between public and private services are blurring, enabling government and private sector to collaborate and share responsibilities. In Belgium, the regional Government of Flanders embedded the re-use of public sector information in its legislation and published a data portal containing well over 4000 Open Datasets. Due to a lack of interoperability, interconnecting and interpreting these sources of information remain challenges for public administrations, businesses and citizens. To dissolve the boundaries between the data silos, the Flemish government applied Linked Data design principles in an operational public sector context. This paper discusses the trends we have identified while ‘rewiring’ the Authentic Source for addresses to a Linked Base Registry. We observed the impact on multiple interoperability levels; namely on the legal, organisational, semantic and technical level. In conclusion Linked Data can increase semantic and technical interoperability and lead to a better adoption of government information in the public and private sector. We strongly believe that the insights from the past thirteen years in the region of Flanders could speed up processes in other countries that are facing the complexity of raising technical and semantic interoperability.}
}
@article{MO2025113484,
title = {Development of a runtime-condition model for proactive intelligent products using knowledge graphs and embedding},
journal = {Knowledge-Based Systems},
volume = {318},
pages = {113484},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113484},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125005301},
author = {Fan Mo and Hamood Ur Rehman and Miriam Ugarte and Angela Carrera-Rivera and Nathaly Rea Minango and Fabio Marco Monetti and Antonio Maffei and Jack C. Chaplin},
keywords = {Runtime condition, Data model, Intelligent system, Knowledge graph},
abstract = {Modern manufacturing processes’ increasing complexity and variability demand advanced systems capable of real-time monitoring, adaptability, and data-driven decision-making. This paper introduces a novel runtime condition model to enhance interoperability, data integration, and decision support within intelligent manufacturing environments. The model encapsulates key manufacturing elements, including asset management, relationships, key performance indicators (KPIs), capabilities, data structures, constraints, and configurations. A key innovation is the integration of a knowledge graph enriched with embedding techniques, enabling the inference of missing relationships, dynamic reasoning, and predictive analytics. The proposed model was validated through a case study conducted in collaboration with TQC Automation Ltd., using their MicroApplication Leak Test System (MALT). A dataset of over 9,000 unique test configurations demonstrated the model’s capabilities in representing runtime conditions, managing operational parameters, and optimising test configurations. The enriched knowledge graph facilitated advanced analyses, providing actionable insights into test outcomes and enabling proactive decision-making. Empirical results showcase the model’s ability to harmonise diverse data sources, infer missing connections, and improve runtime adaptability. This study highlights the potential of combining runtime modelling with knowledge graphs to address the challenges of modern manufacturing. Future research will explore the model’s application to additional domains, integration with larger datasets, and the use of machine learning for enhanced predictive capabilities.}
}
@article{OKUR20242209,
title = {Relational Turkish Text Classification Using Distant Supervised Entities and Relations},
journal = {Computers, Materials and Continua},
volume = {79},
number = {2},
pages = {2209-2228},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.050585},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824002339},
author = {Halil Ibrahim Okur and Kadir Tohma and Ahmet Sertbas},
keywords = {Text classification, relation extraction, NER, distant supervision, deep learning, machine learning},
abstract = {Text classification, by automatically categorizing texts, is one of the foundational elements of natural language processing applications. This study investigates how text classification performance can be improved through the integration of entity-relation information obtained from the Wikidata (Wikipedia database) database and BERT-based pre-trained Named Entity Recognition (NER) models. Focusing on a significant challenge in the field of natural language processing (NLP), the research evaluates the potential of using entity and relational information to extract deeper meaning from texts. The adopted methodology encompasses a comprehensive approach that includes text preprocessing, entity detection, and the integration of relational information. Experiments conducted on text datasets in both Turkish and English assess the performance of various classification algorithms, such as Support Vector Machine, Logistic Regression, Deep Neural Network, and Convolutional Neural Network. The results indicate that the integration of entity-relation information can significantly enhance algorithm performance in text classification tasks and offer new perspectives for information extraction and semantic analysis in NLP applications. Contributions of this work include the utilization of distant supervised entity-relation information in Turkish text classification, the development of a Turkish relational text classification approach, and the creation of a relational database. By demonstrating potential performance improvements through the integration of distant supervised entity-relation information into Turkish text classification, this research aims to support the effectiveness of text-based artificial intelligence (AI) tools. Additionally, it makes significant contributions to the development of multilingual text classification systems by adding deeper meaning to text content, thereby providing a valuable addition to current NLP studies and setting an important reference point for future research.}
}
@article{LAKSHIKA2025102451,
title = {ECS-KG: An event-centric semantic knowledge graph for event-related news articles},
journal = {Data & Knowledge Engineering},
volume = {159},
pages = {102451},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102451},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000461},
author = {MVPT Lakshika and HA Caldera and TNK {De Zoysa}},
keywords = {Semantic KG, Event-centric KG, Contextual embedding, Graph attention networks, Temporal graph neural networks},
abstract = {Recent advances in deep learning techniques and contextual understanding render Knowledge Graphs (KGs) valuable tools for enhancing accessibility and news comprehension. Conventional and news-specific KGs frequently lack the specificity for efficient news-related tasks, leading to limited relevance and static data representation. To fill the gap, this study proposes an Event-Centric Semantic Knowledge Graph (ECS-KG) model that combines deep learning approaches with contextual embeddings to improve the procedural and dynamic knowledge representation observed in news articles. The ECS-KG incorporates several information extraction techniques, a temporal Graph Neural Network (GNN), and a Graph Attention Network (GAT), yielding significant improvements in news representation. Several gold-standard datasets, comprising CNN/Daily Mail, TB-Dense, and ACE 2005, revealed that the proposed model outperformed the most advanced models. By integrating temporal reasoning and semantic insights, ECS-KG not only enhances user understanding of news significance but also meets the evolving demands of news consumers. This model advances the field of event-centric semantic KGs and provides valuable resources for applications in news information processing.}
}
@article{TAN2020592,
title = {Clan/geographical association heritage as a place-based approach for nurturing the sense of place for locals at a World Heritage Site},
journal = {Journal of Hospitality and Tourism Management},
volume = {45},
pages = {592-603},
year = {2020},
issn = {1447-6770},
doi = {https://doi.org/10.1016/j.jhtm.2020.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S1447677020302333},
author = {Siow-Kian Tan and Siow-Hooi Tan},
keywords = {Clan associations, Geographical associations, Heritage, Immigrants, Place-based education, World heritage site (WHS)},
abstract = {Sense of place is important in sustaining the socio-cultural heritage of a place. It can be nurtured through a systematic place-based approach. However, the lack of a critical perspective and often under-theorized place-based education has been criticized as this results in limitations to achieve its original intention. This research aims to comprehend the concepts of place-based learning by identifying the socio-cultural elements, particularly from a clan/geographical heritage perspective, and how it might cultivate a sense of place for locals. Interviews and observations were conducted in Melaka and George Town, the World Heritage Sites of Malaysia. A total of twenty documents, including primary and secondary data, were analysed. The findings reveal five themes to be developed as a place-based approach for nurturing the sense of place of locals. Theoretically, it extends the literature of place-based learning, and the ontological appreciations of place. Practically, related stakeholders might use this as a reference to preserve their clan/geographical association heritage.}
}
@article{CHIU2025111286,
title = {Qualitative and quantitative analysis of the dietary approaches to stop hypertension diet for personalized hypertension management},
journal = {Computers & Industrial Engineering},
volume = {207},
pages = {111286},
year = {2025},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2025.111286},
url = {https://www.sciencedirect.com/science/article/pii/S0360835225004322},
author = {Chun-Wei Chiu and Thi-Phuong-Thanh Le and Chien-Hsiung Huang and Ching-Ter Chang and Chao-Chun Cheng},
keywords = {Dietary approaches to stop hypertension, Hypertension, Fuzzy multi-choice goal programming, Non-linear multi-segment goal programming, Personal healthcare},
abstract = {Hypertension is a major global health concern, yet personalized planning for diet and lifestyle remains a challenge in clinical practice. To address this issue, this study proposes a novel Personal Health Support Model (PHSM) based on the principle of the dietary approaches to stop hypertension (DASH) diet, aiming to improve the precision and adaptability of personalized recommendations. It integrates the analytic hierarchy process (AHP), fuzzy multi-choice goal programming (FMCGP), and nonlinear multi-segment goal programming (NLMSGP) to construct a comprehensive multi-criteria decision-support framework that considers individual characteristics, dietary preferences, and clinical guidelines. The main contributions of this study are as follows: 1. PHSM calculates daily calorie and nutrient requirements based on gender, age, and activity level to ensure dietary plans align with individual needs. 2. PHSM provides a personalized exercise plans tailored to individual health conditions and lifestyle preferences to support blood pressure management and overall well-being. 3. Utilizing FMCGP techniques to accommodate diverse dietary and lifestyle preferences, offering precise personalized blood pressure management solutions. 4. Introducing the NLMSGP approach addresses non-scalar coefficient issues, optimizing dietary plans by considering the timing and quantify of food intake. The study results indicate that PHSM significantly outperforms conventional dietitian-led approaches in term of user satisfaction, engagement, and perceived effectiveness. Its structured design and real-time optimization features are also associated with stronger behavioral intention and adherence. In conclusion, this PHSM shows strong potential for clinical applications and can be integrated with AI-driven health platforms, contributing to both the practical advancement of intelligent dietary planning and the theoretical development of multi-objective health decision-making.}
}
@article{CHEN20222354,
title = {Data Model Classification for Interoperability in the Industry},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {2354-2359},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.060},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322020699},
author = {Y. CHEN and D. ANNEBICQUE and V. CARRE-MENETRIER and A. PHILIPPOT and T. DANEAU},
keywords = {Industry 4.0, data model classification, manufacturing, interoperability},
abstract = {In the context of industry 4.0, interoperability is a major challenge for the manufacturing world. With new use cases heavily depending on industrial data analysis, data structuration through model has become essential for system and process description. However, diversities in data model types due to silo working between domains, represent a challenge for interoperability. Models don't have a clear definition of different domains and there is a need for alignment. This study proposes a classification of different types of data models based on capability criteria to help model definition alignment.}
}
@article{AKKI2024112758,
title = {Advances in Parkinson’s disease research – A computational network pharmacological approach},
journal = {International Immunopharmacology},
volume = {139},
pages = {112758},
year = {2024},
issn = {1567-5769},
doi = {https://doi.org/10.1016/j.intimp.2024.112758},
url = {https://www.sciencedirect.com/science/article/pii/S1567576924012797},
author = {Ali Jawad Akki and Shruti A. Patil and Sphoorty Hungund and R. Sahana and Malini M. Patil and Raghavendra V. Kulkarni and K. {Raghava Reddy} and Farhan Zameer and Anjanapura V. Raghu},
keywords = {Neurodegenerative, Bioinformatics, Phytobiologicals, Systems biology, Epigenetics, Biomarkers},
abstract = {Parkinson’s disease (PD), the second most prevalent neurodegenerative disorder, is projected to see a significant rise in incidence over the next three decades. The precise treatment of PD remains a formidable challenge, prompting ongoing research into early diagnostic methodologies. Network pharmacology, a burgeoning field grounded in systems biology, examines the intricate networks of biological systems to identify critical signal nodes, facilitating the development of multi-target therapeutic molecules. This approach systematically maps the components of Parkinson’s disease, thereby reducing its complexity. In this review, we explore the application of network pharmacology workflows in PD, discuss the techniques employed in this field, and evaluate the current advancements and status of network pharmacology in the context of Parkinson’s disease. The comprehensive insights will pave newer paths to explore early disease biomarkers and to develop diagnosis with a holistic in silico, in vitro, in vivo and clinical studies.}
}
@article{QIU2025108524,
title = {Bio-K-Transformer: A pre-trained transformer-based sequence-to-sequence model for adverse drug reactions prediction},
journal = {Computer Methods and Programs in Biomedicine},
volume = {260},
pages = {108524},
year = {2025},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108524},
url = {https://www.sciencedirect.com/science/article/pii/S0169260724005170},
author = {Xihe Qiu and Siyue Shao and Haoyu Wang and Xiaoyu Tan},
keywords = {Adverse drug reactions, Transformer, Diagonal-masked, Sequence-to-sequence model},
abstract = {Background and Objective:
Adverse drug reactions (ADRs) pose a serious threat to patient health, potentially resulting in severe consequences, including mortality. Accurate prediction of ADRs before drug market release is crucial for early prevention. Traditional ADR detection, relying on clinical trials and voluntary reporting, has inherent limitations. Clinical trials face challenges in capturing rare and long-term reactions due to scale and time constraints, while voluntary reporting tends to neglect mild and common reactions. Consequently, drugs on the market may carry unknown risks, leading to an increasing demand for more accurate predictions of ADRs before their commercial release. This study aims to develop a more accurate prediction model for ADRs prior to drug market release.
Methods:
We frame the ADR prediction task as a sequence-to-sequence problem and propose the Bio-K-Transformer, which integrates the transformer model with pre-trained models (i.e., Bio_ClinicalBERT and K-bert), to forecast potential ADRs. We enhance the attention mechanism of the Transformer encoder structure and adjust embedding layers to model diverse relationships between drug adverse reactions. Additionally, we employ a masking technique to handle target data. Experimental findings demonstrate a notable improvement in predicting potential adverse reactions, achieving a predictive accuracy of 90.08%. It significantly exceeds current state-of-the-art baseline models and even the fine-tuned Llama-3.1-8B and Llama3-Aloe-8B-Alpha model, while being cost-effective. The results highlight the model’s efficacy in identifying potential adverse reactions with high precision, sensitivity, and specificity.
Conclusion:
The Bio-K-Transformer significantly enhances the prediction of ADRs, offering a cost-effective method with strong potential for improving pre-market safety evaluations of pharmaceuticals.}
}
@article{PILKINGTON2022102050,
title = {The London Whale Scandal under new Scrutiny: A Critical Appraisal of the Concept of Retro-causality11Most philosophical issues raised in this article had been sketched out by the quantum macroeconomist Professor Bernard Schmitt (1919-2014), who combined insights borrowed from economics and philosophy. We wish to pay him a tribute with this piece of scholarship.},
journal = {International Review of Financial Analysis},
volume = {80},
pages = {102050},
year = {2022},
issn = {1057-5219},
doi = {https://doi.org/10.1016/j.irfa.2022.102050},
url = {https://www.sciencedirect.com/science/article/pii/S1057521922000278},
author = {Marc Pilkington},
keywords = {Actions, Intentions, Causality, Retro-causality, J.P Morgan chase, London whale scandal},
abstract = {This paper aims to extend the purview of the retro-causality concept, by exploring a complex and under-researched financial accounting scandal, namely the London Whale, a group of traders that operated on account of the London subsidiary of JPMorgan Chase & Co in 2012. We specify the conceptual articulation underlying the discourse on the scandal, between the intentions, the actions and the temporality in which the events have taken place. We then elaborate upon philosophers Elizabeth Anscombe and Donald Davidson. We show the limitations of rationalization as a causal deterministic explanation of actions in light of a methodical review of the JPMorgan Chase Task Force Report in 2013. We put forward the idea that the scandal features flows of awareness, reminiscent of Bergson's duration. The articulation between intentions, actions and causes is performed, by mobilizing quantum macroeconomic theory developed by the economist Bernard Schmitt. We show the relevance of retro-causality for central debates in contemporary accounting (e.g. fair value accounting and the efficient market hypothesis). Our empirical study abundantly draws on Ludwig Wittgenstein's language games, and retro-causality insights borrowed from quantum physics. Our results confirm our theoretical intuitions: there is a common thread and guiding reflection throughout the accounting scandal, namely the idea that the future might influence the past, under certain assumptions. Finally, some groundbreaking consequences are derived for the accounting profession regarding the global crisis in 2008.}
}
@article{AMINEDAOUD2025101314,
title = {A comprehensive meta-analysis of efficiency and effectiveness in the detection community},
journal = {Journal of Computer Languages},
volume = {82},
pages = {101314},
year = {2025},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2024.101314},
url = {https://www.sciencedirect.com/science/article/pii/S2590118424000571},
author = {Mohamed {Amine Daoud} and Sid {Ahmed Mokhtar Mostefaoui} and Abdelkader Ouared and Hadj {Madani Meghazi} and Bendaoud Mebarek and Abdelkader Bouguessa and Hasan Ahmed},
keywords = {Intrusion detection system, Meta-model, Repository, Reuse, Configurations},
abstract = {Creating an intrusion detection system (IDS) is a prominent area of research that continuously draws attention from both scholars and practitioners who tirelessly innovate new solutions. The complexity of IDS naturally escalates alongside technological advancements, whether they are manually implemented within security infrastructures or elaborated upon in academic literature. However, accessing and comparing these IDS solutions requires sifting through a multitude of hypotheses presented in research papers, which is a laborious and error-prone endeavor. Consequently, many researchers encounter difficulties in replicating results or reanalyzing published IDSs. This challenge primarily arises due to the absence of a standardized process for elucidating IDS methodologies. In response, this paper advocates for a framework aimed at enhancing the reproducibility of IDS outcomes, thereby enabling their seamless reuse across diverse cybersecurity contexts, benefiting both end-users and experts alike. The proposed framework introduces a descriptive language for the precise specification of IDS descriptions. Additionally, a model repository facilitates the sharing and reusability of IDS configurations. Lastly, through a case study, we showcase the effectiveness of our framework in addressing challenges associated with data acquisition and knowledge organization and sharing. Our results demonstrate satisfactory prediction accuracy for configuration reuse and precise identification of reusable components.}
}
@article{LAGUNA2025104347,
title = {A platform to support the fast development of digital twins for agricultural holdings},
journal = {Computers in Industry},
volume = {172},
pages = {104347},
year = {2025},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2025.104347},
url = {https://www.sciencedirect.com/science/article/pii/S0166361525001125},
author = {Jorge Laguna and Mario E. Suaza-Medina and Rubén Béjar and Javier Lacasta and F. Javier Zarazaga-Soria},
keywords = {Digital twin, Open-field agriculture, Platform, Machine learning, Smart farming},
abstract = {Industry 4.0 has advanced in agriculture through Smart Agriculture initiatives, yet open-field farming lags in the adoption of digital twins. Although digital twins have transformed manufacturing since 2011, their application in open-field farming remains limited by environmental variability, data scarcity, and financial constraints. This paper addresses four gaps: the lack of affordable platforms for small farms that dominate European agriculture; the need to manage agricultural complexity through data-driven models rather than the physical modelling approaches prevalent in non-agricultural sectors; the absence of open sources solutions adapted to agriculture’s slower innovation pace; the breach between technology and farmers. The platform features innovations in data workflow integration, open data incorporation, a cost-effective shared warehouse, and scalable data pipelines. To validate the proposed platform, a case study with two example digital twins mirroring two fields is conducted. This implementation ran efficiently on modest hardware (2 vCPUs, 4GB RAM). It achieved an average CPU usage of 60%, RAM usage of 2.5 GB, and a deployment time of around one minute. This helps lowering adoption barriers for small holdings and bridging the gap between basic monitoring and complex future systems.}
}
@article{IQBAL2025104157,
title = {Continual and wisdom learning for federated learning: A comprehensive framework for robustness and debiasing},
journal = {Information Processing & Management},
volume = {62},
number = {5},
pages = {104157},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104157},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325000986},
author = {Saeed Iqbal and Xiaopin Zhong and Muhammad Attique Khan and Zongze Wu and Dina Abdulaziz AlHammadi and Weixiang Liu and Imran Arshad Choudhry},
keywords = {Federated Learning (FL), Continual Learning (CL), Wisdom Learning (WL), Label noise mitigation, Model fairness, Client heterogeneity, Debiasing techniques},
abstract = {Federated Learning (FL) has transformed decentralized machine learning, however it remains has concerns with noisy labeled data, diverse clients, and sparse datasets, especially in delicate fields like healthcare. To address these issues, this study introduces a robust FL framework that integrates advanced Continual Learning (CL) and Wisdom Learning (WL) techniques. Elastic Weight Consolidation (EWC) prevents catastrophic forgetting by penalizing deviations from critical weights, while Progressive Neural Networks (PNN) leverage modular architectures with lateral connections to enable knowledge transfer across tasks and isolate client-specific biases. WL incorporates consensus-based aggregation, dynamic model distillation, and adaptive ensemble learning to enhance model robustness against noisy updates and biased data distributions. The framework is rigorously validated on benchmark medical imaging datasets, including ADNI, BraTS, PathMNIST, BreastMNIST, and ChestMNIST, demonstrating significant improvements in fairness metrics, with up to a 94.3% reduction in bias (Demographic Parity) and a 92.7% improvement in accuracy fairness (Accuracy Parity). These results establish the effectiveness of the proposed approach in achieving stable, equitable, and high-performing global models under challenging FL conditions characterized by dynamic client settings, label noise, and class imbalance.}
}
@article{BISCHOF201822,
title = {Enriching integrated statistical open city data by combining equational knowledge and missing value imputation},
journal = {Journal of Web Semantics},
volume = {48},
pages = {22-47},
year = {2018},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2017.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1570826817300355},
author = {Stefan Bischof and Andreas Harth and Benedikt Kämpgen and Axel Polleres and Patrik Schneider},
keywords = {Open data, Linked Data, Data cleaning, Data integration},
abstract = {Several institutions collect statistical data about cities, regions, and countries for various purposes. Yet, while access to high quality and recent such data is both crucial for decision makers and a means for achieving transparency to the public, all too often such collections of data remain isolated and not re-useable, let alone comparable or properly integrated. In this paper we present the Open City Data Pipeline, a focused attempt to collect, integrate, and enrich statistical data collected at city level worldwide, and re-publish the resulting dataset in a re-useable manner as Linked Data. The main features of the Open City Data Pipeline are: (i) we integrate and cleanse data from several sources in a modular and extensible, always up-to-date fashion; (ii) we use both Machine Learning techniques and reasoning over equational background knowledge to enrich the data by imputing missing values, (iii) we assess the estimated accuracy of such imputations per indicator. Additionally, (iv) we make the integrated and enriched data, including links to external data sources, such as DBpedia, available both in a web browser interface and as machine-readable Linked Data, using standard vocabularies such as QB and PROV. Apart from providing a contribution to the growing collection of data available as Linked Data, our enrichment process for missing values also contributes a novel methodology for combining rule-based inference about equational knowledge with inferences obtained from statistical Machine Learning approaches. While most existing works about inference in Linked Data have focused on ontological reasoning in RDFS and OWL, we believe that these complementary methods and particularly their combination could be fruitfully applied also in many other domains for integrating Statistical Linked Data, independent from our concrete use case of integrating city data.}
}
@article{CHAURASIA2023585,
title = {T5LSTM-RNN based Text Summarization Model for Behavioral Biology Literature},
journal = {Procedia Computer Science},
volume = {218},
pages = {585-593},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.040},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923000406},
author = {Shivangi Chaurasia and Debalay Dasgupta and Rajeshkhannan Regunathan},
keywords = {Seq2Seq, Text Summarization, Abstractive Summary, T5, Transformer, Behavior Biology, LSTM, RNN},
abstract = {Behavioral biology is one of the crucial and trending topics these days which needs proper attention by scholars for rapid development in this field. Therefore, the purpose of this paper is to ease the process of collecting all kinds of relevant and vital data available on the internet regarding this topic in different forms of media, such as news articles, research papers, and YouTube lecture videos, all into one place into a single document in a proper summarized form. For proper training of the LSTM model, the lengthy video and journal datasets are pre-processed using the T5 transformer model to generate a uniform training dataset. So, in this work, a comprehensive approach is proposed based on an abstractive form of text summarization using the seq2seq encoder-decoder model combined with a stacked LSTM layer with an attention mechanism and a T5 transformer model pre-processor. Therefore, a proper hybrid model, T5LSTM-RNN, is implemented to generate the summarized data.}
}
@article{EDGELL2024100075,
title = {A monstrous matter: The three faces of artificial creativity},
journal = {Journal of Creativity},
volume = {34},
number = {1},
pages = {100075},
year = {2024},
issn = {2713-3745},
doi = {https://doi.org/10.1016/j.yjoc.2024.100075},
url = {https://www.sciencedirect.com/science/article/pii/S2713374524000013},
author = {Robert A. Edgell},
keywords = {Artificial intelligence, Creativity, Matters of concern, Trust, Creative value, Creative personal identity},
abstract = {Through a focus on artificial creativity (AC), creativity and innovation researchers, practitioners, and educators are beginning to demystify the phenomenon's liminality by exploring and contesting the potential affordances, constraints, and pitfalls brought about by the deployment of powerful AI models for creative endeavors. For the creativity community, AC as a sociotechnical network has become a deeply consternating and contested monster. Given the recency of AC, there has been little theorizing yet. My critical self-reflection paper seeks to understand the community's concerns and, thereby, to discern theoretical insights that conceptually contribute towards a theory of AC. Drawing on autoethnography, I identified three distinct perceived matters of concern represented by anthropomorphic personalities or faces of AC: Trickster, Surveyor, and Harbinger. The findings reveal that the Trickster is the most monstrous and disconcerting face of AC. It may be prankish or deceptive, but can also be beneficent and supportive. While the Surveyor provides surveillance, measurement, and calculation, the Harbinger announces competing future visions, one of utopian hope and the other of dystopian despair. I conclude by discussing the implications of three underlying theoretical variables: trust, creative value, and creative personal identity.}
}
@article{ASSI2019104925,
title = {Context-aware instance matching through graph embedding in lexical semantic space},
journal = {Knowledge-Based Systems},
volume = {186},
pages = {104925},
year = {2019},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.104925},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119303739},
author = {Ali Assi and Hamid Mcheick and Ahmad Karawash and Wajdi Dhifli},
keywords = {Data linking, Instance matching, Lexical semantic vector, RDF graph, Semantic web},
abstract = {In recent years, the growing availability of open-accessed data (e.g., Wikipedia) combined with the advances in algorithmic techniques for information extraction have facilitated the design and structuring of information giving rise to knowledge bases. A major challenge relies in the integration of these independently designed knowledge bases. Instance matching is presented as one of the solutions to facilitate this process. It aims to link co-referent instances with an owl:sameAs connection to allow knowledge bases to complement each other. In this work, we present an approach for automatic alignment of instances in knowledge bases in the form of Resource Description Framework (RDF) graphs. Our approach generates for each instance a virtual document from its local description (i.e., data-type properties) and instances related to it through object-type properties (i.e., neighbors). We transform the instance matching problem into a document matching problem and solve it by a vector space embedding technique. We consider the pre-trained word embeddings to assess words similarities at both the lexical and semantic levels. We evaluate our approach on multiple knowledge bases from the instance track of the Ontology Alignment Evaluation Initiative (OAEI). The experiments show that our approach gets prominent results compared to several state-of-the-art existing approaches.}
}
@article{GAO2024113889,
title = {Integrated building fault detection and diagnosis using data modeling and Bayesian networks},
journal = {Energy and Buildings},
volume = {306},
pages = {113889},
year = {2024},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.113889},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824000057},
author = {Tianyun Gao and Sylvain Marié and Patrick Béguery and Simon Thebault and Stéphane Lecoeuche},
keywords = {Bayesian network, Fault detection and diagnostics, Building, Descriptive metadata, Topology, Haystack, Data-driven modeling, BMS, HVAC},
abstract = {Heating, ventilation, and air-conditioning (HVAC) equipment faults and operational errors result in comfort issues and waste of energy in buildings. An Automatic Fault Detection and Diagnosis (AFDD) tool could help facility managers fix comfort and energy issues more efficiently, by identifying the most probable root causes. Existing AFDD methods mostly focus on equipment-level fault detection and diagnostics; almost no attention is given to building level fault diagnosis, considering inter-dependency between equipment through the energy distribution chain. In this work we propose a methodology to automatically derive a Bayesian network from HVAC system topology description such as Haystack. This Bayesian network models and estimates the state of all elements in the system, helping users to identify the most probable root fault. As it is able to ingest evidence from any source (field data, operators, or other models) and is capable of updating its estimates when new evidence is delivered, such a tool could have a great potential to be used interactively on the field. We applied the proposed methodology on simulated and real-world buildings and present in this paper one specific case.}
}
@article{JOSEFSSON2020101088,
title = {Turning to the dark side: Challenging the hegemonic positivity of the creativity discourse},
journal = {Scandinavian Journal of Management},
volume = {36},
number = {1},
pages = {101088},
year = {2020},
issn = {0956-5221},
doi = {https://doi.org/10.1016/j.scaman.2019.101088},
url = {https://www.sciencedirect.com/science/article/pii/S0956522118300940},
author = {Iva Josefsson and Annika Blomberg},
keywords = {Dark side, Creativity, Discourse theory, Critical management studies, Laclau, Hegemony, Organization studies},
abstract = {Using a Laclauian discourse approach this paper challenges the hegemony of articulations framing creativity as good, necessary, and as a source of valuable outcomes for organizations and individuals. Instead, this paper argues that creativity has a ‘dark side’, referring to that which is harmful and may result in pain, loss or suffering. We analyze and expose the hegemonic positivity of the creativity discourse within organization studies and discuss the implications of this hegemony. We conclude that the dark side of creativity has been subverted in the discourse and requires further scholarly exploration. To promote a greyer research agenda of creativity in organizations, we offer three theorizations of the dark side of creativity as antagonisms to the hegemony – the individual, collective, and critical. By challenging the hegemonic positivity of creativity and by providing a number of research imaginaries, this paper invites scholars to broaden the discourse and to embrace a more greyer understanding of creativity in organization studies.}
}
@article{KENZIE2024102412,
title = {Protocol for an interview-based method for mapping mental models using causal-loop diagramming and realist interviewing},
journal = {Evaluation and Program Planning},
volume = {103},
pages = {102412},
year = {2024},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2024.102412},
url = {https://www.sciencedirect.com/science/article/pii/S0149718924000132},
author = {Erin S. Kenzie and Wayne Wakeland and Antonie Jetter and Kristen Hassmiller Lich and Mellodie Seater and Rose Gunn and Melinda M. Davis},
keywords = {Interview, Causal-loop diagram, Systems science, Mental model, Realist evaluation, System dynamics},
abstract = {Causal-loop diagramming, a method from system dynamics, is increasingly used in evaluation to describe individuals’ understanding of how policies or programs do or could work ("mental models"). The use of qualitative interviews to inform model development is common, but guidance for how to design and conduct these interviews to elicit causal information in participant mental models is scant. A key strength of semi-structured qualitative interviews is that they let participants speak freely; they are not, however, designed to elicit causal information. Moreover, much of human communication about mental models—particularly larger causal structures such as feedback loops—is implicit. In qualitative research, part of the skill and art of effective interviewing and analysis involves listening for information that is expressed implicitly. Similarly, a skilled facilitator can recognize and inquire about implied causal structures, as is commonly done in group model building. To standardize and make accessible these approaches, we have formalized a protocol for designing and conducting semi-structured interviews tailored to eliciting mental models using causal-loop diagramming. We build on qualitative research methods, system dynamics, and realist interviewing. This novel, integrative method is designed to increase transparency and rigor in the use of interviews for system dynamics and has a variety of potential applications.}
}
@article{LISSANDRINI2025100860,
title = {The ESW of Wikidata: Exploratory search workflows on Knowledge Graphs},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100860},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100860},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000465},
author = {Matteo Lissandrini and Gianmarco Prando and Gianmaria Silvello},
keywords = {Wikidata, Exploratory search, SPARQL},
abstract = {Exploratory search on Knowledge Graphs (KGs) arises when a user needs to understand and extract insights from an unfamiliar KG. In these exploratory sessions, the users issue a series of queries to identify relevant portions of the KG that can answer their questions, with each query answer informing the formulation of the next query. Despite the widespread adoption of KGs, the needs of current KG exploration use cases are not well understood. This work presents the “Exploratory Search Workflows” (ESW) collection focusing on real-world exploration sessions of an open-domain KG, Wikidata, conducted by 57 M.Sc. Computer Engineering students in two advanced Graph Database course editions. This resource includes 234 real exploratory workflows, each containing an average of 45 SPARQL queries and reference workflows that serve as gold-standard solutions to the proposed tasks. The ESW collection is also available as an RDF graph and accessible via a public SPARQL endpoint. It allows for analysis of real user sessions, understanding query evolution and complexity, and serves as the first query benchmark for KG management systems for exploratory search.}
}
@article{BAI2024e33304,
title = {Systematic pan-cancer analysis identified RASSF1 as an immunological and prognostic biomarker and validated in lung cancer},
journal = {Heliyon},
volume = {10},
number = {12},
pages = {e33304},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e33304},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024093356},
author = {Yibing Bai and Yuanyong Wang and Jiapei Qin and Ting Wang and Xin Zhou and Zhiqiang Ma and An Wang and Wenyu Yang and Jinliang Wang and Jinfeng Li and Yi Hu},
keywords = {RASSF1, Pan-cancer, Prognosis, Immunity, Lung cancer},
abstract = {Background
Ras association domain family member 1 (RASSF1) encodes the RASSF1A protein, serving as a scaffold protein situated at the intersection of a complex signalling network.
Aims
To evaluate the immunological and prognostic significance of RASSF1 expression in various types of human cancers, with a specific focus on lung cancer.
Methods
Differential expression analysis of RASSF1 was conducted based on data from The Cancer Genome Atlas, Genotype-Tissue Expression, and Cancer Cell Line Encyclopaedia databases. Prognostic analysis was performed using the Cox regression test and Kaplan-Meier test. Spearman's test was utilized for correlation analysis. Gene Ontology (GO) and Kyoto Encyclopaedia of Genes and Genomes (KEGG) gene sets were employed to enrich the associated signaling pathways. Immunohistochemical staining and quantitative real-time PCR were employed to detect protein and mRNA expression levels, respectively.
Results
RASSF1 expression was significantly lower in tumour tissues than in normal tissues in most cancers, and Cox regression analysis demonstrated a significant correlation between RASSF1 expression and the prognosis of over 12 types of cancer. Specifically, high RASSF1 expression was associated with poor OS in nine cancer types, including GBMLGG (HR = 4.98, P = 1.2e-31), LGG (HR = 3.72, P = 2.5e-10), and LAML (HR = 1.48, P = 2.4e-3). Further analysis showed that RASSF1 expression was significantly correlated with immune checkpoint- and immune-related genes. Moreover, RASSF1 expression is involved in tumour microenvironment (TME), RNA modification, genomic heterogeneity, and tumour stemness. GO and KEGG analyses showed that RASSF1 was closely related to tumour immune-related pathways. Finally, RASSF1A was moderately correlated with PD-L1 (R = 0.556), and RASSF1A overexpression significantly affected the expression of several genes involved in the Th17 cell differentiation signalling pathway in lung cancer.
Conclusions
RASSF1 was differentially expressed in 29 human cancers and played a critical role in tumour immunity. Thus, RASSF1 has the potential to be used as a prognostic marker and reference for achieving more precise immunotherapy, particularly in lung cancer.}
}
@article{HADDAD202523,
title = {The value-ladenness of ancestry},
journal = {Studies in History and Philosophy of Science},
volume = {112},
pages = {23-32},
year = {2025},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2025.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0039368125000664},
author = {Yasmin Haddad},
keywords = {Ancestry, Clustering, Values, Value-ladenness, Classification, Descriptors},
abstract = {Clustering humans based on their genetic ancestry is a common practice in human genomics. Genetically similar populations can be seen as statistical constructs that are labeled by population descriptors such as “race,” “ethnicity,” and “genetic ancestry.” Recently, there has been a shift towards replacing the descriptor “race” with “genetic ancestry” because the latter is considered more objective. A descriptor is deemed objective if it adequately captures an underlying feature of the biological world, such as genetic similarities or differences between human sub-populations. However, claims of objectivity do not sufficiently explain the rationale for the choice and use of population descriptors such as “ancestry.” This paper proposes an axiological approach to capture the choice and use of population descriptors in human genomics, by showing that the population descriptor “ancestry” is value-laden and that there is a legitimate role for values in the choice and use of population descriptors in genomics.}
}
@article{TAN2024103878,
title = {Does multilingual packaging influence purchasing in retail segment? Evidence from multiple experiments},
journal = {Journal of Retailing and Consumer Services},
volume = {79},
pages = {103878},
year = {2024},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2024.103878},
url = {https://www.sciencedirect.com/science/article/pii/S0969698924001747},
author = {Fuqiang Tan and Xi Li and Reeti Agarwal and Yatish Joshi and Muhammad Zafar Yaqub},
keywords = {Language familiarity, Multilingual packaging, Green products, Social crowding},
abstract = {The language on green product packaging shapes consumers' shopping experiences and purchase intentions. This study aims to explore the effects of multilingual packaging on consumers' intention to purchase green products through a series of four experiments with data collected using the survey method. Experimental findings reveal that consumers' familiarity with the language used on green product packaging positively influences their likelihood of purchasing such products. This relationship is strengthened in the presence of higher social crowding. Furthermore, the study uncovers that including bilingual or trilingual packaging enhances consumers' propensity to purchase green products. This research demonstrates the positive impact of multilingual packaging on sales of green products, attracting a more extensive consumer base and increasing consumers' purchase intention for environmentally friendly products. Additionally, the study identifies the role of different levels of social crowding in the relationship between consumer language familiarity and their intention to purchase green products. Besides significantly contributing to the scholarly discourse, it provides practical implications for companies and brands in designing future effective green product packaging language. These findings contribute to promoting green products, offering valuable suggestions to assist companies in better designing and packaging their products to attract a broader consumer base and, thus, increase their market share.}
}
@article{CHOWDHURY2020100439,
title = {Systems biology and bioinformatics approach to identify gene signatures, pathways and therapeutic targets of Alzheimer's disease},
journal = {Informatics in Medicine Unlocked},
volume = {21},
pages = {100439},
year = {2020},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2020.100439},
url = {https://www.sciencedirect.com/science/article/pii/S235291482030589X},
author = {Utpala Nanda Chowdhury and M. Babul Islam and Shamim Ahmad and Mohammad Ali Moni},
keywords = {Alzheimer's disease (AD), cis-eQTL, GWAS, Brain, Blood},
abstract = {Alzheimer's disease (AD) develops relentlessly in affected individuals and its occurrence is increasing. A clinical test to diagnose early-stage AD could be an important means of enabling interventions to slow its progression. However, available neuroimaging and cerebrospinal fluid-based diagnoses are very costly. Therefore, detecting AD from blood transcripts that mirror the expression of brain transcripts in the AD could improve the diagnosis. To achieve this goal, we employed a transcriptional analysis of affected tissues and integrated them with cis-eQTL data. In this study, we analyzed microarray gene expression data of brain and blood cells from AD patients and control individuals. Differentially expressed genes (DEGs) common to both brain tissue and blood cells were identified. Potential common genes and molecular pathways were identified using overlapping DEGs through the pathway and gene ontology enrichment analysis. We identified 18 significantly dysregulated genes shared by both brain and blood cells in AD affected individuals. We validated these candidates as disease-associated genes using gold-standard benchmarking databases (gene SNP-disease linkage). Significant molecular pathway and gene ontology indicating AD progression were identified. This study also identified regulatory factors, including transcription factors (TFs), microRNAs and candidate drugs. In sum, we identified new putative links between pathological processes in brain tissue and blood cells in AD that may allow assessment of AD status using blood samples. Thus, our formulated methodologies demonstrate the power of gene and gene expression analysis for brain-related pathologies transcriptomics, cis-eQTL, and epigenetics data from brain and blood cells.}
}
@article{CALISKAN20234895,
title = {Metadata integrity in bioinformatics: Bridging the gap between data and knowledge},
journal = {Computational and Structural Biotechnology Journal},
volume = {21},
pages = {4895-4913},
year = {2023},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2023.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S2001037023003616},
author = {Aylin Caliskan and Seema Dangwal and Thomas Dandekar},
keywords = {Meta-data, Error, Annotation, Error-transfer, Wrong labelling, Patient data, Control group, Tools overview},
abstract = {In the fast-evolving landscape of biomedical research, the emergence of big data has presented researchers with extraordinary opportunities to explore biological complexities. In biomedical research, big data imply also a big responsibility. This is not only due to genomics data being sensitive information but also due to genomics data being shared and re-analysed among the scientific community. This saves valuable resources and can even help to find new insights in silico. To fully use these opportunities, detailed and correct metadata are imperative. This includes not only the availability of metadata but also their correctness. Metadata integrity serves as a fundamental determinant of research credibility, supporting the reliability and reproducibility of data-driven findings. Ensuring metadata availability, curation, and accuracy are therefore essential for bioinformatic research. Not only must metadata be readily available, but they must also be meticulously curated and ideally error-free. Motivated by an accidental discovery of a critical metadata error in patient data published in two high-impact journals, we aim to raise awareness for the need of correct, complete, and curated metadata. We describe how the metadata error was found, addressed, and present examples for metadata-related challenges in omics research, along with supporting measures, including tools for checking metadata and software to facilitate various steps from data analysis to published research.}
}
@article{DAUGAARD2025308,
title = {Characterization of human melanoma skin cancer models: A step towards model-based melanoma research},
journal = {Acta Biomaterialia},
volume = {191},
pages = {308-324},
year = {2025},
issn = {1742-7061},
doi = {https://doi.org/10.1016/j.actbio.2024.11.018},
url = {https://www.sciencedirect.com/science/article/pii/S1742706124006731},
author = {Nicoline Dorothea Daugaard and Rikke Tholstrup and Jakob Rask Tornby and Sofie Marchsteiner Bendixen and Frederik Tibert Larsen and Daniela {De Zio} and Mike Bogetofte Barnkob and Kim Ravnskjaer and Jonathan R. Brewer},
keywords = {3D skin models, Melanoma, Melanoma skin model, RNAseq, Bioimaging},
abstract = {Advancing 3D in vitro human tissue models is crucial for biomedical research and drug development to address the ethical and biological limitations of animal testing. Recently, 3D skin models have proven to be effective for studying serious skin conditions, such as melanoma. For these advanced models to be applicable in preclinical studies, thorough characterization is essential to understand their applicability and limitations. In this study, we used bioimaging and RNA sequencing to assess the architecture and transcriptomic profiles of skin models, including models with melanoma. Our results indicated that these models closely mimicked skin morphology and gene expression patterns. The full-thickness (FT) model shows a superior resemblance to the human skin, particularly in basement membrane formation and cellular interactions. The integrity of the skin-like properties and gene expression signatures of both skin and melanoma cells were preserved upon the integration of melanoma cells, establishing these models as robust platforms for cancer research. The responsiveness of the FT melanoma models to vemurafenib treatment was successfully monitored, demonstrating their validity as a reliable, reproducible, and humane tool for pharmacological testing and drug development. Furthermore, the transcriptomic data showed that skin models with cancer spheroids had upregulated genes linked to aggressive and resilient cancer behavior compared to spheroids alone. This emphasizes the importance of the microenvironment in cancer progression and suggests that 3D skin models can serve to uncover mechanisms and therapeutic targets that are not detectable in simpler systems.
Statement of significance
This study introduces advanced, ethically sound skin and melanoma models as alternatives to animal testing in drug discovery. By thoroughly characterizing these models using bioimaging and RNA sequencing, we demonstrate their close resemblance to human skin, particularly in full-thickness models. These models not only replicate the complex cellular interactions and gene expression patterns of human tissue but also maintain robustness after melanoma integration. Our findings highlight the potential of these models in revealing cancer mechanisms and therapeutic targets, offering a significant impact on melanoma research and preclinical testing.}
}
@article{SHU2023e19406,
title = {Emergency treatment mechanism of laboratory safety accidents in university based on IoT and context aware computing},
journal = {Heliyon},
volume = {9},
number = {9},
pages = {e19406},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e19406},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023066148},
author = {Qiang Shu and Yan Li and Wei Gao},
keywords = {IoT, Context aware, Laboratory safety},
abstract = {In recent years, safety accidents in university laboratories have occurred frequently. Not only do the accidents result in property damage, but also in injuries. Real-time environmental monitoring of the laboratory through IoT enables early detection of potential safety risks such as high temperatures, high humidity and gas leaks, and timely action to reduce the likelihood of accidents. To ensure laboratory safety, in the paper, an emergency treatment mechanism for laboratory safety accidents was proposed based on IoT and context perception. The mechanism uses sensors to collect environmental information and fill a feature characterization architecture for unified safety management. Subsequently, the meta-rule algorithm is used to discover services in the prior knowledge model to form a workflow engine, so as to drive the security business management. Additionally, based on the standard measurement model, we normalize the fuzzy uncertainty measurement model with different granularities and define the fuzzy uncertainty of different emergency decision-making knowledge. Based on this, a knowledge fusion method for emergency decision-making under different fuzzy uncertainties is proposed, which improves laboratory safety emergency response performance based on situational awareness. The implementation of the proposed mechanism in a chemical laboratory demonstrates its efficacy in optimizing operational processes and discovering operational flow through multi-dimensional information analysis. This capability significantly aids safety administrators in their daily laboratory safety management.}
}
@article{REIMER2021104588,
title = {Subcategorizing EHR diagnosis codes to improve clinical application of machine learning models},
journal = {International Journal of Medical Informatics},
volume = {156},
pages = {104588},
year = {2021},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2021.104588},
url = {https://www.sciencedirect.com/science/article/pii/S1386505621002148},
author = {Andrew P. Reimer and Wei Dai and Benjamin Smith and Nicholas K. Schiltz and Jiayang Sun and Siran M. Koroukian},
keywords = {Electronic health records, Data management, Electronic data processing, Machine learning},
abstract = {Background
Electronic health record (EHR) data is commonly used for secondary purposes such as research and clinical decision support. However, reuse of EHR data presents several challenges including but not limited to identifying all diagnoses associated with a patient’s clinical encounter. The purpose of this study was to assess the feasibility of developing a schema to identify and subclassify all structured diagnosis codes for a patient encounter.
Methods
To develop a subclassification schema we used EHR data from an interhospital transport data repository that contained complete hospital encounter level data. Eight discrete data sources containing structured diagnosis codes were identified. Diagnosis codes were normalized using the Unified Medical Language System and additional EHR data were combined with standardized terminologies to create and validate the subcategories. We then employed random forest to assess the usefulness of the new subcategorized diagnoses to predict post-interhospital transfer mortality by building 2 models, one using standard diagnosis codes, and one using the new subcategorized diagnosis codes.
Results
Six subcategories of diagnoses were identified and validated. The subcategories included: primary or admitting diagnoses (10%), past medical, surgical or social history (9%), problem list (20%), comorbidity (24%), discharge diagnoses (6%), and unmapped diagnoses (31%). The subcategorized model outperformed the standard model, achieving a training AUROC of 0.97 versus 0.95 and testing model AUROC of 0.81 versus 0.46.
Discussion
Our work demonstrates that merging structured diagnosis codes with additional EHR data and secondary data sources provides additional information to understand the role of diagnosis throughout a clinical encounter and improves predictive model performance. Further work is necessary to assess if subcategorizing produces benefits in interpreting the results of prognostic models and/or operationalizing the results in clinical decision support applications.}
}
@article{KHOLEIF201960,
title = {The paradox of embedded agency from a strong structuration perspective},
journal = {Qualitative Research in Accounting & Management},
volume = {16},
number = {1},
pages = {60-92},
year = {2019},
issn = {1176-6093},
doi = {https://doi.org/10.1108/QRAM-03-2016-0027},
url = {https://www.sciencedirect.com/science/article/pii/S1176609319000238},
author = {Ahmed Othman Rashwan Kholeif and Lisa Jack},
keywords = {Egypt, Resistance, Strong structuration theory, Embedded agency, Performance-based budgeting},
abstract = {Purpose
This paper aims to use Stones’ strong structuration theory (SST) that combines Giddens’ duality and Archer’s analytical dualism to deal with the paradox of embedded agency, focussing on resistance, in the budgeting literature. It also applies this framework to an illustrative case study that examines a failed attempt to implement performance-based budgeting (PBB) in the Egyptian Sales Tax Department (ESTD).
Design/methodology/approach
The authors have used SST as an analytical framework. Longitudinal case study data were collected from interviews, observations, discussions and documentary analysis and from publicly available reports and other media issued by the World Bank.
Findings
The SST framework identifies the circumstances in which middle managers as embedded agency have limited possibilities to change their dispositions to act and identify opportunities for emancipation in the wider social context in which they are embedded. The official explanation for the failure to implement PBB in Egypt was obstruction by middle managers. The findings of this study provide an alternative explanation to that published by the World Bank for the failure to institutionalise PBB in Egypt. It was found that the middle managers were the real supporters of PBB. Other parties and existing laws and regulations contributed to the failure of PBB.
Research limitations/implications
As a practical implication of the study, the analysis presented here offers an alternative interpretation of the failure of the Egyptian project for monitoring and evaluation to that published by the World Bank. This case and similar cases may enhance the understanding of how and when monitoring and evaluation technologies should be introduced at the global level to manage conflicts of interest between agencies and beneficiaries.
Originality/value
This paper contributes to the extant management accounting literature on the use of ST in addressing the paradox of embedded agency in making or resisting structural change. It uses SST to integrate Giddens’ ST with critical realist theory, incorporating duality and dualism in a stronger model of structuration. The SST framework offers a means of analysing case studies that result from interactions and conjunctures between different groups of actors at different ontological levels. The paper also examines the issue of embedded agency in budgeting research using an illustrative case study from a developing country, Egypt.}
}
@article{NEGROCALDUCH2021104507,
title = {Technological progress in electronic health record system optimization: Systematic review of systematic literature reviews},
journal = {International Journal of Medical Informatics},
volume = {152},
pages = {104507},
year = {2021},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2021.104507},
url = {https://www.sciencedirect.com/science/article/pii/S1386505621001337},
author = {Elsa Negro-Calduch and Natasha Azzopardi-Muscat and Ramesh S. Krishnamurthy and David Novillo-Ortiz},
keywords = {Medical informatics, Electronic health records, eHealth, Artificial intelligence, Blockchain, Phenotyping, Deep learning, Natural language processing},
abstract = {Background
The recent, rapid development of digital technologies offers new possibilities for more efficient implementation of electronic health record (EHR) and personal health record (PHR) systems. A growing volume of healthcare data has been the hallmark of this digital transformation. The large healthcare datasets' complexity and their dynamic nature pose various challenges related to processing, analysis, storage, security, privacy, data exchange, and usability.
Materials and Methods
We performed a systematic review of systematic reviews to assess technological progress in EHR and PHR systems. We searched MEDLINE, Cochrane, Web of Science, and Scopus for systematic literature reviews on technological advancements that support EHR and PHR systems published between January 1, 2010, and October 06, 2020.
Results
The searches resulted in a total of 2,448 hits. Of these, we finally selected 23 systematic reviews. Most of the included papers dealt with information extraction tools and natural language processing technology (n = 10), followed by studies that assessed the use of blockchain technology in healthcare (n = 8). Other areas of digital technology research included EHR and PHR systems in austere settings (n = 1), de-identification methods (n = 1), visualization techniques (n = 1), communication tools within EHR and PHR systems (n = 1), and methodologies for defining Clinical Information Models that promoted EHRs and PHRs interoperability (n = 1).
Conclusions
Technological advancements can improve the efficiency in the implementation of EHR and PHR systems in numerous ways. Natural language processing techniques, either rule-based, machine-learning, or deep learning-based, can extract information from clinical narratives and other unstructured data locked in EHRs and PHRs, allowing secondary research (i.e., phenotyping). Moreover, EHRs and PHRs are expected to be the primary beneficiaries of the blockchain technology implementation on Health Information Systems. Governance regulations, lack of trust, poor scalability, security, privacy, low performance, and high cost remain the most critical challenges for implementing these technologies.}
}
@article{YU202578,
title = {Preparing for an agentic era of human-machine transportation systems: Opportunities, challenges, and policy recommendations},
journal = {Transport Policy},
volume = {171},
pages = {78-97},
year = {2025},
issn = {0967-070X},
doi = {https://doi.org/10.1016/j.tranpol.2025.05.030},
url = {https://www.sciencedirect.com/science/article/pii/S0967070X2500215X},
author = {Jiangbo Yu},
keywords = {AI agents, Human-AI, Machine learning, Autonomous vehicle, Smart cities, Automated construction, Participatory decision-making, Large language model},
abstract = {Human-Machine Transportation Systems (HMTS) refer to transportation systems where humans and machines interact to enable mobility. The history of humans creating and utilizing machines for transportation purposes dates back from the invention of the wheel to more recent innovations such as bicycles, automobiles, traffic signals, handheld navigation devices, asphalt pavers, and computer-aided design and management tools. In recent years, technological advancements have transformed machines from passive tools into more active participants, with humans increasingly delegating complex tasks and responsibilities to them. While these advancements have revolutionized mobility, their siloed, uncoordinated implementation has also introduced critical challenges, including urban sprawl, high fatalities, environmental degradation, and worsened societal disparity. The recent advancements in machine learning, robotics, communication, and computing technologies prompt the emergence of agentic transportation systems (ATS) to potentially address these chronic issues and transform how people access resources and opportunities. In ATS, intelligent machines serve as autonomous intermediaries, facilitating the interactions among humans and between humans and infrastructure. From this new standpoint, early-stage ATS—such as autonomous vehicles, on-demand ridesharing platforms, generative design tools, construction robots, and anomaly detection equipment—have already begun to enter society, calling for an understanding about whether the current research and practice in transportation planning and engineering are ready for ATS. A review of recent literature reveals four main categories of research: (1) co-visioning, co-planning, and co-design; (2) co-construction and co-maintenance; (3) co-control, co-operation, and co-management; and (4) co-usage and co-consumption. The review suggests a significant lack of studies on the proactive integration of agentic machines within and across individual lifecycle phases, risking severe and irreversible consequences. Accordingly, the paper proposes a framework to guide the development of ATS to be justifiable, inclusive, and adaptable (JIA) and ensure the intelligence in and of the next-generation HMTS to be genuinely human-centered and societally beneficial.}
}
@article{RIAZI201841,
title = {Analysis of the empirical research in the journal of second language writing at its 25th year (1992–2016)},
journal = {Journal of Second Language Writing},
volume = {41},
pages = {41-54},
year = {2018},
issn = {1060-3743},
doi = {https://doi.org/10.1016/j.jslw.2018.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S106037431730512X},
author = {Mehdi Riazi and Ling Shi and John Haggerty},
keywords = {L2 writing research, L2 writing theories, L2 writing research methods, Research synthesis},
abstract = {In this historical survey, we review 272 empirical research articles published in the Journal of Second Language Writing (JSLW) over its first quarter century of publication. We report overall and periodic analyses (1992–1999, 2000–2010, 2011–2016) in respect to the following themes: (1) contexts and participants, (2) research foci and theoretical orientations, and (3) research methodology and data sources. The typical research contexts and participants were undergraduates in U.S. universities or colleges. The most common research foci were feedback and writing instruction and the main theoretical orientations were cognitive, social, socio-cognitive, genre, contrastive rhetoric, and critical theories. The most frequently used research methodology was qualitative and the top three data sources used by L2 writing researchers were multiple sources, text samples, and elicitation. Based on the findings, we make suggestions for future research in studies of L2 writing. Along with Tony Silva’s reflections on our results, the present analysis gives readers a birds-eye view of the scholarship on L2 writing over the last 25 years as represented in the JSLW.}
}
@article{VIST2025108962,
title = {ExtractPDF: A data extraction tool for scientific papers applied to a systematic scoping review in public health},
journal = {Computer Methods and Programs in Biomedicine},
volume = {270},
pages = {108962},
year = {2025},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2025.108962},
url = {https://www.sciencedirect.com/science/article/pii/S0169260725003797},
author = {Gunn E. Vist and Trine Husøy and Michael Guy Diemar and Hubert Dirven and Erwin L. Roggen and Maria E. Kalyva},
keywords = {Data extraction tool, Automation, Scientific papers, Environmental chemicals, Systematic scoping review},
abstract = {Background and Objectives
Systematic reviews are widely used to identify the evidence and get an overview of the available knowledge for various questions related to public health and medical topics. They can provide a summary of all the available data and can be used to make knowledge-based decisions about policy, practice, and academic research. The conduct of systematic reviews can often be time‐consuming and costly.
Methods
We have developed a command-line based code in R to extract data in an automated manner from full-text scientific papers. ExtractPDF is a data extraction tool/software that provides a reliable computational workflow for extracting words or combinations of words from numerous portable document format (PDF) files.
Results
The software was applied to extract information from 299 papers that have been screened as included for a published systematic scoping review study within the field of risk assessment in public health. The output of the software is tables of extracted information per type of information of interest per PDF file. The tables were used during the data extraction stage as a second reviewer alongside a human reviewer to assist and/or validate data extraction items.
Conclusions
ExtractPDF tool has a novel pipeline architecture to automate extraction of information from unstructured format types, such as PDF files. ExtractPDF tool assisted in expediting the task of data extraction stage and reducing human related resources as well as errors. The tool’s performance and reliability were found to be very good with metrics of averagely 0.89 for precision, 0.92 for recall, 0.86 for accuracy and 0.91for F1-score.}
}
@article{YAGER20241933,
title = {Towards a science exocortex},
journal = {Digital Discovery},
volume = {3},
number = {10},
pages = {1933-1957},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d4dd00178h},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X2400158X},
author = {Kevin G. Yager},
abstract = {Artificial intelligence (AI) methods are poised to revolutionize intellectual work, with generative AI enabling automation of text analysis, text generation, and simple decision making or reasoning. The impact to science is only just beginning, but the opportunity is significant since scientific research relies fundamentally on extended chains of cognitive work. Here, we review the state of the art in agentic AI systems, and discuss how these methods could be extended to have even greater impact on science. We propose the development of an exocortex, a synthetic extension of a person's cognition. A science exocortex could be designed as a swarm of AI agents, with each agent individually streamlining specific researcher tasks, and whose inter-communication leads to emergent behavior that greatly extend the researcher's cognition and volition.}
}
@article{SHAKED2023100465,
title = {A model-based methodology to support systems security design and assessment},
journal = {Journal of Industrial Information Integration},
volume = {33},
pages = {100465},
year = {2023},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2023.100465},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X23000389},
author = {Avi Shaked},
keywords = {Model-based design, Systems security engineering, Systems specification methodology, Modeling methodology, Threat and risk assessment},
abstract = {Addressing cybersecurity aspects while designing systems is challenging. As our systems increasingly rely on digital technology to perform, security and resilience aspects need to be considered during the system design process. However, the integration of pertinent information into the systems engineering lifecycle is not trivial, as it is characterized by following verbose guidelines and documentation, and has no practical, model-based methodology to support threat-aware design of systems. In this article, we address this gap by presenting an integrative, model-based methodology to support the design and assessment of systems' security aspects. We discuss the methodology's design, specifically with respect to system development scenarios, and detail industrial case studies demonstrating the applicability of the methodology.}
}
@article{YANG2025106514,
title = {Knowledge graph construction with BERT-BiLSTM-IDCNN-CRF and graph algorithms for metallogenic pattern discovery: A case study of pegmatite-type lithium deposits in China},
journal = {Ore Geology Reviews},
volume = {179},
pages = {106514},
year = {2025},
issn = {0169-1368},
doi = {https://doi.org/10.1016/j.oregeorev.2025.106514},
url = {https://www.sciencedirect.com/science/article/pii/S0169136825000745},
author = {Xin Yang and Li Sun and Mei-Ling Liu and Ke-Yan Xiao and Cheng Li and Xu-Chao Dong},
keywords = {Knowledge graph, Bert-BiLSTM-IDCNN-CRF model, Eigenvector centrality, Cosine similarity},
abstract = {Compared to traditional geological data processing methods, knowledge graphs are more effective in calculating and processing the associated information and implicit geological knowledge within the data, helping to accurately grasp the underlying patterns and relationships of geological phenomena. To further optimize the semantic representation of geological text data and extract more detailed feature information, this study introduces the dilated convolutional neural network (IDCNN) layer into the Bert-BiLSTM-CRF model, constructing the Bert-BiLSTM-IDCNN-CRF framework for the precise extraction of lithium deposit named entities.This framework is then used to construct a knowledge graph for granite (pegmatite) lithium deposits in China. Experimental results demonstrate that the Bert-BiLSTM-IDCNN-CRF model exhibits excellent performance in processing Chinese geological text data, achieving a precision of 89%, a recall rate of 87%, and an F1 score of 88%. These results confirm the model's high effectiveness in geological named entity recognition and extraction tasks. Based on this, the study further employs centrality and similarity algorithms from graph theory to deeply analyze the metallogenic characteristics and potential patterns of lithium deposits. This analysis successfully identifies key influencing factors and core nodes for each lithium belt, providing a solid scientific foundation for subsequent lithium exploration target area delineation.}
}
@incollection{EITOBRUN20181,
title = {Chapter 1 - XML: The Basis of the Language},
editor = {Ricardo Eito-Brun},
booktitle = {XML-based Content Management},
publisher = {Chandos Publishing},
pages = {1-30},
year = {2018},
isbn = {978-0-08-100204-9},
doi = {https://doi.org/10.1016/B978-0-08-100204-9.00001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780081002049000019},
author = {Ricardo Eito-Brun},
keywords = {XML, markup language, W3C standards, document standardization},
abstract = {This chapter provides readers with an introduction to the key characteristics of the XML markup language, its evolution, and presents different related specifications published by the W3C that support the development and use of XML-based applications and related technologies. The application of XML is found in different contexts, including editorial and document publishing processes, technical communication, e-learning, translation and localization, and cultural heritage (libraries, archives, and museums). In all these scenarios, organizations and users of information products can benefit from structured content and data management. Relevant savings in the document production process, and reuse opportunities for repurposing the data in response to different, even previously unnoticed needs, are some of the advantages that structured content management offers. This chapter explains the main characteristics of the XML language, its benefits, and the main constituents of XML documents. Key concepts, including well-formed, validity, XML schemas, and XSL stylesheets are also introduced.}
}
@article{RUSSELL2020101169,
title = {Connection as Country: Relational values of billabongs in Indigenous northern Australia},
journal = {Ecosystem Services},
volume = {45},
pages = {101169},
year = {2020},
issn = {2212-0416},
doi = {https://doi.org/10.1016/j.ecoser.2020.101169},
url = {https://www.sciencedirect.com/science/article/pii/S221204162030111X},
author = {Shaina Russell and Emilie Ens},
keywords = {Participatory action research, Human-ecosystem relationships, Relational values, Indigenous Ecological Knowledge, Wetland management, Indigenous water values, Cultural ecosystem services, Ecosystem valuation},
abstract = {Relational values have recently emerged in the sustainability and ecosystem valuation literature as a way to capture the diversity of human-ecosystem relationships. Relational values have been defined as preferences, principles and virtues about human-nature relationships. We describe participatory action research that aimed to elucidate the values associated with freshwater billabongs by Indigenous people in the Ngukurr community in the South East Arnhem land Indigenous Protected Area, northern Australia. Interviews with senior Indigenous knowledge holders revealed that values were strongly relational, i.e. relationships between people, and between people and Country and the inherent reciprocity of these relationships were important. We developed the “Connection as Country” framework towards conceptualising and understanding the relational values of Country from an Indigenous worldview. The “Connection as Country” framework encompasses four domains: spirituality, reciprocal kinship, knowledge and education and cultural subsistence. Broader and contextualised understandings of reciprocal human-ecosystem relationships is required for greater inclusion of Indigenous values in science-policy processes.}
}
@article{GONZALEZGONZALEZ2023101634,
title = {Automatic explanation of the classification of Spanish legal judgments in jurisdiction-dependent law categories with tree estimators},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {7},
pages = {101634},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101634},
url = {https://www.sciencedirect.com/science/article/pii/S131915782300188X},
author = {Jaime González-González and Francisco {de Arriba-Pérez} and Silvia García-Méndez and Andrea Busto-Castiñeira and Francisco J. González-Castaño},
keywords = {Natural language processing, Machine learning, Interpretable and transparent models, Natural language generation, Legal analysis, Human-in-the-loop},
abstract = {Automatic legal text classification systems have been proposed in the literature to address knowledge extraction from judgments and detect their aspects. However, most of these systems are black boxes even when their models are interpretable. This may raise concerns about their trustworthiness. Accordingly, this work contributes with a system combining Natural Language Processing (nlp) with Machine Learning (ml) to classify legal texts in an explainable manner. We analyze the features involved in the decision and the threshold bifurcation values of the decision paths of tree structures and present this information to the users in natural language. This is the first work on automatic analysis of legal texts combining nlp and ml along with Explainable Artificial Intelligence techniques to automatically make the models’ decisions understandable to end users. Furthermore, legal experts have validated our solution, and this knowledge has also been incorporated into the explanation process as “expert-in-the-loop” dictionaries. Experimental results on an annotated data set in law categories by jurisdiction demonstrate that our system yields competitive classification performance, with accuracy values well above 90%, and that its automatic explanations are easily understandable even to non-expert users.}
}
@article{WANG2025,
title = {Intelligent Fake News Detection Leveraging Semantic and Context-Driven Analysis},
journal = {International Journal on Semantic Web and Information Systems},
volume = {21},
number = {1},
year = {2025},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.378676},
url = {https://www.sciencedirect.com/science/article/pii/S1552628325000043},
author = {Dongxiu Wang and Zuxi Chen},
keywords = {Fake News Detection, Collaborative Attention, Cross-Modal Fusion, Graph Convolutional Networks, Graph Attention Mechanism, Semantic Web},
abstract = {ABSTRACT
With the rise of fake news as a societal threat, misinformation detection has become crucial in natural language processing. Traditional methods struggle with inadequate unimodal feature extraction, weak text-image fusion, and limited integration of user context. To address these issues, we suggest an intelligent Fake News Detection Leveraging Semantic and Context-Driven Analysis. Our model extracts text and image features via DeBERTa and CLIP-ViT, while a collaborative attention module enhances cross-modal interactions. Additionally, a graph convolutional network (GCN) captures user dissemination behaviors and social influence within the Semantic Web. By integrating structured user knowledge and multimodal content, the model constructs a holistic, context-aware news representation. Experimental results show that IFN-SC achieves ACC scores of 0.943, 0.963, and 0.911 on Weibo, Twitter, and GossipCop, outperforming state-of-the-art methods and demonstrating the effectiveness of Semantic Web-enhanced multimodal fusion in fake news detection.}
}
@article{DJEBALI2024234,
title = {Survey and insights on digital twins design and smart grid’s applications},
journal = {Future Generation Computer Systems},
volume = {153},
pages = {234-248},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.11.033},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23004466},
author = {Sonia Djebali and Guillaume Guerard and Ihab Taleb},
keywords = {Digital twin, DevOps, Smart grid, Design methodology},
abstract = {Digital twins are a promising technology for simulating complex systems, especially in the smart grid domain. This paper offers a comprehensive literature review on digital twins, focusing on data gathering, data management, and human-in-the-loop control design aspects. Emphasizing the integration of AI and machine learning in big data, it enhances analytics and decision-making capabilities. We introduce a collaborative framework involving multiple stakeholders to maximize the potential of digital twins. The paper examines digital twin applications in smart grids, covering areas like asset management, predictive maintenance, energy optimization, and demand response. By synthesizing research and implementation findings, we identify trends, challenges, and opportunities in the field.}
}
@article{RAJENDRAN2024100913,
title = {Learning across diverse biomedical data modalities and cohorts: Challenges and opportunities for innovation},
journal = {Patterns},
volume = {5},
number = {2},
pages = {100913},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2023.100913},
url = {https://www.sciencedirect.com/science/article/pii/S2666389923003227},
author = {Suraj Rajendran and Weishen Pan and Mert R. Sabuncu and Yong Chen and Jiayu Zhou and Fei Wang},
abstract = {Summary
In healthcare, machine learning (ML) shows significant potential to augment patient care, improve population health, and streamline healthcare workflows. Realizing its full potential is, however, often hampered by concerns about data privacy, diversity in data sources, and suboptimal utilization of different data modalities. This review studies the utility of cross-cohort cross-category (C4) integration in such contexts: the process of combining information from diverse datasets distributed across distinct, secure sites. We argue that C4 approaches could pave the way for ML models that are both holistic and widely applicable. This paper provides a comprehensive overview of C4 in health care, including its present stage, potential opportunities, and associated challenges.}
}
@article{PONOMAREV2021654,
title = {Decision support systems configuration based on knowledge-driven automated service composition: requirements and conceptual model},
journal = {Procedia Computer Science},
volume = {186},
pages = {654-660},
year = {2021},
note = {14th International Symposium "Intelligent Systems},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.04.213},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921010607},
author = {Andrew Ponomarev and Nikolay Mustafin},
keywords = {Service composition, service oriented architecure, dynamic decision support systems, knowledge-driven systems},
abstract = {The development of decision support systems is a complex, multi-stage process that requires considerable effort both methodically and technically. At the same time, with the proliferation of service-oriented architecture (and, more recently, the microservice approach based on it), technical development of such systems is often largely reduced to the construction of compositions of existing services implementing information processing functions, so that eventually the corresponding composition provides the necessary functionality of the decision support system. The paper proposes an approach to the construction of configurable service-oriented decision support systems based on the automated service composition, which will significantly reduce the effort required to develop such systems. Particularly, following results are presented: a) functional framework of different types of decision support systems, typical structures and patterns applicable in different types of decision support systems; b) a set of requirements for configurable service-oriented decision support systems and their main components (tools for describing services, and the purposes of the composite service, methods and algorithms for implementing the composition); c) a conceptual model of a configurable service-oriented decision support system.}
}
@article{BOTTRIGHI201987,
title = {Supporting the distributed execution of clinical guidelines by multiple agents},
journal = {Artificial Intelligence in Medicine},
volume = {98},
pages = {87-108},
year = {2019},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2019.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0933365718306274},
author = {Alessio Bottrighi and Luca Piovesan and Paolo Terenziani},
keywords = {Knowledge representation, Distributed execution of clinical guideline, Support for agent coordination, Management of responsibility, Delegation and execution, Management of agents' qualification and capabilities},
abstract = {Clinical guidelines (GLs) are widely adopted in order to improve the quality of patient care, and to optimize it. To achieve such goals, their application on a specific patient usually requires the interventions of different agents, with different roles (e.g., physician, nurse), abilities (e.g., specialist in the treatment of alcohol-related problems) and contexts (e.g., many chronic patients may be treated at home). Additionally, the responsibility of the application of a guideline to a patient is usually retained by a physician, but delegation of responsibility (of the whole guideline, or of a part of it) is often used\required (e.g., delegation to a specialist), as well as the possibility, for a responsible, to select the executor of an action (e.g., a physician may retain the responsibility of an action, but delegate to a nurse its execution). To manage such phenomena, proper support to agent interaction and communication must be provided, providing agents with facilities for (1) treatment continuity (2) contextualization, (3) responsibility assignment and delegation (4) check of agent “appropriateness”. In this paper we extend GLARE, a computerized GL management system, to support such needs. We illustrate our approach by means of a practical case study.}
}
@article{XU2024104120,
title = {On implementing autonomous supply chains: A multi-agent system approach},
journal = {Computers in Industry},
volume = {161},
pages = {104120},
year = {2024},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2024.104120},
url = {https://www.sciencedirect.com/science/article/pii/S0166361524000484},
author = {Liming Xu and Stephen Mak and Maria Minaricova and Alexandra Brintrup},
keywords = {Autonomous supply chain, Multi-agent system, Autonomous agents, Perishable foods, Resilience},
abstract = {Trade restrictions, the COVID-19 pandemic, and geopolitical conflicts have significantly exposed vulnerabilities within traditional global supply chains. These events underscore the need for organisations to establish more resilient and flexible supply chains. To address these challenges, the concept of the autonomous supply chain (ASC), characterised by predictive and self-decision-making capabilities, has recently emerged as a promising solution. However, research on ASCs is relatively limited, with no existing studies specifically focusing on their implementations. This paper aims to address this gap by presenting an implementation of ASC using a multi-agent approach. It presents a methodology for the analysis and design of such an agent-based ASC system (A2SC). This paper provides a concrete case study, the autonomous meat supply chain, which showcases the practical implementation of the A2SC system using the proposed methodology. Additionally, a system architecture and a toolkit for developing such A2SC systems are presented. Despite limitations, this work demonstrates a promising approach for implementing an effective ASC system.}
}
@article{MOUSSA20222950,
title = {Mixing Static Word Embeddings and RoBERTa for Spatial Role Labeling},
journal = {Procedia Computer Science},
volume = {207},
pages = {2950-2957},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.353},
url = {https://www.sciencedirect.com/science/article/pii/S187705092201242X},
author = {Alaeddine Moussa and Sebastien Fournier and Khaoula Mahmoudi and Bernard Espinasse and Sami Faiz},
keywords = {Spatial Role Labeling, Word Embedding, Transfer Learning, Deep Learning, Natural language processing},
abstract = {Language model pretraining has yielded significant results in diverse natural language processing tasks. RoberTa, an efficient method for pretraining self-supervised NLP systems, is a good example. Our hypothesis in this paper is that the performance of Spatial Role Labeling (SpRL) can be improved by combining static word vectors and bags of features with RoberTa vectors. Furthermore, we show that our method is successful in several SpRL datasets.}
}
@article{CHEN2024e25569,
title = {Integrated analysis of differentially expressed genes and miRNA expression profiles in dilated cardiomyopathy},
journal = {Heliyon},
volume = {10},
number = {4},
pages = {e25569},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e25569},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024016001},
author = {Yu Chen and Wen-Ke Cai and Jie Yu and Ming Shen and Jin-Huan Zhou and Sheng-Yu Yang and Wei Liu and Si Lu and Yan-Kun Shi and Li-Xia Yang},
keywords = {Dilated cardiomyopathy, Genes, microRNAs, mRNA-miRNA network, Bioinformatics},
abstract = {Background
Although dilated cardiomyopathy (DCM) is a prevalent form of cardiomyopathy, the molecular mechanisms underlying its pathogenesis and progression remain poorly understood. It is possible to identify and validate DCM-associated genes, pathways, and miRNAs using bioinformatics analysis coupled with clinical validation methods.
Methods
Our analysis was performed using 3 mRNA datasets and 1 miRNA database. We employed several approaches, including gene ontology (GO) analysis, KEGG pathway enrichment analysis, protein-protein interaction networks analysis, and analysis of hub genes to identify critical genes and pathways linked to DCM. We constructed a regulatory network for DCM that involves interactions between miRNAs and mRNAs. We also validated the differently expressed miRNAs in clinical samples (87 DCM ，83 Normal) using qRT-PCR. The miRNAs' clinical value was evaluated by receiver operating characteristic curves (ROCs).
Results
78 differentially expressed genes (DEGs) and 170 differentially expressed miRNAs (DEMs) were associated with DCM. The top five GO annotations were collagen-containing extracellular matrix, cell substrate adhesion, negative regulation of cell differentiation, and inflammatory response. The most enriched KEGG pathways were the Neurotrophin signaling pathway, Thyroid hormone signaling pathway, Wnt signaling pathway, and Axon guidance. In the PPI network, we identified 10 hub genes, and in the miRNA-mRNA regulatory network, we identified 8 hub genes and 15 miRNAs. In the clinical validation, we found 13 miRNAs with an AUC value greater than 0.9.
Conclusion
Our research offers novel insights into the underlying mechanisms of DCM and has implications for identifying potential targets for diagnosis and treatment of this condition.}
}
@article{JOSEPH202383,
title = {Machine Learning Methods for Predicting Patient-Level Emergency Department Workload},
journal = {The Journal of Emergency Medicine},
volume = {64},
number = {1},
pages = {83-92},
year = {2023},
issn = {0736-4679},
doi = {https://doi.org/10.1016/j.jemermed.2022.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0736467922005686},
author = {Joshua W. Joseph and Evan L. Leventhal and Anne V. Grossestreuer and Paul C. Chen and Benjamin A. White and Larry A. Nathanson and Noémie Elhadad and Leon D. Sanchez},
keywords = {clinical decision support, operations management, machine learning, quality assurance},
abstract = {Background
Work Relative Value Units (wRVUs) are a component of many compensation models, and a proxy for the effort required to care for a patient. Accurate prediction of wRVUs generated per patient at triage could facilitate real-time load balancing between physicians and provide many practical operational and clinical benefits.
Objective
We examined whether deep-learning approaches could predict the wRVUs generated by a patient's visit using data commonly available at triage.
Methods
Adult patients presenting to an urban, academic emergency department from July 1, 2016–March 1, 2020 were included. Deidentified triage information included structured data (age, sex, vital signs, Emergency Severity Index score, language, race, standardized chief complaint) and unstructured data (free-text chief complaint) with wRVUs as outcome. Five models were examined: average wRVUs per chief complaint, linear regression, neural network and gradient-boosted tree on structured data, and neural network on unstructured textual data. Models were evaluated using mean absolute error.
Results
We analyzed 204,064 visits between July 1, 2016 and March 1, 2020. The median wRVUs were 3.80 (interquartile range 2.56–4.21), with significant effects of age, gender, and race. Models demonstrated lower error as complexity increased. Predictions using averages from chief complaints alone demonstrated a mean error of 2.17 predicted wRVUs per visit (95% confidence interval [CI] 2.07–2.27), the linear regression model: 1.00 wRVUs (95% CI 0.97–1.04), gradient-boosted tree: 0.85 wRVUs (95% CI 0.84–0.86), neural network with structured data: 0.86 wRVUs (95% CI 0.85–0.87), and neural network with unstructured data: 0.78 wRVUs (95% CI 0.76–0.80).
Conclusions
Chief complaints are a poor predictor of the effort needed to evaluate a patient; however, deep-learning techniques show promise. These algorithms have the potential to provide many practical applications, including balancing workloads and compensation between emergency physicians, quantify crowding and mobilizing resources, and reducing bias in the triage process.}
}
@article{NAWAZ2025200541,
title = {A review of neuro-symbolic AI integrating reasoning and learning for advanced cognitive systems},
journal = {Intelligent Systems with Applications},
volume = {26},
pages = {200541},
year = {2025},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2025.200541},
url = {https://www.sciencedirect.com/science/article/pii/S2667305325000675},
author = {Uzma Nawaz and Mufti Anees-ur-Rahaman and Zubair Saeed},
abstract = {Neuro-symbolic AI represents the convergence of two principal paradigms in artificial intelligence: neural networks, which are efficient in data-driven learning, and symbolic reasoning, which offers explainability and logical inference. This hybrid methodology combines the adaptability of neural networks with symbolic AI's interpretability and formal reasoning abilities, which provide a practical framework for advanced cognitive systems. This paper analyzes the present condition of neuro-symbolic AI, emphasizing essential techniques that combine reasoning and learning. We explore models such as Logic Tensor Networks, Differentiable Logic Programs, and Neural Theorem Provers. The study analyzes their impact on the advancement of cognitive systems in natural language processing, robotics, and decision-making. The paper examines the challenges faced by neuro-symbolic AI, such as scalability, integration with multimodal data, and maintaining interpretability without compromising efficiency. By evaluating the strengths and weaknesses of many methodologies, we comprehensively understand the field's development and its potential to revolutionize intelligent systems. In addition, we identify emerging research areas, including the incorporation of ethical frameworks and the development of adaptive dynamic neuro-symbolic systems that respond in real-time. This review aims to guide future research by providing insights into the potential of neuro-symbolic AI to influence the development of the next generation of intelligent, explainable, and adaptive systems.}
}
@article{GAO2024105466,
title = {Construction risk identification using a multi-sentence context-aware method},
journal = {Automation in Construction},
volume = {164},
pages = {105466},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105466},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524002024},
author = {Nan Gao and Ali Touran and Qi Wang and Nicholas Beauchamp},
keywords = {Project-level risk, Risk identification, Context-aware text classification, Natural language processing},
abstract = {Knowledge of risk events with potentially negative consequences from previous projects is essential for risk identification in early stages of new infrastructure projects. However, historical risk events are usually scattered in various sources and reports, rendering collecting such risk information time-consuming and expensive. To expand the current risk data sources and facilitate risk events' extraction, the paper presents a synthetic approach that utilizes Natural Language Processing (NLP) techniques to automatically identify and extract risk-related sentences from news articles. A supervised Multi-sentence Context-aware Risk Identification (MCRI) model is devised to exploit both sentence-level and multi-sentence level context to boost the sentence classification performance. The MCRI model outperformed several baseline models with a risk-class F1-score of 87.1% and an accuracy of 86.7%. This paper provides a baseline for future studies aimed at automating the extraction of project-level risk information within the construction domain.}
}
@article{WU2024715,
title = {A review of deep learning methods for ligand based drug virtual screening},
journal = {Fundamental Research},
volume = {4},
number = {4},
pages = {715-737},
year = {2024},
issn = {2667-3258},
doi = {https://doi.org/10.1016/j.fmre.2024.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S2667325824001043},
author = {Hongjie Wu and Junkai Liu and Runhua Zhang and Yaoyao Lu and Guozeng Cui and Zhiming Cui and Yijie Ding},
keywords = {Virtual screening, Deep learning, Drug discovery, Drug-target interaction, Drug-target affinity},
abstract = {Drug discovery is costly and time consuming, and modern drug discovery endeavors are progressively reliant on computational methodologies, aiming to mitigate temporal and financial expenditures associated with the process. In particular, the time required for vaccine and drug discovery is prolonged during emergency situations such as the coronavirus 2019 pandemic. Recently, the performance of deep learning methods in drug virtual screening has been particularly prominent. It has become a concern for researchers how to summarize the existing deep learning in drug virtual screening, select different models for different drug screening problems, exploit the advantages of deep learning models, and further improve the capability of deep learning in drug virtual screening. This review first introduces the basic concepts of drug virtual screening, common datasets, and data representation methods. Then, large numbers of common deep learning methods for drug virtual screening are compared and analyzed. In addition, a dataset of different sizes is constructed independently to evaluate the performance of each deep learning model for the difficult problem of large-scale ligand virtual screening. Finally, the existing challenges and future directions in the field of virtual screening are presented.}
}
@article{MA2025105994,
title = {Knowledge co-creation during urban simulation computation to enable broader participation},
journal = {Sustainable Cities and Society},
volume = {118},
pages = {105994},
year = {2025},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2024.105994},
url = {https://www.sciencedirect.com/science/article/pii/S2210670724008187},
author = {Zaiyang Ma and Hengyue Li and Kai Zhang and Jin Wang and Songshan Yue and Yongning Wen and Guonian Lü and Min Chen},
keywords = {Urban simulation, Collaboration, Information extraction, Knowledge synthesis, Simulation processes},
abstract = {Preparing knowledge on urban simulation computation is necessary to help participants build consensus, reduce expertise gaps, and guide participatory sustainable urban planning. Knowledge co-creation is an effective way to prepare the needed knowledge related to urban simulation computation. However, the procedural and operational information that can help instruct the implementation of urban simulation is extensively hidden in the implementation processes of urban simulation in various forms (e.g., dialog records, configuration parameters, and model operations). Difficulties remain in extracting this implicit information and synthesizing the related knowledge. Therefore, a strategy is proposed to support the co-creation of knowledge during the urban simulation computation. In this strategy, the structural knowledge expression methods are first designed to support information extraction and knowledge synthesis. Based on interaction tracking and natural language understanding techniques, the related information can be obtained from simulation computation processes. Using this information, four main types of knowledge can be generated, optimized and visualized to assist collaborative urban simulation practices. This strategy was implemented in an online collaboration prototype system and verified with two sustainable urban case studies involving the simulation of urban noise environments and solar radiation assessment of photovoltaic noise barriers in cities. The results show that the knowledge co-creation can be effectively implemented by using the information extracted from simulation computation processes, which can benefit broader collaboration in urban simulation and sustainable urban planning.}
}
@article{LIU2019637,
title = {Biologically Inspired Design of Context-Aware Smart Products},
journal = {Engineering},
volume = {5},
number = {4},
pages = {637-645},
year = {2019},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2019.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S2095809918306076},
author = {Ang Liu and Ivan Teo and Diandi Chen and Stephen Lu and Thorsten Wuest and Zhinan Zhang and Fei Tao},
keywords = {Design method, Biologically inspired design, Context-awareness, Intelligent design},
abstract = {The rapid development of information and communication technologies (ICTs) and cyber–physical systems (CPSs) has paved the way for the increasing popularity of smart products. Context-awareness is an important facet of product smartness. Unlike artifacts, various bio-systems are naturally characterized by their extraordinary context-awareness. Biologically inspired design (BID) is one of the most commonly employed design strategies. However, few studies have examined the BID of context-aware smart products to date. This paper presents a structured design framework to support the BID of context-aware smart products. The meaning of context-awareness is defined from the perspective of product design. The framework is developed based on the theoretical foundations of the situated function–behavior–structure ontology. A structured design process is prescribed to leverage various biological inspirations in order to support different conceptual design activities, such as problem formulation, structure reformulation, behavior reformulation, and function reformulation. Some existing design methods and emerging design tools are incorporated into the framework. A case study is presented to showcase how this framework can be followed to redesign a robot vacuum cleaner and make it more context-aware.}
}
@article{RIZZO2020256,
title = {Class expression induction as concept space exploration: From DL-Foil to DL-Focl},
journal = {Future Generation Computer Systems},
volume = {108},
pages = {256-272},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.02.071},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19303991},
author = {Giuseppe Rizzo and Nicola Fanizzi and Claudia d’Amato},
keywords = {Concept learning, Web of data, Semantic knowledge bases, Linked data, Triple stores, Type classification, Description logics, Open-world semantics, Heuristic search, Ontology learning},
abstract = {The Web of Data is one of the perspectives of the Semantic Web. In this context, concept learning services, supported by multirelational machine learning, have been integrated in various tools for knowledge engineers to carry out several tasks related to the construction, completion and maintenance of the knowledge bases: essentially they are used to elicit new candidate concept definitions (i.e. axioms regarding classes) to be incorporated in the knowledge bases possibly also as replacements for previous ones. Sundry reference approaches rely on a covering strategy to generalize input examples that can be regarded as a form of hill-climbing search that explores a huge discrete conceptual space. Methods adopting this strategy are known to be affected by an inherent problem of myopia. In particular, our DL-Foil has been shown to suffer from this problem as its algorithm is based on a stochastic yet informed exploration of the concept space, by means of a refinement operator, to generate partial descriptions iteratively. To tackle this problem and enhance the performance of our system we have introduced a series of extensions of the original DL-Foil algorithm, that have led to various releases of its spin-off DL-Focl. Essentially they aim at reducing the aforementioned problem through specific strategies grounded on either the integration of meta-heuristics, such as repeated hill-climbing and tabu search, or the employment of some form of lookahead. In this work, we present consolidated and extended releases of both DL-Foil and DL-Focl along various dimensions: better heuristics and stop conditions, more complex refinement operators with the possibility to perform the specialization adopting iterative deepening or lookahead strategies, improved versions of the algorithm based on the repeated hill-climbing strategy with new quality criteria and of the tabu search with a different policy for managing the local memory. All the implementations of these approaches have been extensively evaluated in three experimental sessions, involving various publicly available knowledge bases and fragments extracted from the Linked Data cloud, showing interesting results and indicating some lessons to be learned: our approaches outperformed a popular reference system from the DL-Learner framework on learning problems when the open-world semantics is explicitly considered. They also exhibited an analogous performance on a benchmark of datasets from contexts with an intended underlying closed-world semantics.}
}
@article{WANG2022113878,
title = {Autophagy involvement in T lymphocyte signalling induced by nickel with quantitative phosphoproteomic analysis},
journal = {Ecotoxicology and Environmental Safety},
volume = {242},
pages = {113878},
year = {2022},
issn = {0147-6513},
doi = {https://doi.org/10.1016/j.ecoenv.2022.113878},
url = {https://www.sciencedirect.com/science/article/pii/S0147651322007187},
author = {Gong Wang and Tingting Shen and Xueyan Huang and Zhen Luo and Yulong Tan and Genlin He and Zeze Wang and Ping Li and Xiaoqian Liu and Xueting Yu and Boyi Zhang and Huan Zhou and Xue Luo and Xuesen Yang},
keywords = {Nickel, Allergy, TMT, Signalling pathway, Autophagy},
abstract = {Nickel-induced allergic contact dermatitis (ACD) is a common skin disease. The mechanism by which nickel causes ACD is not clear. There is no treatment for it, only symptomatic therapy. However, due to the lifetime sensitization characteristics, the recurrence rate in patients is high. T lymphocytes play a key role in nickel-induced ACD. Elucidating the potential mechanism underlying nickel-induced T lymphocyte signalling might make it possible to achieve targeted treatment of nickel-induced ACD. In our study, a phosphoproteomic approach based on tandem mass tag (TMT) labelling and LCMS/MS analyses was employed. An animal model of nickel allergy was established. Splenic T lymphocytes were purified for quantitative phosphoproteomic analysis. The numbers of phosphoproteins, phosphopeptides and phosphosites identified in this study were 3072, 7977 and 10,200, respectively. Comprehensive gene ontology (GO) analysis combined with Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway enrichment analysis revealed that nickel can significantly affect the phosphorylation of the mTOR signalling pathway in T lymphocytes. Western blotting analysis was used to detect changes in the expression of autophagy-related proteins (Beclin 1, LC3II, and p62). Nickel allergy changed autophagy-related protein expression (p < 0.05). It has been demonstrated that nickel causes autophagy of T lymphocytes in the spleen. Using autophagy inhibitors to intervene, it was found that Th1 differentiation was inhibited, and the expression of Th1-related inflammatory factors was downregulated. Overall, the identification of relevant signalling pathways yielded new insights into the molecular mechanisms underlying nickel allergy and might help in the discovery and development of mechanism-based drugs.}
}
@article{HABERBECK20183,
title = {Harmonized terms, concepts and metadata for microbiological risk assessment models: The basis for knowledge integration and exchange},
journal = {Microbial Risk Analysis},
volume = {10},
pages = {3-12},
year = {2018},
note = {Special issue on 10th International Conference on Predictive Modelling in Food: Interdisciplinary Approaches and Decision-Making Tools in Microbial Risk Analysis},
issn = {2352-3522},
doi = {https://doi.org/10.1016/j.mran.2018.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352352218300100},
author = {Leticia Ungaretti Haberbeck and Carolina Plaza-Rodríguez and Virginie Desvignes and Paw Dalgaard and Moez Sanaa and Laurent Guillier and Maarten Nauta and Matthias Filter},
keywords = {QMRA modelling, Information exchange format, Metadata schema, Controlled vocabularies, Model annotation},
abstract = {In the last decades the microbial food safety community has developed a variety of valuable knowledge (e.g., mathematical models and data) and resources (e.g., databases and software tools) in the areas of quantitative microbial risk assessment (QMRA) and predictive microbiology. However, the reusability of this knowledge and the exchange of information between resources are currently difficult and time consuming. This problem has increased over time due to the lack of harmonized data format and rules for knowledge annotation. It includes the lack of a common understanding of basic terms and concepts and of a harmonized information exchange format to describe and annotate knowledge. The existence of ambiguities and inconsistencies in the use of terms and concepts in the QMRA and predictive microbial (PM) modelling necessitates a consensus on their refinement, which will allow a harmonized exchange of information within these areas. Therefore, this work aims to harmonize terms and concepts used in QMRA and PM modelling spanning from high level concepts as defined by Codex Alimentarius, Food and Agriculture Organization (FAO) and World Health Organization (WHO), up to terms generally used in statistics or data and software science. As a result, a harmonized schema for metadata that allows consistent annotation of data and models from these two domains is proposed. This metadata schema is also a key component of the Food Safety Knowledge Markup Language (FSK-ML), a harmonized format for information exchange between resources in the QMRA and PM modelling domain. This work is carried out within a research project that aims to establish a new community resource called Risk Assessment Modelling and Knowledge Integration Platform (RAKIP). This platform will facilitate the sharing and execution of curated QMRA and PM models using the foundation of the proposed harmonized metadata schema and information exchange format. Furthermore, it will also provide access to related open source software libraries, converter tools and software-specific import and export functions that promote the adoption of FSK-ML by the microbial food safety community. In the future, these resources will hopefully promote both the knowledge reusability and the high-quality information exchange between stakeholders within the areas of QMRA and PM modelling worldwide.}
}
@article{ZHANG2023224,
title = {Rule capture of automated compliance checking of building requirements: a review},
journal = {Proceedings of the Institution of Civil Engineers - Smart Infrastructure and Construction},
volume = {176},
number = {4},
pages = {224-238},
year = {2023},
issn = {2397-8759},
doi = {https://doi.org/10.1680/jsmic.23.00005},
url = {https://www.sciencedirect.com/science/article/pii/S2397875923000121},
author = {Zijing Zhang and Ling Ma and Tim Broyd},
keywords = {Building Information Modelling (BIM), construction management, design, UN SDG 9: Industry, innovation and infrastructure},
abstract = {In the architectural, engineering and construction industry, building design needs to be checked against regulations before it can be finalised and progress to the construction stage. The traditional manual compliance-checking process is error prone and time consuming. As a solution, automated compliance checking (ACC) was proposed. Rule capture is a crucial bottleneck of ACC. Despite many studies in this domain, no research has synthesised the themes and identified future research opportunities. This paper aims to fill this gap by conducting a systematic literature review and identifying challenges facing this field. The findings revealed that the rule capture process had attracted interest in the past years, and more semi-automated and automated methods have been proposed. The current representation development process lacks a methodological backdrop. The existing representations cannot represent unknowns and side effects, lack the ability to deal with ambiguous rules and are typically restricted by the rule engine and/or target data model. The understanding of rules, representations and the relationships between them is insufficient. Further research is required to address these issues.}
}
@article{XU2021102295,
title = {Exploring the research themes and their relationships of LIS in China from 2013 to 2018 using co-word analysis},
journal = {The Journal of Academic Librarianship},
volume = {47},
number = {1},
pages = {102295},
year = {2021},
issn = {0099-1333},
doi = {https://doi.org/10.1016/j.acalib.2020.102295},
url = {https://www.sciencedirect.com/science/article/pii/S0099133320301865},
author = {Fang Xu and Li Ma},
keywords = {LIS in China, Co-word analysis, Research themes, Social network analysis, Cluster analysis},
abstract = {This study aims to examine the research themes and their relationships of library and information science (LIS) in mainland China during the period of 2013–2018. Data were collected from the China Academic Journal Network Publishing Database (CAJD) and co-word analysis method was employed to measure correlation among the extracted key words. Seventeen theme-clusters, network characteristics (centre and density), clustering strategy diagram, and correlation network are revealed through cluster analysis, multidimensional scaling (MDS), and social network analysis. The results show that in recent years research on LIS in China, several important research themes like Electronic Government, Reading Promotion, and Social Media have emerged with high relevance, which can be considered as concentrated, mature and well-developed. Some research topics such as Ontology, Big data, and Cloud Computing have developed on a considerable scale, while others remain isolated and undeveloped like Knowledge Sharing and Virtual Community. In addition, the current research has also identified a few of emerging research topics with great potential for development like Mobile Library, Subject Services, and User Experience. The study is beneficial to understand the overall picture of research themes and their relationships of LIS in China in recent years to provide support for follow-up research.}
}
@incollection{ZWOLINSKI201827,
title = {Chapter 2 - Methods for Assessing Geodiversity},
editor = {Emmanuel Reynard and José Brilha},
booktitle = {Geoheritage},
publisher = {Elsevier},
pages = {27-52},
year = {2018},
isbn = {978-0-12-809531-7},
doi = {https://doi.org/10.1016/B978-0-12-809531-7.00002-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128095317000022},
author = {Zbigniew Zwoliński and Alicja Najwer and Marco Giardino},
keywords = {Geodiversity, assessment, valuation, methods and techniques, geodiversity index},
abstract = {The assessment of geodiversity can be made with qualitative, quantitative and qualitative–quantitative methods. Qualitative methods have a descriptive character and are suitable for nominal and ordinal data. Quantitative methods are based on a set of parameters and indicators to determine a geodiversity index of a certain area. Qualitative–quantitative methods result in a combination of quantitative (i.e., digital) and cause-effect data (i.e., relational and explanatory). At the current stage of development, qualitative–quantitative methods are the most advanced and the ones offering more reliable results. Their main advantage is the integration of data from different sources and with different content and their wide use within geographic information systems, both at the stage of data collection and data integration, as well as during numerical processing and output presentation. The limitation of these methods is related to difficulties concerning the validation of results. The development of qualitative–quantitative methods associated with cognitive issues should be expected in the near future, oriented towards ontology and the Semantic Web.}
}
@incollection{ZARGAR2024133,
title = {Chapter 8 - An introduction to systems biology},
editor = {Sajad Majeed Zargar and Asmat Farooq and Parvaze Ahmad Sofi and Jebi Sudan and Uneeb Urwat and Khursheed Hussain},
booktitle = {Concepts and Techniques in OMICS and System Biology},
publisher = {Academic Press},
pages = {133-166},
year = {2024},
isbn = {978-0-443-21923-8},
doi = {https://doi.org/10.1016/B978-0-443-21923-8.00008-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044321923800008X},
author = {Sajad Majeed Zargar and Asmat Farooq and Parvaze Ahmad Sofi and Jebi Sudan and Uneeb Urwat and Khursheed Hussain},
keywords = {Biological sciences, Computational biology, High-throughput data, Network analysis, Systems biology},
abstract = {Systems biology, a multidisciplinary field leveraging computer models, high-throughput technologies, and computational methods, is explored comprehensively in this chapter. Its historical evolution, importance in modern biology, objectives, and tools are detailed. Tracing its roots to the mid-20th century, key developments in the 1990s, like the Virtual Cell project and the Systems Biology Markup Language (SBML), are highlighted. Acknowledging its crucial role in diverse fields, including biotechnology, medicine, ecology, and environmental science, this chapter underscores systems biology's significance in modern biology. The ability of systems biology to integrate vast data is emphasized for its quantitative and predictive understanding of biological systems. Its applications span across studying diseases, like cancer and diabetes, to engineering plants with improved traits. Challenges, such as complex mathematical models and the need for interdisciplinary collaboration, are acknowledged. The objectives of systems biology, categorized into understanding, prediction, and control, drive efforts to identify the key components, predict system behavior, and manipulate biological systems. This chapter delves into the tools and techniques, covering high-throughput data generation, bioinformatics, and computational approaches. Genome-wide association studies (GWAS) and gene set enrichment analysis (GSEA) are highlighted, and the role of network analysis in understanding cellular networks is explored. In conclusion, this comprehensive overview emphasizes the pivotal role of systems biology in advancing our understanding of complex biological systems. The integration of experimental data with computational analyses, coupled with advancements in high-throughput technologies, positions systems biology as a transformative field with diverse applications in medicine, biotechnology, and beyond.}
}
@article{ASSI202173,
title = {Instance Matching in Knowledge Graphs through random walks and semantics},
journal = {Future Generation Computer Systems},
volume = {123},
pages = {73-84},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21001369},
author = {Ali Assi and Wajdi Dhifli},
keywords = {Affinity-preserving random walk, Data linking, Instance Matching, Knowledge Graph, Web of data},
abstract = {Instance Matching (IM) is the process of matching instances that refer to the same real-world object (e.g., the same person) across different independent Knowledge Bases (KBs). This process is considered as a key step, for instance, in the integration of different KBs. In this paper, we focus on the problem of IM across different KBs represented as Knowledge Graphs (KGs). We propose SBIGMat, a novel approach for the IM problem based on Markov random walks (RW). Our approach leverages both the local and global information mutually calculated from a pairwise similarity graph. Precisely, we first build an expanded association graph consisting of pairs of IM candidates. Then, we rank each candidate pair through the stationary distribution computed from the RW on the association graph. We propose semantic and bipartite graph-based post-processing strategies that operate on the obtained random walk ranks to optimize the final assignment of co-referents. We provide a scalable distributed implementation of our approach on top of the Spark framework and we evaluate it on benchmark datasets from the instance track of the Ontology Alignment Evaluation Initiative (OAEI). The experiments show the efficiency and scalability of SBIGMat compared to several state-of-the-art IM approaches.}
}
@article{KRISHNASIVAPRASAD2022101280,
title = {Exploring intrinsic information content models for addressing the issues of traditional semantic measures to evaluate verb similarity},
journal = {Computer Speech & Language},
volume = {71},
pages = {101280},
year = {2022},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2021.101280},
url = {https://www.sciencedirect.com/science/article/pii/S0885230821000838},
author = {M. {Krishna Siva Prasad} and Poonam Sharma},
keywords = {Semantic similarity, Intrinsic information content model, Information content, Path length, Depth},
abstract = {Semantic similarity measures play an important role in many natural language processing and information retrieval activities. It is highly challenging to measure semantic similarity with higher accuracy. A notable branch of semantic similarity evaluation based on information content (IC) is popular in this aspect. Intrinsic information content (IIC) models are another wing of IC based evaluation. Both IC based and IIC based approaches majorly handled similarity evaluation of nouns. Research related to semantic similarity assessment of verb pairs are rarely discussed. To bridge this gap, this work examines various IC based, IIC based approaches on verb pairs. A detailed discussion of the existing measures and their drawbacks are mentioned in this work. Strategies based on information content, length and depth of the concepts are discussed and tested on benchmark datasets. Existing intrinsic information content models are enhanced by addressing various issues like (a) dealing concepts with no path in WordNet and (b) handling the synonym sets of verb concepts. Measures based on path length, intrinsic information content, combined strategies and non-linear strategies for verb pairs are thoroughly inspected. This paper also presents novel strategies to understand novel aspects that are not addressed before. The strategies are experimented by generating the synonym sets of required parts-of-speech which proved very effective in improving the correlation with human judgment. Results on benchmark datasets specify that the proposed approaches for verb similarity will be a guiding factor for understanding the natural language processing tasks.}
}
@article{FORTH2023100263,
title = {BIM4EarlyLCA: An interactive visualization approach for early design support based on uncertain LCA results using open BIM},
journal = {Developments in the Built Environment},
volume = {16},
pages = {100263},
year = {2023},
issn = {2666-1659},
doi = {https://doi.org/10.1016/j.dibe.2023.100263},
url = {https://www.sciencedirect.com/science/article/pii/S266616592300145X},
author = {Kasimir Forth and Alexander Hollberg and André Borrmann},
keywords = {LCA, BIM, Design decision support, Early design stages},
abstract = {To meet the European climate goals in the building sector, a holistic optimization of embodied greenhouse gas (GHG) emissions using the method of life cycle assessments (LCA) are necessary. The early design stages have high impact on the final performance of the buildings and are characterized by high uncertainty due to the lack of information and not yet taken decisions. Furthermore, most current LCA approaches based on Building Information Models (BIM) require high expertise and experience in both BIM and LCA and do not follow an intuitive visualization approach for other stakeholders and non-experts. This paper presents a novel design-decision-making approach for reducing embodied GHG emissions by interactive, model-based visualizations of uncertain LCA results. The proposed workflow is based on open BIM data formats, such as Industry Foundation Classes (IFC) and BIM Collaboration Format (BCF), and is developed for decision support for non-LCA experts in the early design stages. With the help of a user study, the prototypical implementation is tested by 103 participants with different levels of experience in BIM and LCA based on a case study. We evaluate the proposed approach regarding the support of open BIM data formats, different LCA visualization strategies, and the intuitiveness of different approaches to visualizing uncertain LCA results. The user study results show a broad acceptance and need for open BIM data formats and model-based LCA visualization but less for visualizing uncertainties, which needs further research. In conclusion, this interactive, model-based visualization approach using color coding supports non-LCA experts in the design decision-making process in early design stages.}
}
@article{LOPEZNICOLAS2024101369,
title = {Untangling business model innovation in family firms: Socioemotional wealth and corporate social responsibility perspectives},
journal = {Scandinavian Journal of Management},
volume = {40},
number = {4},
pages = {101369},
year = {2024},
issn = {0956-5221},
doi = {https://doi.org/10.1016/j.scaman.2024.101369},
url = {https://www.sciencedirect.com/science/article/pii/S0956522124000502},
author = {Carolina López-Nicolás and Ángel L. Meroño-Cerdán and Marikka Heikkilä and Harry Bouwman},
keywords = {Family firms, Business model, innovation, multiple case study research},
abstract = {Despite the increasing interest in business model innovation (BMI) as a way to improve the performance of firms, and the predominance of family firms (FFs) in modern economy, these two topics have so far not been combined. Drawing on socioemotional wealth (SEW) theory and the corporate social responsibility (CSR) concept, and on insights from research into BMI, we conduct a qualitative analysis using data from fifteen European FFs, examining the strategic and BM focus, the nature of the BM renewal, and the process and outcomes of BMI on their business models (BMs). Our results identify several BM configurations, with a focus on (1) growth by internationalization in combination with attention to increased quality in value creation, and (2) profit orientation based on increased efficiency, enabled by digitalization, mainly in the value delivery components of a BM. The latter reflects distinctive, innovative capabilities found in FFs, that contribute to the preservation of family objectives, as suggested by SEW theory and business orientation on CSR. Furthermore, there is a link between family involvement and limited, but specific, knowledge-related resources, and the way the dynamic BMI process is governed and executed.}
}
@article{KISHOR2025172,
title = {Development of Grader Provider System Using Deep Learning},
journal = {Procedia Computer Science},
volume = {259},
pages = {172-181},
year = {2025},
note = {Sixth International Conference on Futuristic Trends in Networks and Computing Technologies (FTNCT06), held in Uttarakhand, India},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.03.318},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925010622},
author = {Kaushal Kishor and Prakash Vishwakarma and Lucky Sengar and Vikash Kumar and Vikash Gupta},
keywords = {NLP, Machine learning, BERT model, OCR, Subjective Answer Evaluation, Long Short Term Memory Networks, Bidirectional Long Short Term Memory Networks, Root Mean Square Error},
abstract = {In recent years, the demand for efficient evaluation systems in educational settings has surged, highlighting the need for automation in grading processes. This research presents a Grader pro (Automatic Answer Sheet Evaluation System) that integrates Optical Character Recognition (OCR) with Natural Language Processing (NLP) techniques, specifically leveraging transformer models. The proposed system aims to accurately assess handwritten answer sheets by first converting them into digital text using advanced Transformer based OCR /fine-tuned transformer for text extraction from hand writing. Following text extraction, the system employs NLP based transformer such as BERT model for key-factors evolutions to analyze and grade the responses based on predefined criteria and answer patterns. Our approach addresses key challenges such as varying handwriting styles, contextual understanding, and semantic analysis of answers. Pre-liminary results demonstrate significant improvements in evaluation speed and accuracy compared to traditional manual grading methods. This research not only contributes to the field of automated assessment but also paves the way for scalable solutions in educational technology, ultimately enhancing the learning experience by providing timely feedback to students. Future work will focus on refining the model’s capabilities and expanding its applicability to various educational contexts.}
}
@article{BOWDEN20208,
title = {The historic (wrong) turn in management and organizational studies},
journal = {Journal of Management History},
volume = {27},
number = {1},
pages = {8-27},
year = {2020},
issn = {1751-1348},
doi = {https://doi.org/10.1108/JMH-06-2020-0037},
url = {https://www.sciencedirect.com/science/article/pii/S1751134820000419},
author = {Bradley Bowden},
keywords = {Foucault, Management history, Postmodernism, ANTi-history, Historic turn},
abstract = {Purpose
Management history has in the past 15 years witnessed growing enthusiasm for “critical” research methodologies associated with the so-called “historic turn”. This paper aims to argue, however, that the “historic turn” has proved to an “historic wrong turn”, typically associated with confused and contradictory positions. In consequence, Foucault’s belief that knowledge is rooted in discourse, and that both are rooted in external structures of power, is used while simultaneously professing advocacy of White’s understanding that history is fictive, the product of the historian’s imagination.
Design/methodology/approach
This paper explores the intellectual roots of the historic (wrong) turn in the idealist philosophies of Nietzsche, Croce, Foucault, White and Latour as well as the critiques that have been made of those theories from within “critical” or “Left” theoretical frameworks.
Findings
Failing to properly acknowledge the historical origin of their ideas and/or the critiques of those ideas – and misrepresenting all contrary opinion as “positivist” – those associated with the historic (wrong) turn replicate the errors of their theoretical champions. The author thus witnesses a confusion of ontology (the nature of being) and epistemology (the nature of knowledge) and, consequently, of “facts” (things that exist independently of our fancy), “evidence” (how ascertain knowledge of a fact) and “interpretation” (how I connect evidence to explain an historical outcome).
Originality/value
Directed toward an examination of the conceptual errors that mark the so-called “historic turn” in management studies, this article argues that the holding contradictory positions is not an accidental by-product of the “historic turn”. Rather, it is a defining characteristic of the genre.}
}
@article{ALDABET2021101224,
title = {Enhancing Arabic aspect-based sentiment analysis using deep learning models},
journal = {Computer Speech & Language},
volume = {69},
pages = {101224},
year = {2021},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2021.101224},
url = {https://www.sciencedirect.com/science/article/pii/S0885230821000310},
author = {Saja Al-Dabet and Sara Tedmori and Mohammad AL-Smadi},
keywords = {Aspect-based sentiment analysis, Aspect-category identification, Aspect-sentiment classification, Deep learning, Arabic language},
abstract = {Aspect-based sentiment analysis is a special type of sentiment analysis that aims to identify the discussed aspects and their sentiment polarities in a given review. In this paper, two deep learning models are proposed to address essential aspect-based sentiment analysis tasks: aspect-category identification and aspect-sentiment classification. For the first task, an identification model is proposed based on a convolutional neural network and stacked independent long-short term memory. For the second task, a classification model is proposed based on stacked bidirectional independent long-short term memory, a position-weighting mechanism, and multiple attention mechanism layers. The proposed models are evaluated using the Arabic SemEval-2016 dataset for the Hotels domain. Experimental results demonstrate that the proposed models outperform the baseline and other models, where the first model, C-IndyLSTM, achieves an F1 measure of 58.08%, and the second model, MBRA, achieves an accuracy measure of 87.31%.}
}
@article{FENG2025145382,
title = {Application of MALDI-TOF MS-based peptidome profiling for the identification of Bacillus cereus, Staphylococcus aureus, and Escherichia coli in single and mixed inoculum},
journal = {Food Chemistry},
volume = {492},
pages = {145382},
year = {2025},
issn = {0308-8146},
doi = {https://doi.org/10.1016/j.foodchem.2025.145382},
url = {https://www.sciencedirect.com/science/article/pii/S0308814625026330},
author = {Ying Feng and Aswathi Soni and Gale Brightwell and Marlon M. Reis and Juan Wang and Qingping Wu and Jinxuan Cao and Yu Ding},
keywords = {Peptidome profile, Mixed-species, MALDI-TOF MS, Artificial intelligence, ResNet},
abstract = {The detection of mixed-species bacterial samples plays a vital role in ensuring food safety, yet research in this area remains notably limited. This study investigates the integration of MALDI-TOF MS-derived peptidome profiles with artificial intelligence (AI) to enable accurate identification of mixed bacterial species. The application of formic acid extraction, combined with a residual network (ResNet) significantly improved the prediction accuracy for mixed bacterial samples in multi-scale complex datasets, achieving 96.88 % accuracy in identifying strains from three species: Bacillus cereus, Staphylococcus aureus, and Escherichia coli. Furthermore, peptidome profiling annotation was performed using label-free proteomics based on liquid chromatography-tandem mass spectrometry (LC-MS/MS) and differential protein screening using a random forest-convolutional neural network (RF-CNN) further enhanced strain identification. This advanced strategy improves the detection of targeted strains in mixed samples, supporting MALDI-TOF MS applications in the food industry.}
}
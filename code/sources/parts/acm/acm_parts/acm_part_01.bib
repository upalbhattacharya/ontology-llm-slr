@inproceedings{10.1145/3706598.3713633,
author = {Haghighi, Nava and Yu, Sunny and Landay, James A. and Rosner, Daniela},
title = {Ontologies in Design: How Imagining a Tree Reveals Possibilities and Assumptions in Large Language Models},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713633},
doi = {10.1145/3706598.3713633},
abstract = {Amid the recent uptake of Generative AI, sociotechnical scholars and critics have traced a multitude of resulting harms, with analyses largely focused on values and axiology (e.g., bias). While value-based analyses are crucial, we argue that ontologies—concerning what we allow ourselves to think or talk about—is a vital but under-recognized dimension in analyzing these systems. Proposing a need for a practice-based engagement with ontologies, we offer four orientations for considering ontologies in design: pluralism, groundedness, liveliness, and enactment. We share examples of potentialities that are opened up through these orientations across the entire LLM development pipeline by conducting two ontological analyses: examining the responses of four LLM-based chatbots in a prompting exercise, and analyzing the architecture of an LLM-based agent simulation. We conclude by sharing opportunities and limitations of working with ontologies in the design and development of sociotechnical systems.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {244},
numpages = {20},
keywords = {ontological design, ontologies, generative AI, large language models, foundation models, LLM agents},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3696410.3714816,
author = {Liu, Zhiqiang and Gan, Chengtao and Wang, Junjie and Zhang, Yichi and Bo, Zhongpu and Sun, Mengshu and Chen, Huajun and Zhang, Wen},
title = {OntoTune: Ontology-Driven Self-training for Aligning Large Language Models},
year = {2025},
isbn = {9798400712746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696410.3714816},
doi = {10.1145/3696410.3714816},
abstract = {Existing domain-specific Large Language Models (LLMs) are typically developed by fine-tuning general-purposed LLMs with large-scale domain-specific corpora. However, training on large-scale corpora often fails to effectively organize domain knowledge of LLMs, leading to fragmented understanding. Inspired by how humans connect concepts and organize knowledge through mind maps, we aim to emulate this approach by using ontology with hierarchical conceptual knowledge to reorganize LLM's domain knowledge. From this perspective, we propose an ontology-driven self-training framework called OntoTune, which aims to align LLMs with ontology through in-context learning, enabling the generation of responses guided by the ontology. We leverage in-context learning to identify whether the LLM has acquired the specific concept's ontology knowledge, and select the entries not yet mastered by LLM as the training set to further align the LLM with ontology. Compared to existing domain LLMs based on newly collected large-scale domain-specific corpora, our OntoTune, which relies on the existing, long-term developed ontology and LLM itself, significantly reduces data maintenance costs and offers improved generalization ability. We conduct our study in the medical domain to evaluate the effectiveness of OntoTune, utilizing a standardized medical ontology, SNOMED CT as our ontology source. Experimental results demonstrate that OntoTune achieves state-of-the-art performance in both in-ontology task hypernym discovery and out-of-ontology task medical domain QA. Moreover, compared to the latest direct ontology injection method TaxoLLaMA, our OntoTune better preserves original knowledge of LLM. The code and data are available at https://github.com/zjukg/OntoTune.},
booktitle = {Proceedings of the ACM on Web Conference 2025},
pages = {119–133},
numpages = {15},
keywords = {align with ontology, large language model, self-training},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inbook{10.1145/3730436.3730532,
author = {Zheng, Hanqi and Ouyang, Guige and Huang, Yongzhong},
title = {An Unsupervised Ontology Construction Method Based on Pre-trained Language Model},
year = {2025},
isbn = {9798400713637},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3730436.3730532},
abstract = {With the fast growth of text data, the importance of automatic ontology construction has grown significantly‎. The proposed article provides a novel approach by applying pre-trained ‎language models to automatically construct the ontology. The framework consists of two sequential phases: automatic concept discovery and automatic relation discovery. In the context of automatic concept discovery, instances and their embedding vectors ‎are extracted first through Named Entity Recognition (NER). Then, the unsupervised affinity ‎propagation (AP) clustering algorithm is applied to classify these embedding vectors, resulting in the discovery of the ‎concepts. A denoising method is discussed to obtain higher accuracy with respect to the concepts obtained to reduce noise caused by complete clustering. Related to automatic relation discovery between concepts (mentioned in the next section), the process ‎generates inexplicable concepts that are contextually similar based on the embedded vectors of instances mapping ‎over the two entities. This can enable unsupervised automatic discovery of relations between the contextually related concepts. This method shows a certain feasibility and achieves early effectiveness in unsupervised automatic ontology construction with the experimental results. ‎},
booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence and Computational Intelligence},
pages = {588–595},
numpages = {8}
}

@inproceedings{10.1145/3672608.3707840,
author = {Shin, Yong-Jun and Utz, Wilfrid},
title = {A Platform-Independent Software-Intensive Workflow Modeling Language And An Open-Source Visual Programming Tool: A Bottom-Up Approach Using Ontology Integration Of Industrial Workflow Engines},
year = {2025},
isbn = {9798400706295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672608.3707840},
doi = {10.1145/3672608.3707840},
abstract = {Many contemporary software-intensive services are developed as workflows of collaborative and interdependent tasks. Industrial workflow platforms (i.e., engines) such as Airflow and Kubeflow automatically execute and monitor the workflow specified in platform-specific code. The code-based workflow specification becomes complex and error-prone as services grow in complexity. Furthermore, differences in platform-specific workflow specifications cause inefficiencies when porting workflows between platforms, even if the different platforms handle semantically the same workflow.In this paper, we propose a bottom-up approach for developing a platform-independent software-intensive workflow modeling language. The approach systematically extends the UML activity diagram by building platform-independent ontologies of the workflow specification from the given target industrial workflow engines. Based on the approach, we develop a platform-independent Workflow Modeling Language (WorkflowML) that covers four famous workflow engines (Airflow, Kubeflow, Argo workflow, and Metaflow). Furthermore, we implement an open-source visual programming tool for WorkflowML using the ADOxx metamodeling platform. We validate our approach by evaluating the expressiveness of WorkflowML based on modeling case studies of 42 simple workflows and two real-case workflow-based services. The evaluation results validate that WorkflowML serves as an effective common visual language for target workflow engines, supported by an open-source visual programming tool.},
booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing},
pages = {1421–1430},
numpages = {10},
keywords = {workflow, domain-specific modeling language, metamodeling, visual programming, tool, ADOxx, ontology},
location = {Catania International Airport, Catania, Italy},
series = {SAC '25}
}

@inproceedings{10.1145/3671127.3698792,
author = {Mulayim, Ozan Baris and Paul, Lazlo and Pritoni, Marco and Prakash, Anand Krishnan and Sudarshan, Malavikha and Fierro, Gabe},
title = {Large Language Models for the Creation and Use of Semantic Ontologies in Buildings: Requirements and Challenges},
year = {2024},
isbn = {9798400707063},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3671127.3698792},
doi = {10.1145/3671127.3698792},
abstract = {Semantic ontologies offer a formalized, machine-readable framework for representing knowledge, enabling the structured description of complex systems. In the building domain, the adoption of ontologies like the Brick schema has transformed how buildings and their systems are modeled by providing a standardized, interoperable language. However, the complexity and the steep learning curve involved in developing and querying semantic models present substantial challenges, often requiring a workforce with specialized expertise. This paper builds on our experience in investigating how Large Language Models (LLMs) can help address these challenges, focusing on their role in constructing and querying of semantic models, particularly using the Brick Schema. Our study outlines the requirements and metrics for evaluating the scalability and effectiveness of LLM-based tools, while also discussing the current challenges and limitations in developing such tools. Ultimately, this paper aims to orient research efforts as various groups experiment with diverse techniques, while enabling more effective comparison of emerging solutions and fostering collaboration across the field.},
booktitle = {Proceedings of the 11th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {312–317},
numpages = {6},
keywords = {Knowledge Graphs, Large Language Models, Semantic Ontology},
location = {Hangzhou, China},
series = {BuildSys '24}
}

@article{10.1145/3757923,
author = {Meloni, Antonello and Reforgiato Recupero, Diego and Osborne, Francesco and salatino, angelo and Motta, Enrico and Vahadati, Sahar and Lehmann, Jens},
title = {Exploring Large Language Models for Scientific Question Answering via Natural Language to SPARQL Translation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3757923},
doi = {10.1145/3757923},
abstract = {Translating scientific questions expressed in natural language into SPARQL queries that can be executed over knowledge graphs remains a significant challenge in the field of question answering. Recently, several prominent benchmarks, notably SciQA and DBLP-QuAD, have emerged to evaluate performance in this domain. In this paper, we provide a comprehensive analysis of the performance of language models on these benchmarks, assessing various optimization strategies. Our results indicate that the combined use of fine-tuning and prompting techniques, especially when incorporating strategic few-shot selection, produces excellent results on both benchmarks. These findings underscore an urgent need for more challenging benchmarks to better assess model capabilities. We identify key insights, common error patterns, and potential opportunities for transfer learning, and we discuss their implications for optimizing the performance of large language models in knowledge graph-based question answering tasks.},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
keywords = {Knowledge graphs, Question answering, Language models, Fine-tuning, Few-shot learning}
}

@inproceedings{10.1145/3637528.3671745,
author = {Komarlu, Tanay and Jiang, Minhao and Wang, Xuan and Han, Jiawei},
title = {OntoType: Ontology-Guided and Pre-Trained Language Model Assisted Fine-Grained Entity Typing},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671745},
doi = {10.1145/3637528.3671745},
abstract = {Fine-grained entity typing (FET), which assigns entities in text with context-sensitive, fine-grained semantic types, is a basic but important task for knowledge extraction from unstructured text. FET has been studied extensively in natural language processing and typically relies on human-annotated corpora for training, which is costly and difficult to scale. Recent studies explore the utilization of pre-trained language models (PLMs) as a knowledge base to generate rich and context-aware weak supervision for FET. However, a PLM still requires direction and guidance to serve as a knowledge base as they often generate a mixture of rough and fine-grained types, or tokens unsuitable for typing. In this study, we vision that an ontology provides a semantics-rich, hierarchical structure, which will help select the best results generated by multiple PLM models and head words. Specifically, we propose a novel annotation-free, ontology-guided FET method, OntoType, which follows a type ontological structure, from coarse to fine, ensembles multiple PLM prompting results to generate a set of type candidates, and refines its type resolution, under the local context with a natural language inference model. Our experiments on the Ontonotes, FIGER, and NYT datasets using their associated ontological structures demonstrate that our method outperforms the state-of-the-art zero-shot fine-grained entity typing methods as well as a typical LLM method, ChatGPT. Our error analysis shows that refinement of the existing ontology structures will further improve fine-grained entity typing.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1407–1417},
numpages = {11},
keywords = {fine-grained entity typing, masked language model prompting, natural language understanding, zero-shot entity typing},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.5555/3635637.3663238,
author = {Zhang, Shiyao and Dong, Yuji and Zhang, Yichuan and Payne, Terry R. and Zhang, Jie},
title = {Large Language Model Assissted Multi-Agent Dialogue for Ontology Alignment},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Ontology alignment is critical in cross-domain integration; however, it typically necessitates the involvement of a human domain-expert, which can make the task costly. Although a variety of machine-learning approaches have been proposed that can simplify this task by learning the patterns from experts, such techniques are still susceptible to domain knowledge updates that could potentially change the patterns and lead to extra expert involvement. The use of Large Language Models (LLMs) has demonstrated a general cognitive ability, which has the potential to assist ontology alignment from the cognition level, thus obviating the need for costly expert involvement. However, the process by which the output of LLMs is generated can be opaque and thus the reliability and interpretability of such models is not always predictable. This paper proposes a dialogue model, in which multiple agents negotiate the correspondence between two knowledge sets with the support from an LLM. We demonstrate that this approach not only reduces the need for the involvement of a domain expert for ontology alignment, but that the results are interpretable despite the use of LLMs.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {2594–2596},
numpages = {3},
keywords = {dialogue, large language model, multi-agent system, negotiation, ontology alignment},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@inproceedings{10.1145/3587259.3627571,
author = {Hertling, Sven and Paulheim, Heiko},
title = {OLaLa: Ontology Matching with Large Language Models},
year = {2023},
isbn = {9798400701412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587259.3627571},
doi = {10.1145/3587259.3627571},
abstract = {Ontology (and more generally: Knowledge Graph) Matching is a challenging task where information in natural language is one of the most important signals to process. With the rise of Large Language Models, it is possible to incorporate this knowledge in a better way into the matching pipeline. A number of decisions still need to be taken, e.g., how to generate a prompt that is useful to the model, how information in the KG can be formulated in prompts, which Large Language Model to choose, how to provide existing correspondences to the model, how to generate candidates, etc. In this paper, we present a prototype that explores these questions by applying zero-shot and few-shot prompting with multiple open Large Language Models to different tasks of the Ontology Alignment Evaluation Initiative (OAEI). We show that with only a handful of examples and a well-designed prompt, it is possible to achieve results that are en par with supervised matching systems which use a much larger portion of the ground truth.},
booktitle = {Proceedings of the 12th Knowledge Capture Conference 2023},
pages = {131–139},
numpages = {9},
keywords = {Entity Resolution, Large Language Model, Ontology Matching},
location = {Pensacola, FL, USA},
series = {K-CAP '23}
}

@article{10.1145/3676280,
author = {O'leary, Daniel E.},
title = {Using Large Language Models for Armchair Auditors},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
url = {https://doi.org/10.1145/3676280},
doi = {10.1145/3676280},
abstract = {Armchair auditors are citizens who use open data to investigate and monitor government activities, typically using analytics and other approaches. Armchair auditors provide a valuable role in holding governments and organizations accountable. This paper investigates the potential use of large language models (LLM) to support armchair auditor analyses of different governmental entities. Unfortunately, the literature, prior to the development of LLM suggested several challenges for armchair auditors. However, the analysis in this paper suggests that LLM can provide substantial data and analytic process support for armchair auditors mitigating issues such as providing guidelines for analyses, guiding users to appropriate communities, suggesting potential data availability opportunities, doing analysis, and other issues. As part of an approach to unifying armchair auditor searches, this paper also suggests a prompt library designed to support, standardize and promote best practice analyzes among armchair auditors. In addition to these issues, this paper also analyzes emerging ethical issues associated with armchair auditors and their use of open data and LLMs. Finally, this paper extends the activity theory model to account for LLMs.},
journal = {Digit. Gov.: Res. Pract.},
month = jun,
articleno = {28},
numpages = {13},
keywords = {Activity theory, large language models, armchair auditor, open data, ChatGPT, BARD}
}

@article{10.1145/3735632,
author = {Li, Jiawei and Gao, Yang and Yang, Yizhe and Bai, Yu and Zhou, Xiaofeng and Li, Yinghao and Sun, Huashan and Liu, Yuhang and Si, Xingpeng and Ye, Yuhao and Wu, Yixiao and Lin, Yiguan and Xu, Bin and Ren, Bowen and Feng, Chong and Huang, Heyan},
title = {Fundamental Capabilities and Applications of Large Language Models: A Survey},
year = {2025},
issue_date = {January 2026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {58},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3735632},
doi = {10.1145/3735632},
abstract = {Large Language Models (LLMs) have demonstrated remarkable effectiveness across various domain-specific applications. However, which fundamental capabilities most contribute to their success in different domains remains unclear. This uncertainty complicates LLM evaluation, as existing benchmark-based assessments often fail to capture their real-world performance, where the required capabilities may differ from those measured in the benchmarks. In this survey, we provide a systematic introduction to LLMs’ fundamental capabilities, encompassing their definitions, formation mechanisms, and practical applications. We further explore the relationships among these capabilities and discuss how they collectively support complex problem-solving in domain-specific applications. Building on this foundation, we review recent advances in LLM-driven applications across nine specific domains: medicine, law, computational biology, finance, social sciences and psychology, computer programming and software engineering, robots and agents, AI for disciplines, and creative work. We analyze how specific capabilities are leveraged for each domain to address unique requirements. This perspective enables us to establish connections between these capabilities and domain requirements, and to evaluate the varying importance of different capabilities across different domains. Based on these insights, we propose evaluation strategies tailored to the essential capabilities required in each domain, offering practical guidance for selecting suitable backbone LLMs in real-world applications.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {38},
numpages = {42},
keywords = {Large language model, fundamental capabilities, applications}
}

@article{10.1145/3764579,
author = {Ling, Chen and Zhao, Xujiang and Lu, Jiaying and Deng, Chengyuan and Zheng, Can and Wang, Junxiang and Chowdhury, Tanmoy and Li, Yun and Cui, Hejie and Zhang, Xuchao and Zhao, Tianjiao and Panalkar, Amit and Mehta, Dhagash and Pasquali, Stefano and Cheng, Wei and Wang, Haoyu and Liu, Yanchi and Chen, Zhengzhang and Chen, Haifeng and White, Chris and Gu, Quanquan and Pei, Jian and Yang, Carl and Zhao, Liang},
title = {Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3764579},
doi = {10.1145/3764579},
abstract = {Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). Domain specification techniques are key to making large language models disruptive in many applications. Specifically, to solve these hurdles, there has been a notable increase in research and practices conducted in recent years on the domain specialization of LLMs. This emerging field of study, with its substantial potential for impact, necessitates a comprehensive and systematic review to summarize better and guide ongoing work in this area. In this article, we present a comprehensive survey on domain specification techniques for large language models, an emerging direction critical for large language model applications. First, we propose a systematic taxonomy that categorizes the LLM domain-specialization techniques based on the accessibility to LLMs and summarizes the framework for all the subcategories as well as their relations and differences to each other. Second, we present an extensive taxonomy of critical application domains that can benefit dramatically from specialized LLMs, discussing their practical significance and open challenges. Last, we offer our insights into the current research status and future trends in this area.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = sep,
keywords = {Large Language Models, Natural Language Processing, Domain Specialization}
}

@inproceedings{10.1145/3711896.3736557,
author = {Jiang, Pengcheng and Ouyang, Siru and Jiao, Yizhu and Zhong, Ming and Tian, Runchu and Han, Jiawei},
title = {Retrieval And Structuring Augmented Generation with Large Language Models},
year = {2025},
isbn = {9798400714542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711896.3736557},
doi = {10.1145/3711896.3736557},
abstract = {Large Language Models (LLMs) have revolutionized natural language processing with their remarkable capabilities in text generation and reasoning. However, these models face critical challenges when deployed in real-world applications, including hallucination generation, outdated knowledge, and limited domain expertise. Retrieval And Structuring (RAS) Augmented Generation addresses these limitations by integrating dynamic information retrieval with structured knowledge representations. This survey (1) examines retrieval mechanisms including sparse, dense, and hybrid approaches for accessing external knowledge; (2) explore text structuring techniques such as taxonomy construction, hierarchical classification, and information extraction that transform unstructured text into organized representations; and (3) investigate how these structured representations integrate with LLMs through prompt-based methods, reasoning frameworks, and knowledge embedding techniques. It also identifies technical challenges in retrieval efficiency, structure quality, and knowledge integration, while highlighting research opportunities in multimodal retrieval, cross-lingual structures, and interactive systems. This comprehensive overview provides researchers and practitioners with insights into RAS methods, applications, and future directions.},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
pages = {6032–6042},
numpages = {11},
keywords = {information retrieval, knowledge representation, large language models},
location = {Toronto ON, Canada},
series = {KDD '25}
}

@inproceedings{10.1145/3706468.3706566,
author = {Yeung, Steven},
title = {A comparative study of rule-based, machine learning and large language model approaches in automated writing evaluation (AWE)},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706566},
doi = {10.1145/3706468.3706566},
abstract = {Automated Writing Evaluation (AWE) tools have proved beneficial to writing development. Research on AWE methods is essential for improving tool performance and further comparative studies are needed as new methods emerge. This study examines the performance of several AWE approaches, comparing rule-based and statistical methods, machine learning (ML) models, and a large language model (LLM). These three AWE methods were applied to a representative sample of academic essays from the TOEFL11 dataset to compare their assessment performance. Results show that the selected LLM, GPT-4, outperformed the other two approaches in terms of QWK and Pearson’s correlation coefficient, while the Support Vector Machine (SVM) model in the ML approach had the highest accuracy and the lowest mean absolute error. This paper provides a detailed comparison of these three approaches and discusses implications for educational practice and future research around AWE.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {984–991},
numpages = {8},
keywords = {Rule-based method, machine learning, large language model, automated writing evaluation, automated essay scoring, generative AI},
location = {
},
series = {LAK '25}
}

@article{10.1145/3715318,
author = {Zhang, Qiang and Ding, Keyan and Lv, Tianwen and Wang, Xinda and Yin, Qingyu and Zhang, Yiwen and Yu, Jing and Wang, Yuhao and Li, Xiaotong and Xiang, Zhuoyi and Zhuang, Xiang and Wang, Zeyuan and Qin, Ming and Zhang, Mengyao and Zhang, Jinlu and Cui, Jiyu and Xu, Renjun and Chen, Hongyang and Fan, Xiaohui and Xing, Huabin and Chen, Huajun},
title = {Scientific Large Language Models: A Survey on Biological \&amp; Chemical Domains},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3715318},
doi = {10.1145/3715318},
abstract = {Large Language Models (LLMs) have emerged as a transformative power in enhancing natural language comprehension, representing a significant stride toward artificial general intelligence. The application of LLMs extends beyond conventional linguistic boundaries, encompassing specialized linguistic systems developed within various scientific disciplines. This growing interest has led to the advent of scientific LLMs, a novel subclass specifically engineered for facilitating scientific discovery. As a burgeoning area in the community of AI for Science, scientific LLMs warrant comprehensive exploration. However, a systematic and up-to-date survey introducing them is currently lacking. In this article, we endeavor to methodically delineate the concept of “scientific language,” whilst providing a thorough review of the latest advancements in scientific LLMs. Given the expansive realm of scientific disciplines, our analysis adopts a focused lens, concentrating on the biological and chemical domains. This includes an in-depth examination of LLMs for textual knowledge, small molecules, macromolecular proteins, genomic sequences, and their combinations, analyzing them in terms of model architectures, capabilities, datasets, and evaluation. Finally, we critically examine the prevailing challenges and point out promising research directions along with the advances of LLMs. By offering a comprehensive overview of technical developments in this field, this survey aspires to be an invaluable resource for researchers navigating the intricate landscape of scientific LLMs.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {161},
numpages = {38},
keywords = {Scientific domain, large language models, protein, molecule, genome}
}

@article{10.1145/3733719,
author = {Kreikemeyer, Justin Noah and Jankowski, Mi\l{}osz and Wilsdorf, Pia and Uhrmacher, Adelinde M.},
title = {Using (Not-so) Large Language Models to Generate Simulation Models in a Formal DSL: A Study on Reaction Networks},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-3301},
url = {https://doi.org/10.1145/3733719},
doi = {10.1145/3733719},
abstract = {Formal languages are an integral part of modeling and simulation. They allow the distillation of knowledge into concise simulation models amenable to automatic execution, interpretation, and analysis. However, the arguably most humanly accessible means of expressing models is through natural language, which is not easily interpretable by computers. Here, we evaluate how a Large Language Model (LLM) might be used for formalizing natural language into simulation models. Existing studies only explored using very large LLMs, like the commercial GPT models, without fine-tuning model weights. To close this gap, we show how an open-weights, 7B-parameter Mistral model can be fine-tuned to translate natural language descriptions to reaction network models in a domain-specific language, offering a self-hostable, compute-, and memory efficient alternative. To this end, we develop a synthetic data generator to serve as the basis for fine-tuning and evaluation. Our quantitative evaluation shows that our fine-tuned Mistral model can recover the ground truth simulation model in up to  (84.5\% )  of cases. In addition, our small-scale user study demonstrates the model’s practical potential for one-time generation as well as interactive modeling in various domains. While promising, in its current form, the fine-tuned small LLM cannot catch up with large LLMs. We conclude that higher-quality training data are required, and expect future small and open-source LLMs to offer new opportunities.},
note = {Just Accepted},
journal = {ACM Trans. Model. Comput. Simul.},
month = may,
keywords = {simulation model generation, natural language processing, language model, constrained decoding, knowledge extraction}
}

@inproceedings{10.1145/3587259.3627560,
author = {Schneider, Florian and Dash, Sarthak and Bagchi, Sugato and Mihindukulasooriya, Nandana and Gliozzo, Alfio Massimiliano},
title = {NLFOA: Natural Language Focused Ontology Alignment},
year = {2023},
isbn = {9798400701412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587259.3627560},
doi = {10.1145/3587259.3627560},
abstract = {For Ontology Alignment (OA), the task is to align semantically equivalent concepts and relations from different ontologies. This task plays a crucial role in many downstream tasks and applications in academia and industry. Since manually aligning ontologies is inefficient and costly, numerous approaches exist to do this automatically. However, most approaches are tailored to specific domains, are rule-based systems or based on feature engineering, and require external knowledge. The most recent advances in the field of OA rely on the widely proven effectiveness of pre-trained language models to represent the human-generated language that describes the entities in an ontology. However, these approaches additionally require sophisticated algorithms or Graph Neural Networks to exploit an ontology’s graphical structure to achieve state-of-the-art performance. In this work, we present NLFOA, or Natural Language Focused Ontology Alignment, which purely focuses on the natural language contained in ontologies to process the ontology’s semantics as well as graphical structure. An evaluation of our approach on common OA datasets shows superior results when finetuning with only a small number of training samples. Additionally, it demonstrates strong results in a zero-shot setting which could be employed in an active learning setup to reduce human labor when manually aligning ontologies significantly.},
booktitle = {Proceedings of the 12th Knowledge Capture Conference 2023},
pages = {114–121},
numpages = {8},
keywords = {Ontology Alignment Sentence Transformers Zero-Shot},
location = {Pensacola, FL, USA},
series = {K-CAP '23}
}

@article{10.1145/3762669,
author = {Arazzi, Marco and Marconi Sciarroni, Monica and Nocera, Antonino and Storti, Emanuele},
title = {RAG-IoE: IoT context-aware information retrieval with Large Language Models in Industry 5.0},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3762669},
doi = {10.1145/3762669},
abstract = {Human-centric design, intelligence, and seamless interconnectivity are key pillars of the Industry 5.0. A critical challenge in these scenarios is the efficient retrieval of relevant, context-aware information for workers within Internet of Everything (IoE) networks. Traditional information retrieval techniques struggle with the heterogeneous, dynamic data generated in industrial settings. To address this, we define a context-aware data model for IoE scenarios, on top of which we propose RAG-IoE, a novel Retrieval-Augmented Generation (RAG) solution to enable adaptive, scalable, and context-based information retrieval from both structured and unstructured data sources. Our approach organizes IoE data within a semantic framework, integrating hybrid retrieval methods. It combines structured search on a Knowledge Graph with unstructured data retrieval using embeddings stored in a vector database, followed by LLM-driven reasoning to refine results. This architecture enhances decision-making, reduces cognitive overload, and ensures precise guidance for industrial operators. We validate the efficiency and effectiveness of RAG-IoE using a novel dataset through both a user study and quantitative analysis, demonstrating its potential to optimize human-machine collaboration in Industry 5.0 environments.},
note = {Just Accepted},
journal = {ACM Trans. Internet Things},
month = aug,
keywords = {Context-aware, Knowledge Graph, Large Language Model, Retrieval-Augmented Generation, IoT, IoE, Industry 5.0}
}

@article{10.1145/3735553,
author = {Li, Tao and Cui, Chenhui and Huang, Rubing and Towey, Dave and Ma, Lei},
title = {Large Language Models for Automated Web-Form-Test Generation: An Empirical Study},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3735553},
doi = {10.1145/3735553},
abstract = {Testing web forms is an essential activity for ensuring the quality of web applications. It typically involves evaluating the interactions between users and forms. Automated test-case generation remains a challenge for web-form testing: Due to the complex, multi-level structure of web pages, it can be difficult to automatically capture their inherent contextual information for inclusion in the tests. Large Language Models (LLMs) have shown great potential for contextual text generation. This motivated us to explore how they could generate automated tests for web forms, making use of the contextual information within form elements. To the best of our knowledge, no comparative study examining different LLMs has yet been reported for web-form-test generation. To address this gap in the literature, we conducted a comprehensive empirical study investigating the effectiveness of 11 LLMs on 146 web forms from 30 open-source Java web applications. In addition, we propose three HTML-structure-pruning methods to extract key contextual information. The experimental results show that different LLMs can achieve different testing effectiveness, with the GPT-4, GLM-4, and Baichuan2 LLMs generating the best web-form tests. Compared with GPT-4, the other LLMs had difficulty generating appropriate tests for the web forms: Their successfully-submitted rates (SSRs) — the proportions of the LLMs-generated web-form tests that could be successfully inserted into the web forms and submitted — decreased by 9.10\% to 74.15\%. Our findings also show that, for all LLMs, when the designed prompts include complete and clear contextual information about the web forms, more effective web-form tests were generated. Specifically, when using Parser-Processed HTML for Task Prompt (PH-P), the SSR averaged 70.63\%, higher than the 60.21\% for Raw HTML for Task Prompt (RH-P) and 50.27\% for LLM-Processed HTML for Task Prompt (LH-P). With RH-P, GPT-4’s SSR was 98.86\%, outperforming models like LLaMa2 (7B) with 34.47\% and GLM-4V with 0\%. Similarly, with PH-P, GPT-4 reached an SSR of 99.54\%, the highest among all models and prompt types. Finally, this paper also highlights strategies for selecting LLMs based on performance metrics, and for optimizing the prompt design to improve the quality of the web-form tests.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
keywords = {Automated Web-Form Testing, Large Language Models (LLMs), Web-Form-Test Generation, Java Web Applications, Empirical Study}
}

@inproceedings{10.1145/3722565.3727197,
author = {Mulayim, Ozan Baris and Fierro, Gabe and Berg\'{e}s, Mario and Pritoni, Marco},
title = {Towards Zero-shot Question Answering in CPS-IoT: Large Language Models and Knowledge Graphs},
year = {2025},
isbn = {9798400716089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3722565.3727197},
doi = {10.1145/3722565.3727197},
abstract = {Natural language provides an intuitive interface for querying data, yet its unstructured nature often makes precise retrieval of information challenging. Knowledge graphs (KGs), with their structured and relational representations, offer a powerful solution to structuring knowledge, while large language models (LLMs) are capable of interpreting user intent through language. This combination of KGs and LLMs has been explored extensively for Knowledge Graph Question Answering (KGQA), primarily for open-domain or encyclopedic knowledge. Domain-specific KGQA, instead, presents significant opportunities for Cyber-Physical Systems (CPS) and the Internet of Things (IoT), where the extraction of structured metadata is essential for automation and scalability of control and analytics applications.In this work, we evaluate and improve AutoKGQA, a domain-independent KGQA framework that utilizes LLMs to generate structured queries. Through a case study on KGs of sensor data from buildings, we assess its ability to retrieve time series identifiers, which are a requirement for extracting time series data from large sensory databases. Our results demonstrate that while AutoKGQA performs well in certain cases, its domain-agnostic approach leads to systematic failures particularly in complex queries requiring implicit knowledge. We show that domain-specific prompting significantly enhances query accuracy, allowing even smaller LLMs to perform on par with larger ones. These findings highlight the impact of domain-adapted prompting in KGQA (DA-KGQA) and suggest a path toward more efficient, scalable, and interpretable AI-driven metadata retrieval for CPS-IoT applications.},
booktitle = {Proceedings of the 2nd International Workshop on Foundation Models for Cyber-Physical Systems \&amp; Internet of Things},
pages = {7–12},
numpages = {6},
keywords = {Knowledge Graphs, Large Language Models, Time series Extraction},
location = {Irvine, CA, USA},
series = {FMSys}
}

@inproceedings{10.1145/3701716.3717817,
author = {Jadhav, Suramya and Perumal, Suki and Tadavi, Yasmin and Dash, Bikshita and Parthiban, Srinivasan},
title = {Leveraging Large Language Models for Biomedical Knowledge Graph Construction and Querying: An Advanced NLP Approach},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3717817},
doi = {10.1145/3701716.3717817},
abstract = {This paper introduces a novel methodology for constructing a comprehensive biomedical knowledge graph by applying advanced Natural Language Processing (NLP) techniques. By leveraging Large Language Models (LLMs) and a multifaceted prompt engineering approach, we effectively perform Named Entity Recognition (NER) and Relation Extraction (RE) on biomedical literature, targeting entities such as diseases, drugs, proteins, procedures, and symptoms. Our methodology incorporates eight distinct prompt engineering strategies for NER and a standardized approach for RE, facilitating the extraction of intricate inter-entity relationships. The resulting knowledge graph amalgamates diverse data sources into a unified framework, enabling efficient querying, visualization, and analysis of biomedical information. Furthermore, we present an innovative query processing pipeline that integrates GPT-3.5 turbo with the knowledge graph, allowing users to interact with the graph through natural language. This integrated system empowers the discovery of novel correlations, accelerating scientific research and fostering interdisciplinary collaboration. This represents a substantial contribution to the field of biomedical knowledge graph construction, offering a robust platform for accelerating scientific discovery and informing clinical decision-making.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {2560–2566},
numpages = {7},
keywords = {knowledge graphs, large language models, named entity recognition, prompt engineering, query processing, relationship extraction},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3701716.3717820,
author = {Chen, Haoting and Rodr\'{\i}guez M\'{e}ndez, Sergio Jos\'{e} and Omran, Pouya Ghiasnezhad},
title = {Open Local Knowledge Graph Construction from Academic Papers Using Generative Large Language Models},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3717820},
doi = {10.1145/3701716.3717820},
abstract = {This manuscript introduces paper2lkg, a novel Local Knowledge Graph Construction (KGC) pipeline designed to transform individual academic papers into their structured local Knowledge Graph (KG) representations. The pipeline harnesses Large Language Models (LLMs), particularly generative LLMs, to automate key Natural Language Processing (NLP) tasks in KGC. The constructed local KGs can potentially be used to enrich an existing academic KG that lacks detailed local representations of individual papers or further integrated into new academic KGs through Knowledge Graph Alignment (KGA).},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {2551–2559},
numpages = {9},
keywords = {knowledge graph construction, large language model, natural language processing},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3711896.3737384,
author = {Huang, Tenghao and Lee, Dong Hee and Sweeney, John and Shi, Jiatong and Steliotes, Emily and Lange, Matthew and May, Jonathan and Chen, Muhao},
title = {FoodPuzzle: Toward Developing Large Language Model Agents as Autonomous Flavor Scientists},
year = {2025},
isbn = {9798400714542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711896.3737384},
doi = {10.1145/3711896.3737384},
abstract = {Flavor development in the food industry is increasingly challenged by the need for rapid innovation and precise flavor profile creation. Traditional flavor research methods typically rely on iterative, subjective testing, which lacks the efficiency and scalability required for modern demands. This paper presents three contributions to address these challenges. Firstly, we define a new problem domain for scientific agents in flavor science, conceptualized as the generation of hypotheses for flavor profile sourcing and understanding. By leveraging their capacity to identify relevant evidence and reason within large context spaces, language model-backed agents can perform the labor-intensive tasks of flavor sourcing and understanding with enhanced efficiency and precision. To facilitate research in this area, we introduce the FoodPuzzle dataset, a challenging benchmark consisting of 978 food items and 1,766 flavor molecule profiles. We propose a novel Scientific Agent approach, integrating in-context learning and retrieval augmented techniques to generate grounded hypotheses in the domain of food science. Experimental results indicate that our model significantly surpasses traditional methods in flavor profile prediction tasks, demonstrating its potential to transform flavor development practices.},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
pages = {5493–5504},
numpages = {12},
keywords = {agent, flavor science, in-context learning, large language models, retrieval-augmented generation},
location = {Toronto ON, Canada},
series = {KDD '25}
}

@article{10.1145/3725856,
author = {Civitarese, Gabriele and Fiori, Michele and Choudhary, Priyankar and Bettini, Claudio},
title = {Large Language Models Are Zero-Shot Recognizers for Activities of Daily Living},
year = {2025},
issue_date = {August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3725856},
doi = {10.1145/3725856},
abstract = {The sensor-based recognition of Activities of Daily Living (ADLs) in smart home environments enables several applications in the areas of energy management, safety, well-being, and healthcare. ADL recognition is typically based on deep learning methods requiring large datasets to be trained. Recently, several studies proved that Large Language Models (LLMs) effectively capture common-sense knowledge about human activities. However, the effectiveness of LLMs for ADL recognition in smart home environments still deserves to be investigated. In this work, we propose ADL-LLM, a novel LLM-based ADL recognition system. ADL-LLM transforms raw sensor data into textual representations, that are processed by an LLM to perform zero-shot ADL recognition. Moreover, in the scenario where a small labeled dataset is available, ADL-LLM can also be empowered with few-shot prompting. We evaluated ADL-LLM on two public datasets, showing its effectiveness in this domain.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jun,
articleno = {78},
numpages = {32},
keywords = {Human Activity Recognition, Large Language Models, Smart Home, Activities of Daily Living}
}

@article{10.1145/3756016,
author = {Vasic, Iva and Fill, Hans-Georg and Quattrini, Ramona and Pierdicca, Roberto},
title = {Knowledge Graphs vs. Large Language Models: Competitors or Partners in Supporting Virtual Museums},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1556-4673},
url = {https://doi.org/10.1145/3756016},
doi = {10.1145/3756016},
abstract = {Virtual museums are factual means for the dissemination and documentation of Cultural Heritage (CH) content. They are suitable environments for the semantic annotation of artifacts and automatic virtual guides. To this end, we identify and compare Traditional (ontology-based), Large Language Model (LLM)-extended, and LLM-pure methods for the semantic information strategies of digital CH. The traditional method is described through an application prototype, while the methods that involve LLM are tested experimentally. To investigate the integral tasks related to LLMs, our experiments include (i) semantic annotation using the CIDOC Conceptual Reference Model (CRM) and Knowledge Graph (KG) generation with LLMs for a painting sample, and (ii) painting ranking relying solely on LLMs using catalog descriptions as input. The experiments demonstrate the potential of these methods to enhance artwork interpretation, description, and refinement of the results. Based on the relevant literature on traditional semantic annotation and conducted experiments with LLMs, a combination of ontologies and LLMs may provide an optimal approach, as it offers the accuracy of structured knowledge while providing a tool that interprets these elements into natural language and vice versa. Relying solely on LLMs may be risky due to the lack of domain-specific knowledge in the training data of LLMs, whereas traditional methods demand expertise in a specific domain and are more time-consuming. Our approach shows potential in use cases such as guiding museum visitors to artifacts that match their interests, assisting museum curators with documentation, or helping CH researchers identify similarities in artifact collections.},
note = {Just Accepted},
journal = {J. Comput. Cult. Herit.},
month = jul,
keywords = {Large Language Model, Cultural Heritage, Semantic Annotation, Knowledge Graphs}
}

@article{10.1145/3682069,
author = {Ampel, Benjamin and Yang, Chi-Heng and Hu, James and Chen, Hsinchun},
title = {Large Language Models for Conducting Advanced Text Analytics Information Systems Research},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3682069},
doi = {10.1145/3682069},
abstract = {The exponential growth of digital content has generated massive textual datasets, necessitating the use of advanced analytical approaches. Large Language Models (LLMs) have emerged as tools that are capable of processing and extracting insights from massive unstructured textual datasets. However, how to leverage LLMs for text analytics Information Systems (IS) research is currently unclear. To assist the IS community in understanding how to operationalize LLMs, we propose a Text Analytics for Information Systems Research (TAISR) framework. Our proposed framework provides detailed recommendations grounded in IS and LLM literature on how to conduct meaningful text analytics IS research for design science, behavioral, and econometric streams. We conducted three business intelligence case studies using our TAISR framework to demonstrate its application in several IS research contexts. We also outline the potential challenges and limitations of adopting LLMs for IS. By offering a systematic approach and evidence of its utility, our TAISR framework contributes to future IS research streams looking to incorporate powerful LLMs for text analytics.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = feb,
articleno = {2},
numpages = {27},
keywords = {Large language models, information systems research, text analytics}
}

@inbook{10.1145/3677389.3702562,
author = {Xilong, Hou and Junhan, Zang and Xiaoguang, Wang},
title = {Leveraging Large Language Models for Classification of Cultural Heritage Domain Terms: A Case Study on CIDOC CRM},
year = {2025},
isbn = {9798400710933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677389.3702562},
abstract = {Large language models (LLMs) have recently revolutionized human language understanding and generation. Ontology is considered one of the primary cornerstones for representing knowledge in a more meaningful way on the semantic web. It's significant to explore whether LLMs know and understand such ontological knowledge. In this paper, we report an experiment to investigate the performance of LLMs in the task of classifying cultural heritage domain terms to upper-level ontology. We first probed the understanding and memorization of CIDOC CRM ontological knowledge by LLMs. Then, we further leverage LLMs to classify domain terms into the structure of CRM, and compare the match type with experts. Our initial findings indicate that LLMs demonstrate a certain level of awareness and comprehension of CIDOC CRM ontological knowledge. LLMs have shown potential as valuable assistants in enhancing ontology engineering and knowledge-intensive tasks.},
booktitle = {Proceedings of the 24th ACM/IEEE Joint Conference on Digital Libraries},
articleno = {59},
numpages = {5}
}

@inproceedings{10.1145/3635059.3635104,
author = {Karanikolas, Nikitas and Manga, Eirini and Samaridi, Nikoletta and Tousidou, Eleni and Vassilakopoulos, Michael},
title = {Large Language Models versus Natural Language Understanding and Generation},
year = {2024},
isbn = {9798400716263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3635059.3635104},
doi = {10.1145/3635059.3635104},
abstract = {In recent years, the process humans adopt to learn a foreign language has moved from the strict "Grammar –Translation" method, which is based mainly on grammar and syntax rules, to more innovative processes, resulting to the more modern "Communicative approach". As its name states, this approach focuses on the coherent communication with native speakers and the cultivation of oral skills, without taking into consideration, at least at the first stages, the rules that govern the language. The same trend seems to have been applied to the way machinery can be "educated" to comprehend and reproduce the unfamiliar, human language. The "rule based" Natural Language Generation (NLG) and Natural Language Understanding (NLU) algorithms, on one hand, and the "text based" Large Language Models (LLMs), on the other, are two, analogous to the two human foreign language learning processes, subareas of Natural Language Processing (NLP). This paper presents these two alternative approaches, LLMs (a technology having surfaced as an influential catalyst of NLP, during last years) on the one hand and NLG/NLU on the other, highlighting their applications, their technologies, their capabilities, their differences, their strengths and weaknesses and the challenges they present, contributing to a deeper comprehension of the evolving landscape of Artificial Intelligence and human-computer communication.},
booktitle = {Proceedings of the 27th Pan-Hellenic Conference on Progress in Computing and Informatics},
pages = {278–290},
numpages = {13},
keywords = {Large Language Models, Natural Language Generation, Natural Language Processing, Natural Language Understanding},
location = {Lamia, Greece},
series = {PCI '23}
}

@inproceedings{10.1145/3726302.3729970,
author = {Xiong, Qiushi and Xu, Zhipeng and Liu, Zhenghao and Wang, Mengjia and Chen, Zulong and Sun, Yue and Gu, Yu and Li, Xiaohua and Yu, Ge},
title = {Enhancing the Patent Matching Capability of Large Language Models via the Memory Graph},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3729970},
doi = {10.1145/3726302.3729970},
abstract = {Intellectual Property (IP) management involves strategically protecting and utilizing intellectual assets to enhance organizational innovation, competitiveness, and value creation. Patent matching is a crucial task in intellectual property management, which facilitates the organization and utilization of patents. Existing models often rely on the emergent capabilities of Large Language Models (LLMs) and leverage them to identify related patents directly. However, these methods usually depend on matching keywords and overlook the hierarchical classification and categorical relationships of patents. In this paper, we propose MemGraph, a method that augments the patent matching capabilities of LLMs by incorporating a memory graph derived from their parametric memory. Specifically, MemGraph prompts LLMs to traverse their memory to identify relevant entities within patents, followed by attributing these entities to corresponding ontologies. After traversing the memory graph, we utilize extracted entities and ontologies to improve the capability of LLM in comprehending the semantics of patents. Experimental results on the PatentMatch dataset demonstrate the effectiveness of MemGraph, achieving a 17.68\% performance improvement over baseline LLMs. The further analysis highlights the generalization ability of MemGraph across various LLMs, both in-domain and out-of-domain, and its capacity to enhance the internal reasoning processes of LLMs during patent matching. All data and codes are available at https://github.com/NEUIR/MemGraph.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {337–347},
numpages = {11},
keywords = {large language models, memory graph, patent matching, retrieval-augmented generation},
location = {Padua, Italy},
series = {SIGIR '25}
}

@inproceedings{10.1145/3688671.3688770,
author = {Saketos, Vasileios and Pantazi, Despina-Athanasia and Koubarakis, Manolis},
title = {The Large Language Model GreekLegalRoBERTa},
year = {2024},
isbn = {9798400709821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3688671.3688770},
doi = {10.1145/3688671.3688770},
abstract = {We develop four versions of GreekLegalRoBERTa, which are four large language models trained on Greek legal and nonlegal text. We show that our models surpass the performance of GreekLegalBERT, Greek- LegalBERT-v2, and GreekBERT in two tasks involving Greek legal documents: named entity recognition and multi-class legal topic classification. We view our work as a contribution to the study of domain-specific NLP tasks in low-resource languages, like Greek, using modern NLP techniques and methodologies.},
booktitle = {Proceedings of the 13th Hellenic Conference on Artificial Intelligence},
articleno = {18},
numpages = {7},
keywords = {Natural Language Processing, Pre-trained Language Models, Greek NLP Resources, Greek Legislation, Classification, Named Entity Recognition},
location = {
},
series = {SETN '24}
}

@inproceedings{10.5555/3709347.3743843,
author = {Torshizi, Parisa Ghanad and Hensel, Laura B. and Shapiro, Ari and Marsella, Stacy C.},
title = {Large Language Models for Virtual Human Gesture Selection},
year = {2025},
isbn = {9798400714269},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Co-speech gestures convey a wide variety of meanings and play an important role in face-to-face human interactions. These gestures have been shown to significantly influence the addressee's engagement, recall, comprehension, and attitudes toward the speaker. Similarly, they have been shown to impact human and embodied virtual agent interaction. The process of selecting and animating meaningful gestures has thus become a key focus in designing embodied virtual agents. However, the automation of this gesture selection process poses a significant challenge. Prior gesture generation techniques have attempted to address this challenge in varied ways from fully automated, data-driven techniques -- which often struggle to produce contextually meaningful gestures -- to more manual approaches of crafting gesture expertise, which are time-consuming and lack generalizability. In this paper, we leverage the semantic capabilities of Large Language Models to realize a gesture selection approach that suggests meaningful, appropriate co-speech gestures. We first illustrate the information on gestures encoded into GPT4. Then we perform a study to specifically evaluate alternative prompting approaches for their ability to select meaningful, contextually relevant gestures and to align them appropriately to the co-speech utterance. Finally, we detail and demonstrate how this approach has been implemented within a virtual agent system, automating the selection and subsequent animation of the selected gestures for human-agent interactions.},
booktitle = {Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems},
pages = {2051–2059},
numpages = {9},
keywords = {gesture selection, large language models, virtual humans},
location = {Detroit, MI, USA},
series = {AAMAS '25}
}

@article{10.1145/3711012,
author = {Liu, Yiren and Li, Yerong and Mayfield, Ryan and Huang, Yun},
title = {Improving Emotional Support Delivery in Text-Based Community Safety Reporting Using Large Language Models},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
url = {https://doi.org/10.1145/3711012},
doi = {10.1145/3711012},
abstract = {Emotional support is a crucial aspect of communication between community members and police dispatchers during incident reporting. However, there is a lack of understanding about how emotional support is delivered through text-based systems, especially in various non-emergency contexts. In this study, we analyzed two years of chat logs comprising 57,114 messages across 8,239 incidents from 130 higher education institutions. Our empirical findings revealed significant variations in emotional support provided by dispatchers, influenced by the type of incident, service time, and a noticeable decline in support over time across multiple organizations. To improve the consistency and quality of emotional support, we developed and implemented a fine-tuned Large Language Model (LLM), named dispatcherLLM, designed to suggest replies through simulating human dispatchers' languages with appropriate emotional support. We evaluated dispatcherLLM by comparing its generated responses to those of human dispatchers and other off-the-shelf models using real chat messages. Additionally, we conducted a human evaluation to assess the perceived effectiveness of the support provided by dispatcherLLM. This study not only contributes new empirical understandings of emotional support in text-based dispatch systems but also demonstrates the significant potential of generative AI in improving service delivery.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {CSCW114},
numpages = {31},
keywords = {emotion classification, event argument extraction, large language models, live chat, safety reporting, text-based reporting system}
}

@inproceedings{10.1145/3652620.3686246,
author = {Siddeshwar, Vaishali and Alwidian, Sanaa and Makrehchi, Masoud},
title = {A Comparative Study of Large Language Models for Goal Model Extraction},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3686246},
doi = {10.1145/3652620.3686246},
abstract = {User stories, expressed in snippets of natural language text, are commonly used to elicit stakeholder's needs in agile software development. Requirement engineers model user stories to interpret the relations among goals and requirements. Manual transformation of goal models has challenges such as, difficulty of converting lower-abstraction user stories into higher-level goals, and extraction of goals embedded in user stories depends on the skill of requirements engineers. In this paper we introduce a technique that leverages Large Language Models (LLMs) to automatically generate goal models from user stories. The approach uses Iterative Prompt Engineering that guides LLM to extract intentional elements and generate its XML-compatible representation in Goal-oriented Requirements Language (GRL). The generated models can be visualized using jUCMNav tool. We evaluated our approach using three LLMs: GPT-4, Llama and Cohere. Our qualitative evaluation indicates that GPT-4 or Llama can be used to assist requirements engineers in modeling as they can produce GRL goal models that are understandable. Additionally, these LLMs are capable of exposing soft goals that are not apparent to stakeholders who are new to the domain.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {253–263},
numpages = {11},
keywords = {goal-oriented requirement language (GRL), goal modeling, user story, agile development, requirements engineering, large language models (LLMS), GPT-4, llama, cohere},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@article{10.1145/3735645,
author = {Zhou, Yinghai and Wang, Ziyu and Jiang, Yunxin and Ma, Bingqi and Wang, Rui and Liu, Yuan and Zhao, Yue and Tian, Zhihong},
title = {AEKG4APT: An AI-Enhanced Knowledge Graph for Advanced Persistent Threats with Large Language Model Analysis},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3735645},
doi = {10.1145/3735645},
abstract = {This paper introduces AEKG4APT, an APT Knowledge Graph (KG) enhanced by Large Language Models (LLMs), as a way to deal with the cybersecurity problems caused by Advanced Persistent Threats (APTs). The core of AEKG4APT lies in the combined application of LLMs, Cyber Threat Intelligence (CTI), and KG. The first part of the paper goes into great detail about how the AEKG4APT was constructed, including its ontology schema, data sources, and dataset features. There are also statistics on the AEKG4APT’s nodes, relationships, and key attributes. Secondly, it was shown how to utilize LLMs and public sandboxes for the collection and analysis of CTI Additionally, tests that compare traditional deep learning models to LLM methods show that LLM is both more efficient and more accurate at extracting information. Subsequently, the Decision Making Trial and Evaluation Laboratory - Interpretive Structural Modeling (DEMATEL-ISM) analytical method was introduced to identify and analyse the factors and their interrelationships within the AEKG4APT data, thereby revealing the key dependencies and influence paths within the data structure. Experiments were designed to demonstrate its applications in modeling, computing, and obtaining interpretable computational results on AEKG4APT. In addition, this paper also explores the dynamic expansion capabilities of AEKG4APT, including data expansion, schema expansion, and permanent maintenance strategies, to address the evolving APT threats. Finally, this paper summarizes the competitiveness and application value of AEKG4APT by comparing it with other CTI KGs and platforms in academia and industry, demonstrating its extensive application potential in the field of cybersecurity.},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
keywords = {Advanced Persistent Threat, Large Language Models, Knowledge Graph, Cyber Threat Intelligence, Sandboxes, DEMATEL-ISM}
}

@article{10.1145/3725852,
author = {Bombieri, Marco and Fiorini, Paolo and Ponzetto, Simone Paolo and Rospocher, Marco},
title = {Do LLMs Dream of Ontologies?},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3725852},
doi = {10.1145/3725852},
abstract = {Large Language Models (LLMs) have demonstrated remarkable performance across diverse natural language processing tasks, yet their ability to memorize structured knowledge remains underexplored. In this paper, we investigate the extent to which general-purpose pre-trained LLMs retain and correctly reproduce concept identifier (ID)–label associations from publicly available ontologies. We conduct a systematic evaluation across multiple ontological resources, including the Gene Ontology, Uberon, Wikidata, and ICD-10, using LLMs such as Pythia-12B, Gemini-1.5-Flash, GPT-3.5, and GPT-4. Our findings reveal that only a small fraction of ontological concepts is accurately memorized, with GPT-4 demonstrating the highest performance. To understand why certain concepts are memorized more effectively than others, we analyze the relationship between memorization accuracy and concept popularity on the Web. Our results indicate a strong correlation between the frequency of a concept’s occurrence online and the likelihood of accurately retrieving its ID from the label. This suggests that LLMs primarily acquire such knowledge through indirect textual exposure rather than directly from structured ontological resources. Furthermore, we introduce new metrics to quantify prediction invariance, demonstrating that the stability of model responses across variations in prompt language and temperature settings can serve as a proxy for estimating memorization robustness.},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
keywords = {Large Language Models, Memorization, Ontologies}
}

@inproceedings{10.1145/3716554.3716558,
author = {Maslaris, Ioannis and Karamanou, Areti and Kalampokis, Evangelos and Tarabanis, Konstantinos},
title = {Evaluating Large Language Models in Interaction with Open Government Data},
year = {2025},
isbn = {9798400713170},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716554.3716558},
doi = {10.1145/3716554.3716558},
abstract = {Large Language Models (LLMs) exhibit great abilities in understanding and generating natural language. Open Government Data (OGD) are datasets that while are available to the public, their linked structure makes it difficult to access. Large language models can significantly enhance access to linked open government data by enabling users to interact with OGD portals using natural language. This study examines the use of LLMs to interact with open government linked data effectively and efficiently. Based on the QB vocabulary, we develop a framework that formulates the task. A set of 20 questions is developed to assess the capabilities of LLMs to execute OGD-related tasks. We propose a simple system in which LLMs interact semi-automatically with OGD. Our findings indicate that smaller and quantized versions of popular LLMs are capable of effectively managing these tasks, with Llama-3.1-8B-Instruct-bnb-4bit identified as the most effective model. This paper aims to promote further interest in systems that enhance Open Government Data portals and improve public access to open data.},
booktitle = {Proceedings of the 28th Pan-Hellenic Conference on Progress in Computing and Informatics},
pages = {26–33},
numpages = {8},
keywords = {large language models, open government data, linked data, natural language processing},
location = {
},
series = {PCI '24}
}

@inproceedings{10.1145/3638584.3638635,
author = {Zhou, Yifan and Ding, Yizhou and Dong, Yuwu and He, Hao},
title = {Ontology-Semantic Alignment On Contrastive Video-Language Model for Multimodel Video Retrieval Task},
year = {2024},
isbn = {9798400708688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638584.3638635},
doi = {10.1145/3638584.3638635},
abstract = {Contrastive Learning-based models have shown impressive performance in text-image retrieval tasks. However, when applied in video retrieval, traditional contrastive learning strategies have faced challenges in achieving satisfactory results due to redundancy of video contents. We discern several potential reasons: (1)Current methodologies sometimes overlook the significant information imbalance between videos and query text, specifically neglecting the in-depth textual representation of the content within the videos. (2) Current video matching methodologies typically focus on cross-model alignment at general entity similarity level, without specific consideration for how entity pair preferences and similarity properties affect the task at hand. (3) Previous vectorized retrieval based on video content features have been somewhat flawed. They primarily focused on aligning overall features without having an video content tags feature for meaningful feature discrimination. Considering the shortcomings identified in the mentioned three aspects, we propose an ontology semantic labels augments retrieval model and introduce a method to integrate video ontology semantic labels into the contrastive learning framework. In particular, we have developed ontology semantic descriptions about entities encompassing both human figures and textual elements within the videos. Subsequently, we conducted training and testing on the CMIVQA dataset to assess the performance of our approach. The experimental results show that employing fine-grained ontology labels as sample pairs for contrastive learning leads to an increased level of precision in video retrieval tasks.},
booktitle = {Proceedings of the 2023 7th International Conference on Computer Science and Artificial Intelligence},
pages = {408–413},
numpages = {6},
keywords = {Multimodal alignment, Ontology description, Video content understanding},
location = {Beijing, China},
series = {CSAI '23}
}

@article{10.1145/3748302,
author = {Zhang, Zeyu and Dai, Quanyu and Bo, Xiaohe and Ma, Chen and Li, Rui and Chen, Xu and Zhu, Jieming and Dong, Zhenhua and Wen, Ji-Rong},
title = {A Survey on the Memory Mechanism of Large Language Model based Agents},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1046-8188},
url = {https://doi.org/10.1145/3748302},
doi = {10.1145/3748302},
abstract = {Large language model (LLM) based agents have recently attracted much attention from the research and industry communities. Compared with original LLMs, LLM-based agents are featured in their self-evolving capability, which is the basis for solving real-world problems that need long-term and complex agent-environment interactions. The key component to support agent-environment interactions is the memory of the agents. While previous studies have proposed many promising memory mechanisms, they are scattered in different papers, and there lacks a systematical review to summarize and compare these works from a holistic perspective, failing to abstract common and effective designing patterns for inspiring future studies. To bridge this gap, in this paper, we propose a comprehensive survey on the memory mechanism of LLM-based agents. In specific, we first discuss “what is” and “why do we need” the memory in LLM-based agents. Then, we systematically review previous studies on how to design and evaluate the memory module. In addition, we also present many agent applications, where the memory module plays an important role. At last, we analyze the limitations of existing work and show important future directions. To keep up with the latest advances in this field, we create a repository at .},
note = {Just Accepted},
journal = {ACM Trans. Inf. Syst.},
month = jul,
keywords = {Information Processing, Information System, Large Language Model, Agent, Memory Mechanism}
}

@inproceedings{10.1145/3711896.3736556,
author = {Xu, Ran and Jiang, Patrick and Luo, Linhao and Xiao, Cao and Cross, Adam and Pan, Shirui and Sun, Jimeng and Yang, Carl},
title = {A Survey on Unifying Large Language Models and Knowledge Graphs for Biomedicine and Healthcare},
year = {2025},
isbn = {9798400714542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711896.3736556},
doi = {10.1145/3711896.3736556},
abstract = {In recent years, the landscape of digital biomedicine and healthcare has been reshaped due to the disruptive breakthroughs in AI-facilitated by tremendous data and high-performance computers, large language models (LLMs) have transformed information technology from accessing data to performing analytical tasks. While demonstrating unprecedented capabilities, LLMs have been found unreliable in tasks requiring factual knowledge and rigorous reasoning. Biomedicine and healthcare, as an important vertical domain rapidly benefitting from progress in AI, necessitates strict requirements on the accuracy, controllability, and interpretability of analytical models, posing critical challenges for LLMs. Despite recent studies addressing the hallucination problem of LLMs, research on empowering LLMs with the ability to plan, reason, and ground with explicit knowledge has also started to prosper, especially in the biomedicine and healthcare domain. On the other hand, biomedical data are enormous and notoriously complex, coming from various sources (e.g., biomedical knowledge bases, online literature, and hospitals) and bearing various modalities (e.g., tables, texts, images and time-series). Healthcare professionals have spent decades collecting, cleaning, and curating various types of data. The processes are extremely costly, producing various datasets with different data schemas, coding systems, and quality standards, many privately owned by the creators, making their integrative analysis and utilization through unified AI techniques still rather challenging. The generalizability of LLMs across different types of data endow them strong promises in automating the processing of large-scale complex healthcare data such as into unified knowledge graphs (KGs). Our goal in this survey is to systematically investigate and summarize recent studies on the unification of LLMs and KGs, towards fully utilizing the value of complex data, unleashing the power of generative AI, and expediting next-generation AI for biomedicine and healthcare applications.},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
pages = {6195–6205},
numpages = {11},
keywords = {biomedical sciences, health informatics, knowledge graph, large language model},
location = {Toronto ON, Canada},
series = {KDD '25}
}

@inproceedings{10.1145/3715275.3732129,
author = {Kocyigit, Emre and Rossi, Arianna and Sergeeva, Anastasia and Negri Ribalta, Claudia and Farjami, Ali and Lenzini, Gabriele},
title = {DeceptiLens: an Approach supporting Transparency in Deceptive Pattern Detection based on a Multimodal Large Language Model},
year = {2025},
isbn = {9798400714825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715275.3732129},
doi = {10.1145/3715275.3732129},
abstract = {To detect deceptive design patterns on UIs, traditional artificial intelligence models, such as machine learning, have limited coverage and a lack of multimodality. In contrast, the capabilities of Multimodal Large Language Model (MM-LLM) can achieve wider coverage with superior performance in the detection, while providing reasoning behind each decision. We propose and implement an MM-LLM-based approach (DeceptiLens) that analyzes UIs and assesses the presence of deceptive design patterns. We utilize Retrieval Augmented Generation (RAG) process in our design and task the model with capturing the deceptive patterns, classifying its category, e.g., false hierarchy, confirmshaming, etc., and explaining the reasoning behind the classifications by employing recent prompt engineering techniques, such as Chain-of-Thought (CoT). We first create a dataset by collecting UI screenshots from the literature and web sources and quantify the agreement between the model’s outputs and a few experts’ opinions. We additionally ask experts to gauge the transparency of the system’s explanations for its classifications in terms of recognized metrics of clarity, correctness, completeness, and verifiability. The results indicate that our approach is capable of capturing the deceptive patterns in UIs with high accuracy while providing clear, correct, complete, and verifiable justifications for its decisions. We additionally release two curated datasets, one with expert-labeled UIs with deceptive design patterns, and one with AI-based generated explanations. Lastly, we propose recommendations for future improvement of the approach in various contexts of use.},
booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1942–1959},
numpages = {18},
keywords = {dark patterns, deceptive design patterns, LLMs, multimodal LLMs},
location = {
},
series = {FAccT '25}
}

@article{10.1145/3639372,
author = {Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
title = {Explainability for Large Language Models: A Survey},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3639372},
doi = {10.1145/3639372},
abstract = {Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this article, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional deep learning models.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {20},
numpages = {38},
keywords = {Explainability, interpretability, large language models}
}

@article{10.1145/3721128,
author = {Zhang, Quanjun and Fang, Chunrong and Zheng, Yi and Zhang, Yaxin and Zhao, Yuan and Huang, Rubing and Zhou, Jianyi and Yang, Yun and Zheng, Tao and Chen, Zhenyu},
title = {Improving Deep Assertion Generation via Fine-Tuning Retrieval-Augmented Pre-Trained Language Models},
year = {2025},
issue_date = {September 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3721128},
doi = {10.1145/3721128},
abstract = {Unit testing validates the correctness of the units of the software system under test and serves as the cornerstone in improving software quality and reliability. To reduce manual efforts in writing unit tests, some techniques have been proposed to generate test assertions automatically, including Deep Learning (DL)-based, retrieval-based, and integration-based ones. Among them, recent integration-based approaches inherit from both DL-based and retrieval-based approaches and are considered state-of-the-art. Despite being promising, such integration-based approaches suffer from inherent limitations, such as retrieving assertions with lexical matching while ignoring meaningful code semantics and generating assertions with a limited training corpus.In this article, we propose a novel Retrieval-Augmented Deep Assertion Generation (RetriGen) approach based on a hybrid assertion retriever and a Pre-Trained Language Model (PLM)-based assertion generator. Given a focal-test, RetriGen first builds a hybrid assertion retriever to search for the most relevant test–assert pair from external codebases. The retrieval process takes both lexical similarity and semantical similarity into account via a token-based and an embedding-based retriever, respectively. RetriGen then treats assertion generation as a sequence-to-sequence task and designs a PLM-based assertion generator to predict a correct assertion with historical test–assert pairs and the retrieved external assertion. Although our concept is general and can be adapted to various off-the-shelf encoder–decoder PLMs, we implement RetriGen to facilitate assertion generation based on the recent CodeT5 model. We conduct extensive experiments to evaluate RetriGen against six state-of-the-art approaches across two large-scale datasets and two metrics. The experimental results demonstrate that RetriGen achieves 57.66\% and 73.24\% in terms of accuracy and CodeBLEU, outperforming all baselines with an average improvement of 50.66\% and 14.14\%, respectively. Furthermore, RetriGen generates 1,598 and 1,818 unique correct assertions that all baselines fail to produce, 3.71X and 4.58X more than the most recent approach EditAS. We also demonstrate that adopting other PLMs can provide substantial advancement, e.g., four additionally utilized PLMs outperform EditAS by 7.91\%–12.70\% accuracy improvement, indicating the generalizability of RetriGen. Overall, our study highlights the promising future of fine-tuning off-the-shelf PLMs to generate accurate assertions by incorporating external knowledge sources.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {209},
numpages = {23},
keywords = {Unit Testing, Assertion Generation, Pre-trained Language Models, AI4SE}
}

@inproceedings{10.1145/3706598.3713307,
author = {Haag, David and Kumar, Devender and Gruber, Sebastian and Hofer, Dominik P., MSc and Sareban, Mahdi and Treff, Gunnar and Niebauer, Josef and Bull, Christopher N and Schmidt, Albrecht and Smeddinck, Jan David},
title = {The Last JITAI? Exploring Large Language Models for Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Prospective Cardiac Rehabilitation Setting},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713307},
doi = {10.1145/3706598.3713307},
abstract = {We evaluated the viability of using Large Language Models (LLMs) to trigger and personalize content in Just-in-Time Adaptive Interventions (JITAIs) in digital health. As an interaction pattern representative of context-aware computing, JITAIs are being explored for their potential to support sustainable behavior change, adapting interventions to an individual's current context and needs. Challenging traditional JITAI implementation models, which face severe scalability and flexibility limitations, we tested GPT-4 for suggesting JITAIs in the use case of heart-healthy activity in cardiac rehabilitation. Using three personas representing patients affected by CVD with varying severeness and five context sets per persona, we generated 450 JITAI decisions and messages. These were systematically evaluated against those created by 10 laypersons (LayPs) and 10 healthcare professionals (HCPs). GPT-4-generated JITAIs surpassed human-generated intervention suggestions, outperforming both LayPs and HCPs across all metrics (i.e., appropriateness, engagement, effectiveness, and professionalism). These results highlight the potential of LLMs to enhance JITAI implementations in personalized health interventions, demonstrating how generative AI could revolutionize context-aware computing.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {644},
numpages = {18},
keywords = {JITAIs, LLMs, adaptive interventions, context-aware computing, digital health, generative AI, healthcare AI, human-AI interaction, just-in-time adaptive interventions, large language models},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3696410.3714720,
author = {Bouadi, Mohamed and Alavi, Arta and Benbernou, Salima and Ouziri, Mourad},
title = {Synergizing Large Language Models and Knowledge-Based Reasoning for Interpretable Feature Engineering},
year = {2025},
isbn = {9798400712746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696410.3714720},
doi = {10.1145/3696410.3714720},
abstract = {Feature engineering stands as a pivotal step in enhancing the performance of machine learning (ML) models, particularly with tabular data. However, traditional feature engineering methods are often time-consuming and requires case-by-case domain knowledge. In addition, as ML systems become more common, interpretability becomes increasingly important, especially among domain experts. To this end, we propose ReaGen, an automated feature engineering (AutoFE) approach that combines knowledge graphs (KGs) with large language models (LLMs) to generate interpretable features. ReaGen begins by symbolic REAsoning over the KG to extract relevant information based on datasets description. Then, it uses an LLM to iteratively GENerate meaningful features. Finally, to overcome challenges such as hallucinations and handling long contexts typical in LLMs, our model performs logical reasoning on the KG to ensure that the generated features maintain interpretability. ReaGen provides Python code for automatic feature generation and detailed explanations of feature utility. It leverages both LLM's internal knowledge and retrieved information from KGs. Experiments on public datasets demonstrate that ReaGen significantly improves prediction accuracy while ensuring high interpretability through human-like explanations for each feature. This work highlights the potential of integrating LLMs and KGs in feature engineering, paving the way for interpretable ML models.},
booktitle = {Proceedings of the ACM on Web Conference 2025},
pages = {2606–2620},
numpages = {15},
keywords = {automated feature engineering, interpretable ML, knowledge graphs, large language models, symbolic reasoning},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@article{10.1145/3700597,
author = {Ke, Ping Fan and Ng, Ka Chung},
title = {Human-AI Synergy in Survey Development: Implications from Large Language Models in Business and Research},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3700597},
doi = {10.1145/3700597},
abstract = {This study examines the novel integration of Large Language Models (LLMs) into the survey development process in business and research through the development and evaluation of the Behavioral Research Assistant (BRASS) Bot. We first analyzed the traditional scale development process to identify tasks suitable for LLM integration, including both human-in-the-loop and automated LLM data collection methods. Following this analysis, we developed the details of BRASS Bot, incorporating design principles of falsifiability and reproducibility. We then conducted a comprehensive evaluation of the BRASS Bot across a diverse set of LLMs, including GPT, Claude, Gemini, and Llama, to assess its usability, validity, and reliability. We further demonstrated the practical utility of the BRASS Bot by conducting a user study and a predictive validity simulation. Our research presents both theoretical and practical implications. The augmentation approach of the BRASS Bot enriches the theoretical foundations of behavioral constructs by identifying previously overlooked patterns. Additionally, the BRASS Bot offers significant time and resource efficiency gains while enhancing scale validity. Our work lays the foundation for future research on the broader application of LLMs as both assistants and collaborators in survey analysis and behavioral research design and execution, highlighting their potential for a transformative impact on the field.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = feb,
articleno = {9},
numpages = {39},
keywords = {Large Language Model, generative AI, scale development, behavioral research}
}

@inproceedings{10.1145/3627673.3679156,
author = {Peng, Yiwen and Bonald, Thomas and Alam, Mehwish},
title = {Refining Wikidata Taxonomy using Large Language Models},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3679156},
doi = {10.1145/3627673.3679156},
abstract = {Due to its collaborative nature, Wikidata is known to have a complex taxonomy, with recurrent issues like the ambiguity between instances and classes, the inaccuracy of some taxonomic paths, the presence of cycles, and the high level of redundancy across classes. Manual efforts to clean up this taxonomy are time-consuming and prone to errors or subjective decisions. We present WiKC, a new version of Wikidata taxonomy cleaned automatically using a combination of Large Language Models (LLMs) and graph mining techniques. Operations on the taxonomy, such as cutting links or merging classes, are performed with the help of zero-shot prompting on an open-source LLM. The quality of the refined taxonomy is evaluated from both intrinsic and extrinsic perspectives, on a task of entity typing for the latter, showing the practical interest of WiKC.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {5395–5399},
numpages = {5},
keywords = {graph mining, knowledge graphs, large language model},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3696410.3714667,
author = {Barile, Roberto and d'Amato, Claudia and Fanizzi, Nicola},
title = {LP-DIXIT: Evaluating Explanations for Link Predictions on Knowledge Graphs using Large Language Models},
year = {2025},
isbn = {9798400712746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696410.3714667},
doi = {10.1145/3696410.3714667},
abstract = {Knowledge Graphs provide a machine-readable representation of knowledge conforming to graph-based data models. Link prediction methods predict missing facts in incomplete knowledge graphs, often using scalable embedding based solutions that, however, lack comprehensibility which is crucial in many domains. Filling this gap, explanation methods identify supporting knowledge. For evaluating them, user studies are the obvious choice as users are the main recipients of explanations. However, finding domain experts is often challenging. In contrast, an automated approach is to measure the influence of explanations on the very same link prediction task, thus disregarding the perspective of users. Additionally, current evaluation methods vary across different explanation approaches. We propose LP-DIXIT, the first protocol to evaluate the utility of explanations of link predictions. LP-DIXIT is user-aware, algorithmic and unique for different explanation methods. It builds on a typical setting of user studies, but adopts Large Language Models (LLMs) to mimic users. Specifically, it measures how explanations improve the user (LLM) ability to perform predictions, which is key to trust. We experimentally proved an overall agreement between LP-DIXIT and user evaluations. Moreover, we adopted LP-DIXIT to conduct a comparative study of state-of-the-art explanation methods. The outcomes suggest that less is more: the most effective explanations are those consisting of a single fact.},
booktitle = {Proceedings of the ACM on Web Conference 2025},
pages = {4034–4042},
numpages = {9},
keywords = {explanation, knowledge graphs, large language models, link prediction},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3626772.3657848,
author = {Zhai, ChengXiang},
title = {Large Language Models and Future of Information Retrieval: Opportunities and Challenges},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657848},
doi = {10.1145/3626772.3657848},
abstract = {Recent years have seen great success of large language models (LLMs) in performing many natural language processing tasks with impressive performance, including tasks that directly serve users such as question answering and text summarization. They open up unprecedented opportunities for transforming information retrieval (IR) research and applications. However, concerns such as halluciation undermine their trustworthiness, limiting their actual utility when deployed in real-world applications, especially high-stake applications where trust is vital. How can we both exploit the strengths of LLMs and mitigate any risk caused by their weaknesses when applying LLMs to IR? What are the best opportunities for us to apply LLMs to IR? What are the major challenges that we will need to address in the future to fully exploit such opportunities? Given the anticipated growth of LLMs, what will future information retrieval systems look like? Will LLMs eventually replace an IR system? In this perspective paper, we examine these questions and provide provisional answers to them. We argue that LLMs will not be able to replace search engines, and future LLMs would need to learn how to use a search engine so that they can interact with a search engine on behalf of users. We conclude with a set of promising future research directions in applying LLMs to IR.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {481–490},
numpages = {10},
keywords = {conversational information access, information retrieval models, intelligent agent, large language models, search engines},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3706598.3714069,
author = {Alvarado Garcia, Adriana and Candello, Heloisa and Badillo-Urquiola, Karla and Wong-Villacres, Marisol},
title = {Emerging Data Practices: Data Work in the Era of Large Language Models},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714069},
doi = {10.1145/3706598.3714069},
abstract = {Data is one of the foundational aspects of making Artificial Intelligence (AI) work as intended. As large language models (LLMs) become the epicenter of AI, it is crucial to understand better how the datasets that maintain such models are created. The emergent nature of LLMs makes it critical to understand the challenges practitioners developing Gen AI technologies face to design alternatives for better responding to Gen AI’s ethical issues. In this paper, we provide such understanding by reporting on 25 interviews with practitioners who handle data in three distinct development stages of different LLMs. Our contributions are (1) empirical evidence of how uncertainty, data practices, and reliance mechanisms change across LLMs’ development cycle; (2) how the unique qualities of LLMs impact data practices and their implications for the future of Gen AI technologies; and (3) provide three opportunities for HCI researchers interested in supporting practitioners developing Gen AI technologies.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {846},
numpages = {21},
keywords = {data work, data practices, AI, LLMs, synthetic data, data governance, AI practitioners, GenAI, generative AI},
location = {
},
series = {CHI '25}
}

@article{10.1145/3736408,
author = {Koyuncu, Anil},
title = {Exploring Fine-Grained Bug Report Categorization with Large Language Models and Prompt Engineering: An Empirical Study},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3736408},
doi = {10.1145/3736408},
abstract = {Accurate classification of issues is essential for effective project management and timely responses, as the volume of issue reports continues to grow. Manual classification is labor-intensive and error-prone, necessitating automated solutions. While large language models (LLMs) show promise in automated issue labeling, most research focuses on broad categorization (e.g., bugs, feature requests), with limited attention to fine-grained categorization. Understanding specific bug types is crucial, as different bugs require tailored resolution strategies.This study addresses this gap by evaluating LLMs and prompt engineering strategies for fine-grained bug report categorization. We analyze 221,184 fine-grained bug report category labels generated by selected LLMs using various prompt engineering strategies for 1,024 bug reports. We examine how LLMs and prompt engineering influence output characteristics, control over outputs, and categorization performance. Our findings highlight that LLMs and prompt engineering significantly impact output consistency and classification capability, with some yielding consistent results and others introducing variability. Based on these findings, we analyze the agreements and disagreements between LLM-generated labels and human annotations to assess category correctness. Our results suggest that examining label consistency and discrepancies can serve as a complementary method for validating bug report categories, identifying unclear reports, and detecting misclassifications in human annotations.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
keywords = {Prompt Engineering, Large Language Models, Automatic Bug Report Classification, Label correctness}
}

@inproceedings{10.1145/3637528.3671469,
author = {Zhang, Yunyi and Zhong, Ming and Ouyang, Siru and Jiao, Yizhu and Zhou, Sizhe and Ding, Linyi and Han, Jiawei},
title = {Automated Mining of Structured Knowledge from Text in the Era of Large Language Models},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671469},
doi = {10.1145/3637528.3671469},
abstract = {Massive amount of unstructured text data are generated daily, ranging from news articles to scientific papers. How to mine structured knowledge from the text data remains a crucial research question. Recently, large language models (LLMs) have shed light on the text mining field with their superior text understanding and instruction-following ability. There are typically two ways of utilizing LLMs: fine-tune the LLMs with human-annotated training data, which is labor intensive and hard to scale; prompt the LLMs in a zero-shot or few-shot way, which cannot take advantage of the useful information in the massive text data. Therefore, it remains a challenge on automated mining of structured knowledge from massive text data in the era of large language models. In this tutorial, we cover the recent advancements in mining structured knowledge using language models with very weak supervision. We will introduce the following topics in this tutorial: (1) introduction to large language models, which serves as the foundation for recent text mining tasks, (2) ontology construction, which automatically enriches an ontology from a massive corpus, (3) weakly-supervised text classification in flat and hierarchical label space, (4) weakly-supervised information extraction, which extracts entity and relation structures.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {6644–6654},
numpages = {11},
keywords = {large language models, text mining, weak supervision},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3650400.3650478,
author = {Zhao, Wei and Chen, Qinghui and You, Junling},
title = {LlmRe: A zero-shot entity relation extraction method based on the large language model},
year = {2024},
isbn = {9798400708305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650400.3650478},
doi = {10.1145/3650400.3650478},
abstract = {Entity relation extraction aims to extract knowledge triples from unstructured or semi-structured text data and can be applied to various fields, including medicine, finance knowledge graph construction and intelligent question-answering. Traditional entity relation extraction requires a large amount of labeled data, consumes a lot of labor and time, and the trained model lacks generalization ability, which is difficult to migrate to other fields. Zero-shot entity relation extraction relieves the dependence on labeled data in traditional method. Based on unlabeled text data, zero-shot entity relation extraction has strong domain adaptability, which is a very challenging and practical task. Recent work on large language models shows that large models can effectively complete downstream tasks through natural language instructions and have good generalization ability. Inspired by this, we explore the use of large models for information extraction. Due to the randomness of large language model generation, we introduce in-context learning in entity relation extraction task to guide large language model to output data in a specified format to help obtain structured data. At the same time, we propose a three-stage extraction framework for decomposing entity relation extraction tasks, and each stage is conducted in the form of question and answer to reduce the complexity of extraction. We evaluated the knowledge triples extraction performance of the model on three self-built test datasets in different fields, and the experimental result showed that our proposed method achieved impressive performance in the zero-shot entity relation extraction task, surpassing the comparison model on multiple metrics, proving the effectiveness and domain adaptability of the proposed method.},
booktitle = {Proceedings of the 2023 7th International Conference on Electronic Information Technology and Computer Engineering},
pages = {475–480},
numpages = {6},
location = {Xiamen, China},
series = {EITCE '23}
}

@inproceedings{10.1145/3605098.3635889,
author = {Arrieta, Kutz and Fillottrani, Pablo R and Keet, C. Maria},
title = {CoSMo: A multilingual modular language for Content Selection Modelling},
year = {2024},
isbn = {9798400702433},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605098.3635889},
doi = {10.1145/3605098.3635889},
abstract = {Representing snippets of information abstractly is a task that needs to be performed for various purposes, such as database view specification and the first stage in the natural language generation pipeline for generative AI from structured input, i.e., the content selection stage to determine what needs to be verbalised. For the Abstract Wikipedia project, requirements analysis revealed that such an abstract representation requires multilingual modelling, content selection covering declarative content and functions, and both classes and instances. There is no modelling language that meets either of the three features, let alone a combination. Following a rigorous language design process inclusive of broad stakeholder consultation, we created CoSMo, a novel Content Selection Modeling language that meets these and other requirements so that it may be useful both in Abstract Wikipedia as well as other contexts. We describe the design process, rationale and choices, the specification, and preliminary evaluation of the language.},
booktitle = {Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing},
pages = {706–713},
numpages = {8},
keywords = {modeling language, query language, wikidata, multilingualism},
location = {Avila, Spain},
series = {SAC '24}
}

@inbook{10.1145/3727648.3727757,
author = {Hu, Ruijuan and Yue, Zhihui and Zhou, Yuzhen and Zhou, Huijuan},
title = {Research on the Representation and Construction Technique of Event Causality Graph Based on Large Language Models},
year = {2025},
isbn = {9798400712647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3727648.3727757},
abstract = {Event graph representation aims to construct a highly accurate and comprehensive event knowledge representation system. Existing event graph representation models lack adaptability in representing dynamic knowledge, which results in under utilization of their potential performance. This paper proposes an event causality graph representation model based on large language models(LLMs). By integrating dynamic event knowledge with static data knowledge, the model draws on the 5W1H theory and adopts a top-down approach combined with a bottom-up approach using LLMs retrieval augmented generation(RAG). This approach designs and constructs an event semantic representation model. Through matching the relevance of different argument roles in scenario knowledge, it achieves adaptive representation across different search spaces, thereby realizing the construction of an event causality graph. Experiments demonstrate the effectiveness of generating event types based on RAG and the dynamic adaptability of the constructed event causality graph.},
booktitle = {Proceedings of the 4th International Conference on Computer, Artificial Intelligence and Control Engineering},
pages = {672–679},
numpages = {8}
}

@article{10.1145/3652028,
author = {Spinner, Thilo and Kehlbeck, Rebecca and Sevastjanova, Rita and St\"{a}hle, Tobias and Keim, Daniel A. and Deussen, Oliver and El-Assady, Mennatallah},
title = {-generAItor: Tree-in-the-loop Text Generation for Language Model Explainability and Adaptation},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/3652028},
doi = {10.1145/3652028},
abstract = {Large language models (LLMs) are widely deployed in various downstream tasks, e.g., auto-completion, aided writing, or chat-based text generation. However, the considered output candidates of the underlying search algorithm are under-explored and under-explained. We tackle this shortcoming by proposing a tree-in-the-loop approach, where a visual representation of the beam search tree is the central component for analyzing, explaining, and adapting the generated outputs. To support these tasks, we present generAItor, a visual analytics technique, augmenting the central beam search tree with various task-specific widgets, providing targeted visualizations and interaction possibilities. Our approach allows interactions on multiple levels and offers an iterative pipeline that encompasses generating, exploring, and comparing output candidates, as well as fine-tuning the model based on adapted data. Our case study shows that our tool generates new insights in gender bias analysis beyond state-of-the-art template-based methods. Additionally, we demonstrate the applicability of our approach in a qualitative user study. Finally, we quantitatively evaluate the adaptability of the model to few samples, as occurring in text-generation use cases.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = jun,
articleno = {14},
numpages = {32},
keywords = {Large language models, beam search tree, natural language generation, explainability, language transformers, visual analytics}
}

@inproceedings{10.1145/3716554.3716603,
author = {Zeginis, Dimitris and Kalampokis, Evangelos and Tarabanis, Konstantinos},
title = {Applying an ontology-aware zero-shot LLM prompting approach for information extraction in Greek: the case of DIAVGEIA gov gr},
year = {2025},
isbn = {9798400713170},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716554.3716603},
doi = {10.1145/3716554.3716603},
abstract = {Large Language Models (LLMs) have attracted considerable attention, primarily due to their potential to revolutionize sectors that heavily rely on textual information. Governance is one such sector. Public administrations around the globe produce millions of documents including laws, administrative decisions and acts (e.g., travel/budget approvals) that contain valuable information in unstructured way. The documents are usually stored at document-centered repositories. As a result the actual data of the documents cannot be further searched or processed. The availability of structured metadata of the documents (e.g., who has traveled, where, when) could further enhance the searching and processing of the documents as well as enable data analytics. The construction of metadata can be done through information extraction approaches such as Named Entity Recognition (NER), Relation Extraction (RE) and Event Extraction (EE) on the documents. LLMs are recently used successfully for information extraction tasks, while ontologies are traditionally used for meaningful data modeling. The aim of the paper is to apply and evaluate an ontology-aware zero-shot LLM prompting approach for information extraction in Greek language documents available in DIAVGEIA.gov.gr - the Greek Open Government portal for administrative documents. The evaluation assesses various LLM models/sizes for various difficulties of information extraction tasks. Overall the results are very promising, since most LLM models, even smaller ones, performed very well for all tasks in Greek.},
booktitle = {Proceedings of the 28th Pan-Hellenic Conference on Progress in Computing and Informatics},
pages = {324–330},
numpages = {7},
keywords = {Information extraction, NER, LLM, Ontology, Greek, Public administration},
location = {
},
series = {PCI '24}
}

@article{10.1145/3702234,
author = {Fang, Chen and Wang, Yidong and Song, Yunze and Long, Qingqing and Lu, Wang and Chen, Linghui and Feng, Guihai and Zhou, Yuanchun and Li, Xin},
title = {How do Large Language Models understand Genes and Cells},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3702234},
doi = {10.1145/3702234},
abstract = {Researching genes and their interactions is crucial for deciphering the fundamental laws of cellular activity, advancing disease treatment, drug discovery, and more. Large language Models (LLMs), with their profound text comprehension and generation capabilities, have made significant strides across various natural science fields. However, their application in cell biology remains limited and a systematic evaluation of their performance is lacking. To address this gap, in this paper, we select seven mainstream LLMs and evaluate their performance across nine gene-related problem scenarios. Our findings indicate that LLMs possess a certain level of understanding of genes and cells, but still lag behind domain-specific models in comprehending transcriptional expression profiles. Moreover, we have improved the current method of textual representation of cells, enhancing the LLMs’ ability to tackle cell annotation tasks. We encourage cell biology researchers to leverage LLMs for problem-solving while being mindful of the associated challenges. We release our code and data at .},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
keywords = {large language models, cell biology, gene gene interaction, cell annotation}
}

@article{10.1145/3678470,
author = {Swaileh A. Alzaidi, Muhammad and Alshammari, Alya and Almanea, Manar and Al-khawaja, Haneen A. and Al Sultan, Hanan and Alotaibi, Shoayee and Almukadi, Wafa},
title = {A Text-Inception-Based Natural Language Processing Model for Sentiment Analysis of Drug Experiences},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3678470},
doi = {10.1145/3678470},
abstract = {The study of sentiment in Natural Language Processing (NLP) is among the most successful research areas because of the availability of millions of user opinions online since the turn of the century. The economic, political, and medical fields are just some of the many that have benefited from studies of sentiment research. While numerous studies have examined more mainstream topics like consumer electronics, movies, and restaurants, relatively few have examined health and medical concerns. Considerable insight into where to direct efforts to improve public health might be gained by a study of how people feel about healthcare as a whole and of individual drug experiences in particular. When it comes to medicine, automatic analysis of online user evaluations paves the way for sifting through massive amounts of user feedback to find information regarding medications' efficacy and side effects that might be used to enhance pharmacovigilance programs. Simple rules-based methods have given way to more complex machine learning approaches like deep learning, which is developing as a technology for many natural language processing jobs. The opensource datasets have been analyzed with models that use word embeddings and term frequency-inverse document frequency (TF-IDF). A feature-enhanced text-inception model for sentiment classification was presented to work in tandem with this approach. The model first employed a cutting-edge text-inception module to glean useful shallow features from the text. K-MaxPooling was subsequently employed to reduce the dimensionality of its shallow and deep includes as well as enhance the generalization of characteristics, and a deep feature extraction module was formed using the bidirectional gated recurrent unit (Bi-GRU) and the capsule neural network to comprehend the text's semantic data. By combining traditional methods with cutting-edge artificial intelligence techniques, this hybrid approach can revolutionize public health initiatives, decision-making, and pharmacovigilance in the healthcare industry. This model achieved an exceptional accuracy rate of 99\%, underscoring its effectiveness in sentiment classification and demonstrating its potential to significantly contribute to advancing healthcare and medical research.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
keywords = {Natural Language Processing (NLP), User Opinions, Healthcare, Medical Sentiment, Public Health, Deep learning}
}

@article{10.1145/3611651,
author = {Wang, Benyou and Xie, Qianqian and Pei, Jiahuan and Chen, Zhihong and Tiwari, Prayag and Li, Zhao and Fu, Jie},
title = {Pre-trained Language Models in Biomedical Domain: A Systematic Survey},
year = {2023},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3611651},
doi = {10.1145/3611651},
abstract = {Pre-trained language models (PLMs) have been the de facto paradigm for most natural language processing tasks. This also benefits the biomedical domain: researchers from informatics, medicine, and computer science communities propose various PLMs trained on biomedical datasets, e.g., biomedical text, electronic health records, protein, and DNA sequences for various biomedical tasks. However, the cross-discipline characteristics of biomedical PLMs hinder their spreading among communities; some existing works are isolated from each other without comprehensive comparison and discussions. It is nontrivial to make a survey that not only systematically reviews recent advances in biomedical PLMs and their applications but also standardizes terminology and benchmarks. This article summarizes the recent progress of pre-trained language models in the biomedical domain and their applications in downstream biomedical tasks. Particularly, we discuss the motivations of PLMs in the biomedical domain and introduce the key concepts of pre-trained language models. We then propose a taxonomy of existing biomedical PLMs that categorizes them from various perspectives systematically. Plus, their applications in biomedical downstream tasks are exhaustively discussed, respectively. Last, we illustrate various limitations and future trends, which aims to provide inspiration for the future research.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {55},
numpages = {52},
keywords = {Biomedical domain, pre-trained language models, natural language processing}
}

@article{10.1145/3605943,
author = {Min, Bonan and Ross, Hayley and Sulem, Elior and Veyseh, Amir Pouran Ben and Nguyen, Thien Huu and Sainz, Oscar and Agirre, Eneko and Heintz, Ilana and Roth, Dan},
title = {Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3605943},
doi = {10.1145/3605943},
abstract = {Large, pre-trained language models (PLMs) such as BERT and GPT have drastically changed the Natural Language Processing (NLP) field. For numerous NLP tasks, approaches leveraging PLMs have achieved state-of-the-art performance. The key idea is to learn a generic, latent representation of language from a generic task once, then share it across disparate NLP tasks. Language modeling serves as the generic task, one with abundant self-supervised text available for extensive training. This article presents the key fundamental concepts of PLM architectures and a comprehensive view of the shift to PLM-driven NLP techniques. It surveys work applying the pre-training then fine-tuning, prompting, and text generation approaches. In addition, it discusses PLM limitations and suggested directions for future research.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {30},
numpages = {40},
keywords = {Large language models, foundational models, generative AI, neural networks}
}

@inproceedings{10.1145/3640457.3688171,
author = {Ali, Zafar and Qi, Guilin and Ullah, Irfan and Mohammed, Adam A. Q. and Kefalas, Pavlos and Muhammad, Khan},
title = {GLAMOR: Graph-based LAnguage MOdel embedding for citation Recommendation},
year = {2024},
isbn = {9798400705052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640457.3688171},
doi = {10.1145/3640457.3688171},
abstract = {Digital publishing’s exponential growth has created vast scholarly collections. Guiding researchers to relevant resources is crucial, and knowledge graphs (KGs) are key tools for unlocking hidden knowledge. However, current methods focus on external links between concepts, ignoring the rich information within individual papers. Challenges like insufficient multi-relational data, name ambiguity, and cold-start issues further limit existing KG-based methods, failing to capture the intricate attributes of diverse entities. To solve these issues, we propose GLAMOR, a robust KG framework encompassing entities e.g., authors, papers, fields of study, and concepts, along with their semantic interconnections. GLAMOR uses a novel random walk-based KG text generation method and then fine-tunes the language model using the generated text. Subsequently, the acquired context-preserving embeddings facilitate superior top@k predictions. Evaluation results on two public benchmark datasets demonstrate our GLAMOR’s superiority against state-of-the-art methods especially in solving the cold-start problem.},
booktitle = {Proceedings of the 18th ACM Conference on Recommender Systems},
pages = {929–933},
numpages = {5},
keywords = {Attributed Graph Embedding, Citation Recommendation, Cold-start, GLAMOR, Large Language Model, Recommender Systems},
location = {Bari, Italy},
series = {RecSys '24}
}

@inproceedings{10.1145/3630106.3658981,
author = {Kraft, Angelie and Soulier, Elo\"{\i}se},
title = {Knowledge-Enhanced Language Models Are Not Bias-Proof: Situated Knowledge and Epistemic Injustice in AI},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3658981},
doi = {10.1145/3630106.3658981},
abstract = {The factual inaccuracies ("hallucinations") of large language models have recently inspired more research on knowledge-enhanced language modeling approaches. These are often assumed to enhance the overall trustworthiness and objectivity of language models. Meanwhile, the issue of bias is usually only mentioned as a limitation of statistical representations. This dissociation of knowledge-enhancement and bias is in line with previous research on AI engineers’ assumptions about knowledge, which indicate that knowledge is commonly understood as objective and value-neutral by this community. We argue that claims and practices by actors of the field still reflect this underlying conception of knowledge. We contrast this assumption with literature from social and, in particular, feminist epistemology, which argues that the idea of a universal disembodied knower is blind to the reality of knowledge practices and seriously challenges claims of "objective" or "neutral" knowledge. Knowledge enhancement techniques commonly use Wikidata and Wikipedia as their sources for knowledge, due to their large scales, public accessibility, and assumed trustworthiness. In this work, they serve as a case study for the influence of the social setting and the identity of knowers on epistemic processes. Indeed, the communities behind Wikidata and Wikipedia are known to be male-dominated and many instances of hostile behavior have been reported in the past decade. In effect, the contents of these knowledge bases are highly biased. It is therefore doubtful that these knowledge bases would contribute to bias reduction. In fact, our empirical evaluations of RoBERTa, KEPLER, and CoLAKE, demonstrate that knowledge enhancement may not live up to the hopes of increased objectivity. In our study, the average probability for stereotypical associations was preserved on two out of three metrics and performance-related gender gaps on knowledge-driven task were also preserved. We build on these results and critical literature to argue that the label of "knowledge" and the commonly held beliefs about it can obscure the harm that is still done to marginalized groups. Knowledge enhancement is at risk of perpetuating epistemic injustice, and AI engineers’ understanding of knowledge as objective per se conceals this injustice. Finally, to get closer to trustworthy language models, we need to rethink knowledge in AI and aim for an agenda of diversification and scrutiny from outgroup members.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1433–1445},
numpages = {13},
keywords = {bias, epistemology, fairness, feminism, knowledge enhancement, knowledge graphs, language models, natural language processing, representation},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

@inproceedings{10.1145/3716895.3716900,
author = {Yang, Da and Wang, Hongbo and Shao, Shanzhong and Liu, Shutian},
title = {Knowledge-Enhanced Large Language Model-Based Assistance Training System for Subway Maintenance Personnel},
year = {2025},
isbn = {9798400718007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716895.3716900},
doi = {10.1145/3716895.3716900},
abstract = {To address the various challenges faced in training urban rail transit system maintenance personnel, this paper proposes a solution for developing a training system for subway maintenance personnel using knowledge graphs and a Retrieval-Augmented Generation (RAG)-enhanced large language model. The approach involves first creating a fine-tuning dataset from subway maintenance technical documents to fine-tune the large language model. This fine-tuned model then assists in constructing a subway maintenance knowledge graph. Concurrently, a vector database of subway maintenance knowledge is established. Finally, a question-answering system leveraging both the knowledge graph and the vector database as external knowledge sources is developed to support the training of subway maintenance personnel. Results demonstrate that this system can effectively enhance the learning efficiency of maintenance staff.},
booktitle = {Proceedings of the 5th International Conference on Artificial Intelligence and Computer Engineering},
pages = {25–29},
numpages = {5},
keywords = {Retrieval-Augmented Generation (RAG), knowledge graph (KG), large language model (LLM), subway maintenance},
location = {
},
series = {ICAICE '24}
}

@inproceedings{10.1145/3638530.3664163,
author = {Custode, Leonardo Lucio and Caraffini, Fabio and Yaman, Anil and Iacca, Giovanni},
title = {An investigation on the use of Large Language Models for hyperparameter tuning in Evolutionary Algorithms},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638530.3664163},
doi = {10.1145/3638530.3664163},
abstract = {Hyperparameter optimization is a crucial problem in Evolutionary Computation. In fact, the values of the hyperparameters directly impact the trajectory taken by the optimization process, and their choice requires extensive reasoning by human operators. Although a variety of self-adaptive Evolutionary Algorithms have been proposed in the literature, no definitive solution has been found. In this work, we perform a preliminary investigation to automate the reasoning process that leads to the choice of hyperparameter values. We employ two open-source Large Language Models (LLMs), namely Llama2-70b and Mixtral, to analyze the optimization logs online and provide novel real-time hyperparameter recommendations. We study our approach in the context of step-size adaptation for (1 + 1)-ES. The results suggest that LLMs can be an effective method for optimizing hyperparameters in Evolution Strategies, encouraging further research in this direction.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1838–1845},
numpages = {8},
keywords = {evolutionary algorithms, large language models, landscape analysis, parameter tuning},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}

@article{10.14778/3742728.3742757,
author = {Liu, Yurong and Pena, Eduardo H. M. and Santos, A\'{e}cio and Wu, Eden and Freire, Juliana},
title = {Magneto: Combining Small and Large Language Models for Schema Matching},
year = {2025},
issue_date = {April 2025},
publisher = {VLDB Endowment},
volume = {18},
number = {8},
issn = {2150-8097},
url = {https://doi.org/10.14778/3742728.3742757},
doi = {10.14778/3742728.3742757},
abstract = {Recent advances in language models (LMs) open new opportunities for schema matching (SM). Recent approaches have shown their potential and key limitations: while small LMs (SLMs) require costly, difficult-to-obtain training data, large LMs (LLMs) demand significant computational resources and face context window constraints. We present Magneto, a cost-effective and accurate solution for SM that combines the advantages of SLMs and LLMs to address their limitations. By structuring the SM pipeline in two phases, retrieval and reranking, Magneto can use computationally efficient SLM-based strategies to derive candidate matches which can then be reranked by LLMs, thus making it possible to reduce runtime while improving matching accuracy. We propose (1) a self-supervised approach to fine-tune SLMs which uses LLMs to generate syntactically diverse training data, and (2) prompting strategies that are effective for reranking. We also introduce a new benchmark, developed in collaboration with domain experts, which includes real biomedical datasets and presents new challenges for SM methods. Through a detailed experimental evaluation, using both our new and existing benchmarks, we show that Magneto is scalable and attains high accuracy for datasets from different domains.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {2681–2694},
numpages = {14}
}

@inproceedings{10.1145/3626772.3657966,
author = {Zhang, Wenjia and Gui, Lin and Procter, Rob and He, Yulan},
title = {Multi-Layer Ranking with Large Language Models for News Source Recommendation},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657966},
doi = {10.1145/3626772.3657966},
abstract = {To seek reliable information sources for news events, we introduce a novel task of expert recommendation, which aims to identify trustworthy sources based on their previously quoted statements. To achieve this, we built a novel dataset, called NewsQuote, consisting of 23,571 quote-speaker pairs sourced from a collection of news articles. We formulate the recommendation task as the retrieval of experts based on their likelihood of being associated with a given query. We also propose a multi-layer ranking framework employing Large Language Models to improve the recommendation performance. Our results show that employing an in-context learning based LLM ranker and a multi-layer ranking-based filter significantly improve both the predictive quality and behavioural quality of the recommender system.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2537–2542},
numpages = {6},
keywords = {in-context learning, large language model, recommender system},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@proceedings{10.1145/3643795,
title = {LLM4Code '24: Proceedings of the 1st International Workshop on Large Language Models for Code},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the first edition of the InternationalWorkshop on Large Language Models for Code (LLM4Code). Large Language Models (LLMs), which are large-scale models being trained on massive textual corpora, have achieved significant advances in various domains, including Software Engineering (SE). Recently, there has been a growing interest in applying LLMs to assist software development and maintenance, such as code generation and comprehension, test generation, and program repair. Although the application of LLMs on code-relevant tasks has shown very promising performance, there is a huge potential to explore this growing domain further. The motivation of the LLM4Code workshop is to provide a platform for academics and practitioners to discuss and share their ideas on applying and developing LLMs to solve code-relevant problems in SE activities.The LLM4Code workshop is concerned with the research on how to better apply LLMs to solve code-relevant tasks, how to design better LLMs for code-relevant tasks, and how to better benchmark LLMs on code-relevant tasks. The workshop aims to achieve multiple goals as follows. Firstly, the workshop aims to provide an opportunity for participants to discuss novel ideas and preliminary results on LLMs for solving code-relevant SE problems, to exchange the latest progress in this domain. Secondly, the workshop aims to encourage participants to discuss the open challenges and problems of LLM4code, to identify important future directions in this domain. Finally, the workshop aims to encourage participants to share infrastructures and benchmarks that are foundational and beneficial for future research in this domain.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3627673.3679231,
author = {Afreen, Neda and Balloccu, Giacomo and Boratto, Ludovico and Fenu, Gianni and Malloci, Francesca Maridina and Marras, Mirko and Martis, Andrea Giovanni},
title = {EDGE: A Conversational Interface driven by Large Language Models for Educational Knowledge Graphs Exploration},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3679231},
doi = {10.1145/3627673.3679231},
abstract = {As education adopts digital platforms, the vast amount of information from various sources, such as learning management systems and learning object repositories, presents challenges in navigation and elaboration. Traditional interfaces involve a steep learning curve, limited user accessibility, and lack flexibility. Language models alone cannot address these issues as they do not have access to structured information specific to the educational organization. In this paper, we propose EDGE (EDucational knowledge Graph Explorer), a natural language interface that uses knowledge graphs to organize educational information. EDGE translates natural language requests into queries and converts the results back into natural language responses. We show EDGE's versatility using knowledge graphs built from public datasets, providing example interactions of different stakeholders. Demo video: https://u.garr.it/eYq63.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {5159–5163},
numpages = {5},
keywords = {conversational interface, graph database, information retrieval, knowledge graph, language model, learning management},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3613905.3650844,
author = {Walker, Johanna and Koutsiana, Elisavet and Nwachukwu, Michelle and Mero\~{n}o Pe\~{n}uela, Albert and Simperl, Elena},
title = {The Promise and Challenge of Large Language Models for Knowledge Engineering: Insights from a Hackathon},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650844},
doi = {10.1145/3613905.3650844},
abstract = {Knowledge engineering (KE) is the process of building, maintaining and using knowledge-based systems. This recently takes the form of knowledge graphs (KGs). The advent of new technologies like Large Language Models (LLMs) has the potential to improve automation in KE work due to the richness of their training data and their performance at solving natural language processing tasks. We conducted a multiple-methods study exploring user opinions and needs regarding the use of LLMs in KE. We used ethnographic techniques to observe KE workers using LLMs to solve KE tasks during a hackathon, followed by interviews with some of the participants. This interim study found that despite LLMs’ promising capabilities for efficient knowledge acquisition and requirements elicitation, their effective deployment requires an extended set of capabilities and training, particularly in prompting and understanding data. LLMs can be useful for simple quality assessment tasks, but in complex scenarios, the output is hard to control and evaluation may require novel approaches. With this study, we aim to evidence the interaction of KE stakeholders with LLMs, identify areas of potential, and understand the barriers to their effective use. We find copilot approaches may be valuable in developing processes where the human or a team of humans is assisted by generative AI.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {318},
numpages = {9},
keywords = {Interviews, Knowledge Engineering, Knowledge Graph, Large Language Models},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@article{10.1145/3631392,
author = {Yang, Jian and Hu, Xinyu and Xiao, Gang and Shen, Yulong},
title = {A Survey of Knowledge Enhanced Pre-trained Language Models},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3631392},
doi = {10.1145/3631392},
abstract = {Pre-trained language models learn informative word representations on a large-scale text corpus through self-supervised learning, which has achieved promising performance in fields of natural language processing (NLP) after fine-tuning. These models, however, suffer from poor robustness and lack of interpretability. We refer to pre-trained language models with knowledge injection as knowledge-enhanced pre-trained language models (KEPLMs). These models demonstrate deep understanding and logical reasoning and introduce interpretability. In this survey, we provide a comprehensive overview of KEPLMs in NLP. We first discuss the advancements in pre-trained language models and knowledge representation learning. Then we systematically categorize existing KEPLMs from three different perspectives. Finally, we outline some potential directions of KEPLMs for future research.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = mar,
keywords = {natural language processing, pre-trained language models, symbolic knowledge, knowledge enhanced pre-trained language models}
}

@inproceedings{10.1145/3626772.3657904,
author = {Xie, Yuzhang and Lu, Jiaying and Ho, Joyce and Nahab, Fadi and Hu, Xiao and Yang, Carl},
title = {PromptLink: Leveraging Large Language Models for Cross-Source Biomedical Concept Linking},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657904},
doi = {10.1145/3626772.3657904},
abstract = {Linking (aligning) biomedical concepts across diverse data sources enables various integrative analyses, but it is challenging due to the discrepancies in concept naming conventions. Various strategies have been developed to overcome this challenge, such as those based on string-matching rules, manually crafted thesauri, and machine learning models. However, these methods are constrained by limited prior biomedical knowledge and can hardly generalize beyond the limited amounts of rules, thesauri, or training samples. Recently, large language models (LLMs) have exhibited impressive results in diverse biomedical NLP tasks due to their unprecedentedly rich prior knowledge and strong zero-shot prediction abilities. However, LLMs suffer from issues including high costs, limited context length, and unreliable predictions. In this research, we propose PromptLink, a novel biomedical concept linking framework that leverages LLMs. Empirical results on the concept linking task between two EHR datasets and an external biomedical KG demonstrate the effectiveness of PromptLink. Furthermore, PromptLink is a generic framework without reliance on additional prior knowledge, context, or training data, making it well-suited for concept linking across various types of data sources. The source code of this study is available at https://github.com/constantjxyz/PromptLink.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2589–2593},
numpages = {5},
keywords = {biomedical concept linking, few-shot prompting, large language models for resource-constrained field, retrieve \&amp; re-rank},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3705754.3705790,
author = {Zhu, Sitong and Xia, Baobing and Duan, Fangwei and Zhao, Zhenyang and Zhao, Zhenxia and Xiao, Teliang and Liu, Zhicheng and Liu, Xia},
title = {Automated Framework for Constructing Knowledge Graphs Oriented for Standard Analysis Using Large Language Models},
year = {2025},
isbn = {9798400710193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3705754.3705790},
doi = {10.1145/3705754.3705790},
abstract = {In an era characterized by rapid technological growth and digital transformation, the necessity for efficient and structured knowledge representation has grown more crucial. Standards serve as fundamental cornerstones that offer guidelines, specifications, and frameworks to ensure the quality and interoperability of products, services, as well as systems. Nonetheless, the complexity and extensive nature of standard documents present significant challenges in extraction, alignment, and organization. Traditional manual processing methods frequently prove labor-intensive and susceptible to errors, impeding the capturing of intricate relationships and hierarchies within these standards. Knowledge Graphs (KGs) provide a robust methodology for organizing information, facilitating improved functionalities for advanced search, reasoning, and analytics. Despite their potential, structuring KGs from standard documents continues to be challenging because of unstructured text, domain-specific terminology, and the intricacies in accurate information extraction. Recent advancements in Natural Language Processing (NLP), particularly the emergence of Large Language Models (LLMs) like GPT-3, have opened new avenues for automating the extraction and structuring of information from unstructured content. These models exhibit exceptional proficiency in comprehending and producing human-like text, positioning them as feasible solutions for addressing the complexities associated with standard documents. This paper presents an automated framework named StandardKG Builder, which utilizes LLMs to construct knowledge graphs tailored for standard analysis from multiple perspectives for complex information extraction. Our evaluation on a comprehensive dataset of standard documents highlights the framework’s effectiveness and scalability. By merging sophisticated knowledge representation with advanced NLP techniques, our work significantly enhances the accessibility and analysis of standard documents, paving the way for more efficient and intelligent standard management systems.},
booktitle = {Proceedings of the 2024 2nd International Conference on Electronics, Computers and Communication Technology},
pages = {183–189},
numpages = {7},
keywords = {Knowledge Graph, Large Language Models, Standard Analysis},
location = {
},
series = {CECCT '24}
}

@inproceedings{10.1145/3715014.3722067,
author = {Hu, Jiawei and Jia, Hong and Hassan, Mahbub and Yao, Lina and Kusy, Brano and Hu, Wen},
title = {LightLLM: A Versatile Large Language Model for Predictive Light Sensing},
year = {2025},
isbn = {9798400714795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715014.3722067},
doi = {10.1145/3715014.3722067},
abstract = {We propose LightLLM, a model that fine tunes pre-trained large language models (LLMs) for light-based sensing tasks. It integrates a sensor data encoder to extract key features, a contextual prompt to provide environmental information, and a fusion layer to combine these inputs into a unified representation. This combined input is then processed by the pre-trained LLM, which remains frozen while being fine-tuned through the addition of lightweight, trainable components, allowing the model to adapt to new tasks without altering its original parameters. This approach enables flexible adaptation of LLM to specialized light sensing tasks with minimal computational overhead and retraining effort. We have implemented LightLLM for three light sensing tasks: light-based localization, outdoor solar forecasting, and indoor solar estimation. Using real-world experimental datasets, we demonstrate that LightLLM significantly outperforms state-of-the-art methods, achieving 4.4x improvement in localization accuracy and 3.4x improvement in indoor solar estimation when tested in previously unseen environments. We further demonstrate that LightLLM outperforms ChatGPT-4 with direct prompting, highlighting the advantages of LightLLM's specialized architecture for sensor data fusion with textual prompts.},
booktitle = {Proceedings of the 23rd ACM Conference on Embedded Networked Sensor Systems},
pages = {158–171},
numpages = {14},
location = {UC Irvine Student Center., Irvine, CA, USA},
series = {SenSys '25}
}

@inproceedings{10.1145/3589335.3653009,
author = {Poria, Soujanya},
title = {Understanding, Leveraging, and Improving Large Language Models},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3653009},
doi = {10.1145/3589335.3653009},
abstract = {The emergence of Large Language Models (LLMs) has marked a substantial advancement in Natural Language Processing (NLP), contributing significantly to enhanced task performance both within and outside specific domains. However, amidst these achievements, three key questions remain unanswered: 1) The mechanism through which LLMs accomplish their tasks and their limitations, 2) Effectively harnessing the power of LLMs across diverse domains, and 3) Strategies for enhancing the performance of LLMs. This talk aims to delve into our research group's endeavors to address these pivotal questions. Firstly, I will outline our approach, which involves utilizing ontology-guided prompt perturbations to unravel the primary limitations of LLMs in solving mathematical problems. Moving on to the second question, we will explore the utilization of synthetic data generated by LLMs to bolster challenging downstream tasks, particularly focusing on structured prediction where LLMs face persistent challenges. I will elaborate on our initiatives aimed at improving LLMs by incorporating highly effective retrieval strategies, specifically addressing the prevalent challenge of hallucinations that often plagues contemporary LLMs. Finally, I will present a technique on LLM realignment to restore safety lost during fine-tuning.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {1805},
numpages = {1},
keywords = {keynote},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3613905.3650949,
author = {Oelen, Allard and Auer, S\"{o}ren},
title = {Leveraging Large Language Models for Realizing Truly Intelligent User Interfaces},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650949},
doi = {10.1145/3613905.3650949},
abstract = {The number of published scholarly articles is growing at a significant rate, making scholarly knowledge organization increasingly important. Various approaches have been proposed to organize scholarly information, including describing scholarly knowledge semantically leveraging knowledge graphs. Transforming unstructured knowledge, presented within articles, to structured and semantically represented knowledge generally requires human intelligence and labor since natural language processing methods alone typically do not render sufficient precision and recall for many applications. With the recent developments of Large Language Models (LLMs), it becomes increasingly possible to provide truly intelligent user interfaces guiding humans in the transformation process. We present an approach to integrate non-intrusive LLMs guidance into existing user interfaces. More specifically, we integrate LLM-supported user interface components into an existing scholarly knowledge infrastructure. Additionally, we provide our experiences with LLM integration, detailing best practices and obstacles. Finally, we evaluate the approach using a small-scale user evaluation with domain experts.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {222},
numpages = {8},
keywords = {Intelligent User Interface, LLM Interface, Scholarly Knowledge Graphs},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@article{10.1145/3696379,
author = {Bui, Minh-Thanh and Boffa, Matteo and Valentim, Rodolfo Vieira and Navarro, Jose Manuel and Chen, Fuxing and Bao, Xiaosheng and Houidi, Zied Ben and Rossi, Dario},
title = {A Systematic Comparison of Large Language Models Performance for Intrusion Detection},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CoNEXT4},
url = {https://doi.org/10.1145/3696379},
doi = {10.1145/3696379},
abstract = {We explore the capabilities of Large Language Models (LLMs) to assist or substitute devices (i.e., firewalls) and humans (i.e., security experts) respectively in the detection and analysis of security incidents. We leverage transformer-based technologies, from relatively small to foundational sizes, to address the problem of correctly identifying the attack severity (and accessorily identifying and explaining the attack type). We contrast a broad range of LLM techniques (prompting, retrieval augmented generation, and fine-tuning of several models) using state-of-the-art machine learning models as a baseline. Using proprietary data from commercial deployment, our study provides an unbiased picture of the strengths and weaknesses of LLM for intrusion detection.},
journal = {Proc. ACM Netw.},
month = nov,
articleno = {22},
numpages = {23},
keywords = {computing methodologies, firewalls, intrusion detection systems, machine learning, natural language processing, security and privacy}
}

@inproceedings{10.1145/3637528.3671491,
author = {Alam, Mehwish and Buscaldi, Davide and Cochez, Michael and Gesese, Genet Asefa and Osborne, Francesco and Reforgiato Recupero, Diego},
title = {Workshop on Deep Learning and Large Language Models for Knowledge Graphs (DL4KG)},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671491},
doi = {10.1145/3637528.3671491},
abstract = {The use of Knowledge Graphs (KGs) which constitute large networks of real-world entities and their interrelationships, has grown rapidly. A substantial body of research has emerged, exploring the integration of deep learning (DL) and large language models (LLMs) with KGs. This workshop aims to bring together leading researchers in the field to discuss and foster collaborations on the intersection of KG and DL/LLMs.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {6704–6705},
numpages = {2},
keywords = {artificial intelligence, deep learning, knowledge graphs, large language models},
location = {Barcelona, Spain},
series = {KDD '24}
}

@article{10.1145/3624012,
author = {Jain, Deepak Kumar and Qamar, Shamimul and Sangwan, Saurabh Raj and Ding, Weiping and Kulkarni, Anand J.},
title = {Ontology-Based Natural Language Processing for Sentimental Knowledge Analysis Using Deep Learning Architectures},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3624012},
doi = {10.1145/3624012},
abstract = {When tested with popular datasets, sentiment categorization using deep learning (DL) algorithms will produce positive results. Building a corpus on novel themes to train machine learning methods in sentiment classification with high assurance, however, will be difficult. This study proposes a way for representing efficient features of a dataset into a word embedding layer of DL methods in sentiment classification known as KPRO (knowledge processing and representation based on ontology), a procedure to embed knowledge in the ontology of opinion datasets. This research proposes novel methods in ontology-based natural language processing utilizing feature extraction as well as classification by a DL technique. Here, input text has been taken as web ontology based text and is processed for word embedding. Then the feature mapping is carried out for this processed text using least square mapping in which the sentiment-based text has been mapped for feature extraction. The feature extraction is carried out using a Markov model based auto-feature encoder (MarMod_AuFeaEnCod). Extracted features are classified by utilizing hierarchical convolutional attention networks. Based on this classified output, the sentiment of the text obtained from web data has been analyzed. Results are carried out for Twitter and Facebook ontology-based sentimental analysis datasets in terms of accuracy, precision, recall, F-1 score, RMSE, and loss curve analysis. For the Twitter dataset, the proposed MarMod_AuFeaEnCod_HCAN attains an accuracy of 98\%, precision of 95\%, recall of 93\%, F-1 score of 91\%, RMSE of 88\%, and loss curve of 70.2\%. For Facebook, ontology web dataset analysis is also carried out with the same parameters in which the proposed MarMod_AuFeaEnCod_HCAN acquires accuracy of 96\%, precision of 92\%, recall of 94\%, F-1 score of 91\%, RMSE of 77\%, and loss curve of 68.2\%.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {17},
numpages = {17},
keywords = {Ontology, NLP, KPRO, deep learning, feature extraction, classification}
}

@inproceedings{10.1145/3627673.3680025,
author = {Huang, Jia-Hong and Yang, Chao-Chun and Shen, Yixian and Pacces, Alessio M. and Kanoulas, Evangelos},
title = {Optimizing Numerical Estimation and Operational Efficiency in the Legal Domain through Large Language Models},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3680025},
doi = {10.1145/3627673.3680025},
abstract = {The legal landscape encompasses a wide array of lawsuit types, presenting lawyers with challenges in delivering timely and accurate information to clients, particularly concerning critical aspects like potential imprisonment duration or financial repercussions. Compounded by the scarcity of legal experts, there's an urgent need to enhance the efficiency of traditional legal workflows. Recent advances in deep learning, especially Large Language Models (LLMs), offer promising solutions to this challenge. Leveraging LLMs' mathematical reasoning capabilities, we propose a novel approach integrating LLM-based methodologies with specially designed prompts to address precision requirements in legal Artificial Intelligence (LegalAI) applications. The proposed work seeks to bridge the gap between traditional legal practices and modern technological advancements, paving the way for a more accessible, efficient, and equitable legal system. To validate this method, we introduce a curated dataset tailored to precision-oriented LegalAI tasks, serving as a benchmark for evaluating LLM-based approaches. Extensive experimentation confirms the efficacy of our methodology in generating accurate numerical estimates within the legal domain, emphasizing the role of LLMs in streamlining legal processes and meeting the evolving demands of LegalAI. Github: https://github.com/Jhhuangkay/Optimizing-Numerical-Estimation-and-Operational-Efficiency-in-the-Legal-Domain.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {4554–4562},
numpages = {9},
keywords = {large language models, precision-oriented legal artificial intelligence, tailored prompt design},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@article{10.1145/3729236,
author = {Jin, Kebing and Zhuo, Hankz Hankui},
title = {Integrating AI Planning with Natural Language Processing:&nbsp;A&nbsp;Combination of Explicit and Tacit Knowledge},
year = {2025},
issue_date = {August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3729236},
doi = {10.1145/3729236},
abstract = {Natural language processing (NLP) aims at investigating the interactions between agents and humans, which processes and analyzes large amounts of natural language data. Large-scale language models play an important role in current NLP. However, the challenges of explainability and complexity come along with the development of language models. One way is to introduce logical relations and rules into NLP models, such as making use of Automated Planning. Automated planning (AI planning) focuses on building symbolic domain models and synthesizing plans to transit initial states to goals based on domain models. Recently, there have been plenty of works related to those two fields, which have the abilities to generate explicit knowledge, e.g., preconditions and effects of action models, and learn from tacit knowledge, e.g., neural models, respectively. Integrating AI planning and NLP effectively improves the communication between human and intelligent agents. This article outlines the commons and relations between AI planning and NLP, and it argues that each of them can effectively impact the other one in six areas: (1) planning-based text understanding, (2) planning-based NLP, (3) text-based human–robot interaction, (4) planning-based explainability, (5) evaluation metrics, and (6) applications. We also explore some potential future issues between AI planning and NLP. To the best of our knowledge, this survey is the first that addresses the deep connections between AI planning and NLP.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {97},
numpages = {37},
keywords = {AI planning, Natural language processing, Natural language understanding, Human-robot interaction, Explainability}
}

@inproceedings{10.1145/3631700.3665233,
author = {Biancini, Giorgio and Ferrato, Alessio and Limongelli, Carla},
title = {Multiple-Choice Question Generation Using Large Language Models: Methodology and Educator Insights},
year = {2024},
isbn = {9798400704666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631700.3665233},
doi = {10.1145/3631700.3665233},
abstract = {Integrating Artificial Intelligence (AI) in educational settings has brought new learning approaches, transforming the practices of both students and educators. Among the various technologies driving this transformation, Large Language Models (LLMs) have emerged as powerful tools for creating educational materials and question answering, but there are still space for new applications. Educators commonly use Multiple-Choice Questions (MCQs) to assess student knowledge, but manually generating these questions is resource-intensive and requires significant time and cognitive effort. In our opinion, LLMs offer a promising solution to these challenges. This paper presents a novel comparative analysis of three widely known LLMs - Llama 2, Mistral, and GPT-3.5 - to explore their potential for creating informative and challenging MCQs. In our approach, we do not rely on the knowledge of the LLM, but we inject the knowledge into the prompt to contrast the hallucinations, giving the educators control over the test’s source text, too. Our experiment involving 21 educators shows that GPT-3.5 generates the most effective MCQs across several known metrics. Additionally, it shows that there is still some reluctance to adopt AI in the educational field. This study sheds light on the potential of LLMs to generate MCQs and improve the educational experience, providing valuable insights for the future.},
booktitle = {Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {584–590},
numpages = {7},
keywords = {Generative AI, LLMs, Multiple Choice Question},
location = {Cagliari, Italy},
series = {UMAP Adjunct '24}
}

@inproceedings{10.1145/3747227.3747264,
author = {Mao, Huijuan},
title = {Research on the Construction of Machine Translation Model of Fuzzy Language in Ancient Chinese Medicine Books for the Transmission of Chinese Medicine Culture},
year = {2025},
isbn = {9798400714382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3747227.3747264},
doi = {10.1145/3747227.3747264},
abstract = {In order to improve the translation quality of fuzzy language in ancient Chinese medical texts, a hybrid translation model integrating deep learning and knowledge graph is constructed to perform context modeling and disambiguation optimization for the literary syntax, semantic ambiguities, and terminology of the ancient texts. The Bi-LSTM context modeling module is designed to enhance semantic understanding with knowledge graph, and a multi-strategy fusion translation decision mechanism is used to improve translation accuracy. The results show that the model performs well in the processing of ambiguities such as disease dynamics and treatment principles, improves terminological consistency and semantic retention, and provides effective support for cross-lingual communication of TCM culture.},
booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks},
pages = {229–235},
numpages = {7},
keywords = {ancient Chinese medical books, fuzzy language, knowledge map, machine translation},
location = {
},
series = {MLNN '25}
}

@inproceedings{10.1145/3708036.3708272,
author = {Yang, Guangyuan and Xie, Quanying and Chen, Lei},
title = {A Scientometrics Analysis and Visualization of Large Language Model in China's Library},
year = {2025},
isbn = {9798400709999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708036.3708272},
doi = {10.1145/3708036.3708272},
abstract = {Large Language Model has been researched in the field of library from the following aspects:space reproduction, service reform, library construction and so on. In order to clarify the current research situation of Large Language Model's application research in the field of library, and provide some reference for the further development of research fields related to Large Language Model empowering library in the future. This paper utilizes two methods of scientometrics and data visualization to analyze and study the journal papers on the application of Large Language Model in the field of Chinese libraries from the aspects of the degree of academic focus, the way of creating academic achievements and research topics of academic achievements, and puts forward the research practice of strengthening the application of Large Language Model in library from the aspects of ’Strengthen the practical research of Large Language Model empowering Chinese library’ and ‘Broaden the field of research related to Large Language Model empowering Chinese library’, in order to promote the all-round development of Large Language Model in the field of library.},
booktitle = {Proceedings of the 2024 5th International Conference on Computer Science and Management Technology},
pages = {1403–1407},
numpages = {5},
keywords = {Chinese libraries, Data Visualization, Large Language Model, Library Service, Scientometrics},
location = {
},
series = {ICCSMT '24}
}

@article{10.1145/3743144,
author = {Comuzzi, Marco and Ko, Jonghyeon and Maggi, Fabrizio},
title = {A Language to Model and Simulate Data Quality Issues in Process Mining},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3743144},
doi = {10.1145/3743144},
abstract = {Real-life business process event logs may suffer from significant data quality problems negatively influencing process mining analysis. Over time, a range of approaches has been developed to detect and repair these quality problems. Validation of these approaches tends to be challenging due to the lack of a ground truth. Moreover, the identification and definition of event log quality problems have been tackled mainly through a pattern-based approach, with systematic and extensible methods currently lacking. In this article, we present FLAWD, a formal language for describing event log data quality issues that enables solutions addressing the shortcomings of process mining data quality research identified above. FLAWD can be used to formally describe and possibly reason over event log data quality errors, as well as to guide the development of tools for controlled and sophisticated “polluting” of event logs through which benchmark datasets may be systematically created. We present the abstract syntax grammar of FLAWD and an open-source software tool based on it that allows for the insertion of all so-called event log imperfection patterns in a stochastic manner. We show how FLAWD has been used in our research to generate benchmark datasets and how it can be used to formally describe and replicate a range of errors found in real-life event logs.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {6},
numpages = {36},
keywords = {Event log, data quality, abstract syntax grammar, FLAWD, error injection, error description}
}

@inproceedings{10.1145/3700297.3700331,
author = {Yang, Da and Liu, Shutian and Fu, Haoyang and Shen, Jiayi},
title = {Research and Practice on the Construction of Course Ideological and Political Education Based on Knowledge Graphs and Large Language Models},
year = {2024},
isbn = {9798400707100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3700297.3700331},
doi = {10.1145/3700297.3700331},
abstract = {Knowledge graphs and large language models (LLMs) have become important tools for educational innovation. This paper explores the application of these two technologies in the construction of ideological and political education in university courses. The paper begins by analyzing the importance of course-based ideological and political education and the challenges currently faced. It then introduces the role of knowledge graphs in integrating educational resources and constructing knowledge systems, as well as the potential and current status of LLMs in natural language processing and providing personalized educational content. This study presents a method that integrates the use of knowledge graphs and LLMs to construct resources and application systems for course-based ideological and political education. The results of practical case studies demonstrate that the proposed method improves the efficiency of constructing ideological and political education content, enhances the effectiveness of moral education within courses, and contributes to the innovative development of ideological and political education.},
booktitle = {Proceedings of the 2024 International Symposium on Artificial Intelligence for Education},
pages = {193–198},
numpages = {6},
keywords = {Course Ideological and Political Education, Educational Innovation, Knowledge Graph, Large Language Model (LLM)},
location = {
},
series = {ISAIE '24}
}

@inproceedings{10.1145/3665601.3669844,
author = {Huang, Zezhou},
title = {Disambiguate Entity Matching using Large Language Models through Relation Discovery},
year = {2024},
isbn = {9798400706943},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3665601.3669844},
doi = {10.1145/3665601.3669844},
abstract = {Entity matching is a critical problem in data integration, central to tasks like fuzzy joins for tuple enrichment. Traditional approaches have focused on overcoming fuzzy term representations through methods such as edit distance, Jaccard similarity, and more recently, embeddings and deep neural networks, including advancements from large language models (LLMs) like GPT. However, when integrating with external databases, the core challenge in entity matching extends beyond term fuzziness to the ambiguity in defining what constitutes a "match". This is because external databases contain tuples with varying levels of detail and granularity among entities, and an "exact match" in traditional entity matching rarely happens. As a result, understanding how entities are related and the potential nuances is critical, especially for high-stake tasks for responsible AI. In this work, we study a case problem of entity matching for ESG reporting. We propose a novel approach that shifts focus from purely identifying semantic similarities to understanding and defining the "relations" between entities for resolving ambiguities in matching, with a human-in-the-loop process to make the final decision. By pre-defining a set of relations relevant to the task at hand, our method allows analysts to navigate the spectrum of similarity more effectively, from exact matches to conceptually related entities, and responsibly perform downstream tasks.},
booktitle = {Proceedings of the Conference on Governance, Understanding and Integration of Data for Effective and Responsible AI},
pages = {36–39},
numpages = {4},
keywords = {Data Integration, Entity Matching, Large Language Models},
location = {Santiago, AA, Chile},
series = {GUIDE-AI '24}
}

@article{10.1145/3708326,
author = {Mountantonakis, Michalis and Tzitzikas, Yannis},
title = {Generating SPARQL Queries over CIDOC-CRM Using a Two-Stage Ontology Path Patterns Method in LLM Prompts},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3708326},
doi = {10.1145/3708326},
abstract = {In this article, we focus on the task of exploiting the capabilities of Large Language Models (LLMs) to generate SPARQL Queries for answering natural questions over cultural Knowledge Graphs (KGs) expressed according to the ISO standard ontology CIDOC-CRM. Since CIDOC-CRM is an event-based model, usually we have to follow long paths for answering a question, thereby, the challenge is how to construct the prompt for aiding the LLM to produce the right SPARQL query. We propose and comparatively evaluate methods based on the creation of ontology path patterns of a configurable path radius (or length). Then, we construct a new dedicated benchmark that includes 100 natural questions and the corresponding SPARQL queries over two real KGs from the cultural domain describing artworks. Finally, we present comparative results about the effectiveness and efficiency over the benchmark by using ChatGPT-3.5. The most effective method follows a two-stage process that predicts and uses the most appropriate path patterns of  (rleq 4) . This method achieves 3.5 (times)  higher accuracy than the baseline method (0.66 versus 0.19), that includes in the prompt only the list of properties and classes of the KG.Benchmark:},
journal = {J. Comput. Cult. Herit.},
month = feb,
articleno = {12},
numpages = {20},
keywords = {Question Answering, CIDOC-CRM, Prompt Engineering, Cultural Heritage, LLM}
}

@inproceedings{10.1145/3616855.3635772,
author = {Deng, Cheng and Zhang, Tianhang and He, Zhongmou and Chen, Qiyuan and Shi, Yuanyuan and Xu, Yi and Fu, Luoyi and Zhang, Weinan and Wang, Xinbing and Zhou, Chenghu and Lin, Zhouhan and He, Junxian},
title = {K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization},
year = {2024},
isbn = {9798400703713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616855.3635772},
doi = {10.1145/3616855.3635772},
abstract = {Large language models (LLMs) have achieved great success in general domains of natural language processing. In this paper, we bring LLMs to the realm of geoscience with the objective of advancing research and applications in this field. To this end, we present the first-ever LLM in geoscience, K2, alongside a suite of resources developed to further promote LLM research within geoscience. For instance, we have curated the first geoscience instruction tuning dataset, GeoSignal, which aims to align LLM responses to geoscience-related user queries. Additionally, we have established the first geoscience benchmark, GeoBench, to evaluate LLMs in the context of geoscience. In this work, we experiment with a complete recipe to adapt a pre-trained general-domain LLM to the geoscience domain. Specifically, we further train the LLaMA-7B model on 5.5B tokens of geoscience text corpus, including over 1 million pieces of geoscience literature, and utilize GeoSignal's supervised data to fine-tune the model. Moreover, we share a protocol that can efficiently gather domain-specific data and construct domain-supervised data, even in situations where manpower is scarce. Meanwhile, we equip K2 with the abilities of using tools to be a naive geoscience aide. Experiments conducted on the GeoBench demonstrate the effectiveness of our approach and datasets on geoscience knowledge understanding and utilization.We open-source all the training data and K2 model checkpoints at https://github.com/davendw49/k2},
booktitle = {Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
pages = {161–170},
numpages = {10},
keywords = {foundation model, geoscience knowledge mining, geoscience large language model},
location = {Merida, Mexico},
series = {WSDM '24}
}

@inproceedings{10.1145/3539618.3591667,
author = {Li, Na and Kteich, Hanane and Bouraoui, Zied and Schockaert, Steven},
title = {Distilling Semantic Concept Embeddings from Contrastively Fine-Tuned Language Models},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591667},
doi = {10.1145/3539618.3591667},
abstract = {Learning vectors that capture the meaning of concepts remains a fundamental challenge. Somewhat surprisingly, perhaps, pre-trained language models have thus far only enabled modest improvements to the quality of such concept embeddings. Current strategies for using language models typically represent a concept by averaging the contextualised representations of its mentions in some corpus. This is potentially sub-optimal for at least two reasons. First, contextualised word vectors have an unusual geometry, which hampers downstream tasks. Second, concept embeddings should capture the semantic properties of concepts, whereas contextualised word vectors are also affected by other factors. To address these issues, we propose two contrastive learning strategies, based on the view that whenever two sentences reveal similar properties, the corresponding contextualised vectors should also be similar. One strategy is fully unsupervised, estimating the properties which are expressed in a sentence from the neighbourhood structure of the contextualised word embeddings. The second strategy instead relies on a distant supervision signal from ConceptNet. Our experimental results show that the resulting vectors substantially outperform existing concept embeddings in predicting the semantic properties of concepts, with the ConceptNet-based strategy achieving the best results. These findings are furthermore confirmed in a clustering task and in the downstream task of ontology completion.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {216–226},
numpages = {11},
keywords = {commonsense knowledge, contrastive learning, language models, word embedding},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3727353.3727365,
author = {Zhai, Dongsheng and Du, Ruize and Liang, Guoqiang},
title = {Recommendation System Based on Heterogeneous Graph Neural Network and Large Language Model},
year = {2025},
isbn = {9798400712425},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3727353.3727365},
doi = {10.1145/3727353.3727365},
abstract = {In order to break through technical problems and reduce R&amp;D costs and risks, innovation subjects usually establish cooperative relationships based on common cooperation goals and technological similarities among them. Therefore, based on the analysis of the common cooperation goals of innovation subjects, this chapter proposes a partner identification model for emerging technological innovation, constructs a vector representation of patented technologies obtained by patent heterogeneous networks, and identifies potential partners in the technology field on the basis of technological similarity.},
booktitle = {Proceedings of the 2025 4th International Conference on Big Data, Information and Computer Network},
pages = {67–72},
numpages = {6},
keywords = {Graph Neural Network, Heterogeneous Patent Network, Large Language Model, Potential Partner Identification},
location = {
},
series = {BDICN '25}
}

@inproceedings{10.1145/3716554.3716598,
author = {Antoniou, Christina and Bassiliades, Nick},
title = {Utilizing LLMs and ontologies to query educational knowledge graphs},
year = {2025},
isbn = {9798400713170},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716554.3716598},
doi = {10.1145/3716554.3716598},
abstract = {Knowledge Graphs (KGs) provide knowledge and data in a structured format with content from various fields. But the access to the knowledge graphs is done by experienced users, that is, users who know the syntax of the SPARQL language and the KG vocabulary (either in RDF Schema or in OWL) in order to be able to ask questions to exploit the knowledge graphs. However, this requires a lot of time and effort for most of the users, which makes KGs inaccessible to a large number of users. Large Language Models (LLMs) that have appeared recently can provide an alternative way to query knowledge graphs without the need to learn SPARQL and/or know the schema and vocabulary of them, eliminating the time and effort that ordinary users need to spend in order to use them. In this article, we present some experiments and their results illustrating how ChatGPT can help ordinary users to generate SPARQL queries, without knowing SPARQL, to effectively use knowledge graphs and exploit their wealth of data. We experimented with ChatGPT to explore whether it can generate SPARQL queries based on user's natural language input and a given vocabulary (ontology) about an educational knowledge graph. To this end we have devised a specific prompt template. Results indicate that LLMs can indeed help in this direction, given the fact that they are prompted properly, using good English language. We have also discussed some practical lessons learned through this experiment.},
booktitle = {Proceedings of the 28th Pan-Hellenic Conference on Progress in Computing and Informatics},
pages = {287–295},
numpages = {9},
keywords = {AI application, ChatGPT, RDF, knowledge graphs, large language model use cases},
location = {
},
series = {PCI '24}
}

@inproceedings{10.1145/3675094.3679000,
author = {Fiori, Michele and Civitarese, Gabriele and Bettini, Claudio},
title = {Using Large Language Models to Compare Explainable Models for Smart Home Human Activity Recognition},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3679000},
doi = {10.1145/3675094.3679000},
abstract = {Recognizing daily activities with unobtrusive sensors in smart environments enables various healthcare applications. Monitoring how subjects perform activities at home and their changes over time can reveal early symptoms of health issues, such as cognitive decline. Most approaches in this field use deep learning models, which are often seen as black boxes mapping sensor data to activities. However, non-expert users like clinicians need to trust and understand these models' outputs. Thus, eXplainable AI (XAI) methods for Human Activity Recognition have emerged to provide intuitive natural language explanations from these models. Different XAI methods generate different explanations, and their effectiveness is typically evaluated through user surveys, that are often challenging in terms of costs and fairness. This paper proposes an automatic evaluation method using Large Language Models (LLMs) to identify, in a pool of candidates, the best XAI approach for non-expert users. Our preliminary results suggest that LLM evaluation aligns with user surveys.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {881–884},
numpages = {4},
keywords = {evaluation, human activity recognition, llms, xai},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3640457.3688022,
author = {Afreen, Neda},
title = {Explainable and Faithful Educational Recommendations through Causal Language Modelling via Knowledge Graphs},
year = {2024},
isbn = {9798400705052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640457.3688022},
doi = {10.1145/3640457.3688022},
abstract = {The rapid expansion of digital education has significantly increased the need for recommender systems to help learners navigate the extensive variety of available learning resources. Recent advancements in these systems have notably improved the personalization of course recommendations. However, many existing systems fail to provide clear explanations for their recommendations, making it difficult for learners to understand why a particular suggestion was made. Researchers have emphasized the importance of explanations in various domains such as e-commerce, media, and entertainment, demonstrating how explanations can enhance system transparency, foster user trust, and improve decision-making processes. Despite these insights, such approaches have been rarely applied to the educational domain, and their effectiveness in practical use remains largely unexamined. My research focuses on developing explainable recommender systems for digital education. First, I aim to design knowledge graphs that can support high-quality recommendations in the educational context. Second, I will create models backed by these knowledge graphs that not only deliver accurate recommendations but also provide faithful explanations for each suggestion. Third, I will evaluate the effectiveness of these explainable recommender systems in real-world educational environments. Ultimately, this research aims to advance the development of more transparent and user-centric educational technologies.},
booktitle = {Proceedings of the 18th ACM Conference on Recommender Systems},
pages = {1358–1360},
numpages = {3},
keywords = {Explainability, Knowledge Graph, Language Modeling, Personalization, Recommendation, Recommender Systems, Transparency},
location = {Bari, Italy},
series = {RecSys '24}
}

@inproceedings{10.1145/3664647.3681389,
author = {Zhao, Shengwei and Xu, Linhai and Liu, Yuying and Du, Shaoyi},
title = {Multi-grained Correspondence Learning of Audio-language Models for Few-shot Audio Recognition},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3681389},
doi = {10.1145/3664647.3681389},
abstract = {Large-scale pre-trained audio-language models excel in general multi-modal representation, facilitating their adaptation to downstream audio recognition tasks in a data-efficient manner. However, existing few-shot audio recognition methods based on audio-language models primarily focus on learning coarse-grained correlations, which are not sufficient to capture the intricate matching patterns between the multi-level information of audio and the diverse characteristics of category concepts. To address this gap, we propose multi-grained correspondence learning for bootstrapping audio-language models to improve audio recognition with few training samples. This approach leverages generative models to enrich multi-modal representation learning, mining the multi-level information of audio alongside the diverse characteristics of category concepts. Multi-grained matching patterns are then established through multi-grained key-value cache and multi-grained cross-modal contrast, enhancing the alignment between audio and category concepts. Additionally, we incorporate optimal transport to tackle temporal misalignment and semantic intersection issues in fine-grained correspondence learning, enabling flexible fine-grained matching. Our method achieves state-of-the-art results on multiple benchmark datasets for few-shot audio recognition, with comprehensive ablation experiments validating its effectiveness.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {9244–9252},
numpages = {9},
keywords = {audio-language models, few-shot audio recognition, multi-grained correspondence learning, optimal transport},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@article{10.14778/3681954.3681973,
author = {Sun, Yushi and Xin, Hao and Sun, Kai and Xu, Yifan Ethan and Yang, Xiao and Dong, Xin Luna and Tang, Nan and Chen, Lei},
title = {Are Large Language Models a Good Replacement of Taxonomies?},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3681973},
doi = {10.14778/3681954.3681973},
abstract = {Large language models (LLMs) demonstrate an impressive ability to internalize knowledge and answer natural language questions. Although previous studies validate that LLMs perform well on general knowledge while presenting poor performance on long-tail nuanced knowledge, the community is still doubtful about whether the traditional knowledge graphs should be replaced by LLMs. In this paper, we ask if the schema of knowledge graph (i.e., taxonomy) is made obsolete by LLMs. Intuitively, LLMs should perform well on common taxonomies and at taxonomy levels that are common to people. Unfortunately, there lacks a comprehensive benchmark that evaluates the LLMs over a wide range of taxonomies from common to specialized domains and at levels from root to leaf so that we can draw a confident conclusion. To narrow the research gap, we constructed a novel taxonomy hierarchical structure discovery benchmark named TaxoGlimpse to evaluate the performance of LLMs over taxonomies. TaxoGlimpse covers ten representative taxonomies from common to specialized domains with in-depth experiments of different levels of entities in this taxonomy from root to leaf. Our comprehensive experiments of eighteen LLMs under three prompting settings validate that LLMs perform miserably poorly in handling specialized taxonomies and leaf-level entities. Specifically, the QA accuracy of the best LLM drops by up to 30\% as we go from common to specialized domains and from root to leaf levels of taxonomies.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {2919–2932},
numpages = {14}
}

@inproceedings{10.1145/3663741.3664785,
author = {Barbon Junior, Sylvio and Ceravolo, Paolo and Groppe, Sven and Jarrar, Mustafa and Maghool, Samira and S\`{e}des, Florence and Sahri, Soror and Van Keulen, Maurice},
title = {Are Large Language Models the New Interface for Data Pipelines?},
year = {2024},
isbn = {9798400706790},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663741.3664785},
doi = {10.1145/3663741.3664785},
abstract = {A Language Model is a term that encompasses various types of models designed to understand and generate human communication. Large Language Models (LLMs) have gained significant attention due to their ability to process text with human-like fluency and coherence, making them valuable for a wide range of data-related tasks fashioned as pipelines. The capabilities of LLMs in natural language understanding and generation, combined with their scalability, versatility, and state-of-the-art performance, enable innovative applications across various AI-related fields, including eXplainable Artificial Intelligence (XAI), Automated Machine Learning (AutoML), and Knowledge Graphs (KG). Furthermore, we believe these models can extract valuable insights and make data-driven decisions at scale, a practice commonly referred to as Big Data Analytics (BDA). In this position paper, we provide some discussions in the direction of unlocking synergies among these technologies, which can lead to more powerful and intelligent AI solutions, driving improvements in data pipelines across a wide range of applications and domains integrating humans, computers, and knowledge.},
booktitle = {Proceedings of the International Workshop on Big Data in Emergent Distributed Environments},
articleno = {6},
numpages = {6},
keywords = {Automated Machine Learning, Big Data Analytic, Human-Computer Interaction, Knowledge Graphs, Natural Language Understanding, eXplainable Artificial Intelligence},
location = {Santiago, AA, Chile},
series = {BiDEDE '24}
}

@inproceedings{10.1145/3656650.3656688,
author = {Grigis, Paolo and De Angeli, Antonella},
title = {Playwriting with Large Language Models: Perceived Features, Interaction Strategies and Outcomes},
year = {2024},
isbn = {9798400717642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3656650.3656688},
doi = {10.1145/3656650.3656688},
abstract = {Large Language Models (LLMs) are sparking debates about creativity, intellectual property, and artistic integrity. This paper focuses on creativity, defined as consensual agreement among domain experts. It presents an inductive analysis of seven semi-structured interviews with professional playwrights who engaged in a longitudinal project with the aim of writing a theatre script using commercial systems. Overall, participants regarded LLMs as unsuitable for playwrighting. However, they enjoyed the experience and identified utility for editorial tasks and brainstorming. A significant obstacle was associated with the politics embedded in LLMs. Not only did these systems avoid a language that could offend sensibilities, but they also refused to engage in taboos and conflicts, which are the core of dramaturgy. Other system features (speed, exploitation, and unpredictability) were sometimes considered conducive and sometimes detrimental to creativity. Participants experienced difficulties and tried to build common ground by trial and error. Often, this strategy evolved into role play: the playwright instructed the LLM to enact characters. The interaction provided hints of inspiration and fostered suspension of disbelief and ontological reflection. However, it often led to technology rejection. Comparing and contrasting our insights with related work, we conclude by opening new directions for research at the boundaries of HCI and AI.},
booktitle = {Proceedings of the 2024 International Conference on Advanced Visual Interfaces},
articleno = {38},
numpages = {9},
keywords = {Creative AI, Creativity, Roleplay, Suspension of Disbelief, Theatre, Unpredictability},
location = {Arenzano, Genoa, Italy},
series = {AVI '24}
}

@inproceedings{10.1145/3701716.3717822,
author = {Vakaj, Edlira and Mihindukulasooriya, Nandana and Tiwari, Sanju and Rodriguez-M\'{e}ndez, Sergio J.},
title = {4th International Workshop on Natural Language Processing for Knowledge Graph Construction},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3717822},
doi = {10.1145/3701716.3717822},
abstract = {Knowledge graphs (KG) are becoming increasingly popular, at the heart of Gartner's emerging tech impact radar, especially as a complementary theme for addressing the challenges of recent advances in natural language processing (NLP) with large language models related to responsible AI such as fairness, transparency, accountability, and explainability. Sir Tim Berners-Lee's seminal work ''Weaving the Web: The Original Design and Ultimate Destiny of the World Wide Web'', envisioned a World Wide Web where information is not only accessible but structured, allowing machines to interpret data meaningfully. This vision laid the groundwork for technologies such as RDF (Resource Description Framework) and OWL (Web Ontology Language), which serve as foundational components for modern KGs.However, the process of building domain-specific KGs from extensive text corpora is highly complex and resource-intensive, requiring careful task design for entity recognition, disambiguation, and relationship extraction, among others. These tasks are essential to ensure accuracy and relevance in knowledge representation, but they pose considerable challenges. Addressing these complexities is crucial for the continued advancement and application of KGs across domains.In this context, the 4th NLP4KGC workshop is held to create a collaborative platform for researchers, practitioners, and industry experts in NLP and KG construction. Following the success and growing community engagement in the previous three editions, this year's workshop aims to deepen collaboration and encourage innovative solutions in this rapidly evolving field. The 4th NLP4KGC will continue to bridge academia and industry, fostering the exchange of insights, tools, and methodologies at the intersection of NLP and KG development. The 4th NLP4KGC will consist of five accepted papers and three keynotes from distinguished speakers.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {2545–2548},
numpages = {4},
keywords = {knowledge graph, large language models, natural language understanding, responsible ai, semantic web, web of data},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3577190.3614158,
author = {Hensel, Laura Birka and Yongsatianchot, Nutchanon and Torshizi, Parisa and Minucci, Elena and Marsella, Stacy},
title = {Large language models in textual analysis for gesture selection},
year = {2023},
isbn = {9798400700552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577190.3614158},
doi = {10.1145/3577190.3614158},
abstract = {Gestures perform a variety of communicative functions that powerfully influence human face-to-face interaction. How this communicative function is achieved varies greatly between individuals and depends on the role of the speaker and the context of the interaction. Approaches to automatic gesture generation vary not only in the degree to which they rely on data-driven techniques but also the degree to which they can produce context and speaker specific gestures. However, these approaches face two major challenges: The first is obtaining sufficient training data that is appropriate for the context and the goal of the application. The second is related to designer control to realize their specific intent for the application. Here, we approach these challenges by using large language models (LLMs) to show that these powerful models of large amounts of data can be adapted for gesture analysis and generation. Specifically, we used ChatGPT as a tool for suggesting context-specific gestures that can realize designer intent based on minimal prompts. We also find that ChatGPT can suggests novel yet appropriate gestures not present in the minimal training data. The use of LLMs is a promising avenue for gesture generation that reduce the need for laborious annotations and has the potential to flexibly and quickly adapt to different designer intents.},
booktitle = {Proceedings of the 25th International Conference on Multimodal Interaction},
pages = {378–387},
numpages = {10},
keywords = {gesture analysis, gesture selection, large language models},
location = {Paris, France},
series = {ICMI '23}
}

@inproceedings{10.1145/3696630.3730562,
author = {Klimek, Radoslaw},
title = {RE-oriented Model Development with LLM Support and Deduction-based Verification},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3730562},
doi = {10.1145/3696630.3730562},
abstract = {The requirements engineering (RE) phase is pivotal in developing high-quality software. Integrating advanced modelling techniques with large language models (LLMs) and formal verification in a logical style can significantly enhance this process. We propose a comprehensive framework that focuses on specific Unified Modelling Language (UML) diagrams for preliminary system development. This framework offers visualisations at various modelling stages and seamlessly integrates large language models and logical reasoning engines. The behavioural models generated with the assistance of LLMs are automatically translated into formal logical specifications. Deductive formal verification ensures that logical requirements and interrelations between software artefacts are thoroughly addressed. Ultimately, the framework facilitates the automatic generation of program skeletons, streamlining the transition from design to implementation.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1297–1304},
numpages = {8},
keywords = {requirements engineering, formal IDE, UML modelling, large language models, automated logical reasoning, temporal logic},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1145/3653946.3653961,
author = {Jiang, Yingdi and Yao, Jiarui and Li, Fangfei and Zhang, Yan},
title = {Research on Engineering Management Question-answering System in the Communication Industry Based on Large Language Models and Knowledge Graphs},
year = {2024},
isbn = {9798400716553},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3653946.3653961},
doi = {10.1145/3653946.3653961},
abstract = {In the engineering management of the communication industry, there are many issues, including low efficiency in information acquisition and limitations in the level of intelligence.Large language models, with their powerful text comprehension and generation capabilities, offer new perspectives for the development of this field.This study constructed a question-answering system using a combined approach of large language models and text knowledge bases. The system dynamically leverages abundant external knowledge and enhances the model's reasoning ability and interpretability through knowledge graphs. In response to five categories of issues in engineering management, experiments and in-depth analysis revealed that although large language models may lack granularity in addressing some complex problems, the question-answering system overall achieved intelligent assistance, improving the efficiency of collaborative engineering management.},
booktitle = {Proceedings of the 2024 7th International Conference on Machine Vision and Applications},
pages = {100–105},
numpages = {6},
keywords = {Engineering management, Keywords • Large language models, Knowledge graphs, Question-answering},
location = {Singapore, Singapore},
series = {ICMVA '24}
}

@article{10.1145/3596219,
author = {Zhao, Shuai and Li, Qing and Yang, Yuer and Wen, Jinming and Luo, Weiqi},
title = {From Softmax to Nucleusmax: A Novel Sparse Language Model for Chinese Radiology Report Summarization},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3596219},
doi = {10.1145/3596219},
abstract = {The Chinese radiology report summarization is a crucial component in smart healthcare that employs language models to summarize key findings in radiology reports and communicate these findings to physicians. However, most language models for radiology report summarization utilize a softmax transformation in their output layer, leading to dense alignments and strictly positive output probabilities. This density is inefficient, reducing model interpretability and giving probability mass to many unrealistic outputs. To tackle this issue, we propose a novel approach named nucleusmax. Nucleusmax is able to mitigate dense outputs and improve model interpretability by truncating the unreliable tail of the probability distribution. In addition, we incorporate nucleusmax with a copy mechanism, a useful technique to avoid professional errors in the generated diagnostic opinions. To further promote the research of radiology report summarization, we also have created a Chinese radiology report summarization dataset, which is freely available. Experimental results showed via both automatic and human evaluation that the proposed approach substantially improves the sparsity and overall quality of outputs over competitive softmax models, producing radiology summaries that approach the quality of those authored by physicians. In general, our work demonstrates the feasibility and prospect of the language model to the domain of radiology and smart healthcare.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {180},
numpages = {21},
keywords = {Chinese radiology report summarization, language model, softmax, abstractive summarization}
}

@inproceedings{10.1145/3689492.3690049,
author = {Thiede, Christoph and Taeumel, Marcel and B\"{o}hme, Lukas and Hirschfeld, Robert},
title = {Talking to Objects in Natural Language: Toward Semantic Tools for Exploratory Programming},
year = {2024},
isbn = {9798400712159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689492.3690049},
doi = {10.1145/3689492.3690049},
abstract = {In exploratory programming, programmers often face a semantic gap between their high-level understanding and the low-level interfaces available for interacting with objects in a system. That is, technical object structure and behavior need to be interpreted as abstract domain concepts, which then increases cognitive load and thus impedes exploration progress. We propose semantic object interfaces that bridge this gap by enabling contextual, natural-language conversations with objects. Our approach leverages an exploratory programming agent powered by a large language model (LLM) to translate natural-language questions into low-level experiments and provide high-level answers. We describe a framework for integrating semantic object interfaces into existing exploratory programming systems, including a prototype implementation in Squeak/Smalltalk using GPT-4o. We showcase the potential of semantic object interfaces through case studies and discuss their feasibility, limitations, and impact on the programming experience. While challenges remain, our approach promises to reduce mental effort and empower programmers to explore and understand systems at a higher level of abstraction for a better programming experience.},
booktitle = {Proceedings of the 2024 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {68–84},
numpages = {17},
keywords = {ChatGPT, LLMs, Smalltalk, conversational agents, exploratory programming, generative AI, natural-language programming, object-oriented programming, semantic tools},
location = {Pasadena, CA, USA},
series = {Onward! '24}
}

@article{10.1145/3596597,
author = {Hossain, Bayzid Ashik and Mukta, Md. Saddam Hossain and Islam, Md Adnanul and Zaman, Akib and Schwitter, Rolf},
title = {Natural Language–Based Conceptual Modelling Frameworks: State of the Art and Future Opportunities},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3596597},
doi = {10.1145/3596597},
abstract = {Identifying requirements for an information system is an important task and conceptual modelling is the first step in this process. Conceptual modelling plays a critical role in the information system design process and usually involves domain experts and knowledge engineers who brainstorm together to identify the required knowledge to build an information system. The conceptual modelling process starts with the collection of necessary information from the domain experts by the knowledge engineers. Afterwards, the knowledge engineers use traditional model driven engineering techniques to design the system based on the collected information. Natural language–based conceptual modelling frameworks or systems are used to help domain experts and knowledge engineers in eliciting requirements and building conceptual models from a natural language text. In this article, we discuss the state of the art of some recent conceptual modelling frameworks that are based on natural language. We take a closer look at how these frameworks are built, in particular at the underlying motivation, architecture, types of natural language used (e.g., restricted vs. unrestricted), types of the conceptual model generated, verification support of the requirements specifications as well as the conceptual models, and underlying knowledge representation formalism. We also discuss some future research opportunities that these frameworks offer.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {12},
numpages = {26},
keywords = {Natural language processing, information extraction, conceptual modelling, knowledge representation, semantic round-tripping}
}

@inproceedings{10.1145/3624062.3624172,
author = {Ding, Xianzhong and Chen, Le and Emani, Murali and Liao, Chunhua and Lin, Pei-Hung and Vanderbruggen, Tristan and Xie, Zhen and Cerpa, Alberto and Du, Wan},
title = {HPC-GPT: Integrating Large Language Model for High-Performance Computing},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624172},
doi = {10.1145/3624062.3624172},
abstract = {Large Language Models (LLMs), including the LLaMA model, have exhibited their efficacy across various general-domain natural language processing (NLP) tasks. However, their performance in high-performance computing (HPC) domain tasks has been less than optimal due to the specialized expertise required to interpret the model’s responses. In response to this challenge, we propose HPC-GPT, a novel LLaMA-based model that has been supervised fine-tuning using generated QA (Question-Answer) instances for the HPC domain. To evaluate its effectiveness, we concentrate on two HPC tasks: managing AI models and datasets for HPC, and data race detection. By employing HPC-GPT, we demonstrate comparable performance with existing methods on both tasks, exemplifying its excellence in HPC-related scenarios. Our experiments on open-source benchmarks yield extensive results, underscoring HPC-GPT’s potential to bridge the performance gap between LLMs and HPC-specific tasks. With HPC-GPT, we aim to pave the way for LLMs to excel in HPC domains, simplifying the utilization of language models in complex computing applications.},
booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {951–960},
numpages = {10},
keywords = {Data Race Detection, High-performance Computing, Large Language Model, Neural Network., OpenMP},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@article{10.1145/3715156,
author = {Varagnolo, Davide and Melo, Dora and Pimenta Rodrigues, Irene},
title = {Translating Natural Language Questions into CIDOC-CRM SPARQL Queries to Access Cultural Heritage Knowledge Bases},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1556-4673},
url = {https://doi.org/10.1145/3715156},
doi = {10.1145/3715156},
abstract = {To explore information on the semantic web, SPARQL queries or DL-queries are suitable tools. However, users interested in exploring the content of such knowledge bases often find it challenging to employ formal query languages, as this requires familiarity with the target domain’s representation model. To address these challenges, a question-answering system that automatically translates natural language questions into SPARQL queries, over the Smithsonian American Art Museum CIDOC-CRM representation is presented. The proposed approach uses an ontology, named Query Ontology, defined to represent the natural language concepts and relations specific to the question’s domain. This system’s architecture uses a traditional natural language processing symbolic approach, with a pipeline of modules for the syntactic, semantic, and pragmatic analysis. An evaluation of the proposed system is presented and shows very promising results.},
journal = {J. Comput. Cult. Herit.},
month = apr,
articleno = {21},
numpages = {28},
keywords = {datasets, SPARQL queries, CIDOC-CRM representation, SAAM’s knowledge base}
}

@article{10.1145/3606699,
author = {Pich\'{e}, Dominique and Font, Ludovic and Zouaq, Amal and Gagnon, Michel},
title = {Comparing Heuristic Rules and Masked Language Models for Entity Alignment in the Literature Domain},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3606699},
doi = {10.1145/3606699},
abstract = {The cultural world offers a staggering amount of rich and varied metadata on cultural heritage, accumulated by governmental, academic, and commercial players. However, the variety of involved institutions means that the data are stored in as many complex and often incompatible models and standards, which limits its availability and explorability by the greater public. The adoption of Linked Open Data technologies allows a strong interlinking of these various databases as well as external connections with existing knowledge bases. However, as they often contain references to the same entities, the delicate issue of entity alignment becomes the central challenge, especially in the absence or scarcity of unique global identifiers. To tackle this issue, we explored two approaches, one based on a set of heuristic rules and one based on masked language models, or masked language models (MLMs). We compare these two approaches, as well as different variations of MLMs, including some models trained on a different language, and various levels of data cleaning and labeling. Our results show that heuristics are a solid approach but also that MLM-based entity alignment obtains better performance coupled with the fact that it is robust to the data format and does not require any form of data preprocessing, which was not the case of the heuristic approach in our experiments.},
journal = {J. Comput. Cult. Herit.},
month = aug,
articleno = {62},
numpages = {18},
keywords = {Linked open data, entity matching, masked language models, cultural heritage, literature}
}

@inproceedings{10.1145/3587259.3627572,
author = {Rula, Anisa and D'Souza, Jennifer},
title = {Procedural Text Mining with Large Language Models},
year = {2023},
isbn = {9798400701412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587259.3627572},
doi = {10.1145/3587259.3627572},
abstract = {Recent advancements in the field of Natural Language Processing, particularly the development of large-scale language models that are pretrained on vast amounts of knowledge, are creating novel opportunities within the realm of Knowledge Engineering. In this paper, we investigate the usage of large language models (LLMs) in both zero-shot and in-context learning settings to tackle the problem of extracting procedures from unstructured PDF text in an incremental question-answering fashion. In particular, we leverage the current state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model, accompanied by two variations of in-context learning that involve an ontology with definitions of procedures and steps and a limited number of samples of few-shot learning. The findings highlight both the promise of this approach and the value of the in-context learning customisations. These modifications have the potential to significantly address the challenge of obtaining sufficient training data, a hurdle often encountered in deep learning-based Natural Language Processing techniques for procedure extraction.},
booktitle = {Proceedings of the 12th Knowledge Capture Conference 2023},
pages = {9–16},
numpages = {8},
keywords = {knowledge capture, knowledge representation},
location = {Pensacola, FL, USA},
series = {K-CAP '23}
}

@inproceedings{10.1145/3608298.3608324,
author = {Schl\"{o}r, Daniel and Pfister, Jan and Hotho, Andreas},
title = {Optimizing Medical Service Request Processes through Language Modeling and Semantic Search},
year = {2023},
isbn = {9798400700712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3608298.3608324},
doi = {10.1145/3608298.3608324},
abstract = {Medical service requests are a crucial part of the workflow in hospitals and healthcare organizations. However, the process of requesting medical services can be time consuming and can require physicians and medical personnel to navigate complex interfaces and enter detailed information about the requested service. In this paper, we propose a system that uses machine learning techniques such as large language models and semantic search to optimize the process of requesting medical services. Our approach enables physicians to request medical services using natural language rather than navigating complex interfaces, allowing for more efficient and flexible interactions with hospital information systems. We evaluate our approach on real-world data and discuss the implications of our work for the future of digital health care. Our results suggest that our approach has the potential to streamline the process of requesting medical services and reduce the time and manual effort required in the daily hospital routine.},
booktitle = {Proceedings of the 2023 7th International Conference on Medical and Health Informatics},
pages = {136–141},
numpages = {6},
keywords = {language modeling, medical service optimization, semantic search},
location = {Kyoto, Japan},
series = {ICMHI '23}
}

@inproceedings{10.1145/3661304.3661901,
author = {Sequeda, Juan and Allemang, Dean and Jacob, Bryon},
title = {A Benchmark to Understand the Role of Knowledge Graphs on Large Language Model's Accuracy for Question Answering on Enterprise SQL Databases},
year = {2024},
isbn = {9798400706530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661304.3661901},
doi = {10.1145/3661304.3661901},
abstract = {Enterprise applications of Large Language Models (LLMs) hold promise for question answering on enterprise SQL databases. However, the extent to which LLMs can accurately respond to enterprise questions in such databases remains unclear, given the absence of suitable Text-to-SQL benchmarks tailored to enterprise settings. Additionally, the potential of Knowledge Graphs (KGs) to enhance LLM-based question answering by providing business context is not well understood. This study aims to evaluate the accuracy of LLM-powered question answering systems in the context of enterprise questions and SQL databases, while also exploring the role of knowledge graphs in improving accuracy. To achieve this, we introduce a benchmark comprising an enterprise SQL schema in the insurance domain, a range of enterprise queries encompassing reporting to metrics, and a contextual layer incorporating an ontology and mappings that define a knowledge graph. Our primary finding reveals that question answering using GPT-4, with zero-shot prompts directly on SQL databases, achieves an accuracy of 16\%. Notably, this accuracy increases to 54\% when questions are posed over a Knowledge Graph representation of the enterprise SQL database. Therefore, investing in Knowledge Graph provides higher accuracy for LLM powered question answering systems.},
booktitle = {Proceedings of the 7th Joint Workshop on Graph Data Management Experiences \&amp; Systems (GRADES) and Network Data Analytics (NDA)},
articleno = {5},
numpages = {12},
location = {Santiago, AA, Chile},
series = {GRADES-NDA '24}
}

@proceedings{10.1145/3677779,
title = {CMNM '24: Proceedings of the International Conference on Modeling, Natural Language Processing and Machine Learning},
year = {2024},
isbn = {9798400709760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xi'an, China}
}

@inproceedings{10.1145/3652620.3688348,
author = {Gerhold, Marcus and Kouzel, Aliaksei and Mangal, Haroun and Mehmed, Selin and Zaytsev, Vadim},
title = {Modelling of Cyber-Physical Systems through Domain-Specific Languages: Decision, Analysis, Design},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688348},
doi = {10.1145/3652620.3688348},
abstract = {Cyber-Physical Systems (CPS) integrate computational algorithms and physical components, requiring sophisticated modelling techniques to address complex interactions and dynamics. This paper explores the creation of Domain-Specific Languages (DSLs) tailored for CPS, focusing on the initial three critical phases: decision, analysis, design. We present four key aspects to address in the decision phase, design an ontology as a domain model for the analysis phase, and collect some advice for the design phase. By systematically addressing these phases, we provide a comprehensive framework for developing DSLs that can efficiently model CPS, facilitating improved design, verification, and deployment of these intricate systems.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {1170–1179},
numpages = {10},
keywords = {cyber-physical systems, ontological analysis, domain-specific languages},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3696500.3696523,
author = {He, Yudong and Tang, Yinqiu and Chen, Tianhong},
title = {A Study on Large Language Model-Based Approach for Construction Contract Risk Detection},
year = {2024},
isbn = {9798400710278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696500.3696523},
doi = {10.1145/3696500.3696523},
abstract = {Construction projects typically involve large-scale operations and are subject to complex external conditions, making it essential to safeguard the interests of contractor enterprises through well-crafted contract clauses. However, the current reliance on expert judgment for identifying contract risks presents several challenges, including lengthy processing times, heavy workloads, and inconsistent results. To address these issues, this study introduces a Large Language Model (LLM)-based approach for automating the identification of risks in construction contracts. The proposed method was rigorously validated on 26 actual contracts, achieving an average accuracy of 76.7\% across four state-of-the-art LLMs. This research advances the application of LLMs in construction contract management, providing practical solutions to existing challenges and setting the stage for further exploration in LLM-driven contract analysis.},
booktitle = {Proceedings of the 2024 International Conference on Big Data and Digital Management},
pages = {136–141},
numpages = {6},
location = {Shanghai, China},
series = {ICBDDM '24}
}

@article{10.1145/3589131,
author = {Van Thin, Dang and Hao, Duong Ngoc and Nguyen, Ngan Luu-Thuy},
title = {Vietnamese Sentiment Analysis: An Overview and Comparative Study of Fine-tuning Pretrained Language Models},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3589131},
doi = {10.1145/3589131},
abstract = {Sentiment Analysis (SA) is one of the most active research areas in the Natural Language Processing (NLP) field due to its potential for business and society. With the development of language representation models, numerous methods have shown promising efficiency in fine-tuning pre-trained language models in NLP downstream tasks. For Vietnamese, many available pre-trained language models were also released, including the monolingual and multilingual language models. Unfortunately, all of these models were trained on different architectures, pre-trained data, and pre-processing steps; consequently, fine-tuning these models can be expected to yield different effectiveness. In addition, there is no study focusing on evaluating the performance of these models on the same datasets for the SA task up to now. This article presents a fine-tuning approach to investigate the performance of different pre-trained language models for the Vietnamese SA task. The experimental results show the superior performance of the monolingual PhoBERT model and ViT5 model in comparison with previous studies and provide new state-of-the-art performances on five benchmark Vietnamese SA datasets. To the best of our knowledge, our study is the first attempt to investigate the performance of fine-tuning Transformer-based models on five datasets with different domains and sizes for the Vietnamese SA task.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {166},
numpages = {27},
keywords = {Vietnamese Sentiment Analysis, fine-tuning language models, monolingual BERT model, multilingual BERT model, T5 architecture}
}

@article{10.1145/3688089,
author = {Zhou, Kyrie Zhixuan and Kilhoffer, Zachary and Sanfilippo, Madelyn Rose and Underwood, Ted and Gumusel, Ece and Wei, Mengyi and Choudhry, Abhinav and Xiong, Jinjun},
title = {Ethics, Governance, and User Mental Models for Large Language Models in Computing Education},
year = {2024},
issue_date = {Fall 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/3688089},
doi = {10.1145/3688089},
abstract = {Large language models like ChatGPT are disrupting many industries, including computing education. How should policy evolve to improve learning outcomes?},
journal = {XRDS},
month = oct,
pages = {46–51},
numpages = {6}
}

@article{10.1145/3529755,
author = {Zini, Julia El and Awad, Mariette},
title = {On the Explainability of Natural Language Processing Deep Models},
year = {2022},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3529755},
doi = {10.1145/3529755},
abstract = {Despite their success, deep networks are used as black-box models with outputs that are not easily explainable during the learning and the prediction phases. This lack of interpretability is significantly limiting the adoption of such models in domains where decisions are critical such as the medical and legal fields. Recently, researchers have been interested in developing methods that help explain individual decisions and decipher the hidden representations of machine learning models in general and deep networks specifically. While there has been a recent explosion of work on Explainable Artificial Intelligence (ExAI) on deep models that operate on imagery and tabular data, textual datasets present new challenges to the ExAI community. Such challenges can be attributed to the lack of input structure in textual data, the use of word embeddings that add to the opacity of the models and the difficulty of the visualization of the inner workings of deep models when they are trained on textual data.Lately, methods have been developed to address the aforementioned challenges and present satisfactory explanations on Natural Language Processing (NLP) models. However, such methods are yet to be studied in a comprehensive framework where common challenges are properly stated and rigorous evaluation practices and metrics are proposed.Motivated to democratize ExAI methods in the NLP field, we present in this work a survey that studies model-agnostic as well as model-specific explainability methods on NLP models. Such methods can either develop inherently interpretable NLP models or operate on pre-trained models in a post hoc manner. We make this distinction and we further decompose the methods into three categories according to what they explain: (1) word embeddings (input level), (2) inner workings of NLP models (processing level), and (3) models’ decisions (output level). We also detail the different evaluation approaches interpretability methods in the NLP field. Finally, we present a case-study on the well-known neural machine translation in an appendix, and we propose promising future research directions for ExAI in the NLP field.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {103},
numpages = {31},
keywords = {ExAI, NLP, language models, transformers, neural machine translation, transparent embedding models, explaining decisions}
}

@inproceedings{10.1145/3635059.3635068,
author = {Mamalis, Marios Evangelos and Kalampokis, Evangelos and Karamanou, Areti and Brimos, Petros and Tarabanis, Konstantinos},
title = {Can Large Language Models Revolutionalize Open Government Data Portals? A Case of Using ChatGPT in statistics.gov.scot},
year = {2024},
isbn = {9798400716263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3635059.3635068},
doi = {10.1145/3635059.3635068},
abstract = {Large language models possess tremendous natural language understanding and generation abilities. However, they often lack the ability to discern between fact and fiction, leading to factually incorrect responses. Open Government Data are repositories of, often times linked, information that is freely available to everyone. By combining these two technologies in a proof of concept designed application utilizing the GPT3.5 OpenAI model and the Scottish open statistics portal, we show that not only is it possible to augment the large language model’s factuality of responses, but also propose a novel way to effectively access and retrieve statistical information from the data portal just through natural language querying. We anticipate that this paper will trigger a discussion regarding the transformation of Open Government Portals through large language models.},
booktitle = {Proceedings of the 27th Pan-Hellenic Conference on Progress in Computing and Informatics},
pages = {53–59},
numpages = {7},
keywords = {chatgpt, large language model, linked data, natural language processing, open government data},
location = {Lamia, Greece},
series = {PCI '23}
}

@inproceedings{10.1145/3584371.3612953,
author = {Quintana, Felix and Treangen, Todd and Kavraki, Lydia},
title = {Leveraging Large Language Models for Predicting Microbial Virulence from Protein Structure and Sequence},
year = {2023},
isbn = {9798400701269},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584371.3612953},
doi = {10.1145/3584371.3612953},
abstract = {In the aftermath of COVID-19, screening for pathogens has never been a more relevant problem. However, computational screening for pathogens is challenging due to a variety of factors, including (i) the complexity and role of the host, (ii) virulence factor divergence and dynamics, and (iii) population and community-level dynamics. Considering a potential pathogen's molecular interactions, specifically individual proteins and protein interactions can help pinpoint a potential protein of a given microbe to cause disease. However, existing tools for pathogen screening rely on existing annotations (KEGG, GO, etc), making the assessment of novel and unannotated proteins more challenging. Here, we present an LLM-inspired approach that considers protein sequence and structure to predict protein virulence. We present a two-stage model incorporating evolutionary features captured from the DistilProtBert language model and protein structure in a graph convolutional network. Our model performs better than sequence alone for virulence function when high-quality structures are present, thus representing a path forward for virulence prediction of novel and unannotated proteins.},
booktitle = {Proceedings of the 14th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
articleno = {103},
numpages = {6},
keywords = {protein function, virulence prediction, graph-based models, large language models},
location = {Houston, TX, USA},
series = {BCB '23}
}

@article{10.14778/3712221.3712222,
author = {Qiang, Zhangcheng and Wang, Weiqing and Taylor, Kerry},
title = {Agent-OM: Leveraging LLM Agents for Ontology Matching},
year = {2024},
issue_date = {November 2024},
publisher = {VLDB Endowment},
volume = {18},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3712221.3712222},
doi = {10.14778/3712221.3712222},
abstract = {Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {516–529},
numpages = {14}
}

@article{10.1145/3705617,
author = {Qu\'{e}r\'{e}, Marianne Aubin Le and Schroeder, Hope and Randazzo, Casey and Gao, Jie},
title = {The State of Large Language Models in HCI Research: Workshop Report},
year = {2025},
issue_date = {January - February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1072-5520},
url = {https://doi.org/10.1145/3705617},
doi = {10.1145/3705617},
abstract = {In this section, we feature reports from conferences, symposia, workshops, and similar events, focusing on discussions where the boundaries of HCI and UX are being challenged and where debate is lively and ongoing.},
journal = {Interactions},
month = jan,
pages = {8–9},
numpages = {2}
}

@article{10.1109/TASLP.2024.3407575,
author = {Chen, Weize and Han, Xu and Lin, Yankai and He, Kaichen and Xie, Ruobing and Zhou, Jie and Liu, Zhiyuan and Sun, Maosong},
title = {Hyperbolic Pre-Trained Language Model},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3407575},
doi = {10.1109/TASLP.2024.3407575},
abstract = {In recent years, we have witnessed significant improvements in pre-trained language models (PLM) brought about by the scaling of parameter sizes and data amounts. However, this also brings high computational and storage costs. In this paper, we present a new direction to improve PLMs without scaling parameters and data: adopting a geometric feature space that is more suitable for encoding the intrinsic structured features of text. Although text is generally considered unstructured data, it possesses rich intrinsic structured features that signify syntactic and semantic relationships. Leveraging these structured features is vital for text understanding. Given that structured features are better encoded in hyperbolic spaces than in the Euclidean spaces used by conventional PLMs, we propose that PLMs should operate entirely within hyperbolic spaces. Our experiments demonstrate the superiority of hyperbolic PLMs over Euclidean PLMs across a wide variety of tasks, using the same parameter and data settings. This suggests that altering the geometry of model representation is a promising direction for model enhancement.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {3101–3112},
numpages = {12}
}

@article{10.1145/3600230,
author = {Katwe, Praveen Kumar and Khamparia, Aditya and Gupta, Deepak and Dutta, Ashit Kumar},
title = {Methodical Systematic Review of Abstractive Summarization and Natural Language Processing Models for Biomedical Health Informatics: Approaches, Metrics and Challenges},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3600230},
doi = {10.1145/3600230},
abstract = {Text summarization tasks are primarily very useful for decision support systems and provide a source for useful data for training of bots as they can reduce and retain the useful information from the large corpus. This review article is for studying the literature that already exists in context of abstractive summarization and application of NLP language models in biomedical and associated healthcare applications. In past decade with trends like bigdata, IOT, enormous amount of data is getting processed in all structured, unstructured and semi structured formats. This review provides a comprehensive literature survey in research trends for abstractive summarization, foundations of machine translation and evolution of language models. This review identifies the potential of language model to provide a possible methodology for improving the performance and accuracy of various tasks in summarization. Deep neural network-based language models have now been the widely accepted state of art for various abstractive summarization and there exists an enormous scope to improvise and tune the language models for domain specific use case. This study shows current systems lack in faithfulness to original content and control of degree of hallucination. This review also details on the evaluation criteria and need for automated metrics and attempts to provide guideline for evaluation for abstractive summarization for health informatics.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
keywords = {NLP, abstractive summarization, sentence compression, sentence fusion, document summarization, language model, ROUGE}
}

@inproceedings{10.1145/3544548.3581441,
author = {Ashby, Trevor and Webb, Braden K and Knapp, Gregory and Searle, Jackson and Fulda, Nancy},
title = {Personalized Quest and Dialogue Generation in Role-Playing Games: A Knowledge Graph- and Language Model-based Approach},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581441},
doi = {10.1145/3544548.3581441},
abstract = {Procedural content generation (PCG) in video games offers unprecedented opportunities for customization and user engagement. Working within the specialized context of role-playing games (RPGs), we introduce a novel framework for quest and dialogue generation that places the player at the core of the generative process. Drawing on a hand-crafted knowledge base, our method grounds generated content with in-game context while simultaneously employing a large-scale language model to create fluent, unique, accompanying dialogue. Through human evaluation, we confirm that quests generated using this method can approach the performance of hand-crafted quests in terms of fluency, coherence, novelty, and creativity; demonstrate the enhancement to the player experience provided by greater dynamism; and provide a novel, automated metric for the relevance between quest and dialogue. We view our contribution as a critical step toward dynamic, co-creative narrative frameworks in which humans and AI systems jointly collaborate to create unique and user-specific playable experiences.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {290},
numpages = {20},
keywords = {English, GPT-2, MMORPG, NPC dialogue, RPG, World of Warcraft, computational creativity, dynamic quest generation, human-AI co-creativity, human-computer interaction, knowledge graph, knowledge-grounded text generation, language model, large-scale language models, narrative, natural language processing, procedural content generation, quest, quests, text generation, transformers, video games},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3605098.3636026,
author = {Jamil, Hasan and Krawetz, Stephen and Gow, Alexander},
title = {Knowledge Synthesis using Large Language Models for a Computational Biology Workflow Ecosystem},
year = {2024},
isbn = {9798400702433},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605098.3636026},
doi = {10.1145/3605098.3636026},
abstract = {An understanding of the molecular basis of musculoskeletal pain is necessary for the development of therapeutics, their management, and possible personalization. One-in-three Americans use OTC pain killers, and one tenth use prescription drugs to manage pain. The CDC also estimates that about 20\% Americans suffer from chronic pain. As the experience of acute or chronic pain varies due to individual genetics and physiology, it is imperative that researchers continue to find novel therapeutics to treat or manage symptoms. In this paper, our goal is to develop a seed knowledgebase computational platform, called BioNursery, that will allow biologists to computationally hypothesize, define and test molecular mechanisms underlying pain. In our knowledge ecosystem, we accumulate curated information from users about the relationships among biological databases, analysis tools, and database contents to generate biological analyses modules, called π-graphs, or process graphs. We propose a mapping function from a natural language description of a hypothesized molecular model to a computational workflow for testing in BioNursery. We use a crowd computing feedback and curation system, called Explorer, to improve proposed computational models for molecular mechanism discovery, and growing the knowledge ecosystem. Since the pain knowledge ecosystem does not yet exist, we validate our approach over a similar application in fertility research.},
booktitle = {Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing},
pages = {523–530},
numpages = {8},
keywords = {knowledge ecosystem, crowdsourcing, query reformulation},
location = {Avila, Spain},
series = {SAC '24}
}

@inproceedings{10.1145/3711954.3711961,
author = {Mukanova, Assel and Yergesh, Banu and Yelibayeva, Gaziza and Bekmanova, Gulmira and Razakhova, Bibigul},
title = {Developing an Ontological Model for Pre-election Advertising},
year = {2025},
isbn = {9798400717369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711954.3711961},
doi = {10.1145/3711954.3711961},
abstract = {The article discusses the process and methods of creating ontological modeling in the field of “Pre-election advertising”. The work includes a structural-semantic analysis of the text corpus of political discourse and the creation of an ontological model of this subject area. The document discusses key concepts, their properties and relationships necessary for building the model, as well as the use of the formal language OWL to describe axioms and relationships between concepts. The ontological model was created using the Prot\'{e}g\'{e} platform and includes various classes, properties, objects and individuals specific to the subject area of pre-election advertising. Methods of constructing logical rules for extracting knowledge from a database, such as production rules and logical programming, are also considered.},
booktitle = {Proceedings of the 2024 9th International Conference on Information Systems Engineering},
pages = {23–28},
numpages = {6},
keywords = {Formal model, Knowledge base, Ontology, Political discourse, Pre-election advertising},
location = {
},
series = {ICISE '24}
}

@inproceedings{10.1145/3613904.3642026,
author = {Reitmaier, Thomas and Raju, Dani Kalarikalayil and Klejch, Ondrej and Wallington, Electra and Markl, Nina and Pearson, Jennifer and Jones, Matt and Bell, Peter and Robinson, Simon},
title = {Cultivating Spoken Language Technologies for Unwritten Languages},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642026},
doi = {10.1145/3613904.3642026},
abstract = {We report on community-centered, collaborative research that weaves together HCI, natural language processing, linguistic, and design insights to develop spoken language technologies for unwritten languages. Across three visits to a Banjara farming community in India, we use participatory, technical, and creative methods to engage community members, collect spoken language photo annotations, and develop an information retrieval (IR) system. Drawing on orality theory, we interrogate assumptions and biases of current speech interfaces and create a simple application that leverages our IR system to match fluidly spoken queries with recorded annotations and surface corresponding photos. In-situ evaluations show how our novel approach returns reliable results and inspired the co-creation of media retrieval use-cases that are more appropriate in oral contexts. The very low (&lt; 4h) spoken data requirements makes our approach adaptable to other contexts where languages are unwritten or have no digital language resources available.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {614},
numpages = {17},
keywords = {Speech/language, co-creation, field study, zero-resource information retrieval},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3652620.3687809,
author = {G\"{o}bel, Susanne and L\"{a}mmel, Ralf},
title = {Model-Based Trust Analysis of LLM Conversations},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687809},
doi = {10.1145/3652620.3687809},
abstract = {LLM-based chatbots are routinely advertised as supporting the collaboration of humans and AI. We study LLM conversations from a knowledge elicitation perspective with the objective of being able to understand and assess the human's trust in knowledge elicited from the LLM and complementary sources. Our approach is supported by the DSML KEML, the Knowledge Elicitation Modeling Language, subject to abstract and visual syntax as well as a model transformation-based model semantics for trust analysis. Conversations are modeled by a combination of sequence diagrams and enhanced argumentation graphs --- the latter for the purpose of relating information pieces (facts and instructions) that are extracted from messages. The analysis of the corresponding models entails trust scores for gathered information (i.e., elicited knowledge).},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {602–610},
numpages = {9},
keywords = {MDE for AI, knowledge representation models, model-based analysis of LLMS, dsmls for AI usage},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.5555/3635637.3662942,
author = {Ichida, Alexandre Yukio and Meneguzzi, Felipe and Cardoso, Rafael C.},
title = {BDI Agents in Natural Language Environments},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Developing autonomous agents to deal with real-world problems is challenging, especially when developers are not necessarily specialists in artificial intelligence. This poses two key challenges regarding the interface of the programming with the developer, and the efficiency of the resulting agents. In this paper we tackle both challenges in an efficient agent architecture that leverages recent developments in natural language processing, and the intuitive folk psychology abstraction of the beliefs, desires, intentions (BDI) architecture. The resulting architecture uses existing reinforcement learning techniques to bootstrap the agent's reasoning capabilities while allowing a developer to instruct the agent more directly using natural language as its programming interface. We empirically show the efficiency gains of natural language plans over a pure machine learning approach in the ScienceWorld environment.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {880–888},
numpages = {9},
keywords = {bdi agents, large language models, natural language, reinforcement learning},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@inproceedings{10.1145/3544548.3580907,
author = {Petridis, Savvas and Diakopoulos, Nicholas and Crowston, Kevin and Hansen, Mark and Henderson, Keren and Jastrzebski, Stan and Nickerson, Jeffrey V and Chilton, Lydia B},
title = {AngleKindling: Supporting Journalistic Angle Ideation with Large Language Models},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580907},
doi = {10.1145/3544548.3580907},
abstract = {News media often leverage documents to find ideas for stories, while being critical of the frames and narratives present. Developing angles from a document such as a press release is a cognitively taxing process, in which journalists critically examine the implicit meaning of its claims. Informed by interviews with journalists, we developed AngleKindling, an interactive tool which employs the common sense reasoning of large language models to help journalists explore angles for reporting on a press release. In a study with 12 professional journalists, we show that participants found AngleKindling significantly more helpful and less mentally demanding to use for brainstorming ideas, compared to a prior journalistic angle ideation tool. AngleKindling helped journalists deeply engage with the press release and recognize angles that were useful for multiple types of stories. From our findings, we discuss how to help journalists customize and identify promising angles, and extending AngleKindling to other knowledge-work domains.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {225},
numpages = {16},
keywords = {Brainstorming, Generative AI, Ideation, Journalism, Large Language Models},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3711896.3736879,
author = {Zhang, Yiqing and Liu, Xiaozhong and Murai, Fabricio},
title = {CLaDMoP: Learning Transferrable Models from Successful Clinical Trials via LLMs},
year = {2025},
isbn = {9798400714542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711896.3736879},
doi = {10.1145/3711896.3736879},
abstract = {Many existing models for clinical trial outcome prediction are optimized using task-specific loss functions on trial phase-specific data. While this scheme may boost prediction for common diseases and drugs, it can hinder the learning of generalizable representations, leading to more false positives/negatives. To address this limitation, we introduce CLaDMoP, a new pre-training approach for clinical trial outcome prediction, alongside the Successful Clinical Trials dataset (SCT), specifically designed for this task. CLaDMoP leverages a Large Language Model-to encode trials' eligibility criteria-linked to a lightweight Drug-Molecule branch through a novel multi-level fusion technique. To efficiently fuse long embeddings across levels, we incorporate a grouping block, drastically reducing computational overhead. CLaDMoP avoids reliance on task-specific objectives by pre-training on a ''pair matching'' proxy task. Compared to established zero-shot and few-shot baselines, our method significantly improves both PR-AUC and ROC-AUC, especially for phase I and phase II trials. We further evaluate and perform ablation on CLaDMoP after Parameter-Efficient Fine-Tuning, comparing it to state-of-the-art supervised baselines, including MEXA-CTP, on the Trial Outcome Prediction (TOP) benchmark. CLaDMoP achieves up to 10.5\% improvement in PR-AUC and 3.6\% in ROC-AUC, while attaining comparable F1 score to MEXA-CTP, highlighting its potential for clinical trial outcome prediction. Code and SCT dataset can be downloaded from https://github.com/murai-lab/CLaDMoP.},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
pages = {3901–3911},
numpages = {11},
keywords = {clinical trial outcome prediction, llms, multi-modal data fusion, representation learning, self-supervised pre-training},
location = {Toronto ON, Canada},
series = {KDD '25}
}

@inproceedings{10.1145/3584371.3612942,
author = {Kabir, Anowarul and Moldwin, Asher and Shehu, Amarda},
title = {A Comparative Analysis of Transformer-based Protein Language Models for Remote Homology Prediction},
year = {2023},
isbn = {9798400701269},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584371.3612942},
doi = {10.1145/3584371.3612942},
abstract = {Protein language models based on the transformer architecture are increasingly shown to learn rich representations from protein sequences that improve performance on a variety of downstream protein prediction tasks. These tasks encompass a wide range of predictions, including prediction of secondary structure, subcellular localization, evolutionary relationships within protein families, as well as superfamily and family membership. There is recent evidence that such models also implicitly learn structural information. In this paper we put this to the test on a hallmark problem in computational biology, remote homology prediction. We employ a rigorous setting, where, by lowering sequence identity, we clarify whether the problem of remote homology prediction has been solved. Among various interesting findings, we report that current state-of-the-art, large models are still underperforming in the "twilight zone" of very low sequence identity.},
booktitle = {Proceedings of the 14th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
articleno = {97},
numpages = {9},
keywords = {remote homology, transformer, large language model},
location = {Houston, TX, USA},
series = {BCB '23}
}

@article{10.1109/TASLP.2022.3153268,
author = {Wang, Chengyu and Dai, Suyang and Wang, Yipeng and Yang, Fei and Qiu, Minghui and Chen, Kehan and Zhou, Wei and Huang, Jun},
title = {ARoBERT: An ASR Robust Pre-Trained Language Model for Spoken Language Understanding},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3153268},
doi = {10.1109/TASLP.2022.3153268},
abstract = {Spoken Language Understanding (SLU) aims to interpret the meanings of human speeches in order to support various human-machine interaction systems. A key technique for SLU is Automatic Speech Recognition (ASR), which transcribes speech signals into text contents. As the output texts of modern ASR systems unavoidably contain errors, mainstream SLU models either trained or tested on texts transcribed by ASR systems would not be sufficiently error robust. We present ARoBERT, an ASR Robust BERT model, which can be fine-tuned to solve a variety of SLU tasks with noisy inputs. To guarantee the robustness of ARoBERT, during pretraining, we decrease the fluctuations of language representations when some parts of the input texts are replaced by homophones or synophones. Specifically, we propose two novel self-supervised pre-training tasks for ARoBERT, namely Phonetically-aware Masked Language Modeling (PMLM) and ASR Model-adaptive Masked Language Modeling (AMMLM). The PMLM task explicitly fuses the knowledge of word phonetic similarities into the pre-training process, which forces homophones and synophones to share similar representations. In AMMLM, a data-driven algorithm is further introduced to mine typical ASR errors such that ARoBERT can tolerate ASR model errors. In the experiments, we evaluate ARoBERT over multiple datasets. The results show the superiority of ARoBERT, which consistently outperforms strong baselines. We have also shown that ARoBERT outperforms state-of-the-arts on a public benchmark. Currently, ARoBERT has been deployed in an online production system with significant improvements.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {1207–1218},
numpages = {12}
}

@inproceedings{10.1145/3573428.3573598,
author = {Yao, Shunyu and Hu, Jie and Sun, Chuxiong and Gao, Zhiqiao and Liu, Ning},
title = {Key Phrase Extraction based on Pre-trained Language Models},
year = {2023},
isbn = {9781450397148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573428.3573598},
doi = {10.1145/3573428.3573598},
abstract = {With the explosion of information and a large amount of data appearing every moment, it is a meaningful task to quickly find the information people want to know in a large amount of text and to present long texts in a streamlined form. Key phrase extraction, which aims to extract from documents a collection of key phrases that express the topic and content of the document, is important for text processing tasks such as information retrieval and document classification and can provide readers with a more comprehensive overview of the topic. We use two types of pre-trained language models for key phrase extraction, namely DeBERTa and RoBERTa, which are first pre-trained on the dataset and then fine-tuned, and the experimental results of these models proved that DeBERTa-V3-Large has reached an F1 score of 0.8925, which is the best result among these models.},
booktitle = {Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering},
pages = {941–945},
numpages = {5},
keywords = {Artificial Intelligence, Key Phrase Extraction, Natural Language Processing, Neural Network},
location = {Xiamen, China},
series = {EITCE '22}
}

@inproceedings{10.1145/3664647.3681472,
author = {Sun, Luoyi and Xu, Xuenan and Wu, Mengyue and Xie, Weidi},
title = {Auto-ACD: A Large-scale Dataset for Audio-Language Representation Learning},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3681472},
doi = {10.1145/3664647.3681472},
abstract = {Recently, the AI community has made significant strides in developing powerful foundation models, driven by large-scale multimodal datasets. However, for audio representation learning, existing datasets suffer from limitations in the following aspects: insufficient volume, simplistic content, and arduous collection procedures. To establish an audio dataset with high-quality captions, we propose an innovative, automatic approach leveraging multimodal inputs, such as video frames, audio streams. Specifically, we construct a large-scale, high-quality, audio-language dataset, named as Auto-ACD, comprising over 1.5M audio-text pairs. We exploit a series of pre-trained models or APIs, to determine audio-visual synchronisation, generate image captions, object detection, or audio tags for specific videos. Subsequently, we employ LLM to paraphrase a congruent caption for each audio, guided by the extracted multi-modality clues. To demonstrate the effectiveness of the proposed dataset, we train widely used models on our dataset and show performance improvement on various downstream tasks, for example, audio-language retrieval, audio captioning, zero-shot classification. In addition, we establish a novel benchmark with environmental information and provide a benchmark for audio-text tasks.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {5025–5034},
numpages = {10},
keywords = {audio captioning, audio-language dataset, audio-language representation learning},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@inproceedings{10.1145/3591196.3593516,
author = {Ding, Zijian and Srinivasan, Arvind and Macneil, Stephen and Chan, Joel},
title = {Fluid Transformers and Creative Analogies: Exploring Large Language Models’ Capacity for Augmenting Cross-Domain Analogical Creativity},
year = {2023},
isbn = {9798400701801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3591196.3593516},
doi = {10.1145/3591196.3593516},
abstract = {Cross-domain analogical reasoning is a core creative ability that can be challenging for humans. Recent work has shown some proofs-of-concept of Large language Models’ (LLMs) ability to generate cross-domain analogies. However, the reliability and potential usefulness of this capacity for augmenting human creative work has received little systematic exploration. In this paper, we systematically explore LLMs capacity to augment cross-domain analogical reasoning. Across three studies, we found: 1) LLM-generated cross-domain analogies were frequently judged as helpful in the context of a problem reformulation task (median 4 out of 5 helpfulness rating), and frequently (∼ 80\% of cases) led to observable changes in problem formulations, and 2) there was an upper bound of ∼ 25\% of outputs being rated as potentially harmful, with a majority due to potentially upsetting content, rather than biased or toxic content. These results demonstrate the potential utility — and risks — of LLMs for augmenting cross-domain analogical creativity.},
booktitle = {Proceedings of the 15th Conference on Creativity and Cognition},
pages = {489–505},
numpages = {17},
keywords = {Analogy, Creativity Support Tools, Large Language Models},
location = {Virtual Event, USA},
series = {C&amp;C '23}
}

@inproceedings{10.1145/3650400.3650405,
author = {Wang, Chen and Hua, Min and Song, Jiale and Tang, Xue-song},
title = {Knowledge Graphs Enhanced Large Language Model Prompt for Electric Power Question Answering},
year = {2024},
isbn = {9798400708305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650400.3650405},
doi = {10.1145/3650400.3650405},
abstract = {With the continuous development and digital transformation in the field of electric power, the application of large language models in the electric power industry has become a remarkable trend. The electric power industry is an information-intensive domain involving extensive data processing, predictive analysis, and decision-making. Therefore, the application of large language models in the electric power sector is of great significance. Current large language models such as GPT3.5 and GLM can perform well in tasks such as question answering dialogues. However, these models still face challenges such as answer hallucination and inaccurate responses. This paper proposes a method to enhance question answering in large language models using knowledge graphs, aiming to improve the accuracy and reliability of these models in question answering tasks in the electric power domain.The proposed method first utilizes local electric power data to extract triplets and generate a question answering dataset specific to the electric power domain using a large language model. Then, the relationships of the knowledge graph triplets are incorporated into the question prompt to enhance the quality of the model's answers. Furthermore, we fine-tune the large language model using the expanded question set derived from the triplets as knowledge enhanced data. Subsequently, we conduct experiments on both an electric power question answering dataset and a knowledge graph question answering dataset. The experimental results demonstrate that our method significantly improves various metrics of the large language model in the electric power question answering task. This research provides new insights and approaches to enhance the effectiveness of question answering systems in the electric power domain. Future studies can further explore and optimize this prompt expansion method for application in broader domains and tasks.},
booktitle = {Proceedings of the 2023 7th International Conference on Electronic Information Technology and Computer Engineering},
pages = {24–29},
numpages = {6},
location = {Xiamen, China},
series = {EITCE '23}
}

@inproceedings{10.1145/3722212.3725628,
author = {Setlur, Vidya},
title = {Supporting Human-Centric Data Exploration Through Semantics and Natural Language Interaction},
year = {2025},
isbn = {9798400715648},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3722212.3725628},
doi = {10.1145/3722212.3725628},
abstract = {Data science plays an increasingly central role in decision-making across domains, yet the effectiveness of these decisions hinges not only on sophisticated algorithms but also on how well systems support human interpretation, exploration, and communication of data. This tutorial explores the intersection of semantics, natural language processing (NLP), and human-computer interaction in creating human-centric data exploration tools that promote accessibility, trust, and transparency. This includes techniques for generating perceptually meaningful visual encodings, using NLP for query interpretation and ambiguity resolution, and designing conversational interfaces that align with users' intent. The tutorial will highlight research from a broad set of contributors in the SIGMOD/VLDB, HCI, and visualization communities, spanning natural language interfaces for databases, multimodal interaction systems, semantic search for data repositories, and the use of AI and large language models to augment visual and textual analysis.As part of this 1.5-hour session, we will present case studies and systems that demonstrate how human-centric design can be integrated into the data analysis pipeline, including tools that support mixed-initiative interaction, adaptive defaults, and subjective query interpretation. We will also discuss open challenges and research opportunities, such as semantic inferencing for unstructured data, retrieval-augmented generation (RAG), and ethical considerations around fairness, explainability, and user agency. Drawing from principles in perception, linguistics, AI, and HCI, this tutorial aims to equip attendees with both conceptual frameworks and practical techniques to build more inclusive, interpretable, and intelligent data systems. It is intended for a broad audience in the SIGMOD/VLDB community interested in designing the next generation of data exploration tools that are aligned with human needs.},
booktitle = {Companion of the 2025 International Conference on Management of Data},
pages = {851–854},
numpages = {4},
keywords = {conversational analytics, human-centered data exploration, natural language interfaces, semantic enrichment},
location = {Berlin, Germany},
series = {SIGMOD/PODS '25}
}

@article{10.1145/3609429.3609433,
author = {Bergami, Giacomo and Zegad\l{}o, Wiktor},
title = {Towards a Generalised Semistructured Data Model and Query Language},
year = {2023},
issue_date = {Summer 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2023},
number = {Summer},
issn = {1931-1745},
url = {https://doi.org/10.1145/3609429.3609433},
doi = {10.1145/3609429.3609433},
abstract = {Although current efforts are all aimed at re-defining new ways to harness old data representations, possibly with new schema features, the challenges still open provide evidence of the need for a "diametrically opposite" approach: in fact, all information generated in real contexts is to be understood lacking of any form of schema, where the schema associated with such data is only determined a posteriori based on either a specific application context, or from some data's facets of interest. This solution should still enable recommendation systems to manipulate the aforementioned data semantically. After providing evidence of these limitations from current literature, we propose a new Generalized Semistructured data Model that makes possible queries expressible in any data representation through a Generalised Semistructured Query Language, both relying upon script v2.0 as a MetaModel language manipulating types as terms as well as allowing structural aggregation functions.},
journal = {SIGWEB Newsl.},
month = aug,
articleno = {4},
numpages = {22}
}

@inproceedings{10.1145/3685650.3685667,
author = {Wanna, Selma and Solovyev, Nicholas and Barron, Ryan and Eren, Maksim E. and Bhattarai, Manish and Rasmussen, Kim \O{}. and Alexandrov, Boian S.},
title = {TopicTag: Automatic Annotation of NMF Topic Models Using Chain of Thought and Prompt Tuning with LLMs},
year = {2024},
isbn = {9798400711695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3685650.3685667},
doi = {10.1145/3685650.3685667},
abstract = {Topic modeling is a technique for organizing and extracting themes from large collections of unstructured text. Non-negative matrix factorization (NMF) is a common unsupervised approach that decomposes a term frequency-inverse document frequency (TF-IDF) matrix to uncover latent topics and segment the dataset accordingly. While useful for highlighting patterns and clustering documents, NMF does not provide explicit topic labels, necessitating subject matter experts (SMEs) to assign labels manually. We present a methodology for automating topic labeling in documents clustered via NMF with automatic model determination (NMFk). By leveraging the output of NMFk and employing prompt engineering, we utilize large language models (LLMs) to generate accurate topic labels. Our case study on over 34,000 scientific abstracts on Knowledge Graphs demonstrates the effectiveness of our method in enhancing knowledge management and document organization.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2024},
articleno = {8},
numpages = {4},
keywords = {chain of thought, llm, nmf, prompt tuning, topic labeling},
location = {San Jose, CA, USA},
series = {DocEng '24}
}

@article{10.14778/3665844.3665857,
author = {Feuer, Benjamin and Liu, Yurong and Hegde, Chinmay and Freire, Juliana},
title = {ArcheType: A Novel Framework for Open-Source Column Type Annotation Using Large Language Models},
year = {2024},
issue_date = {May 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/3665844.3665857},
doi = {10.14778/3665844.3665857},
abstract = {Existing deep-learning approaches to semantic column type annotation (CTA) have important shortcomings: they rely on semantic types which are fixed at training time; require a large number of training samples per type; incur high run-time inference costs; and their performance can degrade when evaluated on novel datasets, even when types remain constant. Large language models have exhibited strong zero-shot classification performance on a wide range of tasks and in this paper we explore their use for CTA. We introduce ArcheType, a simple, practical method for context sampling, prompt serialization, model querying, and label remapping, which enables large language models to solve CTA problems in a fully zero-shot manner. We ablate each component of our method separately, and establish that improvements to context sampling and label remapping provide the most consistent gains. ArcheType establishes a new state-of-the-art performance on zero-shot CTA benchmarks (including three new domain-specific benchmarks which we release along with this paper), and when used in conjunction with classical CTA techniques, it outperforms a SOTA DoDuo model on the fine-tuned SOTAB benchmark.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {2279–2292},
numpages = {14}
}

@inbook{10.1145/3674127.3674136,
author = {Galu\v{s}\v{c}\'{a}kov\'{a}, Petra and Oard, Douglas W. and Nair, Suraj},
title = {Cross-language Retrieval},
year = {2024},
isbn = {9798400710506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3674127.3674136},
booktitle = {Information Retrieval: Advanced Topics and Techniques},
pages = {321–357},
numpages = {37}
}

@article{10.14778/3626292.3626294,
author = {Arora, Simran and Yang, Brandon and Eyuboglu, Sabri and Narayan, Avanika and Hojel, Andrew and Trummer, Immanuel and R\'{e}, Christopher},
title = {Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes},
year = {2023},
issue_date = {October 2023},
publisher = {VLDB Endowment},
volume = {17},
number = {2},
issn = {2150-8097},
url = {https://doi.org/10.14778/3626292.3626294},
doi = {10.14778/3626292.3626294},
abstract = {A long standing goal in the data management community is developing systems that input documents and output queryable tables without user effort. Given the sheer variety of potential documents, state-of-the art systems make simplifying assumptions and use domain specific training. In this work, we ask whether we can maintain generality by using the in-context learning abilities of large language models (LLMs). We propose and evaluate Evaporate, a prototype system powered by LLMs. We identify two strategies for implementing this system: prompt the LLM to directly extract values from documents or prompt the LLM to synthesize code that performs the extraction. Our evaluations show a cost-quality tradeoff between these two approaches. Code synthesis is cheap, but far less accurate than directly processing each document with the LLM. To improve quality while maintaining low cost, we propose an extended implementation, Evaporate-Code+, which achieves better quality than direct extraction. Our insight is to generate many candidate functions and ensemble their extractions using weak supervision. Evaporate-Code+ outperforms the state-of-the art systems using a sublinear pass over the documents with the LLM. This equates to a 110X reduction in the number of documents the LLM needs to process across our 16 real-world evaluation settings.},
journal = {Proc. VLDB Endow.},
month = oct,
pages = {92–105},
numpages = {14}
}

@inproceedings{10.1145/3528588.3528658,
author = {Alchokr, Rand and Borkar, Manoj and Thotadarya, Sharanya and Saake, Gunter and Leich, Thomas},
title = {Supporting systematic literature reviews using deep-learning-based language models},
year = {2023},
isbn = {9781450393430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528588.3528658},
doi = {10.1145/3528588.3528658},
abstract = {Background: Systematic Literature Reviews are an important research method for gathering and evaluating the available evidence regarding a specific research topic. However, the process of conducting a Systematic Literature Review manually can be difficult and time-consuming. For this reason, researchers aim to semi-automate this process or some of its phases. Aim: We aimed at using a deep-learning based contextualized embeddings clustering technique involving transformer-based language models and a weighted scheme to accelerate the conduction phase of Systematic Literature Reviews for efficiently scanning the initial set of retrieved publications. Method: We performed an experiment using two manually conducted SLRs to evaluate the performance of two deep-learning-based clustering models. These models build on transformer-based deep language models (i.e., BERT and S-BERT) to extract contextualized embeddings on different text levels along with a weighted scheme to cluster similar publications. Results: Our primary results show that clustering based on embedding at paragraph-level using S-BERT-paragraph represents the best performing model setting in terms of optimizing the required parameters such as correctly identifying primary studies, number of additional documents identified as part of the relevant cluster and the execution time of the experiments. Conclusions: The findings indicate that using natural-language-based deep-learning architectures for semi-automating the selection of primary studies can accelerate the scanning and identification process. While our results represent first insights only, such a technique seems to enhance SLR process, promising to help researchers identify the most relevant publications more quickly and efficiently.},
booktitle = {Proceedings of the 1st International Workshop on Natural Language-Based Software Engineering},
pages = {67–74},
numpages = {8},
keywords = {BERT, deep learning, language models, systematic literature review},
location = {Pittsburgh, Pennsylvania},
series = {NLBSE '22}
}

@inproceedings{10.1145/3689492.3690054,
author = {Marron, Mark},
title = {A Programming Language for Data and Configuration!},
year = {2024},
isbn = {9798400712159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689492.3690054},
doi = {10.1145/3689492.3690054},
abstract = {A day in the life of a developer often involves more time working with schemas, configurations, and data description systems than writing code and logic in a classical programming language. As more systems move into distributed worlds, e.g. cloud and microservices, and developers make increasing use of libraries and frameworks, the need to interact with a range of data formats and configuration mechanisms is only increasing. This is a treacherous world, where a misspelled property name or missing field can render an entire service inoperable, a mistake that a number in an API represents     seconds instead of milli-seconds can lead to a message being set for delivery in several months instead of in an hour, misconfigured schema can lead to public exposure of sensitive data, and corrupt or erroneous results from a misunderstood data format could result in massive financial and/or reputational damage.        To address these challenges this paper casts the problems of data and configuration descriptions, not as a problem of data representation, but as a type system problem, that can be addressed with well understood and highly effective programming language techniques! The novel challenge is that data representation and configuration are universal concerns in a system and, particularly in modern cloud or micro-service systems, these systems may involve many programming languages. In the past this has led to specification systems that use a least-common-denominator set of data types, often little more than strings and numbers, and then rely on conventions or (out-of-date) documentation to ensure that the data is interpreted correctly. This paper shows that, with careful design, it is possible to create a rich universal system that can be used to express data and configuration specifications in a way that is human readable/writable and that can be produced/consumed, much like JSON, by a wide range of programming languages and systems.},
booktitle = {Proceedings of the 2024 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {147–161},
numpages = {15},
keywords = {Configuration, Data Specification, Programming Language},
location = {Pasadena, CA, USA},
series = {Onward! '24}
}

@inproceedings{10.1145/3723366.3723405,
author = {Qiu, Shunli and Jiang, Wenxia},
title = {Research on the Construction of Semantic Information Retrieval Model Based on Logistics Ontology},
year = {2025},
isbn = {9798400718298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3723366.3723405},
doi = {10.1145/3723366.3723405},
abstract = {Along with the popularity of the Internet and the increase in the amount of information, how to quickly and effectively retrieve the expected information in the massive information has become the focus of information retrieval concerns and research. The current retrieval system is mainly based on the search keywords of the full text matching query, the results often return a large number of useless information, in the search rate and accuracy rate can not meet the user's retrieval needs. In this paper, based on the logistics field, we study the construction method of domain ontology, semantic annotation of knowledge document information based on ontology, and semantic query expansion of user's query statement using domain ontology, and propose a framework structure of semantic information retrieval model based on domain ontology, as well as designing and realising a prototype system.},
booktitle = {Proceedings of the 2024 4th International Symposium on Big Data and Artificial Intelligence},
pages = {247–252},
numpages = {6},
keywords = {Ontology, model building, semantic information retrieval},
location = {
},
series = {ISBDAI '24}
}

@inproceedings{10.1145/3650400.3650526,
author = {Li, Wenqing and Qi, Xiaoman and Zhao, Qi and Wang, Chen and Wu, Qiongyu and Tang, Xue-song},
title = {Knowledge Graph-Based Credibility Evaluation Method for Electric Grid Large Language Model Knowledge Question-Answering},
year = {2024},
isbn = {9798400708305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650400.3650526},
doi = {10.1145/3650400.3650526},
abstract = {In the field of electricity, specialized terminology is often intricate and complex, making it challenging for non-experts to comprehend. However, with the advancement of artificial intelligence technology, the emergence of large language models provides a new technological solution to address this issue. Large language models, based on deep learning techniques, have the capability to quickly understand and interpret specialized terminology in the electricity domain through learning from a vast corpus of professional literature and data. They can then be applied to various domains, including question-answering systems. However, existing large language models still face issues of unreliable outputs, necessitating a method to evaluate their results and improve the quality of their applications. We propose a knowledge graph-based credibility evaluation method for electric grid large language model knowledge question-answering. This method aligns the answers generated by large language models with the knowledge graph of a local knowledge base and calculates their cosine similarity and Pearson correlation coefficient. We batch-process the answers from the large language model into an electricity dataset and validate them using this method. Experimental results demonstrate that this method can accurately and efficiently reflect the relevance between texts, providing a reliable scoring basis for question-answering by large models in vertical domains. Future research can focus on exploring other embedding methods that can better extract semantic relationships between texts and validating the feasibility of this method in vertical domains other than electricity.},
booktitle = {Proceedings of the 2023 7th International Conference on Electronic Information Technology and Computer Engineering},
pages = {754–759},
numpages = {6},
location = {Xiamen, China},
series = {EITCE '23}
}

@inproceedings{10.1145/3625156.3625157,
author = {Liu, Gang and Jiang, Wenhua and Hu, Yulin and Zhan, Kai and Wang, Tongli},
title = {CGIR: a Model of Cross Language Information Retrieval based on Concept Graph by Fusing Attention Mechanism},
year = {2023},
isbn = {9798400708206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625156.3625157},
doi = {10.1145/3625156.3625157},
abstract = {Cross language information retrieval faces challenges such as language differences, data scarcity, contextual disparities, and machine translation errors. To enhance retrieval accuracy and effectiveness, this paper proposes a similarity evaluation framework called Concept Graph Information Retrieval (CGIR). The framework includes the creation of concept graphs, quantized representations of these graphs, and retrieval processes. The construction process of CGIR incorporates an attention mechanism, significantly boosting its performance and accuracy. Through this fusion, concept graphs offer a comprehensive representation of texts, capturing the essence of the entire content while minimizing the displayed information, all while preserving the original meaning of the text to the fullest extent possible. The experimental results clearly demonstrate that the generated concept graphs effectively function as semantic representations of the entire texts. In comparison to keyword-based, ontology-based, and term-based retrieval methods, CGIR exhibits a remarkable improvement in accuracy, surpassing them by over 10\%.CCS CONCEPTS • Information systems∼Information retrieval},
booktitle = {Proceedings of the 2023 6th International Conference on Information Science and Systems},
pages = {1–7},
numpages = {7},
keywords = {attention mechanism, concept graph, information retrieval},
location = {Edinburgh, United Kingdom},
series = {ICISS '23}
}

@article{10.1145/3575803,
author = {Di, Donglin and Song, Xianyang and Zhang, Weinan and Zhang, Yue and Wang, Fanglin},
title = {Building Dialogue Understanding Models for Low-resource Language Indonesian from Scratch},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3575803},
doi = {10.1145/3575803},
abstract = {Using off-the-shelf resources from resource-rich languages to transfer knowledge to low-resource languages has received a lot of attention. The requirements of enabling the model to achieve the reliable performance, including the scale of required annotated data and the effective framework, are not well guided. To address the first question, we empirically investigate the cost-effectiveness of several methods for training intent classification and slot-filling models from scratch in Indonesia (ID) using English data. Confronting the second challenge, we propose a Bi-Confidence-Frequency Cross-Lingual transfer framework (BiCF), which consists of “BiCF Mixing”, “Latent Space Refinement” and “Joint Decoder”, respectively, to overcome the lack of low-resource language dialogue data. BiCF Mixing based on the word-level alignment strategy generates code-mixed data by utilizing the importance-frequency and translating-confidence. Moreover, Latent Space Refinement trains a new dialogue understanding model using code-mixed data and word embedding models. Joint Decoder based on Bidirectional LSTM (BiLSTM) and Conditional Random Field (CRF) is used to obtain experimental results of intent classification and slot-filling. We also release a large-scale fine-labeled Indonesia dialogue dataset (ID-WOZ1) and ID-BERT for experiments. BiCF achieves 93.56\% and 85.17\% (F1 score) on intent classification and slot filling, respectively. Extensive experiments demonstrate that our framework performs reliably and cost-efficiently on different scales of manually annotated Indonesian data.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {105},
numpages = {20},
keywords = {Dialogue datasets, intent classification, slot-filling, indonesian}
}

@inproceedings{10.1145/3709025.3712213,
author = {Sreedharan, Sreekant and Akda\u{g}, Melih and Ramachandran, Muthu and R\o{}seag, Erik and Rokseth, B\o{}rge},
title = {Legata - A domain language for maritime regulatory compliance},
year = {2025},
isbn = {9798400714214},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3709025.3712213},
doi = {10.1145/3709025.3712213},
abstract = {The paper addresses the challenge of ensuring that increasingly powerful autonomous maritime vessels operate safely and conform to regulatory standards. We presents Legata, a domain language designed to ensure regulatory compliance in autonomous maritime vessels. By leveraging large-scale simulations, Legata translates legal regulations into computable terms, enabling precise evaluation of vessel behavior across diverse scenarios. The framework quantifies risk based on regulatory violations, providing a structured method for assessing compliance. A case study on the Istanbul Strait demonstrates Legata's practical application.},
booktitle = {Proceedings of the 2025 Symposium on Computer Science and Law},
pages = {89–107},
numpages = {19},
keywords = {AI \&amp; Law, Autonomous Vessels, Maritime Regulations, Risk Evaluation, Safety Assurance},
location = {Munich, Germany},
series = {CSLAW '25}
}

@inproceedings{10.1145/3460620.3460631,
author = {Yelibayeva, Gaziza and Sharipbay, Altynbek and Bekmanova, Gulmira and Omarbekova, Assel},
title = {Ontology-Based Extraction of Kazakh Language Word Combinations in Natural Language Processing},
year = {2021},
isbn = {9781450388382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460620.3460631},
doi = {10.1145/3460620.3460631},
abstract = {This article provides an ontological model of nominative word combinations in the Kazakh language. It is necessary for creation of the automated templates for search of nominative word combinations of the Kazakh language in text corpora. The presented model expands the theory of applied linguistics in the field of extracting information from the text during corpus studies. The results will be used in semantic searches, Q&amp;A systems and in the development of software applications for obtaining knowledge, as well as for training and evaluation of knowledge on the syntax of the Kazakh language in the system of e-learning.},
booktitle = {International Conference on Data Science, E-Learning and Information Systems 2021},
pages = {58–59},
numpages = {2},
location = {Ma'an, Jordan},
series = {DATA'21}
}

@inproceedings{10.1145/3613905.3651051,
author = {Moore, Nicole and Amith, Muhammad and Neumann, Ana and Hamilton, Jane and Tang, Lu and Savas, Lara and Tao, Cui},
title = {Translating motivational interviewing for the HPV vaccine into a computable ontology model for automated AI conversational interaction},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3651051},
doi = {10.1145/3613905.3651051},
abstract = {Human papillomavirus (HPV) vaccinations are lower than expected. To protect the onset of head and neck cancers, innovative strategies to improve the rates are needed. Artificial intelligence may offer some solutions, specifically conversational agents to perform counseling methods. We present our efforts in developing a dialogue model for automating motivational interviewing (MI) to encourage HPV vaccination. We developed a formalized dialogue model for MI using an existing ontology-based framework to manifest a computable representation using OWL2. New utterance classifications were identified along with the ontology that encodes the dialogue model. Our work is available on GitHub under the GPL v.3. We discuss how an ontology-based model of MI can help standardize/formalize MI counseling for HPV vaccine uptake. Our future steps will involve assessing MI fidelity of the ontology model, operationalization, and testing the dialogue model in a simulation with live participants.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {341},
numpages = {12},
keywords = {cancer, chat bots, conversational agents, dialogue systems, human papillomavirus, ontology, oral health, patient-provider communication},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@inproceedings{10.1145/3696230.3696248,
author = {Asmawi, Adelina and Alam, Md. Saiful},
title = {Understanding the Digital Epistemologies of Chat GPT: Towards a Decolonial Language Pedagogy},
year = {2024},
isbn = {9798400717574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696230.3696248},
doi = {10.1145/3696230.3696248},
abstract = {Since its emergence, research around Chat GPT and language teaching has trended into an asymmetry of opportunities and challenges from both utopian and dystopian perspectives. Chat GPT has Western data-based inherent coloniality and thus carries invisible colonial perpetuation when used in language education. However, Chat GPT has context-awareness and personalization capacity and is open to user control. Therefore, rather than decolonizing Chat GPT itself, decolonizing by Chat GPT can be a flipped approach to materialize decolonial persuasion in language pedagogy. Grounded in Santos's epistemology of the south, this paper attempts to conceptualize Chat GPT-assisted decolonial pedagogy. Using the authors’ constructivist ideation, the study employed simulated text data generated through a series of Chat GPT-author conversations. The collected data were analyzed by applying the educational data mining (EDM) method to support the primary conceptualization of the proposed decolonial pedagogy. The findings serve as a breakthrough with a novelty discovered in Chat GPT-facilitated decolonization of language pedagogy empowering decolonially charged educators working in the global south.},
booktitle = {Proceedings of the 2024 8th International Conference on Digital Technology in Education (ICDTE)},
pages = {277–283},
numpages = {7},
keywords = {AI, Chat GPT, Chat GPT Epistemology, Decolonizing ELT, Decolonizing Education},
location = {
},
series = {ICDTE '24}
}

@inproceedings{10.1145/3744367.3744411,
author = {Yan, Chengmin},
title = {Innovation and Development of Chinese Language International Education in the Digital Age},
year = {2025},
isbn = {9798400715068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3744367.3744411},
doi = {10.1145/3744367.3744411},
abstract = {To study the development of China's digital applications and international education, modern technological methods, innovative education models and challenges were analysed. The solution explains several examples of key technology applications such as AI language assessment, internal learning and blockchain, and shows that integrating technology into an educational innovation in China offers huge opportunities, but the supply of resources needs to be further improved. Technical adaptation and ethical issues.},
booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence and Educational Systems},
pages = {272–277},
numpages = {6},
keywords = {Digital education, Chinese language international education, technological empowerment, innovative teaching and learning, ethical risks},
location = {
},
series = {ICAIES '25}
}

@inproceedings{10.1145/3637528.3671742,
author = {Jiang, Wenyuan and Wu, Wenwei and Zhang, Le and Yuan, Zixuan and Xiang, Jian and Zhou, Jingbo and Xiong, Hui},
title = {Killing Two Birds with One Stone: Cross-modal Reinforced Prompting for Graph and Language Tasks},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671742},
doi = {10.1145/3637528.3671742},
abstract = {In recent years, Graph Neural Networks (GNNs) and Large Language Models (LLMs) have exhibited remarkable capability in addressing different graph learning and natural language tasks, respectively. Motivated by this, integrating LLMs with GNNs has been increasingly studied to acquire transferable knowledge across modalities, which leads to improved empirical performance in language and graph domains. However, existing studies mainly focused on a single-domain scenario by designing complicated integration techniques to manage multimodal data effectively. Therefore, a concise and generic learning framework for multi-domain tasks, i.e., graph and language domains, is highly desired yet remains under-exploited due to two major challenges. First, the language corpus of downstream tasks differs significantly from graph data, making it hard to bridge the knowledge gap between modalities. Second, not all knowledge demonstrates immediate benefits for downstream tasks, potentially introducing disruptive noise to context-sensitive models like LLMs. To tackle these challenges, we propose a novel plug-and-play framework for incorporating a lightweight cross-domain prompting method into both language and graph learning tasks. Specifically, we first convert the textual input into a domain-scalable prompt, which not only preserves the semantic and logical contents of the textual input, but also highlights related graph information as external knowledge for different domains. Then, we develop a reinforcement learning-based method to learn the optimal edge selection strategy for useful knowledge extraction, which profoundly sharpens the multi-domain model capabilities. In addition, we introduce a joint multi-view optimization module to regularize agent-level collaborative learning across two domains. Finally, extensive empirical justifications over 23 public and synthetic datasets demonstrate that our approach can be applied to diverse multi-domain tasks more accurately, robustly, and reasonably, and improve the performances of the state-of-the-art graph and language models in different learning paradigms.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1301–1312},
numpages = {12},
keywords = {graph neural networks, large language models, prompt learning, reinforcement learning},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3706598.3714073,
author = {Singh, Divyanshu Kumar and Das, Dipto and Semaan, Bryan},
title = {The Power of Language: Resisting Western Heteropatriarchal Normative Writing Standards},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714073},
doi = {10.1145/3706598.3714073},
abstract = {Language is more than communication; it is a form of power. Whereas science has been scrutinized for privileging Western values and norms, what has been less explored is scientific linguistic performance (e.g. writing). The enforcement of English as the “normative standard” has prioritized hegemonic values and assumptions, thereby shaping the expectations of scientific performance. HCI/CSCW is dominated by heteropatriarchal Western practices, overlooking entangled values and assumptions impacting non-Western colleagues. Our work presents a design fiction (fictitious case study) envisioning a research contribution which embodies non-Western linguistic nuances as an alternative “normative standard” for scientific communication. Through this work, not only are we championing care in developing responsible linguistic practices in HCI/CSCW, but also epistemically challenging readers with intentional confusion. We establish a call to action for acknowledging and embracing different writing practices that are more inclusive of the diverse representation of scholars in HCI/CSCW.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {491},
numpages = {17},
keywords = {Language, Design Fiction, Coloniality, Human-Computer Interaction, Decolonization, Feminism, People of Color, Power, Justice},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3391274.3393639,
author = {Erekhinskaya, Tatiana and Strebkov, Dmitriy and Patel, Sujal and Balakrishna, Mithun and Tatu, Marta and Moldovan, Dan},
title = {Ten ways of leveraging ontologies for natural language processing and its enterprise applications},
year = {2020},
isbn = {9781450379748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3391274.3393639},
doi = {10.1145/3391274.3393639},
abstract = {In the last years, Artificial Intelligence and Deep Learning have matured from a facinating research area to real-word applications across multiple domains. Enterprises adopt data-driven approaches for various use cases. With the increased adoption, such issues as governance of the models, deployment, scalability, reusablity and maintenance are widely addressed on the engineering side, but not so much on the knowledge side. In this paper, we demonstrate 10 ways of leveraging ontology for Natural Language Processing. Specifically, we explore the usage of ontologies and related standards for labeling schema, configuration, providing lexical data, powering rule engine and automated generation of rules, as well as providing a standard output format. Additionally, we discuss three NLP-based applications: semantic search, question answering and natural language querying and show how they can benefit from ontology usage. The paper summarizes our experience of using ontology in a number of projects for medical, enterprise, financial, legal and security domains.},
booktitle = {Proceedings of The International Workshop on Semantic Big Data},
articleno = {8},
numpages = {6},
keywords = {domain-specific knowledge, labeling, natural language processing, natural language querying, ontologies, semantic graph},
location = {Portland, Oregon},
series = {SBD '20}
}

@inproceedings{10.1145/3487553.3524923,
author = {Negreanu, Carina and Karaoglu, Alperen and Williams, Jack and Chen, Shuang and Fabian, Daniel and Gordon, Andrew and Lin, Chin-Yew},
title = {Rows from Many Sources: Enriching row completions from Wikidata with a pre-trained Language Model},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524923},
doi = {10.1145/3487553.3524923},
abstract = {Row completion is the task of augmenting a given table of text and numbers with additional, relevant rows. The task divides into two steps: subject suggestion, the task of populating the main column; and gap filling, the task of populating the remaining columns. We present state-of-the-art results for subject suggestion and gap filling measured on a standard benchmark (WikiTables). Our idea is to solve this task by harmoniously combining knowledge base table interpretation and free text generation. We interpret the table using the knowledge base to suggest new rows and generate metadata like headers through property linking. To improve candidate diversity, we synthesize additional rows using free text generation via GPT-3, and crucially, we exploit the metadata we interpret to produce better prompts for text generation. Finally, we verify that the additional synthesized content can be linked to the knowledge base or a trusted web source such as Wikipedia.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {1272–1280},
numpages = {9},
keywords = {free text generation, knowledge base linking, language models, natural language applications, semantic knowledge, tabular data},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3653081.3653117,
author = {Xing, Xueyang and Jia, Bo and Huang, Zhicheng and Chen, Yongzhi and Wang, Junjie and Fan, Anfei and Chen, Xin and Cao, Lei},
title = {A fusion inference method for large language models and knowledge graphs based on structured injection and causal inference},
year = {2024},
isbn = {9798400716485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3653081.3653117},
doi = {10.1145/3653081.3653117},
abstract = {In this paper, we propose a large language model and knowledge graph fusion reasoning method based on structured injection and causal reasoning (LKFSC) to address the limitations of existing large language models and knowledge graphs in practical applications. The approach effectively mitigates the problems of long-distance dependency and limited contextual information, and improves the reasoning capability of the large language model. Meanwhile, by fusing the generative ability of the large language model and the inference ability of the knowledge graph, the method realizes intelligent reasoning for complex problems. The main contributions of this paper include proposing a structured injection method that introduces causality for reasoning, and constructing a fusion reasoning framework that effectively mitigates the illusory problem of large language models and provides powerful and intelligent decision support for practical applications.},
booktitle = {Proceedings of the 2023 5th International Conference on Internet of Things, Automation and Artificial Intelligence},
pages = {208–213},
numpages = {6},
location = {Nanchang, China},
series = {IoTAAI '23}
}

@inproceedings{10.1145/3717934.3718013,
author = {Wang, Gang and Wang, He and Xu, Min and Zhou, Aihua and Yu, Hai and Qian, Zhonghao},
title = {Multi Domain Ontology Model Fusion and Interoperability Method of Digital Twin in Distribution Network},
year = {2025},
isbn = {9798400707087},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3717934.3718013},
doi = {10.1145/3717934.3718013},
abstract = {The digital twin of distribution network covers models including topology, geography, space, production, operation, control, measurement, marketing, electric field, etc. There is no unified rule framework for multi domain ontology modeling methods, and there is an urgent need for model fusion and comprehensive utilization. This article focuses on the cross domain joint modeling problem of distribution network ontology, conducts research on the fusion method of multi domain ontology models of distribution network digital twins, proposes a multi type model resource fusion and comprehensive utilization method with equipment and facilities as the core, integrates business models and power grid models, studies multi domain ontology model fusion interoperability technology for digital twins, and realizes the comprehensive utilization of distribution network digital twin model resources.},
booktitle = {Proceedings of the 7th International Conference on Information Technologies and Electrical Engineering},
pages = {516–522},
numpages = {7},
keywords = {Digital twin, Distribution network, Model fusion, Multi domain ontology, Operation method},
location = {
},
series = {ICITEE '24}
}

@inproceedings{10.1145/3701716.3715194,
author = {Frey, Johannes and Ferraz, Lucas and Hofer, Marvin},
title = {POTS - A Polyparadigmatic Ontology Term Search with Fine-Grained Context Steering using Hyper-Level Vector Spaces},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715194},
doi = {10.1145/3701716.3715194},
abstract = {We present a novel microservice-based system, that facilitates a polyparadigmatic ontology term search (leveraging semantic search via vector embeddings, keyword search, and attribute filters). The search index strategy intends to preserve important semantic aspects of the ontological context of a term (selected attributes and term relationships) using structured search fields and multilevel vector spaces assembling hyper-level vector spaces. The flexible, yet simple query API allows fine-grained search requests based on a combination of fuzzy and exact filters. The architecture is based on a highly automatable and flexible Docker Compose setup strategy. While deploying the system for a local ontology is only one command away, the setup also allows ingesting a configurable subset of over 1,800 published ontologies in over 12,000 versions via DBpedia Archivo.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {2831–2834},
numpages = {4},
keywords = {graph retrieval augmented generation, llms, ontology, ontology retrieval, ontology terms embedding, owl, semantic search, terminology lookup service},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@proceedings{10.1145/3732771,
title = {SLE '25: Proceedings of the 18th ACM SIGPLAN International Conference on Software Language Engineering},
year = {2025},
isbn = {9798400718847},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Koblenz, Germany}
}

@inproceedings{10.5555/3586210.3586396,
author = {Shuttleworth, David and Padilla, Jose},
title = {From Narratives to Conceptual Models via Natural Language Processing},
year = {2023},
publisher = {IEEE Press},
abstract = {This paper explores the use of natural language processing (NLP) towards the semi-automatic generation of conceptual models, and eventual simulation specifications, from descriptions of a phenomenon. Narratives describing the problem are transformed into a list of concepts and relationships and visualized using a network graph. The process relies on pattern-based grammatical rules and an NLP dependency parser identifying important concept types, namely actors, factors, and mechanisms. We use three conceptualizations, created by potential users, to understand how the NLP-generated model should and could be adjusted. The objective of the research is to develop potential standard approaches users can use to generate conceptual models; develop a conceptual modeling assistant that subject matter experts can use to make them participant in the simulation creation process; and to identify how narratives should be written so an NLP-based conceptual modeling assistant may provide a thorough description of a phenomenon.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2222–2233},
numpages = {12},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.1145/3733155.3733192,
author = {Pinna, Simone and Massa, Silvia Maria and Fenu, Matteo and Casti, Giulio and Riboni, Daniele},
title = {Integration of Retrieval-Augmented Generation Technique for LLM-based Differential Diagnosis Assistant},
year = {2025},
isbn = {9798400714023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3733155.3733192},
doi = {10.1145/3733155.3733192},
abstract = {Artificial Intelligence (AI) is increasingly transforming the medical field, offering significant potential for diagnosis, treatment, and patient care. However, its successful integration relies on healthcare professionals, such as doctors, psychologists, and nurses, trusting the technology’s reliability and accuracy. For Large Language Models (LLMs), this trust requires transparent, verifiable, and rigorously reviewed information sources. This paper presents an AI-powered tool for differential diagnosis and disease comparison, utilizing an LLM enhanced by Retrieval-Augmented Generation (RAG). RAG overcomes traditional LLM limitations by enabling access to external, domain-specific knowledge, ensuring accurate and contextually relevant responses. The system leverages PubMed, a biomedical article aggregator, to extract symptom-related information from scientific literature on various disorders. Evaluations involving psychologist-administered questionnaires demonstrate that combining a similarity score with detailed symptom descriptions provides a clear understanding of relationships between disorders. This approach may enhance diagnostic precision and build trust in AI-driven tools, encouraging their broader adoption in clinical practice.},
booktitle = {Proceedings of the 18th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {277–284},
numpages = {8},
keywords = {Large Language Models, Retrieval-Augmented Generation, e-Health, Differential diagnosis},
location = {
},
series = {PETRA '25}
}

@inproceedings{10.1145/3436829.3436833,
author = {Negm, Eman and Makady, Soha and Salah, Akram},
title = {Towards Ontology-based Domain Specific Language for Internet of Things},
year = {2021},
isbn = {9781450377218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3436829.3436833},
doi = {10.1145/3436829.3436833},
abstract = {Development of Internet of Things (IoT) applications is considered as a complex task. It requires knowledge in the different software layers starting from the low level perception layer to the high level application layer. The domain expert should be involved from the start of the project to its end, to ensure that the delivered system satisfies the user needs. Such involvement results from the continuous need for the domain knowledge throughout the software development lifecycle. Such long development time along with the high cost of IoT applications, cause a slow progress in the IoT development. In this paper, a Domain Specific Language (DSL), called OntIoT, is proposed that contributes in reducing the complexity of IoT application development through providing the needed domain knowledge in an automated manner. OntIoT is an ontology-based DSL that utilizes the Semantic Sensor Network (SSN) ontology to catch the IoT domain concepts and constraints.},
booktitle = {Proceedings of the 9th International Conference on Software and Information Engineering},
pages = {146–151},
numpages = {6},
keywords = {Domain Specific Language, Internet of Things, Ontology},
location = {Cairo, Egypt},
series = {ICSIE '20}
}

@inproceedings{10.1145/3652620.3687805,
author = {Netz, Lukas and Reimer, Jan and Rumpe, Bernhard},
title = {Using Grammar Masking to Ensure Syntactic Validity in LLM-based Modeling Tasks},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687805},
doi = {10.1145/3652620.3687805},
abstract = {Low-code development platforms (LCDPs) are becoming increasingly important in industry, which confronts us in academic teaching with the challenge of educating students in the basic principles, critical engagement, and evaluation of LCDPs. This leads us to the question, how to teach the usage of different LCDPs during an university course. The short time frame of university-level courses makes it challenging to teach more than only one LCDP. In our teaching approach, students use two different LCDPs and create a web-application with both of them. Firstly, we require the students to define a target application with common modeling languages, next they use the first LCDP, at about half the time they switch to the second LCDP and present their findings of the differences in methodology and development processes at the end. We discuss this approach, show survey results from the participants, and explain lessons learned. This concept allows students critical engagement with LCDPs and model-driven software engineering. Supervisors get an insight into the learnability of each LCDP and how novices adapt to different domain-specific languages and their notations.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {115–122},
numpages = {8},
keywords = {low-code development platforms, education, university-level courses, model-driven software engineering, problem-based learning},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@article{10.1109/TCBB.2023.3248797,
author = {Jha, Kanchan and Saha, Sriparna and Karmakar, Sourav},
title = {Prediction of Protein-Protein Interactions Using Vision Transformer and Language Model},
year = {2023},
issue_date = {Sept.-Oct. 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {5},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2023.3248797},
doi = {10.1109/TCBB.2023.3248797},
abstract = {The knowledge of protein-protein interaction (PPI) helps us to understand proteins’ functions, the causes and growth of several diseases, and can aid in designing new drugs. The majority of existing PPI research has relied mainly on sequence-based approaches. With the availability of multi-omics datasets (sequence, 3D structure) and advancements in deep learning techniques, it is feasible to develop a deep multi-modal framework that fuses the features learned from different sources of information to predict PPI. In this work, we propose a multi-modal approach utilizing protein sequence and 3D structure. To extract features from the 3D structure of proteins, we use a pre-trained vision transformer model that has been fine-tuned on the structural representation of proteins. The protein sequence is encoded into a feature vector using a pre-trained language model. The feature vectors extracted from the two modalities are fused and then fed to the neural network classifier to predict the protein interactions. To showcase the effectiveness of the proposed methodology, we conduct experiments on two popular PPI datasets, namely, the human dataset and the &lt;italic&gt;S. cerevisiae&lt;/italic&gt; dataset. Our approach outperforms the existing methodologies to predict PPI, including multi-modal approaches. We also evaluate the contributions of each modality by designing uni-modal baselines. We perform experiments with three modalities as well, having gene ontology as the third modality.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = feb,
pages = {3215–3225},
numpages = {11}
}

@article{10.1145/3606706,
author = {Mikhaylova, Daria and Metilli, Daniele},
title = {Extending RiC-O to Model Historical Architectural Archives: The ITDT Ontology},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1556-4673},
url = {https://doi.org/10.1145/3606706},
doi = {10.1145/3606706},
abstract = {Historical architectural archives enjoy attention from diverse audiences, acting as a primary source of information for architects, historians, public authorities, and common citizens alike. In Italy, the interest in architectural archives has grown slowly but steadily for the last 20 years. However, architectural archives do not generally follow the trend common for museums and galleries in publishing digitized materials and providing standard metadata for individual records. The information that is available online usually includes only an archival finding aid, instead of metadata about the individual records, or fully digital versions of the records. While cataloguing standards for archival descriptions of architectural records have existed at least since the 1980s, the rise of Linked Open Data as a framework for publishing cultural heritage data has allowed archivists to enhance these archival descriptions with richer contextual information and links to external knowledge bases. In this paper we present the ITDT ontology, an extension of the Records in Contexts Ontology that facilitates the representation of architectural records and of the context related to architectural projects, its process, and participating entities. We discuss the application of the ontology to the project files of Italian architect and engineer Dino Tamburini (1924–2011), and the creation of a digital archive offering multiple perspectives over the records.},
journal = {J. Comput. Cult. Herit.},
month = aug,
articleno = {67},
numpages = {15},
keywords = {Historical archives, ontology, semantic annotations}
}

@inproceedings{10.1145/3297001.3297059,
author = {Deshpande, Ameet and Jegadeesan, Monisha},
title = {Leveraging Ontological Knowledge for Neural Language Models},
year = {2019},
isbn = {9781450362078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297001.3297059},
doi = {10.1145/3297001.3297059},
abstract = {Neural Language Models such as Word2Vec and GloVe have been shown to encode semantic relatedness between words. Improvements in unearthing these embeddings can ameliorate performance in numerous downstream applications such as sentiment analysis, question answering, and dialogue generation. Lexical ontologies such as WordNet are known to supply information about semantic similarity rather than relatedness. Further, extracting word em-beddings from small corpora is daunting for data-hungry neural networks. This work shows how methods that conflate Word2Vec and Ontologies can achieve better performance, reduce training time and help adapt to domains with a minimum amount of data.},
booktitle = {Proceedings of the ACM India Joint International Conference on Data Science and Management of Data},
pages = {350–353},
numpages = {4},
keywords = {Domain-transfer, Hierarchy, Ontology, Word Vectors},
location = {Kolkata, India},
series = {CODS-COMAD '19}
}

@proceedings{10.1145/3687997,
title = {SLE '24: Proceedings of the 17th ACM SIGPLAN International Conference on Software Language Engineering},
year = {2024},
isbn = {9798400711800},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 17th ACM SIGPLAN International Conference on Software Language Engineering (SLE), held in Pasadena, California, USA, October 20–21 2024, as part of SPLASH 2024. The SLE conference is devoted to the principles of software languages: their design, their implementation, and their evolution.},
location = {Pasadena, CA, USA}
}

@inproceedings{10.1145/3696410.3714672,
author = {Yang, Hui and Chen, Jiaoyan and Sattler, Uli},
title = {TransBox: EL++-closed Ontology Embedding},
year = {2025},
isbn = {9798400712746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696410.3714672},
doi = {10.1145/3696410.3714672},
abstract = {OWL (Web Ontology Language) ontologies, which are able to represent both relational and type facts as standard knowledge graphs and complex domain knowledge in Description Logic (DL) axioms, are widely adopted in domains such as healthcare and bioinformatics. Inspired by the success of knowledge graph embeddings, embedding OWL ontologies has gained significant attention in recent years. Current methods primarily focus on learning embeddings for atomic concepts and roles, enabling the evaluation based on normalized axioms through specially designed score functions. However, they often neglect the embedding of complex concepts, making it difficult to infer with more intricate axioms. This limitation reduces their effectiveness in advanced reasoning tasks, such as Ontology Learning and ontology-mediated Query Answering. In this paper, we propose EL++-closed ontology embeddings which are able to represent any logical expressions in DL EL++ via composition. Furthermore, we develop TransBox, an effective EL++ -closed ontology embedding method that can handle many-to-one, one-to-many and many-to-many relations. Our extensive experiments demonstrate that TransBox often achieves state-of-the-art performance across various real-world datasets for predicting complex axioms.},
booktitle = {Proceedings of the ACM on Web Conference 2025},
pages = {22–34},
numpages = {13},
keywords = {description logic, ontology completion, ontology embedding, ontology learning, web ontology language},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3679431.3679514,
author = {Gao, Xiang and Zhou, Yuanchao},
title = {Exploring Computational Visual Interfaces for Artificial Intelligence Language Modeling User Experience among College Students: A Rooted Theoretical Approach},
year = {2024},
isbn = {9798400709951},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3679431.3679514},
doi = {10.1145/3679431.3679514},
abstract = {Artificial Intelligence (AI) is an emerging technology with the aim of developing intelligent applications that have broad applications in various fields such as healthcare, education, and design. AI design research is presently in an exploratory phase, yet its influence on computer vision interfaces for subjective user experience is becoming increasingly significant. Focused on Chinese university students, the research delves into AI user experience, emphasizing NLP, HCI, and SU. Data was collected via surveys and interviews, with deep learning techniques aiding data processing. A substantial volume of user data was gathered through user surveys and in-depth interviews, with deep learning techniques employed for data preprocessing and feature extraction. Results show a preference for personalized services and data mining in interface design, while technical features and operational fluency are priorities in programming. Enhancing HCI design can improve operational efficiency and meet individual needs, bolstered by clear visual interfaces and HCI technologies, thus enhancing overall user experience quality and effectiveness.},
booktitle = {Proceedings of the 2024 3rd International Symposium on Control Engineering and Robotics},
pages = {516–522},
numpages = {7},
location = {Changsha, China},
series = {ISCER '24}
}

@article{10.1145/3664609,
author = {Bhagwat, Suvarna Rajesh. and Bhavsar, R. P. and Pawar, B. V.},
title = {Marathi to Indian Sign Language Machine Translation},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3664609},
doi = {10.1145/3664609},
abstract = {Machine translation has been a prominent field of research, contributing significantly to human life enhancement. Sign language machine translation, a subfield, focuses on translating spoken language content into sign language and vice versa, thereby facilitating communication between the normal hearing and hard-of-hearing communities, promoting inclusivity.This study presents the development of a ‘sign language machine translation system’ converting simple Marathi sentences into Indian Sign Language (ISL) glosses and animation. Given the low-resource nature of both languages, a phrase-level rule-based approach was employed for the translation. Initial encoding of translation rules relied on basic linguistic knowledge of Marathi and ISL, with subsequent incorporation of rules to address 'simultaneous morphological' features in ISL. These rules were applied during the ‘generation phase’ of translation to dynamically adjust phonological sign parameters, resulting in improved target sentence fluency.The paper provides a detailed description of the system architecture, translation rules, and comprehensive experimentation. Rigorous evaluation efforts were undertaken, encompassing various linguistic features, and the findings are discussed herein.The web-based version of the system serves as an interpreter for brief communications and can support the teaching and learning of sign language and its grammar in schools for hard-of-hearing students.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
keywords = {Marathi, Indian Sign Language, Phrase-level Translation, Rule-based Translation, Inclusion of specially-abled community, Sign languages’ simultaneous morphological features}
}

@article{10.1145/3593804,
author = {Huang, Tao and Hu, Shengze and Lin, Keke and Yang, Huali and Zhang, Hao and Song, Houbing and Lv, Zhihan},
title = {Sequence Generation Model Integrating Domain Ontology for Mathematical question tagging},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3593804},
doi = {10.1145/3593804},
abstract = {In online learning systems, tagging knowledge points for questions is a fundamental task. Automatic tagging technology uses intelligent algorithms to automatically tag knowledge points for questions to reduce manpower and time costs. However, the current knowledge point tagging technology cannot satisfy the situation that mathematics questions often involve a variable number of knowledge points, lacks the consideration of the characteristics of the mathematics field, and ignores the internal connection between knowledge points. To address the above issues, we propose a Sequence Generation Model Integrating Domain Ontology for Mathematical question tagging (SOMPT). SOMPT performs data augmentation for text and then obtains intermediate text based on domain ontology replacement to facilitate deep learning model to understand mathematical question text. SOMPT is able to obtain dynamic word vector embedding to optimize the textual representation for math questions. What’s more, our model can capture the relationship between tags to generate knowledge points more accurately in the way of sequence generation. The comparative experimental results show that our proposed model has an excellent tagging ability for mathematical questions. Moreover, the sequence generation module in SOMPT can be applied on other multi-label classification tasks and be on par with the state-of-the-art performance models.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
keywords = {Mathematical question tagging, Deep learning, Language models, Sequence generation}
}

@article{10.1145/3687306,
author = {Wu, Xiaobing},
title = {Unveiling Transformative Insights via Cross-Modal Learning and Natural Language Processing for Enhanced Supply Chain Intelligence},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3687306},
doi = {10.1145/3687306},
abstract = {This day's quickly developing business landscape, supply chains have become more globalized, intricate, and multi-covering, making them crucial for companies to navigate through disruptions and unpredictability. The major which are addressed in the supply chain process are lack of transparency and visibility of the supply chain network and that's leads to delay and inefficiency in the process. In order to overcome those drawbacks in the supply chain process, in this article an enhanced supply chain intelligence is developed which performs Unveiling Transformative Insights using the learning process like Cross-Modal Learning (CML) and Natural Language Processing (NLP). The implementation of these techniques is carried out in the software Python. This analysis consists of certain calculation called enhanced supply chain analysis, sales revenue Vs SKU analysis, various modes cost analysis, Lead time vs different supplier and location. The comparative analysis is performed among the technique like RF regression, SARIMA-LSTM-BP and BiLSTM model. The parameters which are involved in this performance analysis are MAE, MSE, RMSE and R^2.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = sep,
keywords = {Enhanced Supply Chain Intelligence, Unveiling Transformative Insights, Cross-Modal Learning (CML) and Natural Language Processing (NLP)}
}

@article{10.1145/3709727,
author = {Luoma, Kyle and Kumar, Arun},
title = {SNAILS: Schema Naming Assessments for Improved LLM-Based SQL Inference},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1145/3709727},
doi = {10.1145/3709727},
abstract = {Large Language Models (LLMs) have revolutionized Natural Language to SQL (NL-to-SQL), dominating most NL-to-SQL benchmarks. But LLMs still face limitations due to hallucinations, semantic ambiguity, and lexical mismatches between an NL query and the database schema. Naturally, a lot of work in the ML+DB intersection aims to mitigate such LLM limitations. In this work, we shine the light on a complementary data-centric question: How should DB schemas evolve in this era of LLMs to boost NL-to-SQL? The intuition is that more NL-friendly schema identifiers can help LLMs work better with DBs. We dive deeper into this seemingly obvious, but hitherto underexplored and important, connection between schema identifier ''naturalness'' and the behavior of LLM-based NL-to-SQL by creating a new integrated benchmark suite we call SNAILS. SNAILS has 4 novel artifacts: (1) A collection of real-world DB schemas not present in prior NL-to-SQL benchmarks; (2) A set of labeled NL-SQL query pairs on our collection not seen before by public LLMs; (3) A notion of naturalness level for schema identifiers and a novel labeled dataset of modified identifiers; and (4) AI artifacts to automatically modify identifier naturalness. Using SNAILS, we perform a comprehensive empirical evaluation of the impact of schema naturalness on LLM-based NL-to-SQL accuracy, and present a method for improving LLM-based NL-to-SQL with natural views. Our results reveal statistically significant correlations across multiple public LLMs from OpenAI, Meta, and Google on multiple databases using both zero-shot prompting as well as more complex NL-to-SQL workflows: DIN SQL, and CodeS. We present several fine-grained insights and discuss pathways for DB practitioners to better exploit LLMs for NL-to-SQL.},
journal = {Proc. ACM Manag. Data},
month = feb,
articleno = {77},
numpages = {26},
keywords = {benchmark, database, llm, natural language to sql, relational database schema, schema design, schema linking, schema naturalness}
}

@article{10.1007/s00165-021-00554-3,
author = {de Lara, Juan and Guerra, Esther},
title = {Language Family Engineering with Product Lines of Multi-level Models},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {6},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-021-00554-3},
doi = {10.1007/s00165-021-00554-3},
abstract = {Modelling is an essential activity in software engineering. It typically involves two meta-levels: one includes meta-models that describe modelling languages, and the other contains models built by instantiating those meta-models.  Multi-level modelling generalizes this approach by allowing models to span an arbitrary number of meta-levels. A scenario that profits from multi-level modelling is the definition of language families that can be specialized (e.g., for different domains) by successive refinements at subsequent meta-levels, hence promoting language reuse. This enables an  open set of variability options given by all possible specializations of the language family. However, multi-level modelling lacks the ability to express closed variability regarding the availability of language primitives or the possibility to opt between alternative primitive realizations. This limits the reuse opportunities of a language family. To improve this situation, we propose a novel combination of product lines with multi-level modelling to cover both open and closed variability. Our proposal is backed by a formal theory that guarantees correctness, enables top-down and bottom-up language variability design, and is implemented atop the MetaDepth multi-level modelling tool.},
journal = {Form. Asp. Comput.},
month = dec,
pages = {1173–1208},
numpages = {36},
keywords = {Meta-modelling, Multi-level modelling, Product lines, Domain-specific languages, Software language engineering, MetaDepth}
}

@inproceedings{10.1145/3575879.3576017,
author = {Fitsilis, Panos and Iatrellis, Omiros and Tsoutsa, Paraskevi},
title = {Using TOSCA language to model personalized educational content: Introducing eduTOSCA},
year = {2023},
isbn = {9781450398541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575879.3576017},
doi = {10.1145/3575879.3576017},
abstract = {Students attending Higher Education Institutions (HEIs) of Vocational Educational and Training (VET) are faced with a variety of complex decisions and procedures. To provide students with more sustained and personalized advising, many HEIs/VETs use academic advising systems and tools as a way to minimize costs and streamline their advising services. Furthermore, it is quite common for educational programs to include and combine educational content from different educational providers, while they are managed and executed on different platforms. Therefore, the ability to develop conceptual models for personalized learning based on educational content produced by heterogeneous educational service providers is a pressing need to address. A similar issue is confronted when deploying applications across diverse cloud computing platforms. A solution that is provided in these situations is the development of specialized languages for defining the topology and the orchestration of applications such as TOSCA, CAMP, Open-CSA, etc. In this paper, we propose to use similar conceptual models for modelling heterogeneous educational offerings toward personalized learning, which are presented along with the overall architecture of a system, named cc-coach, able to support these concepts. Further, this paper is a proposal for the standardization efforts needed for creating a multi-vendor educational ecosystem with diverse stakeholders, able to support personalized learning at various levels.},
booktitle = {Proceedings of the 26th Pan-Hellenic Conference on Informatics},
pages = {355–360},
numpages = {6},
keywords = {TOSCA, curriculum modelling, e-learning, eduTOSCA, orchestration, personalized learning},
location = {Athens, Greece},
series = {PCI '22}
}

@article{10.1145/3737459,
author = {Ghimire, Sujan and Lin, Yu-Zheng and Mamun, Muntasir and Chowdhury, Muhtasim Alam and Alemi, Farhad and Cai, Shuyu and Guo, Jinduo and Zhu, Mingyu and Li, Honghui and Saber Latibari, Banafsheh and Rafatirad, Setareh and Satam, Pratik and Salehi, Soheil},
title = {HWREx: AI-enabled Hardware Weakness and Risk Exploration and Storytelling Framework with LLM-assisted Mitigation Suggestion},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1084-4309},
url = {https://doi.org/10.1145/3737459},
doi = {10.1145/3737459},
abstract = {Abstract:The growing complexity of modern computing frameworks has led to an increase in cybersecurity vulnerabilities reported to the National Vulnerability Database (NVD). Extracting meaningful trends from this vast amount of unstructured data is challenging without proper tools and methodologies. Existing approaches lack a holistic strategy for vulnerability mitigation and prediction and effective knowledge extraction from the Common Weakness Enumeration (CWE), Common Vulnerability Exposure (CVE), and Common Attack Pattern Enumeration and Classification (CAPEC) databases. We introduce the AI-enabled Hardware Weakness and Risk Exploration and Storytelling Framework with LLM-assisted Mitigation Suggestion (HWREx), designed to address hardware vulnerabilities and IoT security. Our architecture features an Ontology-driven Storytelling capability that automates ontology updates to track vulnerability patterns and evolution over time, while offering mitigation strategies. It also clarifies the complex interrelations among CVEs, CWEs, and CAPECs through interactive visual knowledge graphs. Our framework achieved accuracy rates of 62\% for CWE-CWE, 83\% for CWE-CVE, and 77\% for CWE-CAPEC linkage predictions. These graphs are instrumental for in-depth hardware weakness analysis and enable HWREx to deliver comprehensive assessments and actionable mitigation strategies. Additionally, HWREx utilizes Generative Pre-trained Transformers (GPT) to offer tailored mitigation suggestions.},
note = {Just Accepted},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = may,
keywords = {Hardware Security, Electronic Design Automation (EDA), Ontology Learning, Large Langauge Model (LLM), Natural Language Processing (NLP), National Vulnerability Database (NVD), Common Vulnerability and Exposure (CVE), Common Weakness Enumeration (CWE), Common Attack Pattern Enumeration and Classification (CAPEC), Internet of Things (IoT)}
}

@article{10.1145/3658451,
author = {Dou, Yutao and Huang, Yuwei and Zhao, Xiongjun and Zou, Haitao and Shang, Jiandong and Lu, Ying and Yang, Xiaolin and Xiao, Jian and Peng, Shaoliang},
title = {ShennongMGS: An LLM-based Chinese Medication Guidance System},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3658451},
doi = {10.1145/3658451},
abstract = {The rapidly evolving field of Large Language Models (LLMs) holds immense promise for healthcare, particularly in medication guidance and adverse drug reaction prediction. Despite their potential, existing LLMs face challenges in dealing with complex polypharmacy scenarios and often grapple with data lag issues. To address these limitations, we introduce an LLM-based Chinese medication guidance system, called ShennongMGS, specifically tailored for robust medication guidance and adverse drug reaction predictions. Our system transforms multi-source heterogeneous medication information into a knowledge graph and employs a two-stage training strategy to construct a specialized LLM (ShennongGPT). This method enables the simulation of professional pharmacists’ decision-making processes and incorporates the capability for knowledge self-updating, thereby significantly enhancing drug safety and the overall quality of medical services. Rigorously evaluated by medical professionals and artificial intelligence experts, our method demonstrates superiority, outperforming existing general and specialized LLMs in performance.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = mar,
articleno = {15},
numpages = {14},
keywords = {Large language model, model fine-tuning, medication guidance, Chinese medical system, natural language processing, software system}
}

@article{10.1145/3706057,
author = {Jayasundara, Sakuna Harinda and Gamagedara Arachchilage, Nalin Asanka and Russello, Giovanni},
title = {SoK: Access Control Policy Generation from High-level Natural Language Requirements},
year = {2024},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3706057},
doi = {10.1145/3706057},
abstract = {Administrator-centered access control failures can cause data breaches, putting organizations at risk of financial loss and reputation damage. Existing graphical policy configuration tools and automated policy generation frameworks attempt to help administrators configure and generate access control policies by avoiding such failures. However, graphical policy configuration tools are prone to human errors, making them unusable. On the other hand, automated policy generation frameworks are prone to erroneous predictions, making them unreliable. Therefore, to find ways to improve their usability and reliability, we conducted a Systematic Literature Review analyzing 49 publications. The thematic analysis of the publications revealed that graphical policy configuration tools are developed to write and visualize policies manually. Moreover, automated policy generation frameworks are developed using machine learning (ML) and natural language processing (NLP) techniques to automatically generate access control policies from high-level requirement specifications. Despite their utility in the access control domain, limitations of these tools, such as the lack of flexibility, and limitations of frameworks, such as the lack of domain adaptation, negatively affect their usability and reliability, respectively. Our study offers recommendations to address these limitations through real-world applications and recent advancements in the NLP domain, paving the way for future research.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {102},
numpages = {37},
keywords = {Access control, policy engineering, system administrator, user interfaces, frameworks, usability, reliability}
}

@inproceedings{10.1145/3716554.3716557,
author = {Ali, Mohsan and Giallousi, Nina and Melidis, Alexandros and Alexopoulos, Charalampos and Charalabidis, Yannis},
title = {GlossAPI: Architecturing the Greek Data Pile for LLM development},
year = {2025},
isbn = {9798400713170},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716554.3716557},
doi = {10.1145/3716554.3716557},
abstract = {With the release of large language models such as ChatGPT, there has been a surge in demand for national language data sources. Greek language resources, in particular, have become widely available and are frequently discussed in the context of LLM development for tailored or custom use. The development of large language models presents several challenges, one of which is data preparation, which must be of high quality and grounded in linguistic, conceptual, and historical foundations. Creating data sets of this nature is a highly complex task. This study focuses on the collection, curation, and management of data sets for the Greek language, as well as making the data provision for use in training custom Greek LLMs smoother, not only for custom LLM development but also for fine-tuning existing LLM models. Immediately after the ChatGPT invention, the GlossAPI project initiative started in 2023. This research encourages the need for a Greek data pile creation, which is further dependent upon the data collection, acquisition, data cleaning, annotation, and classification steps. Upon cleaning the data pile, storage of the data pile and serving it to the LLMs or other users is the main concern. This concern is resolved by the use of an integrative, capable database solution that can facilitate in-database inference, including data collection, annotation, classification, and storage. For each of these stages, a special protocol requires, for instance, the use of existing LLMs or other artificial intelligence models for data preprocessing, annotation, and classification. To store data, we need a database that can seamlessly integrate with other data resources. Our study aim at the development of an architecture to unite Greek data pile and intelligent AI models at a one place using MindsDB integrative capabilities. MindsDB is an emerging database with an integration functionality to wide variety of AI models which can facilitate the data processing, annotation, classification through the integration of Supervised, unsupervised, and even though LLMs models.},
booktitle = {Proceedings of the 28th Pan-Hellenic Conference on Progress in Computing and Informatics},
pages = {16–25},
numpages = {10},
keywords = {Generative Pre-Trained Transformers, Large Language Models, Large Scale Language Corpora, Natural Language Processing, Pre-Trained Language Models},
location = {
},
series = {PCI '24}
}

@inproceedings{10.1145/3605098.3636053,
author = {Alharbi, Reham and Tamma, Valentina and Grasso, Floriana and Payne, Terry},
title = {An Experiment in Retrofitting Competency Questions for Existing Ontologies},
year = {2024},
isbn = {9798400702433},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605098.3636053},
doi = {10.1145/3605098.3636053},
abstract = {Competency Questions (CQs) are a form of ontology functional requirements expressed as natural language questions. Inspecting CQs together with the axioms in an ontology provides critical insights into the intended scope and applicability of the ontology. CQs also underpin a number of tasks in the development of ontologies e.g. ontology reuse, ontology testing, requirement specification, and the definition of patterns that implement such requirements. Although CQs are integral to the majority of ontology engineering methodologies, the practice of publishing CQs alongside the ontological artefacts is not widely observed by the community.In this context, we present an experiment in retrofitting CQs from existing ontologies. We propose RETROFIT-CQs, a method to extract candidate CQs directly from ontologies using Generative AI. In the paper we present the pipeline that facilitates the extraction of CQs by leveraging Large Language Models (LLMs) and we discuss its application to a number of existing ontologies.},
booktitle = {Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing},
pages = {1650–1658},
numpages = {9},
keywords = {ontology engineering, competency questions, large language models},
location = {Avila, Spain},
series = {SAC '24}
}

@inproceedings{10.1145/3726302.3730148,
author = {Kantz, Benedikt and Innerebner, Kevin and Waldert, Peter and Lengauer, Stefan and Lex, Elisabeth and Schreck, Tobias},
title = {OnSET: Ontology and Semantic Exploration Toolkit},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3730148},
doi = {10.1145/3726302.3730148},
abstract = {Retrieval over knowledge graphs is typically performed using specialized, complex query languages such as SPARQL. We propose a novel system, Ontology and Semantic Exploration Toolkit (OnSET), that allows novice users to quickly build queries with visual user guidance provided by topic modeling and semantic search throughout the application. OnSET enables users without prior knowledge of the ontology or networked knowledge to start exploring topics of interest over knowledge graphs, including the retrieval and detailed exploration of prototypical sub-graphs and their instances. Existing systems either focus on direct graph exploration or do not foster further exploration of the result set. We, however, provide a node-based editor that can extend these missing properties of existing systems to support search over large ontologies with sub-graph instances. Furthermore, OnSET combines efficient and open platforms to deploy the system on commodity hardware.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3980–3984},
numpages = {5},
keywords = {graph retrieval, natural language, ontology, user guidance, visualization},
location = {Padua, Italy},
series = {SIGIR '25}
}

@inproceedings{10.1145/3582768.3582778,
author = {Chowdhury, Md Towhidul Absar and Sharma, Naveen},
title = {Community Asset Ontology for Modeling Community Data using Information Extraction},
year = {2023},
isbn = {9781450397629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582768.3582778},
doi = {10.1145/3582768.3582778},
abstract = {In this paper, we analyze some data-related challenges to building resilient and sustainable communities, particularly how to computationally model the social and economical dynamic that exists within a community. To that end, we propose the Community Asset Ontology (CAO) for a knowledge graph that can encapsulate community data as modeled in existing social science literature. We utilize existing information extraction paradigms to map natural language community data to CAO and evaluate the usefulness of such an ontology-based approach compared to a baseline open information extraction approach.},
booktitle = {Proceedings of the 2022 6th International Conference on Natural Language Processing and Information Retrieval},
pages = {195–199},
numpages = {5},
keywords = {information extraction, knowledge graph, ontologies},
location = {Bangkok, Thailand},
series = {NLPIR '22}
}

@inproceedings{10.1145/3696630.3728697,
author = {Chom\k{a}tek, \L{}ukasz and S\l{}abosz, Wojciech and Poniszewska-Mara\'{n}da, Aneta},
title = {From Words to Wisdom: LLMs Summarizing Instructional Content},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728697},
doi = {10.1145/3696630.3728697},
abstract = {This study explores the effectiveness of large language models (LLMs) in summarizing instructional video transcriptions, a key application in educational technology. We assessed nine LLMs using two prompts—a simple base prompt and an enhanced, structured prompt—across 62 instructional videos. Two evaluating models, gpt-4o-mini and gemini-1.5-flash, scored the summaries based on seven criteria tailored to instructional content: overall structure, presence of examples, availability of sources, relevance, coherence, narration, and ACCURACY. Results showed notable performance differences, with models like Mistral Large and Claude 3.5 Sonnet performing best, especially with the enhanced prompt. However, the enhanced prompt improved narrative quality at the expense of structural clarity in some cases. Evaluator bias was also observed, with gpt-4o-mini assigning higher scores than gemini-1.5-flash, highlighting the need for multiple evaluators. These findings underscore the role of prompt design and model choice in educational LLM applications and suggest future research into optimizing prompts and standardizing evaluation methods.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1623–1630},
numpages = {8},
keywords = {experience, instructional materials, summarization, large language models},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1145/3643795.3648384,
author = {Koziolek, Heiko and Gr\"{u}ner, Sten and Hark, Rhaban and Ashiwal, Virendra and Linsbauer, Sofia and Eskandani, Nafise},
title = {LLM-based and Retrieval-Augmented Control Code Generation},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648384},
doi = {10.1145/3643795.3648384},
abstract = {Control code is designed and implemented for industrial automation applications that manage power plants, petrochemical processes, or steel production. Popular large language models (LLM) can synthesize low-level control code in the Structured Text programming notation according to the standard IEC 61131-3, but are not aware of proprietary control code function block libraries, which are often used in practice. To automate control logic implementation tasks, we proposed a retrieval-augmented control code generation method that can integrate such function blocks into the generated code. With this method control engineers can benefit from the code generation capabilities of LLMs, re-use proprietary and well-tested function blocks, and speed up typical programming tasks significantly. We have evaluated the method using a prototypical implementation based on GPT-4, LangChain, Open-PLC, and the open-source OSCAT function block library. In several spot sample tests, we successfully generated IEC 61131-3 ST code that integrated the desired function blocks, could be compiled, and validated through simulations.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {22–29},
numpages = {8},
keywords = {large language models, code generation, IEC 61131-3, industrial automation, PLC, DCS, ChatGPT, GPT-4},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1145/3486001.3486240,
author = {Hagen, Morten and Arora, Piyush and Ghosh, Rahul and Thomas, Dawn and Joshi, Salil R},
title = {Class-Based Order-Independent Models of Natural Language for Bayesian Auto-Complete Inference},
year = {2021},
isbn = {9781450385947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486001.3486240},
doi = {10.1145/3486001.3486240},
abstract = {We introduce a model for auto-complete of general queries via Bayesian inference. To that end, we address three issues: First, the problem of predicting a word given previous words in a text. Usually, the context words are treated as a directional sequence. In our approach, we introduce a set-based class language model with order-independence, modeling the context words as a set of classes. Second, towards the task of predicting the next word’s class based on the classes of previous words plus an incomplete word prefix, we present a Bayesian framework that incorporates the set-based class language model in conjunction with an ontology. Third, regarding the auto-complete problem, we provide complete query suggestions via abstract class-space search which determines similar historical queries that contain the classes of previous words plus the next word’s predicted class. Subsequently, we apply the model to auto-complete inference in a system setting, in which users can access data via natural language queries.},
booktitle = {Proceedings of the First International Conference on AI-ML Systems},
articleno = {20},
numpages = {7},
keywords = {Bayesian inference, Class-based language model, auto-complete, order-independence},
location = {Bangalore, India},
series = {AIMLSystems '21}
}

@article{10.1145/3617371,
author = {Banerjee, Anasua and Kumar, Vinay and Shankar, Achyut and Jhaveri, Rutvij H. and Banik, Debajyoty},
title = {Automatic Resource Augmentation for Machine Translation in Low Resource Language: EnIndic Corpus},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3617371},
doi = {10.1145/3617371},
abstract = {Parallel corpus is the primary ingredient of machine translation. It is required to train the statistical machine translation (SMT) and neural machine translation (NMT) systems. There is a lack of good quality parallel corpus for Hindi to English. Comparable corpora for a given language pair are comparatively easy to find, but this cannot be used directly in SMT or NMT systems. As a result, we generate a parallel corpus from the comparable corpus. For this purpose, the sentences (which are translations of each other) are mined from the comparable corpus to prepare the parallel corpus. The proposed algorithm uses the length of the sentence and word translation model to align sentence pairs that are translations of each other. Then, the sentence pairs that are poor translations of each other (measured by a similarity score based on IBM model 1 translation probability) are filtered out. We apply this algorithm to comparable corpora, which are crawled from speeches of the President and Vice-President of India, and mined parallel corpora out of them. The prepared parallel corpus contains good quality aligned sentences (with 96.338\% f-score). Subsequently, incorrect sentence pairs are filtered out manually to make the corpus in qualitative practical use. Finally, we gather various sentences from different sources to prepare the EnIndic corpus, which comprises 1,656,207 English-Hindi sentence pairs (miscellaneous domain). We have deployed this prepared largest English-Hindi parallel corpus at https://github.com/debajyoty/EnIndic.git and the source code at https://github.com/debajyoty/EnIndicSourceCode.git.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
keywords = {Parallel Corpus, Comparable Corpus, Machine Translation, Linguistic Resources and Natural Language Processing}
}

@article{10.1145/3586080,
author = {Fafalios, Pavlos and Kritsotaki, Athina and Doerr, Martin},
title = {The SeaLiT Ontology – An Extension of CIDOC-CRM for the Modeling and Integration of Maritime History Information},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3586080},
doi = {10.1145/3586080},
abstract = {We describe the construction and use of the SeaLiT Ontology, an extension of the ISO standard CIDOC-CRM for the modelling and integration of maritime history information. The ontology has been developed gradually, following a bottom-up approach that required the analysis of large amounts of real primary data (archival material) as well as knowledge and validation by domain experts (maritime historians). We present the specification of the ontology, RDFS and OWL implementations, as well as knowledge graphs that make use of this data model for integrating information originating from a large and diverse set of archival documents, such as crew lists, sailors registers, naval ship registers, and payrolls. We also describe an application that operates over these knowledge graphs and which supports historians in exploring and quantitatively analysing the integrated data through a user-friendly interface. Finally, we discuss aspects related to the use, evolution, and sustainability of the ontology.},
journal = {J. Comput. Cult. Herit.},
month = aug,
articleno = {60},
numpages = {21},
keywords = {Ontologies, maritime history, CIDOC-CRM, data integration, semantic interoperability}
}

@inproceedings{10.1145/3627050.3630732,
author = {Gui, Zhou and Freund, Michael and Harth, Andreas},
title = {A Natural Language Interface for IoT Systems Using the Web of Things Abstraction},
year = {2024},
isbn = {9798400708541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627050.3630732},
doi = {10.1145/3627050.3630732},
abstract = {We present a demo of a Natural Language Interface (NLI) for controlling Internet of Things (IoT) devices using the Web of Things (WoT) specification as an intermediate abstraction layer. All interaction information of a device is stored in a Knowledge Graph using the thing description ontology. The central component of the NLI is a sequence-to-sequence neural network model for text to code translation. We build a data corpus based on the functionalities of a Philips Hue smart lamp and use the corpus to train the text to code model. Our demonstration illustrates how to control the power state, the light colour, and the brightness of a Philips Hue smart lamp using natural language commands. The implementation of an NLI system based on the WoT specification represents an approach towards the development of easy-to-use and interoperable IoT systems.},
booktitle = {Proceedings of the 13th International Conference on the Internet of Things},
pages = {186–188},
numpages = {3},
keywords = {Knowledge Graphs, Natural Language Understanding, Text to Code, Web of Things},
location = {Nagoya, Japan},
series = {IoT '23}
}

@inproceedings{10.1145/3708557.3716342,
author = {Westh\"{a}u\ss{}er, Rebecca and Zepf, Sebastian and Minker, Wolfgang},
title = {CAIM: A Cognitive AI Memory Framework for Long-term Interaction with LLMs},
year = {2025},
isbn = {9798400714092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708557.3716342},
doi = {10.1145/3708557.3716342},
abstract = {The concept of cognitive artificial intelligence (AI) intends to simulate the human thought process in a computerized model. When applied as an extension to large language models (LLMs), cognitive AI has the potential to support fostering long-term relationships by improving the contextual relevance of generated responses. In this context, we propose CAIM, a framework inspired by cognitive AI principles. CAIM aims to enhance the memory capabilities of LLMs by integrating aspects of cognitive AI, such as thoughts, memory mechanisms, and decision-making. CAIM consists of three modules: 1.) The Memory Controller as central decision unit 2.) the Memory Retrieval, which filters relevant data for an interaction upon request, and 3.) the Post-Thinking, which maintains the memory storage. In this paper, we describe the core architecture of CAIM and outline potential extensions aiming to stimulate discussions around holistic memory modeling for LLMs inspired by cognitive AI.},
booktitle = {Companion Proceedings of the 30th International Conference on Intelligent User Interfaces},
pages = {22–25},
numpages = {4},
keywords = {Large Language Models, Long-term Memory, Cognitive AI},
location = {
},
series = {IUI '25 Companion}
}

@inproceedings{10.1145/3711896.3737138,
author = {Wu, Wei and Wang, Chao and Chen, Liyi and Yin, Mingze and Zhu, Yiheng and Fu, Kun and Ye, Jieping and Xiong, Hui and Wang, Zheng},
title = {Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose Protein Understanding with LLMs},
year = {2025},
isbn = {9798400714542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711896.3737138},
doi = {10.1145/3711896.3737138},
abstract = {Proteins, as essential biomolecules, play a central role in biological processes, including metabolic reactions and DNA replication. Accurate prediction of their properties and functions is crucial in biological applications. Recent development of protein language models (pLMs) with supervised fine tuning provides a promising solution to this problem. However, the fine-tuned model is tailored for particular downstream prediction task, and achieving general-purpose protein understanding remains a challenge. In this paper, we introduce Structure-Enhanced Protein Instruction Tuning (SEPIT) framework to bridge this gap. Our approach incorporates a novel structure-aware module into pLMs to enrich their structural knowledge, and subsequently integrates these enhanced pLMs with large language models (LLMs) to advance protein understanding. In this framework, we propose a novel instruction tuning pipeline. First, we warm up the enhanced pLMs using contrastive learning and structure denoising. Then, caption-based instructions are used to establish a basic understanding of proteins. Finally, we refine this understanding by employing a mixture of experts (MoEs) to capture more complex properties and functional information with the same number of activated parameters. Moreover, we construct the largest and most comprehensive protein instruction dataset to date, which allows us to train and evaluate the general-purpose protein understanding model. Extensive experiments on both open-ended generation and closed-set answer tasks demonstrate the superior performance of SEPIT over both closed-source general LLMs and open-source LLMs trained with protein knowledge.},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
pages = {3216–3227},
numpages = {12},
keywords = {insturction tuning, large language models, protein},
location = {Toronto ON, Canada},
series = {KDD '25}
}

@inproceedings{10.1145/3625704.3625720,
author = {Bekmanova, Gulmira and Omarbekova, Assel and Mukanova, Assel and Zulkhazhav, Altanbek and Zakirova, Alma and Ongarbayev, Yerkin},
title = {Development of an Ontological Model of Words in Public Political Discourse},
year = {2023},
isbn = {9798400709142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625704.3625720},
doi = {10.1145/3625704.3625720},
abstract = {The aim of the research is to develop methods for analyzing political discourse in social networks in the Kazakh language in order to identify official and unofficial information sources of political discourse, as well as to determine the mood of the discussion in these sources. The article presents an ontological model of the subject area of elections, a referendum.},
booktitle = {Proceedings of the 7th International Conference on Education and Multimedia Technology},
pages = {362–367},
numpages = {6},
keywords = {Artificial intelligence, knowledge base, discourse, ontology, formalization},
location = {Tokyo, Japan},
series = {ICEMT '23}
}

@inproceedings{10.1145/3701716.3715870,
author = {Jiao, Yizhu and Ouyang, Siru and Zhong, Ming and Zhang, Yunyi and Ding, Linyi and Zhou, Sizhe and Han, Jiawei},
title = {Retrieval and Structuring Augmented Generation with LLMs for Web Applications},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715870},
doi = {10.1145/3701716.3715870},
abstract = {Although Large Language Models (LLMs) have revolutionized natural language processing, they face significant challenges in web applications, including hallucinations, outdated knowledge, and limited specialization in niche domains. To address these issues, this tutorial explores how integrating retrieval mechanisms and structured knowledge can enhance LLM performance for web use. By leveraging Retrieval-Augmented Generation (RAG), we can ground LLM outputs with relevant external data, mitigating limitations in applications like search engines, chatbots, and recommendation systems. We delve into text structuring techniques-such as taxonomy construction, multi-level text classification, and taxonomy-guided information retrieval-that improve the effectiveness of information retrieval processes. Furthermore, we examine how structure-guided augmented generation through information extraction and knowledge graph construction can reduce hallucinations and enhance factual accuracy. By bridging the gap between unstructured language models and structured knowledge, we aim to unlock new potentials for dynamic web content generation and personalized user experiences. Finally, we highlight future directions for seamlessly integrating retrieval and generation, enhancing personalization, and incorporating multimodal data to expand the capabilities of LLMs in web applications.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {25–28},
numpages = {4},
keywords = {large language models, retrieval augmented generation, text mining, text structuring},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3550356.3561606,
author = {Huang, Yining and Dhouib, Saadia and Medinacelli, Luis Palacios and Malenfant, Jacques},
title = {Enabling semantic interoperability of asset administration shells through an ontology-based modeling method},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3561606},
doi = {10.1145/3550356.3561606},
abstract = {Digital twin technology establishes the future development vision for Industry 4.0, and is also an important exploration direction for the Model-Driven Engineering (MDE) paradigm. Because it builds a more flexible and communicative production system through models that spans life cycle, hierarchy and architecture. The standard proposed under the concept of Industry 4.0, the Asset Administration Shell (AAS), provides a syntactic interoperability interface for all assets involved in smart factories. However, there is still a need to fill the gap regarding semantic interoperability, in order to allow efficient interactions between Industry 4.0 components. Ontologies are a good candidate because they provide formal semantics expressed using a knowledge representation language, and in addition, there are many associated mature tools for reasoning and inference. Therefore, we propose a modeling approach that provides semantic interoperability for AAS-based digital twins using ontologies.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {497–502},
numpages = {6},
keywords = {asset administration shell, digital twins, industry 4.0, model-driven engineering, ontology, semantic interoperability, smart manufacturing},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@article{10.1145/3554727,
author = {Dong, Chenhe and Li, Yinghui and Gong, Haifan and Chen, Miaoxin and Li, Junxin and Shen, Ying and Yang, Min},
title = {A Survey of Natural Language Generation},
year = {2022},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3554727},
doi = {10.1145/3554727},
abstract = {This article offers a comprehensive review of the research on Natural Language Generation (NLG) over the past two decades, especially in relation to data-to-text generation and text-to-text generation deep learning methods, as well as new applications of NLG technology. This survey aims to (a) give the latest synthesis of deep learning research on the NLG core tasks, as well as the architectures adopted in the field; (b) detail meticulously and comprehensively various NLG tasks and datasets, and draw attention to the challenges in NLG evaluation, focusing on different evaluation methods and their relationships; (c) highlight some future emphasis and relatively recent research issues that arise due to the increasing synergy between NLG and other artificial intelligence areas, such as computer vision, text, and computational creativity.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {173},
numpages = {38},
keywords = {Natural language generation, data-to-text generation, text-to-text generation, deep learning, evaluation}
}

@inproceedings{10.1145/3583780.3615126,
author = {Dong, Hang and Chen, Jiaoyan and He, Yuan and Horrocks, Ian},
title = {Ontology Enrichment from Texts: A Biomedical Dataset for Concept Discovery and Placement},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615126},
doi = {10.1145/3583780.3615126},
abstract = {Mentions of new concepts appear regularly in texts and require automated approaches to harvest and place them into Knowledge Bases (KB), e.g., ontologies and taxonomies. Existing datasets suffer from three issues, (i) mostly assuming that a new concept is pre-discovered and cannot support out-of-KB mention discovery; (ii) only using the concept label as the input along with the KB and thus lacking the contexts of a concept label; and (iii) mostly focusing on concept placement w.r.t a taxonomy of atomic concepts, instead of complex concepts, i.e., with logical operators. To address these issues, we propose a new benchmark, adapting MedMentions dataset (PubMed abstracts) with SNOMED CT versions in 2014 and 2017 under the Diseases sub-category and the broader categories of Clinical finding, Procedure, and Pharmaceutical / biologic product. We provide usage on the evaluation with the dataset for out-of-KB mention discovery and concept placement, adapting recent Large Language Model based methods.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {5316–5320},
numpages = {5},
keywords = {SNOMED CT, biomedical ontologies, concept placement, entity linking, language models, ontology enrichment, text mining},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@article{10.1145/3652952,
author = {Degbelo, Auriol},
title = {Prolegomena to a Description Language for GenAI Tools in Cities},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
url = {https://doi.org/10.1145/3652952},
doi = {10.1145/3652952},
abstract = {The potential of generative AI has been recently demonstrated through different applications. The open government and smart city initiatives can leverage this potential to produce innovations that improve government workflows and the lives of citizens. This commentary makes the case for a description language enabling the structured documentation of these upcoming innovations. The description language would facilitate the communication between governments, citizens, and innovators. The key elements of the description language are briefly sketched and its usefulness is shown by the generation of ideas for GenAI tools related to interactive maps in cities.},
journal = {Digit. Gov.: Res. Pract.},
month = feb,
articleno = {8},
numpages = {8},
keywords = {Smart cities, open government, GenAI tools, metadata generation, interactive maps, human-computer interaction}
}

@article{10.1145/3548457,
author = {Lahoti, Pawan and Mittal, Namita and Singh, Girdhari},
title = {A Survey on NLP Resources, Tools, and Techniques for Marathi Language Processing},
year = {2022},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3548457},
doi = {10.1145/3548457},
abstract = {Natural Language Processing (NLP) has been in practice for the past couple of decades, and extensive work has been done for the Western languages, particularly the English language. The Eastern counterpart, especially the languages of the Indian subcontinent, needs attention as not much language processing work has been done on these languages. Western languages are rich in dictionaries, WordNet, and associated tools, while Indian languages are lagging behind in this segment. Marathi is the third most spoken language in India and the 15th most spoken language worldwide. Lack of resources, complex linguistic facts, and the inclusion of prevalent dialects of neighbors have resulted in limited work for Marathi. The aim of this study is to provide an insight into the various linguistic resources, tools, and state-of-the-art techniques applied to the processing of the Marathi language. Initially, morphological descriptions of the Marathi language are provided, followed by a discussion on the characteristics of the Marathi language. Thereafter, for Marathi language, the availability of corpus, tools, and techniques to be used to develop NLP tasks is reviewed. Finally, gap analysis is discussed in current research and future directions for this new and dynamic area of research are listed that will benefit the Marathi Language Processing research community.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {47},
numpages = {34},
keywords = {Marathi language, Marathi morphology, Marathi resources, Part-of-Speech (POS) tagging, Named Entity Recognition (NER), Word Sense Disambiguation (WSD)}
}

@proceedings{10.1145/3711542,
title = {NLPIR '24: Proceedings of the 2024 8th International Conference on Natural Language Processing and Information Retrieval},
year = {2024},
isbn = {9798400717383},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@article{10.1145/3519298,
author = {Xue, Xingsi and Liu, Wenyu},
title = {Integrating Heterogeneous Ontologies in Asian Languages Through Compact Genetic Algorithm&nbsp;with Annealing Re-sample Inheritance Mechanism},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3519298},
doi = {10.1145/3519298},
abstract = {An ontology is a state-of-the-art knowledge modeling technique in the natural language domain, which has been widely used to overcome the linguistic barriers in Asian and European countries’ intelligent applications. However, due to the different knowledge backgrounds of ontology developers, the entities in the ontologies could be defined in different ways, which hamper the communications among the intelligent applications built on them. How to find the semantic relationships among the entities that are lexicalized in different languages is called the Cross-lingual Ontology Matching problem (COM), which is a challenge problem in the ontology matching domain. To face this challenge, being inspired by the success of the Genetic Algorithm&nbsp;(GA) in the ontology matching domain, this work proposes a Compact GA with Annealing Re-sample Inheritance mechanism (CGA-ARI) to efficiently address the COM problem. In particular, a Cross-lingual Similarity Metric (CSM) is presented to distinguish two cross-lingual entities, a discrete optimal model is built to define the COM problem, and the compact encoding mechanism and the Annealing Re-sample Inheritance mechanism (ARI) are introduced to improve CGA’s searching performance. The experiment uses Multifarm track to test CGA-ARI’s performance, which includes 45 ontology pairs in different languages. The experimental results show that CGA-ARI is able to significantly improve the performance of GA and CGA and determine better alignments than state-of-the-art ontology matching systems.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = mar,
articleno = {73},
numpages = {21},
keywords = {Cross-lingual ontology alignment, compact genetic algorithm, Annealing Re-sample Inheritance Mechanism}
}

@inproceedings{10.1145/3724504.3724548,
author = {Yang, Xiaoqing and Liang, Xiaobo},
title = {A Visual Analysis of the Hotspots of the National Security and Language Studies in the Context of Big Data},
year = {2025},
isbn = {9798400711732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3724504.3724548},
doi = {10.1145/3724504.3724548},
abstract = {This paper explores the convergence of national security and language, a critical but under-explored area of contemporary scholarship. It adopts a visual analytical approach, using bibliometric techniques and CiteSpace software to analyse 2,039 journal articles from the Web of Science Core Collection. All of these articles have been published over the past 38 years (1986-2024) on the topic of national security and language. The aim of this paper is to map knowledge using big data mining, to identify research hotspots in the field of language and national security, and to visualize the results using knowledge mapping software. The analysis focuses on publication time, keyword co-occurrence, citation bursts and their clustering relationships. The findings reveal that: (1)there has been a significant rise in academic interest in the field of language and national security since 1986, with a notable increase after 2014, which underscores the growing awareness of the critical role of language in national security discourse and practice; (2) the research hotspots of national security and language research are divided into nine main clusters, namely, #0 food security, #1 European Union, #2politics, #3 discourse analysis, #4 national language processing, #5 security, #6 national security, #7 national identity and #8 globalization. Future research should aim to further explore the intersection of language and national security by incorporating a wider range of sources, including non-English literature, in order to enrich the understanding of this field and to examine the potential impact of emerging technologies such as artificial intelligence and machine learning on the dynamics of language in the security contexts.},
booktitle = {Proceedings of the 2024 2nd International Conference on Information Education and Artificial Intelligence},
pages = {267–272},
numpages = {6},
keywords = {Bibliometric Analysis, CiteSpace, Language, National Security},
location = {
},
series = {ICIEAI '24}
}

@inproceedings{10.1145/3701716.3717818,
author = {Yang, Can and Pereira Nunes, Bernardo and Rodr\'{\i}guez M\'{e}ndez, Sergio},
title = {LLM as Auto-Prompt Engineer: Automated NER Prompt Optimisation},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3717818},
doi = {10.1145/3701716.3717818},
abstract = {The emergence of Large Language Models (LLMs) has revolutionised natural language processing capabilities. However, despite these advances, effectively optimising prompts for knowledge extraction tasks like Named Entity Recognition (NER) remains challenging. This paper presents a zero-shot automated prompt engineering approach that decomposes the NER task into two phases: entity boundary detection and entity classification. Our method incorporates structured task analysis, automated prompt generation, test case generation, and iterative optimisation, requiring no labelled training examples. This decomposition allows for more precise entity recognition while maintaining the efficiency. Through experimentation on the CoNLL-2003 dataset using standard exact-match evaluation metrics, our approach demonstrates improvements over unified methods, achieving a 75.39\% F1 score compared to baseline approaches (72.90\%). The key contributions include: (1) A structured pipeline for zero-shot automated prompt engineering in NER tasks that addresses the challenges of prompt design and optimisation; (2) A two-phase approach to NER tasks that separates boundary detection from entity classification; and (3) Experimental results demonstrating the effectiveness of our approach compared to existing zero-shot approaches in NER tasks.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {2574–2578},
numpages = {5},
keywords = {automated prompt engineering, large language models, named entity recognition, prompt optimisation},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@article{10.1145/3709681,
author = {Omar, Reham and Mangukiya, Omij and Mansour, Essam},
title = {Dialogue Benchmark Generation from Knowledge Graphs with Cost-Effective Retrieval-Augmented LLMs},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1145/3709681},
doi = {10.1145/3709681},
abstract = {Dialogue benchmarks are crucial in training and evaluating chatbots engaging in domain-specific conversations. Knowledge graphs (KGs) represent semantically rich and well-organized data spanning various domains, such as DBLP, DBpedia, and YAGO. Traditionally, dialogue benchmarks have been manually created from documents, neglecting the potential of KGs in automating this process. Some question-answering benchmarks are automatically generated using extensive preprocessing from KGs, but they do not support dialogue generation. This paper introduces Chatty-Gen, a novel multi-stage retrieval-augmented generation platform for automatically generating high-quality dialogue benchmarks tailored to a specific domain using a KG. Chatty-Gen decomposes the generation process into manageable stages and uses assertion rules for automatic validation between stages. Our approach enables control over intermediate results to prevent time-consuming restarts due to hallucinations. It also reduces reliance on costly and more powerful commercial LLMs. Chatty-Gen eliminates upfront processing of the entire KG using efficient query-based retrieval to find representative subgraphs based on the dialogue context. Our experiments with several real and large KGs demonstrate that Chatty-Gen significantly outperforms state-of-the-art systems and ensures consistent model and system performance across multiple LLMs of diverse capabilities, such as GPT-4o, Gemini 1.5, Llama 3, and Mistral.},
journal = {Proc. ACM Manag. Data},
month = feb,
articleno = {31},
numpages = {26},
keywords = {assertion-based validation, benchmarking, conversational question answering, cost-effecive inference, graph serialization, knowledge graphs (kgs), large language models (llms), retrieval-augumented generation (rag)}
}

@inproceedings{10.1145/3587281.3587700,
author = {Deng, Xiang},
title = {A More Accessible Web with Natural Language Interface},
year = {2023},
isbn = {9798400707483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587281.3587700},
doi = {10.1145/3587281.3587700},
abstract = {The past decade has witnessed the rapid growth and evolution of the Web. Today, people can perform a multitude of tasks through the use of a single browser. Despite the immense power and capabilities of the Web, its increasing complexity and the overwhelming amount of information pose significant challenges for users to easily access the information they need and achieve their goals. Especially for those who are less technologically proficient or have disabilities. In this work, we propose to tackle this issue by building a general natural language interface for the Web, which enables users to express their needs in natural language and have the system carry out the arduous actions. We examine the key components crucial for building the natural language interface. On top of that, we present our ongoing efforts in curating a new benchmark dataset covering a diverse range of websites and tasks, and establishing baselines to demonstrate the feasibility of building such a system.},
booktitle = {Proceedings of the 20th International Web for All Conference},
pages = {153–155},
numpages = {3},
keywords = {Natural Language Interface, Web Accessibility},
location = {Austin, TX, USA},
series = {W4A '23}
}

@inproceedings{10.1145/3735654.3735942,
author = {Hoseini, Sayed and Herrmann, Vincent and Quix, Christoph},
title = {End-To-End ML with LLMs and Semantic Data Management: Experiences from Chemistry 4.0},
year = {2025},
isbn = {9798400719240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3735654.3735942},
doi = {10.1145/3735654.3735942},
abstract = {Machine Learning (ML) in industrial chemistry is often hindered by the complexity of preprocessing heterogeneous datasets. In this proof-of-concept study, we explore the use of semantic data management to support LLM-driven automation of end-to-end ML pipelines in a real-world Chemistry 4.0 setting. A semantic model is used to capture domain knowledge and metadata in a machine-readable form, guiding LLMs through natural language prompts to generate complete data wrangling and ML modeling code. We evaluate several state-of-the-art LLMs on their ability to autonomously produce functionally correct Python code for preprocessing and Gaussian Process modeling. Our results show that, when guided by structured semantic context, larger LLMs can reliably generate accurate pipelines, significantly reducing the need for manual intervention. These findings provide an encouraging starting point for further exploration toward leveraging the semantic model to improve the robustness of code generation by systematically integrating relevant information into the generation process, rather than relying solely on the raw intelligence of the LLM.},
booktitle = {Proceedings of the Workshop on Data Management for End-to-End Machine Learning},
articleno = {6},
numpages = {10},
keywords = {AutoML, Data Wrangling, LLMs, Semantic Data Management},
location = {Berlin, Germany},
series = {DEEM '25}
}

@inproceedings{10.1145/3652620.3687806,
author = {Burgue\~{n}o, Lola and Keet, Maria and Kienzle, J\"{o}rg and Michael, Judith and Babur, \"{O}nder},
title = {A Human Behavior Exploration Approach Using LLMs for Cyber-Physical Systems},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687806},
doi = {10.1145/3652620.3687806},
abstract = {In the early phases of Cyber-Physical Systems (CPS) development, scoping human behavior plays a significant role, especially when interactions extend beyond expected behavior. Here, it is especially challenging to develop cases that capture the full spectrum of human behavior. Up to now, identifying such behavior of humans remains a task for domain experts. We explore how one can use Large Languages Models (LLMs) in the design phase of systems to provide additional information about human-CPS interaction. Our approach proposes a preliminary ontology describing a hierarchy of types of behavior and relevant CPS components as input for prompt templates. It uses them to generate parts of human behavior descriptions, as well as a canned prompt with one variable about behavior. For demonstration, we take a smart building with a Home Energy System as the use case.An initial user evaluation shows that the behavior descriptions generated with standard and ontology-driven prompts complement each other and are useful when assisting humans. The discovered uncommon behaviors can be used to complete interaction scenarios that eventually result in a more robust CPS implementation.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {578–586},
numpages = {9},
keywords = {human behavior, large language models, cyber-physical systems, user scenario, digital twin},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3727582.3728689,
author = {Yamani, Asma and Baslyman, Malak and Ahmed, Moataz},
title = {Leveraging LLMs for User Stories in AI Systems: UStAI Dataset},
year = {2025},
isbn = {9798400715945},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3727582.3728689},
doi = {10.1145/3727582.3728689},
abstract = {AI systems are gaining widespread adoption across various sectors and domains. Creating high-quality AI system requirements is crucial for aligning the AI system with business goals and consumer values and for social responsibility. However, with the uncertain nature of AI systems and the heavy reliance on sensitive data, more research is needed to address the elicitation and analysis of AI systems requirements. With the proprietary nature of many AI systems, there is a lack of open-source requirements artifacts and technical requirements documents for AI systems, limiting broader research and investigation. With Large Language Models (LLMs) emerging as a promising alternative to human-generated text, this paper investigates the potential use of LLMs to generate user stories for AI systems based on abstracts from scholarly papers. We conducted an empirical evaluation using three LLMs and generated 1260 user stories from 42 abstracts from 26 domains. We assess their quality using the Quality User Story (QUS) framework. Moreover, we identify relevant non-functional requirements (NFRs) and ethical principles. Our analysis demonstrates that the investigated LLMs can generate user stories inspired by the needs of various stakeholders, offering a promising approach for generating user stories for research purposes and for aiding in the early requirements elicitation phase of AI systems. We have compiled and curated a collection of stories generated by various LLMs into a dataset (UStAI), which is now publicly available for use.},
booktitle = {Proceedings of the 21st International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {21–30},
numpages = {10},
keywords = {User stories, large language models, quality requirements, requirements elicitation, requirements generation},
location = {Trondheim, Norway},
series = {PROMISE '25}
}

@inproceedings{10.1145/3568364.3568388,
author = {Zeng, Jinghan},
title = {Cross-Language Vocabulary Teaching Based on Information Network Technology: The Example of Loanwords},
year = {2022},
isbn = {9781450396950},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568364.3568388},
doi = {10.1145/3568364.3568388},
abstract = {Loanwords have always been a hot topic in the study and teaching of Chinese as a foreign language, but there are also many different views and debates. This article summarises and outlines the hotspots of research into and teaching of loanwords in linguistics in recent decades and discusses the current issues of language teaching and cross-linguistic education in the context of the new COVID-19 pandemic in relation to information network technology. First, the concept and scope of loanwords are defined, and the rationale of loanwords is discussed from the perspective of word formation. On this basis, the characteristics of loanwords are summarised, the online teaching of loanwords in the context of the new pandemic is discussed from the perspective of information technology and the Sinicization of loanwords is discussed from the perspective of language development.},
booktitle = {Proceedings of the 4th World Symposium on Software Engineering},
pages = {155–160},
numpages = {6},
keywords = {information-based teaching, language education, loanwords, word-building rationale},
location = {Xiamen, China},
series = {WSSE '22}
}

@inproceedings{10.1145/3701716.3717556,
author = {Ma, Ao and Li, Zhiyuan and Liang, Zhuonan and Gu, Tiancheng and Fan, Jianan and Long, Jieting and M\"{u}ller, Henning and Cai, Weidong},
title = {Seek Inner: LLM-Enhanced Information Mining for Medical Visual Question Answering},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3717556},
doi = {10.1145/3701716.3717556},
abstract = {Medical visual question answering (Med-VQA) focuses on analyzing medical images to accurately respond to clinicians' specific questions. Although integrating prior knowledge can enhance VQA reasoning, current methods often struggle to extract relevant information from the vast and complex medical knowledge base, thereby limiting the models' ability to learn domain-specific features. To overcome this limitation, our study presents a novel information mining approach that leverages large language models (LLMs) to efficiently retrieve pertinent data. Specifically, we design a latent knowledge generation module that employs LLMs to separately extract and filter information from questions and answers, enhancing the model's inference capabilities. Furthermore, we propose a multi-level prompt fusion module in which an initial prompt interacts with the extracted latent knowledge to draw clinically relevant details from both unimodal and multimodal features. Experimental results demonstrate that our approach outperforms current state-of-the-art models on multiple Med-VQA benchmark datasets.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {2297–2305},
numpages = {9},
keywords = {knowledge injection, large language models, medical visual question answering, multimodal learning},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@proceedings{10.1145/3640310,
title = {MODELS '24: Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Linz, Austria}
}

@inproceedings{10.1145/3706599.3720126,
author = {Zhang, Shengchen and Guo, Weiwei and Sun, Xiaohua},
title = {Align with Me, Not TO Me: How People Perceive Concept Alignment with LLM-Powered Conversational Agents},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3720126},
doi = {10.1145/3706599.3720126},
abstract = {Concept alignment—building a shared understanding of concepts—is essential for human and human-agent communication. While large language models (LLMs) promise human-like dialogue capabilities for conversational agents, the lack of studies to understand people’s perceptions and expectations of concept alignment hinders the design of effective LLM agents. This paper presents results from two lab studies with human-human and human-agent pairs using a concept alignment task. Quantitative and qualitative analysis reveals and contextualizes potentially (un)helpful dialogue behaviors, how people perceived and adapted to the agent, as well as their preconceptions and expectations. Through this work, we demonstrate the co-adaptive and collaborative nature of concept alignment and identify potential design factors and their trade-offs, sketching the design space of concept alignment dialogues. We conclude by calling for designerly endeavors on understanding concept alignment with LLMs in context, as well as technical efforts to combine theory-informed and LLM-driven approaches.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {67},
numpages = {10},
keywords = {Concept Alignment, Grounding, Conversational Agents, Large Language Models, Human-Agent Interaction},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3705328.3748751,
author = {Ferrero, Fabio},
title = {Narrative-Driven Itinerary Recommendation: LLM Integration for Immersive Urban Walking},
year = {2025},
isbn = {9798400713644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3705328.3748751},
doi = {10.1145/3705328.3748751},
abstract = {Sedentary behavior, dubbed the disease of the 21st century, is a ubiquitous force driving chronic illness. Yet, traditional itinerary and Point-of-Interest (POI) Recommender Systems (RSs) lack engaging elements that motivate routine urban walking. This research proposes a novel framework combining narrative-driven storytelling with location-based RSs to promote physical activity and immersive urban exploration. This approach introduces a bidirectional alignment between POI and itinerary recommendations and LLM-generated narratives, transforming routine urban walks into dynamic journeys where contextually relevant stories unfold across city locations. Unlike sequential POI recommendations, this framework embeds location suggestions within contextually relevant narratives of various genres, simultaneously promoting health benefits and deeper city exploration. The research addresses three research questions using a method that builds a structured knowledge base by extracting entities (e.g., POIs, and characters) and semantic links from narrative corpora, enabling semantic alignment between recommended physical locations and story elements. The core aspects of this work are: (i) context-aware itinerary recommendations and personalized story generation, (ii) bidirectional mapping between RSs and story generation, and (iii) systems design bridging user’s needs to promote urban walking as a health activity. Evaluation employs comparative user studies measuring quality and engagement, route-narrative semantic alignment, and narrative analysis to validate the integrated proposed approach.},
booktitle = {Proceedings of the Nineteenth ACM Conference on Recommender Systems},
pages = {1485–1491},
numpages = {7},
keywords = {Walking Route Recommender Systems, Large Language Models, Geographically Anchored Interactive Narratives.},
location = {
},
series = {RecSys '25}
}

@article{10.1109/TASLP.2022.3181350,
author = {An, Jinwon and Cho, Sungzoon and Bang, Junseong and Kim, Misuk},
title = {Domain-Slot Relationship Modeling Using a Pre-Trained Language Encoder for Multi-Domain Dialogue State Tracking},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3181350},
doi = {10.1109/TASLP.2022.3181350},
abstract = {Dialogue state tracking for multi-domain dialogues is challenging because the model should be able to track dialogue states across multiple domains and slots. As using pre-trained language models is the de facto standard for natural language processing tasks, many recent studies use them to encode the dialogue context for predicting the dialogue states. Model architectures that have certain inductive biases for modeling the relationship among different domain-slot pairs are also emerging. Our work is based on these research approaches on multi-domain dialogue state tracking. We propose a model architecture that effectively models the relationship among domain-slot pairs using a pre-trained language encoder. Inspired by the way the special &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$[CLS]$&lt;/tex-math&gt;&lt;/inline-formula&gt; token in BERT is used to aggregate the information of the whole sequence, we use multiple special tokens for each domain-slot pair that encodes information corresponding to its domain and slot. The special tokens are run together with the dialogue context through the pre-trained language encoder, which effectively models the relationship among different domain-slot pairs. Our experimental results on the datasets MultiWOZ-2.0 and MultiWOZ-2.1 show that our model outperforms other models with the same setting. Our ablation studies incorporate three main parts. The first component shows the effectiveness of our approach exploiting the relationship modeling. The second component compares the effect of using different pre-trained language encoders. The final component involves comparing different initialization methods that could be used for the special tokens. Qualitative analysis of the attention map of the pre-trained language encoder shows that our special tokens encode relevant information through the encoding process by attending to each other.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {2091–2102},
numpages = {12}
}

@inproceedings{10.1145/3726302.3731964,
author = {Zhao, Mengyue and Nokleby, Matthew and Shen, Bo and Dong, Wenbo and Pachauri, Deepti and Yang, Andrew},
title = {Ontology-Guided Knowledge Graph Retrieval for Multi-Hop and Cross-Granularity Store Fulfillment Queries},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3731964},
doi = {10.1145/3726302.3731964},
abstract = {Answering complex queries in store fulfillment, such as ''What percentage of employee-assigned actions remain unresolved?'' or ''How many worklists for a specific product type were completed within a timeframe at each location?'' requires precise, multi-hop reasoning across datasets with varying granularities. This paper introduces an ontology-based knowledge graph (KG) approach integrated with a structured text-to-Cypher generation pipeline, enabling accurate retrieval for such queries. Benchmarking against a robust hybrid search baseline combining BM25 and semantic search, our method demonstrates superior performance in addressing multi-hop and cross-granularity questions. Leveraging a KG schema designed to capture intricate relationships (e.g. (OrderLineItem)-[:INVOLVES_ACTION]-&gt;(Action)-[:INVOLVES]-&gt;(BatchProcess)-[:IS_COMPLETED_AT]-&gt;(Location)), we reveal universal patterns for constructing and querying highly relational data. This work highlights the transformative potential of ontology-driven KGs to improve reasoning, data aggregation, and decision-making, with broader implications for any domain requiring structured, multi-relational data analysis.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {4360–4364},
numpages = {5},
keywords = {hybrid search, knowledge graph, large language model, multi-hop reasoning, natural language processing, ontology-driven retrieval, store fulfillment., text-to-cypher},
location = {Padua, Italy},
series = {SIGIR '25}
}

@inproceedings{10.1145/3639233.3639234,
author = {Okgetheng, Boago and Malema, Gabofetswe},
title = {Named Entity Recognition for Setswana Language: A conditional Random Fields (CRF) Approach},
year = {2024},
isbn = {9798400709227},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639233.3639234},
doi = {10.1145/3639233.3639234},
abstract = {Named Entity Recognition (NER) is a fundamental task in Natural Language Processing (NLP) focused on identifying entities like individuals, organizations, and locations within text. Locating these entities can present initial challenges, and subsequent classification can be equally daunting. This complexity is exemplified in Setswana, where shared naming between locations and personal names adds an extra layer of intricacy. This study introduces a Setswana NER approach, featuring a Setswana Regex Annotator (SERxA) for preliminary entity classification, followed by BRAT tool annotation. Employing the Conditional Random Fields (CRF) algorithm, we establish a supervised statistical machine learning NER model for Setswana. Evaluation using standard metrics on a held-out test set attains impressive F1-scores of 0.94 for person entities and 0.79 for location entities. Our findings underscore the viability of NER in Setswana and emphasize the necessity of nurturing NLP resources for less-resourced languages.},
booktitle = {Proceedings of the 2023 7th International Conference on Natural Language Processing and Information Retrieval},
pages = {240–244},
numpages = {5},
location = {Seoul, Republic of Korea},
series = {NLPIR '23}
}

@inproceedings{10.1145/3555776.3578739,
author = {Ihsan, Ahmad Zainul and Fathalla, Said and Sandfeld, Stefan},
title = {DISO: A Domain Ontology for Modeling Dislocations in Crystalline Materials},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3578739},
doi = {10.1145/3555776.3578739},
abstract = {Crystalline materials, such as metals and semiconductors, nearly always contain a special defect type called dislocation. This defect decisively determines many important material properties, e.g., strength, fracture toughness, or ductility. Over the past years, significant effort has been put into understanding dislocation behavior across different length scales via experimental characterization techniques and simulations. This paper introduces the dislocation ontology (DISO), which defines the concepts and relationships related to linear defects in crystalline materials. We developed DISO using a top-down approach in which we start defining the most general concepts in the dislocation domain and subsequent specialization of them. DISO is published through a persistent URL following W3C best practices for publishing Linked Data. Two potential use cases for DISO are presented to illustrate its usefulness in the dislocation dynamics domain. The evaluation of the ontology is performed in two directions, evaluating the success of the ontology in modeling a real-world domain and the richness of the ontology.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1746–1753},
numpages = {8},
keywords = {ontology, dislocation, crystallographic defects, linked data, materials science and engineering},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/3698205.3733952,
author = {Priyanka Rani, FNU and Alomair, Maryam and Pan, Shimei and Chen, Lujie K.},
title = {Systematically Identifying, Defining and Organizing Knowledge Components for Data Science Problem Solving through Human-LLM Collaboration},
year = {2025},
isbn = {9798400712913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698205.3733952},
doi = {10.1145/3698205.3733952},
abstract = {As demand grows for job-ready data science professionals, there is increasing recognition that traditional training often falls short in cultivating the higher-order reasoning and real-world problem-solving skills essential to the field. A foundational step toward addressing this gap is the identification and organization of knowledge components (KCs) that underlie data science problem solving (DSPS). KCs represent conditional knowledge-knowing about appropriate actions given particular contexts or conditions-and correspond to the critical decisions data scientists must make throughout the problem-solving process. While existing taxonomies in data science education support curriculum development, they often lack the granularity and focus needed to support the assessment and development of DSPS skills. In this paper, we present a novel framework that combines the strengths of large language models (LLMs) and human expertise to identify, define, and organize KCs specific to DSPS. We treat LLMs as ''knowledge engineering assistants'' capable of generating candidate KCs by drawing on their extensive training data, which includes a vast amount of domain knowledge and diverse sets of real-world DSPS cases. Our process involves prompting multiple LLMs to generate decision points, synthesizing and refining KC definitions across models, and using sentence-embedding models to infer the underlying structure of the resulting taxonomy. Human experts then review and iteratively refine the taxonomy to ensure validity. This human-AI collaborative workflow offers a scalable and efficient proof-of-concept for LLM-assisted knowledge engineering. The resulting KC taxonomy lays the groundwork for developing fine-grained assessment tools and adaptive learning systems that support deliberate practice in DSPS. Furthermore, the framework illustrates the potential of LLMs not just as content generators but as partners in structuring domain knowledge to inform instructional design. Future work will involve extending the framework by generating a directed graph of KCs based on their input-output dependencies and validating the taxonomy through expert consensus and learner studies. This approach contributes to both the practical advancement of DSPS coaching in data science education and the broader methodological toolkit for AI-supported knowledge engineering.},
booktitle = {Proceedings of the Twelfth ACM Conference on Learning @ Scale},
pages = {341–345},
numpages = {5},
keywords = {data science education, data science problem solving, domain analysis, knowledge components, knowledge engineering, large language models, problem solving},
location = {Palermo, Italy},
series = {L@S '25}
}

@inproceedings{10.1145/3589335.3651557,
author = {Venkatakrishnan, Radhakrishnan and Tanyildizi, Emrah and Canbaz, M. Abdullah},
title = {Semantic interlinking of Immigration Data using LLMs for Knowledge Graph Construction},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3651557},
doi = {10.1145/3589335.3651557},
abstract = {The challenge of managing immigration data is exacerbated by its reliance on paper-based, evidence-driven records maintained by legal professionals, creating obstacles for efficient processing and analysis due to inherent trust issues with AI-based systems. This paper introduces a cutting-edge framework to surmount these hurdles by synergizing Large Language Models (LLMs) with Knowledge Graphs (KGs), revolutionizing traditional data handling methods. Our method transforms archaic, paper-based immigration records into a structured, interconnected knowledge network that intricately mirrors the legal and procedural nuances of immigration, ensuring a dynamic and trustworthy platform for data analysis. Utilizing LLMs, we extract vital entities and relationships from diverse legal documents to forge a comprehensive knowledge graph, encapsulating the complex legalities and procedural disparities in immigration processes and mapping the multifaceted interactions among stakeholders like applicants, sponsors, and legal experts. This graph not only facilitates a deep dive into the legal stipulations but also incorporates them, significantly boosting the system's reliability and precision. With the integration of Retrieval Augmented Generation (RAG) for exact, context-aware data retrieval and Augmented Knowledge Creation for developing a conversational interface via LLMs, our framework offers a scalable, adaptable solution to immigration data management. This innovative amalgamation of LLMs, KGs, and RAG techniques marks a paradigm shift towards more informed, efficient, and trustworthy decision-making in the sphere of global migration, setting a new benchmark for legal technology and data source management.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {605–608},
numpages = {4},
keywords = {data restructuring, document processing, information retrieval, knowledge graphs, large language models, legal tech},
location = {Singapore, Singapore},
series = {WWW '24}
}

@article{10.1145/3610582,
author = {Sethi, Nandini and Dev, Amita and Bansal, Poonam and Sharma, Deepak Kumar and Gupta, Deepak},
title = {A Pragmatic Analysis of Machine Translation Techniques for Preserving the Authenticity of the Sanskrit Language},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3610582},
doi = {10.1145/3610582},
abstract = {Machine Translation has been a field of study for over six decades, but it has acquired substantial prominence in the last decade as processing capacity in personal computers has increased. The purpose of this paper is to discuss the usage of Sanskrit as a source, target, or supporting language in various Machine Translation systems. To investigate Machine Translation, researchers use a variety of strategies, including corpus-based, direct, and rule-based approaches. The primary goal of employing Sanskrit in Machine Translation is to evaluate its appropriateness, lexicon, and performance when proper Machine Translation methods are used. The research examines various modelling strategies for developing a machine translation system, specifically Statistical and Neural Machine Translation, in order to bridge the gap between Sanskrit and its current successor, Hindi. Interpretations are formed in Statistical Machine Translation by matching words from the source and target languages with statistical models and bilingual text corpora to learn parameters. Neural Machine Translation, on the other hand, uses an artificial neural network to predict the likelihood of a word sequence, frequently modelling entire phrases within a single integrated model. Neural Machine Translation is implemented using an encoder-decoder architecture with an attention mechanism. One of the most significant contributions of this paper is the use of different data sources, data collecting, and scraping to create a complete dataset. According to the study's findings, Neural Machine Translation outperforms the Statistical Machine Translation modelling technique. Furthermore, the paper examines the distinctive qualities of the Sanskrit language as well as the difficulties encountered by researchers in digesting Sanskrit while constructing the machine translation system. This study investigates the use of Sanskrit in Machine Translation and analyses several modelling methods, such as Statistical and Neural Machine Translation. The paper emphasizes the advantages of Neural Machine Translation and discusses the unique characteristics and challenges of the Sanskrit language in machine translation development.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
keywords = {Sanskrit, Corpus-Based Machine Translation (CBMT), Bilingual Dictionary, Interlingua, Parallel Corpora, Natural Language Processing (NLP), part of speech (POS)}
}

@inproceedings{10.1145/3640310.3674093,
author = {Morales, Sergio and Claris\'{o}, Robert and Cabot, Jordi},
title = {A DSL for Testing LLMs for Fairness and Bias},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674093},
doi = {10.1145/3640310.3674093},
abstract = {Large language models (LLMs) are increasingly integrated into software systems to enhance them with generative AI capabilities. But LLMs may reflect a biased behavior, resulting in systems that could discriminate against gender, age or ethnicity, among other ethical concerns. Society and upcoming regulations will force companies and development teams to ensure their AI-enhanced software is ethically fair. To facilitate such ethical assessment, we propose LangBiTe, a model-driven solution to specify ethical requirements, and customize and automate the testing of ethical biases in LLMs. The evaluation can raise awareness on the biases of the LLM-based components of the system and/or trigger a change in the LLM of choice based on the requirements of that particular application. The model-driven approach makes both the requirements specification and the test generation platform-independent, and provides end-to-end traceability between the requirements and their assessment. We have implemented an open-source tool set, available on GitHub, to support the application of our approach.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {203–213},
numpages = {11},
keywords = {Bias, Domain-Specific Language, Ethics, Large Language Models, Model-Driven Engineering, Red Teaming, Testing},
location = {Linz, Austria},
series = {MODELS '24}
}

@article{10.14778/3636218.3636225,
author = {Zhang, Yi and Deriu, Jan and Katsogiannis-Meimarakis, George and Kosten, Catherine and Koutrika, Georgia and Stockinger, Kurt},
title = {ScienceBenchmark: A Complex Real-World Benchmark for Evaluating Natural Language to SQL Systems},
year = {2023},
issue_date = {December 2023},
publisher = {VLDB Endowment},
volume = {17},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3636218.3636225},
doi = {10.14778/3636218.3636225},
abstract = {Natural Language to SQL systems (NL-to-SQL) have recently shown improved accuracy (exceeding 80\%) for natural language to SQL query translation due to the emergence of transformer-based language models, and the popularity of the Spider benchmark. However, Spider mainly contains simple databases with few tables, columns, and entries, which do not reflect a realistic setting. Moreover, complex real-world databases with domain-specific content have little to no training data available in the form of NL/SQL-pairs leading to poor performance of existing NL-to-SQL systems.In this paper, we introduce ScienceBenchmark, a new complex NL-to-SQL benchmark for three real-world, highly domain-specific databases. For this new benchmark, SQL experts and domain experts created high-quality NL/SQL-pairs for each domain. To garner more data, we extended the small amount of human-generated data with synthetic data generated using GPT-3. We show that our benchmark is highly challenging, as the top performing systems on Spider achieve a very low performance on our benchmark. Thus, the challenge is many-fold: creating NL-to-SQL systems for highly complex domains with a small amount of hand-made training data augmented with synthetic data. To our knowledge, ScienceBenchmark is the first NL-to-SQL benchmark designed with complex real-world scientific databases, containing challenging training and test data carefully validated by domain experts.},
journal = {Proc. VLDB Endow.},
month = dec,
pages = {685–698},
numpages = {14}
}

@inproceedings{10.1145/3701716.3715558,
author = {Khurana, Udayan and Suneja, Sahil and Samulowitz, Horst},
title = {Table Retrieval using LLMs and Semantic Table Similarity},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715558},
doi = {10.1145/3701716.3715558},
abstract = {Searching for relevant tables in response to a textual phrase or a question is an important task for large tabular data repositories, such as relational databases, CSV files in data lakes, etc. It is somewhat different from the problem of web document search because the subjects of search are tables rather than documents, while the query remains textual. In this paper, we explore a novel technique for table search on large repositories using natural language queries. It is based on a generative methodology that aims to maximize the semantic connection between the query and the resulting tables. Unlike traditional keyword search approaches, our technique can find the needed tables more effectively through deeper semantic concept discovery rather than simply searching for exact keyword matches. Additionally, our technique supports natural language queries rather than plain keyword queries. In this paper, we describe the core ideas, implementation, and effectiveness of our method using two different benchmarks with diverse queries.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {1072–1076},
numpages = {5},
keywords = {generative ai, large language models, semantic similarity, structured data search, table retrieval},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3705328.3748100,
author = {Zhu, Sinan and Simonovikj, Sanja and Edmonds, Darren and Sun, Yang},
title = {Metadata Generation and Evaluation using LLMs - Case Study on Canonical Titles},
year = {2025},
isbn = {9798400713644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3705328.3748100},
doi = {10.1145/3705328.3748100},
abstract = {In online job search platforms, autocomplete plays a crucial role in providing immediate, structured suggestions that guide users through their query process. However, inconsistencies in job title expressions, such as ’sr data scientist’ versus ’data scientist senior’, or embellished forms such as ’superstar software engineer’, can undermine the quality of autocomplete suggestions and diminish user satisfaction. Traditional normalization methods rely on manually curated vocabularies, which are labor intensive and often insufficient to capture the diverse variations in raw job titles.We present an automated and scalable framework for canonical title generation that leverages large language models (LLMs) alongside embedding-based similarity measures to derive normalized job titles directly from raw data. Our approach generalizes to domains with unstructured or inconsistently formatted titles (e.g. product catalogs or course titles): we systematically remove irrelevant information, enforce a consistent format, and eliminate overly generic or redundant titles by combining LLM-generated normalization with a two-stage deduplication process. Our method demonstrates significant improvements in normalization quality, with offline accuracy gains of 18.6\% over baseline methods and online A/B tests showing an improvement of 160\% in user engagement metrics.},
booktitle = {Proceedings of the Nineteenth ACM Conference on Recommender Systems},
pages = {1010–1013},
numpages = {4},
keywords = {canonical job titles, occupation taxonomy, data cleaning with LLMs},
location = {
},
series = {RecSys '25}
}

@inproceedings{10.1145/3637528.3671857,
author = {Ouyang, Siru and Huang, Jiaxin and Pillai, Pranav and Zhang, Yunyi and Zhang, Yu and Han, Jiawei},
title = {Ontology Enrichment for Effective Fine-grained Entity Typing},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671857},
doi = {10.1145/3637528.3671857},
abstract = {Fine-grained entity typing (FET) is the task of identifying specific entity types at a fine-grained level for entity mentions based on their contextual information. Conventional methods for FET require extensive human annotation, which is time-consuming and costly given the massive scale of data. Recent studies have been developing weakly supervised or zero-shot approaches. We study the setting of zero-shot FET where only an ontology is provided. However, most existing ontology structures lack rich supporting information and even contain ambiguous relations, making them ineffective in guiding FET. Recently developed language models, though promising in various few-shot and zero-shot NLP tasks, may face challenges in zero-shot FET due to their lack of interaction with task-specific ontology. In this study, we propose \o{}urs, where we (1) enrich each node in the ontology structure with two categories of extra information:instance information for training sample augmentation andtopic information to relate types with contexts, and (2) develop a coarse-to-fine typing algorithm that exploits the enriched information by training an entailment model with contrasting topics and instance-based augmented training samples. Our experiments show that \o{}urs achieves high-quality fine-grained entity typing without human annotation, outperforming existing zero-shot methods by a large margin and rivaling supervised methods. \o{}urs also enjoys strong transferability to unseen and finer-grained types. We will open source this work upon acceptance.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2318–2327},
numpages = {10},
keywords = {fine-grained entity typing, language models, natural language inference, zero-shot learning},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3736229.3736261,
author = {Almuntashiri, Abdullah Hamed and Ib\'{a}\~{n}ez, Luis-Daniel and Chapman, Adriane},
title = {Using LLMs to infer provenance information},
year = {2025},
isbn = {9798400719417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3736229.3736261},
doi = {10.1145/3736229.3736261},
abstract = {Having a provenance record facilitates data reuse and experimental reuse. However, provenance capture requires either: specific provenance-enabled systems to be used or human documentation. While there have been many examples of provenance-enabled systems for scientific usage, they are still the exception, not the norm. The one, standard place for provenance information of scientific experiments remains the scientific publication. Unfortunately, provenance buried in text is not immediately useful for computational purposes. Large Language Models (LLMs) have demonstrated exceptional capability across various tasks, particularly in information extraction. In this paper, we explore the potential of LLMs to infer a provenance record for scientific experiments from scientific papers. We develop an extractor, identify the most effective prompt for provenance extraction. Our results emphasise the capability of ChatGPT-4o in accessing and extracting provenance information from biomedical research papers. Additionally, we assess the scalability of the extractor for use in extracting provenance information across a set of biomedical research papers.},
booktitle = {Proceedings of the ProvenanceWeek 2025},
pages = {1–10},
numpages = {10},
keywords = {Dataset Search, Provenance, Information Extraction, LLMs},
location = {
},
series = {PW' 25}
}

@article{10.1145/3748323,
author = {Al-Thubaity, AbdulMohsen},
title = {A Novel Dataset for Arabic Domain Specific Term Extraction and Comparative Evaluation of BERT-Based Models for Arabic Term Extraction},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3748323},
doi = {10.1145/3748323},
abstract = {Automatic term extraction from domain-specific corpora is a well-known challenge in natural language processing, with applications in machine translation, information retrieval, text classification, ontology building, and thesaurus construction. Unlike English, where various approaches have been explored, Arabic automatic term extraction has relied heavily on rule-based or statistical methods due to the lack of annotated datasets. This paper introduces the first annotated dataset for Arabic automatic term extraction (AraATE) in the field of Arabic linguistics. AraATE comprises 4,148 sentences and 155,502 tokens, annotated with 4,362 single and multi-word Arabic linguistic terms. The dataset covers diverse areas of Arabic linguistics, including lexicography, semantics, pragmatics, phonetics, and semiotics. Additionally, this paper presents the results of fine-tuning five BERT-based models using AraATE. The findings indicate that AraBERTv0.2-base, CAMeLBERT-MSA, and AraBERTv0.2-large exhibit comparable F1 scores (0.82, 0.81, and 0.81). However, no statistically significant difference was observed in the performance of these models. The availability of AraATE will facilitate Arabic term extraction by serving as a benchmarking dataset for different approaches. Nevertheless, the field still requires additional benchmarking datasets that cover other domains.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
keywords = {Arabic term extraction, BERT, fine-tuning language models, information extraction, and natural language processing}
}

@inproceedings{10.1145/3648050.3648051,
author = {Wang, Shengyao},
title = {Research on Natural Language Recognition Processing System in Computer Intelligent Graphics},
year = {2024},
isbn = {9798400708251},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3648050.3648051},
doi = {10.1145/3648050.3648051},
abstract = {This paper proposes a natural language processing method based on grammar rule matching. Then the construction model of NLP system based on this algorithm is given. Set up an interactive editing environment to compress and encode the recorded information and upload it to the server. After cross-editing, it is input into the embedded system to complete the identification process and then achieve the identification of the session semantic block, the identification of the session topic, and the extraction of the session content. The proposed method is tested systematically, and the results show that the accuracy of the proposed method reaches 93\%.},
booktitle = {Proceedings of the 2023 International Conference on AI and Metaverse in Supply Chain Management},
articleno = {24},
numpages = {6},
keywords = {Embedded system, Grammar rule matching, Natural language, Parameter extraction, Speech recognition},
location = {Bhubaneswar, India},
series = {AIMSCM '23}
}

@article{10.1145/3617993,
author = {Xu, Jin},
title = {Research on the Diversification of Language Ability in Applied Linguistics and Foreign Linguistics Based on New Media},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3617993},
doi = {10.1145/3617993},
abstract = {In order to improve the research effect of the diversification of language ability in applied linguistics and foreign linguistics, this paper conducts research on the diversification of language ability in applied linguistics and foreign linguistics based on new media and intelligent data processing technology. In the process of language learning data processing, combining the adjacency degree and the network structure entropy, this paper proposes the adjacency structure entropy based on the super network to identify the key nodes in the super network. Moreover, this paper applies this indicator to the competitive hypernetwork, and accurately obtains the key nodes of the hypernetwork. In addition, this paper constructs a scientific research cooperation hypernetwork, and applies the adjacency structure entropy in the hypernetwork to this dataset. From the experimental research, it can be seen that the algorithm proposed in this paper can play a certain role in the diversification analysis of linguistic ability in linguistics and foreign linguistics. At the same time, the model proposed in this paper has a certain effect on the diversification of language ability in applied linguistics and foreign linguistics.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = sep,
keywords = {new media, applied linguistics, foreign linguistics, language ability, diversification}
}

@inproceedings{10.1145/3589132.3625600,
author = {Liu, Mengyi and Wang, Xieyang and Xu, Jianqiu and Lu, Hua},
title = {NALSpatial: An Effective Natural Language Transformation Framework for Queries over Spatial Data},
year = {2023},
isbn = {9798400701689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589132.3625600},
doi = {10.1145/3589132.3625600},
abstract = {Spatial databases play a vital role in many applications that access spatial data via appropriate queries. However, most application users lack the expertise necessary for formulating spatial queries. To fill in this gap, we propose an effective framework called NALSpatial that translates natural language queries over spatial data into executable database queries. NALSpatial consists of two core phases. The natural language understanding phase extracts key entity information, comprehends the query intent and determines the query type. The key entities and query type are passed to the subsequent natural language translation phase, which employs entity mapping rules and structured language models to construct executable database queries accordingly. We implement NALSpatial on the open-source extensible database system SECONDO to support range queries, nearest neighbor queries, spatial joins and aggregation queries. Extensive experiments show that NALSpatial on average achieves response time of about 2.5 seconds, translatability of 95\% and translation precision of 92\%, outperforming state-of-the-art natural language transformation methods.},
booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
articleno = {57},
numpages = {4},
keywords = {spatial database, natural language interface, semantic parsing, query processing},
location = {Hamburg, Germany},
series = {SIGSPATIAL '23}
}

@inproceedings{10.1145/3587828.3587840,
author = {Nordin, Noratikah and Zainol, Zurinahni and Mohd Noor, Mohd Halim and Chan, Lai Fong},
title = {An Ontology-based Modeling for Classifying Risk of Suicidal Behavior},
year = {2023},
isbn = {9781450398589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587828.3587840},
doi = {10.1145/3587828.3587840},
abstract = {Classifying an individual with suicidal behavior is a complex problem. A clinical decision support system (CDSS) helps medical experts in their daily work and supports them in effective decision-making. The huge amount of medical information and the complex correlation between the risk factors and the level of risk for suicidal behavior makes the representation of data is challenging. Therefore, this paper proposes an ontology-based modeling to classify an individual with at-risk of suicidal behavior for effective clinical decision support system. The case study is conducted to evaluate the ontology model and provides a general approach to knowledge sharing and reusing knowledge for suicide risk prevention and management. The finding shows that the ontology model can be used as a knowledge base for classification, and it is suitable to capture medical knowledge, detailed concepts, and relationships in a formal way using Web Ontology Language (OWL). The results of the proposed ontology model in terms of accuracy, specificity, and sensitivity are 83\%, 84\%, and 82\% respectively.},
booktitle = {Proceedings of the 2023 12th International Conference on Software and Computer Applications},
pages = {71–76},
numpages = {6},
keywords = {Clinical Decision Support System, Data Mining, Knowledge Base, Ontological Model, Suicidal Behavior},
location = {Kuantan, Malaysia},
series = {ICSCA '23}
}

@inproceedings{10.1145/3492547.3492612,
author = {A. Abdelnabi, Esra and M. Maatuk, Abdelsalam and M. Abdelaziz, Tawfig},
title = {An Algorithmic Approach for Generating Behavioral UML Models Using Natural Language Processing},
year = {2021},
isbn = {9781450390446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3492547.3492612},
doi = {10.1145/3492547.3492612},
abstract = {The process of transformation from informal requirements stated in natural language into a formal specification such as Unified Modeling Language (UML) is an important challenge. User requirements that are expressed in natural language can be very problematic, which makes the requirements analysis a difficult task. In this paper, we propose a method to analyze the natural language requirements and generate sequence and collaboration diagrams from these requirements, which are commonly used to describe the behavior of software systems. A case study was accomplished to compare the diagrams generated by the proposed approach to the diagrams produced by other approaches. The results showed that the elements of the sequence and collaboration diagrams extracted through our approach are very satisfactory and they would be acceptable as initial analysis models.},
booktitle = {The 7th International Conference on Engineering \&amp; MIS 2021},
articleno = {36},
numpages = {6},
keywords = {NLP tools, Requirement analysis, Sequence and Collaboration diagrams, UML diagrams},
location = {Almaty, Kazakhstan},
series = {ICEMIS'21}
}

@inproceedings{10.1145/3652988.3673932,
author = {Steenstra, Ian and Nouraei, Farnaz and Arjmand, Mehdi and Bickmore, Timothy},
title = {Virtual Agents for Alcohol Use Counseling: Exploring LLM-Powered Motivational Interviewing},
year = {2024},
isbn = {9798400706257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652988.3673932},
doi = {10.1145/3652988.3673932},
abstract = {We introduce a novel application of large language models (LLMs) in developing a virtual counselor capable of conducting motivational interviewing (MI) for alcohol use counseling. Access to effective counseling remains limited, particularly for substance abuse, and virtual agents offer a promising solution by leveraging LLM capabilities to simulate nuanced communication techniques inherent in MI. Our approach combines prompt engineering and integration into a user-friendly virtual platform to facilitate realistic, empathetic interactions. We evaluate the effectiveness of our virtual agent through a series of studies focusing on replicating MI techniques and human counselor dialog. Initial findings suggest that our LLM-powered virtual agent matches human counselors’ empathetic and adaptive conversational skills, presenting a significant step forward in virtual health counseling and providing insights into the design and implementation of LLM-based therapeutic interactions.},
booktitle = {Proceedings of the 24th ACM International Conference on Intelligent Virtual Agents},
articleno = {20},
numpages = {10},
keywords = {Alcohol Use Counseling, Embodied Conversational Agents, Intelligent Virtual Agents, Large Language Models, Motivational Interviewing, Persuasive Technology},
location = {GLASGOW, United Kingdom},
series = {IVA '24}
}

@inproceedings{10.1145/3711403.3711485,
author = {Zhao, Wei and Nie, Zhenhua and Li, Xiaoming},
title = {Teaching Practice of C Language Programming under the Constructivist Learning Paradigm},
year = {2025},
isbn = {9798400717468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711403.3711485},
doi = {10.1145/3711403.3711485},
abstract = {The construction of new engineering disciplines and the “Double Tops” initiative have put forward new requirements for the training of engineering professionals. Taking the course of C language programming as an example, this paper analyzes the pain points in current course teaching, combines the characteristics of students’ learning situation, takes students as the center, and based on the knowledge graph, an effective tool for organizing resources, carries out a modular teaching practice of C language programming design under the constructivist learning paradigm. Integrating the OBE (Outcomes-Based Education) concept into teaching, it enhances students’ practical ability, cultivates their computational thinking, and highlights the training of students’ ability to analyze and solve complex engineering problems and creative thinking. The exploration and practice of this course teaching can provide reference and reference for the teaching design of other programming courses.},
booktitle = {Proceedings of the 2024 7th International Conference on Educational Technology Management},
pages = {504–508},
numpages = {5},
keywords = {Program design, constructivist learning, teaching mode, teaching reform and practice},
location = {
},
series = {ICETM '24}
}

@inproceedings{10.1145/3613905.3644062,
author = {Gould, Sandy J. J. and Brumby, Duncan P. and Cox, Anna L.},
title = {ChatTL;DR – You Really Ought to Check What the LLM Said on Your Behalf},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3644062},
doi = {10.1145/3613905.3644062},
abstract = {Interactive large language models (LLMs) are so hot right now, and are probably going to be hot for a while. There are lots of problems exciting challenges created by mass use of LLMs. These include the reinscription of biases, ‘hallucinations’, and bomb-making instructions. Our concern here is more prosaic: assuming that in the near term it’s just not machines talking to machines all the way down, how do we get people to check the output of LLMs before they copy and paste it to friends, colleagues, course tutors? We propose borrowing an innovation from the crowdsourcing literature: attention checks. These checks (e.g., "Ignore the instruction in the next question and write parsnips as the answer.") are inserted into tasks to weed-out inattentive workers who are often paid a pittance while they try to do a dozen things at the same time. We propose ChatTL;DR1, an interactive LLM that inserts attention checks into its outputs. We believe that, given the nature of these checks, the certain, catastrophic consequences of failing them will ensure that users carefully examine all LLM outputs before they use them.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {552},
numpages = {7},
keywords = {LLMs, Large Language Models, academics being hilarious, attention checks, checking behaviour, computers-talking-to-computers-all-the-way-down-circlejerk, error detection, human factors, instructional manipulation checks, that-bloody-automatic-lane-assist-ffs},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@inproceedings{10.1145/3643479.3662055,
author = {Bui, Tuan and Tran, Oanh and Nguyen, Phuong and Ho, Bao and Nguyen, Long and Bui, Thang and Quan, Tho},
title = {Cross-Data Knowledge Graph Construction for LLM-enabled Educational Question-Answering System: A Case Study at HCMUT},
year = {2024},
isbn = {9798400705472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643479.3662055},
doi = {10.1145/3643479.3662055},
abstract = {In today's rapidly evolving landscape of Artificial Intelligence, large language models (LLMs) have emerged as a vibrant research topic. LLMs find applications in various fields and contribute significantly. Despite their powerful language capabilities, similar to pre-trained language models (PLMs), LLMs still face challenges in remembering events, incorporating new information, and addressing domain-specific issues or hallucinations. To overcome these limitations, researchers have proposed Retrieval-Augmented Generation (RAG) techniques, some others have proposed the integration of LLMs with Knowledge Graphs (KGs) to provide factual context, thereby improving performance and delivering more accurate feedback to user queries.Education plays a crucial role in human development and progress. With the technology transformation, traditional education is being replaced by digital or blended education. Therefore, educational data in the digital environment is increasing day by day. Data in higher education institutions are diverse, comprising various sources such as unstructured/structured text, relational databases, web/app-based API access, etc. Constructing a Knowledge Graph from these cross-data sources is not a simple task. This article proposes a method for automatically constructing a Knowledge Graph from multiple data sources and discusses some initial applications (experimental trials) of KG in conjunction with LLMs for question-answering tasks.},
booktitle = {Proceedings of the 1st ACM Workshop on AI-Powered Q&amp;A Systems for Multimedia},
pages = {36–43},
numpages = {8},
keywords = {Education, Knowledge Graph, Large language model, Open Intent Discovery, Question-Answering System},
location = {Phuket, Thailand},
series = {AIQAM '24}
}

@inproceedings{10.1145/3726302.3731950,
author = {Zheng, Qi and Zhong, Mingjie and Gong, Saisai and Jiang, Huimin and Wu, Kaixin and Liu, Hong and Xu, Jia and Mo, Linjian},
title = {MAAQR: An LLM-based Multi-Agent Framework for Adaptive Query Rewriting in Alipay Search},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3731950},
doi = {10.1145/3726302.3731950},
abstract = {Query rewriting is essential in e-commerce search, as it bridges the lexical gap between user queries and item descriptions, thereby enhancing search performance. Despite recent advancements, current rewriting approaches are still limited by an inadequate comprehension of domain-specific knowledge and a lack of mechanisms for adaptive refinement in response to new or changing query-item relationships. To overcome these limitations, we propose a large language model (LLM) based Multi-Agent Framework for Adaptive Query Rewriting (MAAQR) in Alipay Search. Initially, we perform knowledge-enhanced fine-tuning to improve the LLM's understanding of query and item semantics. Subsequently, a multi-agent collaborative rewriting architecture is employed to enhance rewrite quality and adaptability. MAAQR has been successfully deployed to serve Alipay's mini-app search since December 2024. Through offline experiments and online A/B testing, MAAQR significantly improves click-through rates (CTR) and the number of transactions for target queries, while substantially reducing the zero-results rate (ZRR).},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {4289–4293},
numpages = {5},
keywords = {information retrieval, large language models, multi-agent, query rewriting},
location = {Padua, Italy},
series = {SIGIR '25}
}

@article{10.1145/3732784,
author = {Xie, Shuyi and Yao, Wenlin and Dai, Yong and Wang, Shaobo and Xu, Zishan and Lin, Fan and Zhou, Donglin and Jin, Lifeng and Feng, Xinhua and Wei, Pengzhi and Lin, Yujie and Hu, Zhichao and Yu, Dong and Zhang, Zhengyou and Nie, Jing and Liu, Yuhong},
title = {TencentLLMEval: A Hierarchical Evaluation of Real-World Capabilities for Human-Aligned LLMs},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3732784},
doi = {10.1145/3732784},
abstract = {Large language models (LLMs) have shown impressive capabilities across various natural language tasks. However, evaluating their alignment with human preferences remains a challenge. To this end, we propose a comprehensive human evaluation framework to assess LLMs’ proficiency in following instructions on diverse real-world tasks. We construct a hierarchical task tree encompassing 7 major areas covering over 200 categories and over 800 tasks, which covers diverse capabilities such as question answering, reasoning, multiturn dialogue, and text generation, to evaluate LLMs in a comprehensive and in-depth manner. We also design detailed evaluation standards and processes to facilitate consistent, unbiased judgments from human evaluators. A test set of over 3,000 instances is released, spanning different difficulty levels and knowledge domains. Our work provides a standardized methodology to evaluate human alignment in LLMs for both English and Chinese. We also analyze the feasibility of automating parts of evaluation with a strong LLM (GPT-4). Our framework supports a thorough assessment of LLMs as they are integrated into real-world applications. We have made publicly available the task tree, TencentLLMEval dataset, and evaluation methodology which have been demonstrated as effective in assessing the performance of Tencent Hunyuan LLMs 1. By doing so, we aim to facilitate the benchmarking of advances in the development of safe and human-aligned LLMs.},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
keywords = {human-aligned, LLMs, Evaluation}
}

@inproceedings{10.1145/3625007.3631503,
author = {Abrouk, Lylia and Chergui, Hamza and Ahaggach, Hamid},
title = {OntoFiC : an ontology for financial fraud detection and customer behavior modeling},
year = {2024},
isbn = {9798400704093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625007.3631503},
doi = {10.1145/3625007.3631503},
abstract = {Fraud detection is a complex issue for financial institutions. They must have tools for the prevention and detection of fraud. In this article, we present our approach to detect fraudulent transactions in SWIFT network based on the domain ontology. Firstly, we present the OntoFiC ontology constructed for the modeling of SWIFT transactions and actors. This ontology is populated with a real dataset. We developed our rules-based approach with rules associated to fraud scenarios to label our transactions as legitimate or fraudulent. Finally, we made SPARQL requests to visualize these transactions through graphs. Our work is part of a collaboration project with a financial company, SKAIZen Group.},
booktitle = {Proceedings of the 2023 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {509–513},
numpages = {5},
keywords = {fraud detection, SWIFT network, domain ontology, ontology population, rules-based approach, SPARQL},
location = {Kusadasi, Turkiye},
series = {ASONAM '23}
}

@inproceedings{10.1145/3600160.3605069,
author = {Castiglione, Gianpietro and Bella, Giampaolo and Santamaria, Daniele Francesco},
title = {Towards Grammatical Tagging for the Legal Language of Cybersecurity},
year = {2023},
isbn = {9798400707728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600160.3605069},
doi = {10.1145/3600160.3605069},
abstract = {Legal language can be understood as the language typically used by those engaged in the legal profession and, as such, it may come both in spoken or written form. Recent legislation on cybersecurity obviously uses legal language in writing, thus inheriting all its interpretative complications due to the typical abundance of cases and sub-cases as well as to the general richness in detail. This paper faces the challenge of the essential interpretation of the legal language of cybersecurity, namely of the extraction of the essential Parts of Speech (POS) from the legal documents concerning cybersecurity. The challenge is overcome by our methodology for POS tagging of legal language. It leverages state-of-the-art open-source tools for Natural Language Processing (NLP) as well as manual analysis to validate the outcomes of the tools. As a result, the methodology is automated and, arguably, general for any legal language following minor tailoring of the preprocessing step. It is demonstrated over the most relevant EU legislation on cybersecurity, namely on the NIS 2 directive, producing the first, albeit essential, structured interpretation of such a relevant document. Moreover, our findings indicate that tools such as SpaCy and ClausIE reach their limits over the legal language of the NIS 2.},
booktitle = {Proceedings of the 18th International Conference on Availability, Reliability and Security},
articleno = {82},
numpages = {9},
keywords = {Act, Data Protection, NLP, POS tagging, Privacy, Pronouncement},
location = {Benevento, Italy},
series = {ARES '23}
}

@article{10.1145/3643505,
author = {King, Evan and Yu, Haoxiang and Lee, Sangsu and Julien, Christine},
title = {Sasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
url = {https://doi.org/10.1145/3643505},
doi = {10.1145/3643505},
abstract = {Smart home assistants function best when user commands are direct and well-specified---e.g., "turn on the kitchen light"---or when a hard-coded routine specifies the response. In more natural communication, however, human speech is unconstrained, often describing goals (e.g., "make it cozy in here" or "help me save energy") rather than indicating specific target devices and actions to take on those devices. Current systems fail to understand these under-specified commands since they cannot reason about devices and settings as they relate to human situations. We introduce large language models (LLMs) to this problem space, exploring their use for controlling devices and creating automation routines in response to under-specified user commands in smart homes. We empirically study the baseline quality and failure modes of LLM-created action plans with a survey of age-diverse users. We find that LLMs can reason creatively to achieve challenging goals, but they experience patterns of failure that diminish their usefulness. We address these gaps with Sasha, a smarter smart home assistant. Sasha responds to loosely-constrained commands like "make it cozy" or "help me sleep better" by executing plans to achieve user goals---e.g., setting a mood with available devices, or devising automation routines. We implement and evaluate Sasha in a hands-on user study, showing the capabilities and limitations of LLM-driven smart homes when faced with unconstrained user-generated scenarios.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = mar,
articleno = {12},
numpages = {38},
keywords = {ambient intelligence, large language models, pervasive computing, smart environments}
}

@inproceedings{10.1145/3744915.3748460,
author = {Zine, Nada and Quinton, Cl\'{e}ment and Rouvoy, Romain},
title = {LLM-based Co-Evolution of Configurable Software Systems},
year = {2025},
isbn = {9798400720246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3744915.3748460},
doi = {10.1145/3744915.3748460},
abstract = {Software Product Lines (SPLs) and s are de&nbsp;facto standards for managing variability in software systems. However, maintaining an up-to-date during software evolution is particularly challenging. Ensuring its consistency with the artifacts of an SPL requires co-evolving them alongside the developed system. When performed manually, this co-evolution process is tedious and error-prone, highlighting the need for automated support. Yet, little attention has been given to the automation of co-evolution between and source code. In this paper, we explore the potential of open-source s to fill this gap. Specifically, we investigate the extent to which s can support bidirectional co-evolution: from to source code—where modifications in the drive changes in the code—and from source code to —where updates in the code are reflected back into the. We evaluate our -based approach on a real-world configurable system. Our results demonstrate that co-evolution from source code to achieves F1 scores ranging from 0.93 to 1.0, while co-evolution from to source code achieves F1 scores between 0.41 and 0.99. These findings highlight the potential of s to support this co-evolution process, while also showing limitations and suggesting areas for improvement, particularly for the co-evolution from to code. Additionally, we conduct a comparative study across various s, revealing how choice affects co-evolution and, incidentally, how it affects model and code generation. Up to a certain size limit, larger s tend to produce more accurate and stable outputs than smaller ones, however, this influence is less pronounced in the code generation task. Overall, our work opens a new research avenue where s are leveraged for automating the co-evolution between configurable software systems and variability models.},
booktitle = {Proceedings of the 29th ACM International Systems and Software Product Line Conference - Volume A},
pages = {27–38},
numpages = {12},
keywords = {Software Product Lines, Large Language Models, Feature Models, Co-evolution},
location = {
},
series = {SPLC-A '25}
}

@inproceedings{10.1145/3486608.3486907,
author = {Barash, Mikhail},
title = {Vision: the next 700 language workbenches},
year = {2021},
isbn = {9781450391115},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486608.3486907},
doi = {10.1145/3486608.3486907},
abstract = {Language workbenches (LWBs) are tools to define software languages together with tailored Integrated Development Environments for them. A comprehensive review of language workbenches by Erdweg et al. (Comput. Lang. Syst. Struct. 44, 2015) presented a feature model of functionality of LWBs from the point of view of "languages that can be defined with a LWB, and not the definition mechanism of the LWB itself". This vision paper discusses possible functionality of LWBs with regard to language definition mechanisms. We have identified five groups of such functionality, related to: metadefinitions, metamodifications, metaprocess, LWB itself, and programs written in languages defined in a LWB. We design one of the features ("ability to define dependencies between language concerns") based on our vision.},
booktitle = {Proceedings of the 14th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {16–21},
numpages = {6},
keywords = {Language workbenches, algebraic specifications, metaprogramming, software languages},
location = {Chicago, IL, USA},
series = {SLE 2021}
}

@article{10.14778/3632093.3632111,
author = {Singh, Mukul and Cambronero, Jos\'{e} and Gulwani, Sumit and Le, Vu and Negreanu, Carina and Nouri, Elnaz and Raza, Mohammad and Verbruggen, Gust},
title = {FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language},
year = {2023},
issue_date = {November 2023},
publisher = {VLDB Endowment},
volume = {17},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3632093.3632111},
doi = {10.14778/3632093.3632111},
abstract = {Formatting is an important property in tables for visualization, presentation, and analysis. Spreadsheet software allows users to automatically format their tables by writing data-dependent conditional formatting (CF) rules. Writing such rules is often challenging for users as it requires understanding and implementing the underlying logic. We present FormaT5, a transformer-based model that can generate a CF rule given the target table and a natural language description of the desired formatting logic. We find that user descriptions for these tasks are often under-specified or ambiguous, making it harder for code generation systems to accurately learn the desired rule in a single step. To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective. These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system. To evaluate FormaT5 on diverse and real scenarios, we create an extensive benchmark of 1053 CF tasks, containing real-world descriptions collected from four different sources. We release our benchmarks to encourage research in this area. Abstention and filling allow FormaT5 to outperform 8 different neural approaches on our benchmarks, both with and without examples. Our results illustrate the value of building domain-specific learning systems.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {497–510},
numpages = {14}
}

@inproceedings{10.1145/3543873.3587601,
author = {Amith, Muhammad and Cui, Licong and Roberts, Kirk and Tao, Cui},
title = {Application of an ontology for model cards to generate computable artifacts for linking machine learning information from biomedical research},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587601},
doi = {10.1145/3543873.3587601},
abstract = {Model card reports provide a transparent description of machine learning models which includes information about their evaluation, limitations, intended use, etc. Federal health agencies have expressed an interest in model cards report for research studies using machine-learning based AI. Previously, we have developed an ontology model for model card reports to structure and formalize these reports. In this paper, we demonstrate a Java-based library (OWL API, FaCT++) that leverages our ontology to publish computable model card reports. We discuss future directions and other use cases that highlight applicability and feasibility of ontology-driven systems to support FAIR challenges.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {820–825},
numpages = {6},
keywords = {FAIR, artificial intelligence, description logic, document engineering, inference, machine learning, model card reports, ontology, semantic web, transparency},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@article{10.1145/3749116.3749132,
author = {Khan, Arijit and Wu, Tianxing and Chen, Xi},
title = {LLM+KG@VLDB 24 Workshop Summary},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/3749116.3749132},
doi = {10.1145/3749116.3749132},
abstract = {The unification of large language models (LLMs) and knowledge graphs (KGs) has emerged as a hot topic. At the LLM+KG'24 workshop, co-located with VLDB 2024 in Guangzhou, China, the key theme explored was important data management challenges and opportunities due to the effective interaction between LLMs and KGs. The report outlines major directions and approaches presented by various speakers during the workshop.},
journal = {SIGMOD Rec.},
month = jul,
pages = {60–65},
numpages = {6}
}

@inproceedings{10.1145/3690712.3690723,
author = {Wasi, Azmine Toushik and Islam, Mst Rafia and Islam, Raima},
title = {LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning},
year = {2024},
isbn = {9798400710315},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3690712.3690723},
doi = {10.1145/3690712.3690723},
abstract = {Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn’t directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-centered aspects in writing and improve writing aid systems.},
booktitle = {Proceedings of the Third Workshop on Intelligent and Interactive Writing Assistants},
pages = {38–42},
numpages = {5},
keywords = {Human computer interaction, Large Language Models, Sense of Ownership, Writing Assistants},
location = {Honolulu, HI, USA},
series = {In2Writing '24}
}

@inproceedings{10.1145/3723498.3723840,
author = {Xu, Kaijie and Verbrugge, Clark},
title = {Constraint Is All You Need: Optimization-Based 3D Level Generation with LLMs},
year = {2025},
isbn = {9798400718564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3723498.3723840},
doi = {10.1145/3723498.3723840},
abstract = {Procedural Content Generation (PCG) has long enabled efficient and varied game level creation. However, integrating high-level design intentions and game mechanics into complex 3D environments remains challenging. This paper introduces a comprehensive framework that transforms narrative-level descriptions into playable 3D game levels. First, Large Language Models (LLMs) parse natural language descriptions of game environments into a structured Game Level Description Language (GLDL), capturing essential spatial constraints. Next, we model level generation as a Facility Layout Optimization problem, ensuring that facility placements and configurations adhere to specified design criteria. Through comprehensive experiments, including automated constraint evaluations and agent-based simulations, our approach ensures both the feasibility and stability of the constraints extracted from textual descriptions. We confirm that the resulting game levels remain interactive, reasonable, and controllable to their original specifications.},
booktitle = {Proceedings of the 20th International Conference on the Foundations of Digital Games},
articleno = {66},
numpages = {13},
keywords = {Procedural Content Generation, Facility Layout Problem, Level Generation, Large Language Models, Game Design},
location = {
},
series = {FDG '25}
}

@article{10.1145/3626960,
author = {Tsaneva, Stefani and Sabou, Marta},
title = {Enhancing Human-in-the-Loop Ontology Curation Results through Task Design},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3626960},
doi = {10.1145/3626960},
abstract = {The success of artificial intelligence (AI) applications is heavily dependent on the quality of data they rely on. Thus, data curation, dealing with cleaning, organising, and managing data, has become a significant research area to be addressed. Increasingly, semantic data structures such as ontologies and knowledge graphs empower the new generation of AI systems. In this article, we focus on ontologies as a special type of data. Ontologies are conceptual data structures representing a domain of interest and are often used as a backbone to knowledge-based intelligent systems or as an additional input for machine learning algorithms. Low-quality ontologies, containing incorrectly represented information or controversial concepts modelled from a single viewpoint, can lead to invalid application outputs and biased systems. Thus, we focus on the curation of ontologies as a crucial factor for ensuring trust in the enabled AI systems. While some ontology quality aspects can be automatically evaluated, others require a human-in-the-loop evaluation. Yet, despite the importance of the field, several ontology quality aspects have not yet been addressed and there is a lack of guidelines for optimal design of human computation tasks to perform such evaluations. In this article, we advance the state-of-the-art by making two novel contributions. First, we propose a human computation (HC)–based approach for the verification of ontology restrictions —an ontology evaluation aspect that has not yet been addressed with HC techniques. Second, by performing two controlled experiments with a junior expert crowd, we empirically derive task design guidelines for achieving high-quality evaluation results related to (i) the formalism for representing ontology axioms and (ii) crowd qualification testing. We find that the representation format of the ontology does not significantly influence the campaign results. Nevertheless, contributors expressed a preference in working with a graphical ontology representation. Additionally, we show that an objective qualification test is better fitted at assessing contributors’ prior knowledge rather than a subjective self-assessment and that prior modelling knowledge of the contributors had a positive effect on their judgements. We make all artefacts designed and used in the experimental campaign publicly available.},
journal = {J. Data and Information Quality},
month = mar,
articleno = {4},
numpages = {25},
keywords = {Ontology evaluation, human-in-the-loop, human computation}
}

@inproceedings{10.1145/3627508.3638340,
author = {Gohsen, Marcel and Stein, Benno},
title = {Assisted Knowledge Graph Authoring: Human-Supervised Knowledge Graph Construction from Natural Language},
year = {2024},
isbn = {9798400704345},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627508.3638340},
doi = {10.1145/3627508.3638340},
abstract = {Encyclopedic knowledge graphs, such as Wikidata, host an extensive repository of millions of knowledge statements. However, domain-specific knowledge from fields such as history, physics, or medicine is significantly underrepresented in those graphs. Although few domain-specific knowledge graphs exist (e.g., Pubmed for medicine), developing specialized retrieval applications for many domains still requires constructing knowledge graphs from scratch. To facilitate knowledge graph construction, we introduce WAKA: a Web application that allows domain experts to create knowledge graphs through the medium with which they are most familiar: natural language.},
booktitle = {Proceedings of the 2024 Conference on Human Information Interaction and Retrieval},
pages = {376–380},
numpages = {5},
keywords = {Information Extraction, Knowledge Graph Construction, Semantic Web},
location = {Sheffield, United Kingdom},
series = {CHIIR '24}
}

@inproceedings{10.1145/3706599.3719683,
author = {Sch\"{a}fer, Ren\'{e} and Preuschoff, Paul Miles and Niewianda, Rene and Hahn, Sophie and Fiedler, Kevin and Borchers, Jan},
title = {Don't Detect, Just Correct: Can LLMs Defuse Deceptive Patterns Directly?},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3719683},
doi = {10.1145/3706599.3719683},
abstract = {Deceptive (or dark) patterns, UI design strategies manipulating users against their best interests, have become widespread. We introduce an idea for technical countermeasures against such patterns. It feeds the HTML code of web elements that may contain deceptive patterns into a large language model (LLM) and iteratively prompts it to make these elements less manipulative. We evaluated our approach with GPT-4o and self-created web elements. The most consistent results appeared after three iterations, with 91\% of deceptive elements being less manipulative and 96\% not more manipulative than originally. We contribute our minimal and improved prompts and a labeled dataset of all 2,600 redesigns with the LLM’s justifications for its changes. We also performed preliminary tests on real websites to show and discuss the feasibility of our approach in the field. Our findings suggest that LLMs can defuse certain deceptive patterns without prior model training, promising a major advance in fighting these manipulations.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {199},
numpages = {11},
keywords = {deceptive patterns, dark patterns, large language models, countermeasures},
location = {
},
series = {CHI EA '25}
}

@article{10.1145/3707637,
author = {Hamed, Naeima and Rana, Omer and Orozco Ter Wengel, Pablo and Goossens, Benoit and Perera, Charith},
title = {FooDS: Ontology-based Knowledge Graphs for Forest Observatories},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1145/3707637},
doi = {10.1145/3707637},
abstract = {Wildlife research activities generate data on ecosystems and species interactions from varied independent projects. Forest Observatories are online platforms that curate, integrate, and analyze wildlife research data for forest monitoring. However, integrating data from disparate sources can be challenging due to data heterogeneity. This study, in collaboration with a research facility in the forest of Sabah, Malaysian Borneo, proposes a novel approach to integrate heterogeneous wildlife data for Forest Observatories. We used the Forest Observatory Ontology (FOO) to standardize wildlife data entities generated by sensors. Four semantically modeled wildlife datasets populated FOO, resulting in an ontology-based knowledge graph named FooDS (Forest Observatory Ontology Data Store). We evaluated FOO and FooDS using specialized open-source ontology scanners, domain experts’ feedback, and applied use cases. This study contributes FooDS, the first ontology-based knowledge graph for Forest Observatories, which provides accurate query responses, reasoning about data, and granular data acquisition from diverse datasets. FOO in turtle format, FOO’s documentation and FooDS in turtle format and their resource website are published at , , , and .},
journal = {ACM J. Comput. Sustain. Soc.},
month = jan,
articleno = {2},
numpages = {42},
keywords = {Wildlife data, Internet of Things, ontology, knowledge graph}
}

@inproceedings{10.1145/3652620.3688197,
author = {Silva Mercado, Jonathan},
title = {AI Assisted Domain Modeling Explainability and Traceability},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688197},
doi = {10.1145/3652620.3688197},
abstract = {Domain Models are abstract representations of selected elements in a domain that is created in a collaborative process between domain and modeler experts. The participants share domain knowledge to conceptualize and reason about the elements that will create the domain models. Through this exchange, a comprehensive and accurate representation of the domain is achieved, ensuring that the model captures the relevant aspects and relationships in the domain. Research in Artificial Intelligence (AI) has explored various methods to assist in the creation of domain models from text using Natural Language Processing (NLP) and Machine Learning (ML). Recent advancements with Large Language Models (LLMs) have shown that it is possible to create domain models using prompting techniques; however, the generated domain models contain errors and remain constrained by the performance of the LLM used.Despite the impressive capabilities of LLMs to create domain models, it is evident that it does not address the needs of domain and modelers experts that participate in the creation of domain models. Every AI technique has its advantages and limitations that must be integrated with human feedback in a collaboration process. Therefore, we propose an approach that incorporates human-AI collaboration supported by AI assistants that follows a dialogue approach to understand the users needs and purpose to suggest relevant models. Our proposal combines symbolic and subsymbolic AI techniques with explainability and traceability of the decisions that assist to create domain models that are relevant for the users.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {130–135},
numpages = {6},
keywords = {domain modeling, large language models, uncertainty, explainability, traceability},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3701716.3715531,
author = {Frey, Johannes and Ciuciu-Kiss, Jenifer Tabita and Arndt, Natanael},
title = {Breaking Accessibility Barriers: An Ontology Proxy with Failure Recovery and Time Travel Capabilities},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715531},
doi = {10.1145/3701716.3715531},
abstract = {This paper introduces a novel concept and implementation of an ontology proxy designed to seamlessly enhance accessibility and reliability of the Web of Ontologies by addressing challenges such as link rot, evolution inconsistencies, and communication failures. The proxy features a time travel mode, powered by DBpedia Archivo, that provides access to archived and versioned snapshots of ontologies. This enables failure recovery and the emulation of a consistent state in time, supporting reproducible research and enhancing the FAIRness of ontologies and associated (meta)data in a plug-and-play manner. Initial evaluations show significant improvements in ontology retrieval success rates, underscoring the proxy's potential as a viable interface for breaking accessibility barriers.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {966–970},
numpages = {5},
keywords = {accessibility issues, fair data, linked data, ontology wayback machine},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3570991.3571051,
author = {Ghosh, Sohom and Naskar, Sudip Kumar},
title = {Using Natural Language Processing to Enhance Understandability of Financial Texts},
year = {2023},
isbn = {9781450397971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570991.3571051},
doi = {10.1145/3570991.3571051},
abstract = {Dealing with money has always been one of the basic skills one needs to live a comfortable life. However, financial literacy rates across the nations are extremely low. Furthermore, over the years the returns from traditional investment avenues like bank fixed deposits (FD), real estate, etc. have been diminishing. This entices new-age investors to trade and reap profits from the ever-growing stock markets. Nevertheless, in reality, only a handful of active traders are able to earn more than the FD rates. This is due to the lack of financial knowledge. The presence of complex concepts and jargons further reduces comprehensibility. In this paper, we present how financial texts can be demystified using Natural Language Processing (NLP). It consists of neural-based readability assessment and hypernym extraction tools to improve the readability of financial texts. Other modules include financial domain specific systems for automated claim detection, sustainability assessment, etc.},
booktitle = {Proceedings of the 6th Joint International Conference on Data Science \&amp; Management of Data (10th ACM IKDD CODS and 28th COMAD)},
pages = {301–302},
numpages = {2},
keywords = {claim detection, financial text processing, hypernym detection, natural language processing, readability},
location = {Mumbai, India},
series = {CODS-COMAD '23}
}

@inproceedings{10.1145/3681772.3698217,
author = {Tupayachi, Jose and Li, Xueping},
title = {Conversational Geographic Question Answering for Route Optimization: An LLM and Continuous Retrieval-Augmented Generation Approach},
year = {2025},
isbn = {9798400711510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3681772.3698217},
doi = {10.1145/3681772.3698217},
abstract = {We present a pilot study exploring the potential of Large Language Models (LLMs) to interface with application programming interfaces through logical instructions, specifically within the domain of Geographic Question Answering for route optimization. This study employs a Continuous Retrieval-Augmented Generation approach combined with fine-tuned LLMs, featuring customized node-based storage and vector search retrieval. We also provide a comparative analysis of the method's effectiveness and adaptability in handling diverse textual queries.},
booktitle = {Proceedings of the 17th ACM SIGSPATIAL International Workshop on Computational Transportation Science GenAI and Smart Mobility Session},
pages = {56–59},
numpages = {4},
keywords = {Geographical Information, Large Language Models, Question Answering, Retrieval Augmented Generation},
location = {Atlanta, GA, USA},
series = {IWCTS'24}
}

@inproceedings{10.1145/3472307.3484649,
author = {Kim, Sujeong and Tamrakar, Amir},
title = {“How to best say it?” : Translating Directives in Machine Language into Natural Language in the Blocks World},
year = {2021},
isbn = {9781450386203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472307.3484649},
doi = {10.1145/3472307.3484649},
abstract = {We propose a method to generate optimal natural language for block placement directives generated by a machine’s planner during human-agent interactions in the blocks world. A non user-friendly machine directive, e.g., move(ObjId, toPos), is transformed into visually and contextually grounded referring expressions that are much easier for the user to comprehend. We describe an algorithm that progressively and generatively transforms the machine’s directive in ECI (Elementary Composable Ideas)-space, generating many alternative versions of the directive. We then define a cost function to evaluate the ease of comprehension of these alternatives and select the best option. The parameters for this cost function were derived empirically from a user study that measured utterance-to-action timings.},
booktitle = {Proceedings of the 9th International Conference on Human-Agent Interaction},
pages = {289–293},
numpages = {5},
keywords = {cost, natural language generation, optimization, perception},
location = {Virtual Event, Japan},
series = {HAI '21}
}

@inproceedings{10.1145/3582437.3587185,
author = {van Rozen, Riemer and Bouwer, Anders and Millenaar, Karel},
title = {Towards a Unified Language for Card Game Design},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582437.3587185},
doi = {10.1145/3582437.3587185},
abstract = {Card game creation is a powerful tool for game&nbsp;design. Using&nbsp;playing cards, game designers can rapidly prototype and iteratively playtest a game’s core mechanisms to explore alternatives and improve the gameplay. However, this process is time-consuming, imprecise and challenging to steer and focus. We aim to empower designers with solutions that automate game design processes. In particular, we study to what extent a unified game design language can offer theoretical foundations, systematic techniques and practical solutions. We propose a novel approach towards a solution that leverages the expressive power of playing cards. Initially focusing on well-known card games, we illustrate the steps for creating CardScript, a formal language and toolkit that supports game design processes. The approach also has the potential to impact a wider research area. When fully developed, a unified language with a common tool set can enable reuse, and eventually support joint research agendas. We start the discussion by highlighting perspectives that relate open challenges to opportunities for future collaboration.},
booktitle = {Proceedings of the 18th International Conference on the Foundations of Digital Games},
articleno = {47},
numpages = {4},
keywords = {design tools, domain-specific languages, game design},
location = {Lisbon, Portugal},
series = {FDG '23}
}

@inproceedings{10.1145/3340555.3356099,
author = {Tan, Chaohong and Ling, Zhenhua},
title = {Multi-Classification Model for Spoken Language Understanding},
year = {2019},
isbn = {9781450368605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340555.3356099},
doi = {10.1145/3340555.3356099},
abstract = {The spoken language understanding (SLU) is an important part of spoken dialogue system (SDS). In the paper, we focus on how to extract a set of act-slot-value tuples from users’ utterances in the 1st Chinese Audio-Textual Spoken Language Understanding Challenge (CATSLU). This paper adopts the pretrained BERT model to encode users’ utterances and builds multiple classifiers to get the required tuples. In our framework, finding acts and values of slots are recognized as classification tasks respectively. Such multi-task training is expected to help the encoder to get better understanding of the utterance. Since the system is built on the transcriptions given by automatic speech recognition (ASR), some tricks are applied to correct the errors of the tuples. We also found that using the minimum edit distance (MED) between results and candidates to rebuild the tuples was beneficial in our experiments.},
booktitle = {2019 International Conference on Multimodal Interaction},
pages = {526–530},
numpages = {5},
keywords = {BERT, Spoken language understanding, classification, multi-task learning, text tagging},
location = {Suzhou, China},
series = {ICMI '19}
}

@inproceedings{10.1145/3719160.3728624,
author = {Desai, Smit and Dubiel, Mateusz and Zargham, Nima and Mildner, Thomas and Spillner, Laura},
title = {Personas Evolved: Designing Ethical LLM-Based Conversational Agent Personalities},
year = {2025},
isbn = {9798400715273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719160.3728624},
doi = {10.1145/3719160.3728624},
abstract = {The emergence of Large Language Models (LLMs) has revolutionized Conversational User Interfaces (CUIs), enabling more dynamic, context-aware, and human-like interactions across diverse domains, from social sciences to healthcare. However, the rapid adoption of LLM-based personas raises critical ethical and practical concerns, including bias, manipulation, and unforeseen social consequences. Unlike traditional CUIs, where personas are carefully designed with clear intent, LLM-based personas generate responses dynamically from vast datasets, making their behavior less predictable and harder to govern. This workshop aims to bridge the gap between CUI and broader AI communities by fostering a cross-disciplinary dialogue on the responsible design and evaluation of LLM-based personas. Bringing together researchers, designers, and practitioners, we will explore best practices, develop ethical guidelines, and promote frameworks that ensure transparency, inclusivity, and user-centered interactions. By addressing these challenges collaboratively, we seek to shape the future of LLM-driven CUIs in ways that align with societal values and expectations.},
booktitle = {Proceedings of the 7th ACM Conference on Conversational User Interfaces},
articleno = {32},
numpages = {4},
keywords = {Conversational User Interfaces, Interaction Design, Personas, LLMs},
location = {
},
series = {CUI '25}
}

@inproceedings{10.1145/3656156.3665133,
author = {Haghighi, Nava},
title = {Ontological Breakdown: Toward a World of Many Worlds},
year = {2024},
isbn = {9798400706325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3656156.3665133},
doi = {10.1145/3656156.3665133},
abstract = {Examining the taken-for-granted assumptions and views of the world underlying the design of technological artifacts, this work posits that a lack of ontological self-reflection can constrain imagination, impeding movement toward a world of many worlds. I propose ontological breakdown as an analytic lens for interrogating the default assumptions underlying the design of technology, using LLMs as a case-study and drawing parallels to the discourse on values in design. Then, I share three ways in which I have used ontological breakdowns generatively to (1) surface ontological difference and create spaces for experiencing ontological alternatives to our defaults, (2) explore ontological alternatives in the design of artifacts and enable the end-users to notice their own ontological defaults, and (3) expand ontological diversity by empowering the end-users to move beyond the prescribed defaults. I demonstrate the generative potential of ontological breakdowns by providing examples of my work in personal informatics.},
booktitle = {Companion Publication of the 2024 ACM Designing Interactive Systems Conference},
pages = {70–73},
numpages = {4},
keywords = {LLM, generative AI, ontological breakdown, ontological design, ontologies, personal informatics},
location = {IT University of Copenhagen, Denmark},
series = {DIS '24 Companion}
}

@article{10.1145/3717067,
author = {Jiang, Shuyu and Chen, Xingshu and Tang, Rui},
title = {Deceiving LLM through Compositional Instruction with Hidden Attacks},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1556-4665},
url = {https://doi.org/10.1145/3717067},
doi = {10.1145/3717067},
abstract = {Recently, large language models (LLMs) have demonstrated promising applications in the autonomous driving (AD) domain, including language-based interactions and decision-making. Ensuring they safely handle harmful inputs is crucial before formal deployment. However, research reveals emerging hand-crafted jailbreak attacks, which pack harmful prompts into harmless instructions, can bypass LLMs’ security mechanisms and elicit harmful responses. To deeply understand such jailbreaks, this paper introduces a Compositional Instruction Attack (CIA) framework to generalize them, and develop two CIA jailbreaking methods to automatically generate tailored jailbreak prompts for each harmful prompt. Then, this paper builds the first CIA question-answering (CIAQA) dataset with 2.7K multiple-choice questions of 900 successful jailbreaks, for assessing LLMs’ ability to identify underlying harmful intents, harmfulness, and task priority in CIA jailbreaks. Combined with experimental analysis on CIAQA and other datasets, this paper concludes three possible reasons for the failure of LLM defenses against CIAs. Finally, we propose an intent-based defense paradigm (IBD), enabling LLMs to defend against CIA by leveraging its capability to identify intents. Experimental results show CIA can achieve attack success rates (ASR) of 95\%+ and 85\%+ in AD and common harmful scenarios for three well-known LLMs (GPT-4, GPT-3.5, and Llama2-70b-chat), and IBD reduces ASR by 74\%+.},
note = {Just Accepted},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = feb,
keywords = {Large language model, autonomous driving, adversarial attack, harmful prompt}
}

@article{10.1145/3564769,
author = {Kumar Attar, Rakesh and Goyal, Vishal and Goyal, Lalit},
title = {State of the Art of Automation in Sign Language: A Systematic Review},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3564769},
doi = {10.1145/3564769},
abstract = {Sign language is the fundamental communication language of deaf people. Efforts to develop sign language generation systems can make the life of these people smooth and effortless. Despite the importance of sign language generation systems, there is a paucity of a systematic literature review. This is the foremost recognizable scholastic literature review of sign language generation systems. It presents a scholastic database of the literature between 1998 and 2020 and suggests classification criteria to systematize research studies. Four hundred fourteen research studies were recognized and reviewed for their direct pertinence to sign language generation systems. One hundred sixty-two research studies were subsequently chosen, examined, and classified. Each of the 162 chosen research papers was categorized based on 30 sign languages and was further comparatively analyzed based on seven comparison parameters (input form, translation technologies, application domain, use of parsers/grammars, manual/non-manual features, accuracy, and output form). It is evident from our research findings that the majority of research on sign language generation was carried out using data-driven approaches in the absence of proper grammar rules and generated only manual signs. This research study may provide researchers a roadmap toward future research directions and facilitate the compilation of information in the field of sign language generation.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {94},
numpages = {80},
keywords = {Machine translation, Interlingua, virtual avatar, SiGML, HamNoSys}
}

@inproceedings{10.1145/3307630.3342417,
author = {Achtaich, Asmaa and Roudies, Ounsa and Souissi, Nissrine and Salinesi, Camille and Mazo, Ra\'{u}l},
title = {Evaluation of the State-Constraint Transition Modelling Language: A Goal Question Metric Approach},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342417},
doi = {10.1145/3307630.3342417},
abstract = {Self-adaptive systems (SAS) are exceptional systems, on account of their versatile composition, dynamic behavior and evolutive nature. Existing formal languages for the specification of SAS focus on adapting system elements to achieve a target goal, following specific rules, without much attention on the adaptation of requirements themselves. The State-Constraint Transition (SCT) modeling language enables the specification of dynamic requirements, both at the domain and application level, as a result of space or time variability. This language, evaluated in this paper, enables the specification of a variety of requirement types, for SASs from different domains, while generating a configuration, all configurations, and number of possible configurations, in milliseconds. This paper presents these results, namely; expressiveness, domain independence and scalability, from the viewpoint of designers and domain engineers, following a goal-question-metric approach. However, being primarily based on constraint programming (CP), the language suffers from drawbacks inherited from this paradigm, specifically time related requirements, like (e.g. order, frequency and staged requirements).},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {106–113},
numpages = {8},
keywords = {IoT, constraint programming, dynamic software product lines, modeling language, state machine},
location = {Paris, France},
series = {SPLC '19}
}

@inbook{10.1145/3677389.3702541,
author = {Wang, Chunying and Ji, Heng and Han, Wenying},
title = {Construction and Application of Emotion Ontology for Narrative Text},
year = {2025},
isbn = {9798400710933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677389.3702541},
abstract = {Emotional information in narrative texts is an important resource for deepening text interpretation and promoting literature utilization, but there is still a lack of dedicated information organization schemes. Based on the systematic analysis of related theories, this study constructed the narrative text emotion ontology (NTEO), which contains three core components of event, appraisal, and emotion, as well as three related components of emotion category, emotion expression, and action tendency, for structuring and organizing the emotion information. Meanwhile, six relations of temporal, cause, circumstance, regulation, change, and trigger were summarized for embedding emotional units into narrative sequences. Furthermore, taking Tsen Shui-fang's Diary, an archive related to the Nanjing Massacre in China, as an example, we explored the innovative applications of fine-grained emotion classification, fine-grained emotion visualization, and emotion attribution analysis supported by this emotion ontology.},
booktitle = {Proceedings of the 24th ACM/IEEE Joint Conference on Digital Libraries},
articleno = {61},
numpages = {5}
}

@inproceedings{10.1145/3307630.3342401,
author = {Villota, Angela and Mazo, Ra\'{u}l and Salinesi, Camille},
title = {The High-Level Variability Language: An Ontological Approach},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342401},
doi = {10.1145/3307630.3342401},
abstract = {Given its relevance, there is an extensive body of research for modeling variability in diverse domains. Regretfully, the community still faces issues and challenges to port or share variability models among tools and methodological approaches. There are researchers, for instance, implementing the same algorithms and analyses again because they use a specific modeling language and cannot use some existing tool. This paper introduces the High-Level Variability Language (HLVL), an expressive and extensible textual language that can be used as a modeling and an intermediate language for variability. HLVL was designed following an ontological approach, i.e., by defining their elements considering the meaning of the concepts existing on different variability languages. Our proposal not only provides a unified language based on a comprehensive analysis of the existing ones but also sets foundations to build tools that support different notations and their combination.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {162–169},
numpages = {8},
keywords = {domain specific language, variability language, variability specification},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.5555/3643142.3643333,
author = {Tu, Ming-Yu and Ehm, Hans and Ismail, Abdelgafar and Ulrich, Philipp},
title = {Reusable Ontology Generation and Matching from Simulation Models},
year = {2024},
isbn = {9798350369663},
publisher = {IEEE Press},
abstract = {As simulating semiconductor manufacturing grows complex, model reuse becomes appealing since it can reduce the time incurred in developing future models. Also, considering a large network of the semiconductor supply chain, knowledge sharing can enable the efficient development of simulation models in a collaborative organization. Such necessity of reusability and interoperability of simulation models motivates this paper. We will address these challenges through ontological modeling and linking of the simulation components. The first application is generating reusable ontologies from simulation models. Another discussed application is ontology matching for knowledge sharing between simulation components and a meta-model of the semiconductor supply chain. The proposed approach succeeds in automatically transforming simulation into reusable knowledge and identifying interconnection in a semiconductor manufacturing system.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2298–2309},
numpages = {12},
location = {San Antonio, Texas, USA},
series = {WSC '23}
}

@inproceedings{10.1145/3544548.3580964,
author = {Ruoff, Marcel and Myers, Brad A and Maedche, Alexander},
title = {ONYX: Assisting Users in Teaching Natural Language Interfaces Through Multi-Modal Interactive Task Learning},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580964},
doi = {10.1145/3544548.3580964},
abstract = {Users are increasingly empowered to personalize natural language interfaces (NLIs) by teaching how to handle new natural language (NL) inputs. However, our formative study found that when teaching new NL inputs, users require assistance in clarifying ambiguities that arise and want insight into which parts of the input the NLI understands. In this paper we introduce ONYX, an intelligent agent that interactively learns new NL inputs by combining NL programming and programming-by-demonstration, also known as multi-modal interactive task learning. To address the aforementioned challenges, ONYX provides suggestions on how ONYX could handle new NL inputs based on previously learned concepts or user-defined procedures, and poses follow-up questions to clarify ambiguities in user demonstrations, using visual and textual aids to clarify the connections. Our evaluation shows that users provided with ONYX’s new features achieved significantly higher accuracy in teaching new NL inputs (median: 93.3\%) in contrast to those without (median: 73.3\%).},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {417},
numpages = {16},
keywords = {Data Visualization Tools, End User Development, Interactive Task Learning, Natural Language Interfaces},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3670474.3685948,
author = {Batten, Christopher and Pinckney, Nathaniel and Liu, Mingjie and Ren, Haoxing and Khailany, Brucek},
title = {PyHDL-Eval: An LLM Evaluation Framework for Hardware Design Using Python-Embedded DSLs},
year = {2024},
isbn = {9798400706998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3670474.3685948},
doi = {10.1145/3670474.3685948},
abstract = {Embedding hardware design frameworks within Python is a promising technique to improve the productivity of hardware engineers. At the same time, there is significant interest in using large-language models (LLMs) to improve key chip design tasks. This paper describes PyHDL-Eval, a new framework for evaluating LLMs on specification-to-RTL tasks in the context of Python-embedded domain-specific languages (DSLs). The framework includes 168 problems, Verilog reference solutions, Verilog test benches, Python test scripts, and workflow orchestration scripts. We use the framework to conduct a detailed case study comparing five LLMs (CodeGemma 7B, Llama3 8B/70B, GPT4, and GPT4 Turbo) targeting Verilog and five Python-embedded DSLs (PyMTL3, PyRTL, MyHDL, Migen, and Amaranth). Our results demonstrate the promise of in-context learning when applied to smaller models (e.g., pass rate for CodeGemma 7B improves from 14.9\% to 32.7\% on Verilog) and Python-embedded DSLs (e.g., pass rate for LLama3 70B improves from 0.6\% to 33.0\% on PyMTL3). We find LLMs perform better when targeting Verilog as compared to Python-embedded DSLs (e.g., pass rate for GPT4 Turbo is 72.2\% on Verilog and 29.8-62.0\% on the Python-embedded DSLs) despite using a popular general-purpose host language. PyHDL-Eval will serve as a useful framework for future research at the intersection of Python-embedded DSLs and LLMs.},
booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD},
articleno = {10},
numpages = {17},
keywords = {Python-embedded domain-specific languages, hardware description languages, large language models},
location = {Salt Lake City, UT, USA},
series = {MLCAD '24}
}

@article{10.1145/3374217,
author = {Zhang, Wei Emma and Sheng, Quan Z. and Alhazmi, Ahoud and Li, Chenliang},
title = {Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3374217},
doi = {10.1145/3374217},
abstract = {With the development of high computational devices, deep neural networks (DNNs), in recent years, have gained significant popularity in many Artificial Intelligence (AI) applications. However, previous efforts have shown that DNNs are vulnerable to strategically modified samples, named adversarial examples. These samples are generated with some imperceptible perturbations, but can fool the DNNs to give false predictions. Inspired by the popularity of generating adversarial examples against DNNs in Computer Vision (CV), research efforts on attacking DNNs for Natural Language Processing (NLP) applications have emerged in recent years. However, the intrinsic difference between image (CV) and text (NLP) renders challenges to directly apply attacking methods in CV to NLP. Various methods are proposed addressing this difference and attack a wide range of NLP applications. In this article, we present a systematic survey on these works. We collect all related academic works since the first appearance in 2017. We then select, summarize, discuss, and analyze 40 representative works in a comprehensive way. To make the article self-contained, we cover preliminary knowledge of NLP and discuss related seminal works in computer vision. We conclude our survey with a discussion on open issues to bridge the gap between the existing progress and more robust adversarial attacks on NLP DNNs.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {24},
numpages = {41},
keywords = {Deep neural networks, adversarial examples, natural language processing, textual data}
}

@inproceedings{10.1145/3307630.3342403,
author = {Berger, Thorsten and Collet, Philippe},
title = {Usage Scenarios for a Common Feature Modeling Language},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342403},
doi = {10.1145/3307630.3342403},
abstract = {Feature models are recognized as a de facto standard for variability modeling. Presented almost three decades ago, dozens of different variations and extensions to the original feature-modeling notation have been proposed, together with hundreds of variability management techniques building upon feature models. Unfortunately, despite several attempts to establish a unified language, there is still no emerging consensus on a feature-modeling language that is both intuitive and simple, but also expressive enough to cover a range of important usage scenarios. There is not even a documented and commonly agreed set of such scenarios.Following an initiative among product-line engineering researchers in September 2018, we present 14 usage scenarios together with examples and requirements detailing each scenario. The scenario descriptions are the result of a systematic process, where members of the initiative authored original descriptions, which received feedback via a survey, and which we then refined and extended based on the survey results, reviewers' comments, and our own expertise. We also report the relevance of supporting each usage scenario for the language, as perceived by the initiative's members, prioritizing each scenario. We present a roadmap to build and implement a first version of the envisaged common language.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {174–181},
numpages = {8},
keywords = {feature models, software product lines, unified language},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1145/3745022,
author = {Lu, Kezhi and Lu, Jie and Xu, Hanshi and Guo, Kairui and Zhang, Qian and Lin, Hua and Grosser, Mark and Zhang, Yi and Zhang, Guangquan},
title = {Genomics-Enhanced Cancer Risk Prediction for Personalized LLMs-Driven Healthcare Recommender Systems},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1046-8188},
url = {https://doi.org/10.1145/3745022},
doi = {10.1145/3745022},
abstract = {Cancer risk prediction is a cornerstone of personalized medicine that offers opportunities for early detection and preventive interventions. However, the current models are designed to predict cancer risk face several challenges. First, most rely on traditional statistical methods, which struggle to capture the complexity of genetic, family medical history, and lifestyle factors. Hence, the accuracy of these models is limited. Additionally, the models neglect to integrate multidimensional data sources, particularly genetic information like single nucleotide polymorphisms (SNPs), which could enhance prediction accuracy. Third, while the system might effectively predict risk, it cannot translate those predictions into actionable healthcare recommendations to reduce cancer risk.In this study, we address all three of these limitations. With a focus on six prevalent cancers – we extracted SNPs data from the UK Biobank and designed a novel risk prediction model for cancer and personalized healthcare recommendations based upon the mixture of experts (MoE) paradigm and large language models (LLMs) respectively. Named MoE-HRS, experts based two router networks for separate processing by the Transformer and the convolutional neural network (CNN). Experiments on UK Biobank data show that our model outperforms state-of-the-art cancer risk prediction models. To bridge the gap between risk prediction and practical healthcare applications, we devised a healthcare recommender system powered by LLMs. This approach holds promise for enhancing early detection rates and promoting preventive healthcare management1.},
note = {Just Accepted},
journal = {ACM Trans. Inf. Syst.},
month = jun,
keywords = {Healthcare Recommender Systems, LLMs-Driven Recommender Systems, Genetic Risk Prediction, Recommendation}
}

@inproceedings{10.5555/3712729.3712892,
author = {Dimitrakopoulos, George and Ehm, Hans and Tsaousi, Eleni},
title = {Enhanced Ontology Extraction: Integrating GPT AI with Human Knowledge on the Example of EU Standards Related to Semiconductor Supply Chains},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {This paper addresses challenges in creating ontologies for the semiconductor supply chain. Ontologies are crucial for seamless data exchange within the semantic web, enabling initiatives like GAIA-X and CatenA-X. Traditionally, ontology creation is complex. Here, we propose a novel AI-assisted method using large language models (LLMs) like ChatGPT 4 Turbo to support human experts. This collaboration aims to expedite ontology generation while maintaining quality. While initial tests show promise, refining the human-AI interface for clear content generation remains a focus. By improving this collaboration, we expect to create more accurate and complete ontologies, fostering efficient information sharing and strengthening the meaningfulness of standards within the semiconductor supply chain.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1955–1965},
numpages = {11},
location = {Orlando, Florida, USA},
series = {WSC '24}
}

@inproceedings{10.1145/3708635.3708655,
author = {Yu, Hong Qing and Sutton, Jack and O'Neill, Sam and Reiff-Marganiec, Stephan},
title = {Case Studies on LLM Centric and Services Oriented Data Analytics Agent Development},
year = {2025},
isbn = {9798400717765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708635.3708655},
doi = {10.1145/3708635.3708655},
abstract = {This paper presents a novel service orchestration framework for a chatbot application focused on data analytics questions. The framework integrates Large Language Models (LLMs) with service-oriented computing to transform data analytics into a dynamic, conversational experience. The approach leverages advancements in LLM technology to enable real-time, automated data insights via chatbot interfaces, making complex data analytics accessible across various industries. In addition, the data will be processed and analysis at edge-machine rather than post all the data directly to the LLMs on the cloud. Therefore, the Central to the framework is the local Micro Analytics Service (MAS) and a dynamic service-data coordination framework, which together facilitate the decoupling of data from business logic, allowing for intuitive engagement with analytics processes. Through two case studies, retail data analysis and regional healthcare planning, the ability of the framework to provide actionable insights through natural language prompts is demonstrated, showcasing its potential to significantly reduce barriers to sophisticated data analytics. The evaluation reveals strong performance in data connection and code generation, with identified areas for improvement in visualizations and handling complex data scenarios.},
booktitle = {Proceedings of the 2024 13th International Conference on Software and Information Engineering},
pages = {69–76},
numpages = {8},
keywords = {LLM-driven service orchestration, Dynamic data analytics services, Services Computing},
location = {
},
series = {ICSIE '24}
}

@inproceedings{10.1145/3543873.3587617,
author = {Zaitoun, Antonio and Sagi, Tomer and Hose, Katja},
title = {Automated Ontology Evaluation: Evaluating Coverage and Correctness using a Domain Corpus},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587617},
doi = {10.1145/3543873.3587617},
abstract = {Ontologies conceptualize domains and are a crucial part of web semantics and information systems. However, re-using an existing ontology for a new task requires a detailed evaluation of the candidate ontology as it may cover only a subset of the domain concepts, contain information that is redundant or misleading, and have inaccurate relations and hierarchies between concepts. Manual evaluation of large and complex ontologies is a tedious task. Thus, a few approaches have been proposed for automated evaluation, ranging from concept coverage to ontology generation from a corpus. Existing approaches, however, are limited by their dependence on external structured knowledge sources, such as a thesaurus, as well as by their inability to evaluate semantic relationships. In this paper, we propose a novel framework to automatically evaluate the domain coverage and semantic correctness of existing ontologies based on domain information derived from text. The approach uses a domain-tuned named-entity-recognition model to extract phrasal concepts. The extracted concepts are then used as a representation of the domain against which we evaluate the candidate ontology’s concepts. We further employ a domain-tuned language model to determine the semantic correctness of the candidate ontology’s relations. We demonstrate our automated approach on several large ontologies from the oceanographic domain and show its agreement with a manual evaluation by domain experts and its superiority over the state-of-the-art.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {1127–1137},
numpages = {11},
keywords = {BERT, knowledge engineering, natural language processing, ontology},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{10.1145/3672608.3707960,
author = {Malandri, Lorenzo and Mercorio, Fabio and Serino, Antonio},
title = {SkiLLMo: Normalized ESCO Skill Extraction through Transformer Models},
year = {2025},
isbn = {9798400706295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672608.3707960},
doi = {10.1145/3672608.3707960},
abstract = {In recent years, natural language processing (NLP) technologies have made a significant contribution in addressing a number of labour market tasks. One of the most interesting challenges is the automatic extraction of competences from unstructured texts.This paper presents a pipeline for efficiently extracting and standardizing skills from job advertisements using NLP techniques. The proposed methodology leverages open-source Transformer and Large Language Models to extract skills and map them to the European labour market taxonomy, ESCO.To address the computational challenges of processing lengthy job advertisements, a BERT model was fine-tuned to identify text segments likely containing skills. This filtering step reduces noise and ensures that only relevant content is processed further. The filtered text is then passed to an LLM, which extracts implicit and explicit hard and soft skills through prompt engineering. The extracted skills are subsequently matched with entries in a vector store containing the ESCO taxonomy to achieve standardization.Evaluation by domain experts shows that the pipeline achieves a precision of 91\% for skill extraction, 80\% for skill standardization and a combined overall precision of 79\%. These results demonstrate the effectiveness of the proposed approach in facilitating structured and standardized skill extraction from job postings.},
booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing},
pages = {1969–1978},
numpages = {10},
keywords = {skill extraction, large language models, transformer models, information extraction, labor market},
location = {Catania International Airport, Catania, Italy},
series = {SAC '25}
}

@article{10.1613/jair.1.19388,
author = {Russo, Mayra and Vidal, Maria-Esther},
title = {Towards an Ontology-Driven Approach to Document Bias},
year = {2025},
issue_date = {Aug 2025},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {83},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.19388},
doi = {10.1613/jair.1.19388},
abstract = {Machine learning (ML)-powered systems are capable of reproducing and often amplifying undesired biases embedded in society, emphasizing the importance of operating under practices that enable the study and understanding of the intrinsic characteristics of ML pipelines. This supports the emergence of documentation frameworks with the idea that “any remedy for bias starts with awareness of its existence.” However, a resource that can formally describe ML pipelines in terms of detected biases is still missing. To address this gap, we present the Doc-BiasO ontology, a resource that sets out to create an integrated vocabulary of biases defined in the Trustworthy AI literature and their measures, as well as to incorporate relevant domain terminology and relationships between them. Overseeing ontology engineering best practices, we reuse existing vocabularies on machine learning and AI to foster knowledge sharing and interoperability between the actors concerned with its research, development, regulation, and others. In addition, we demonstrate the potential of Doc-BiasO with an experiment on an existing benchmark and as part of a neuro-symbolic system. Overall, our main objective is to contribute towards clarifying existing terminology on bias research as it rapidly expands to all areas of AI and to improve the interpretation of bias in data and downstream impact through its documentation.},
journal = {J. Artif. Int. Res.},
month = aug,
numpages = {35},
keywords = {Explainable AI, AI in Healthcare, Trustworthy AI}
}

@inproceedings{10.1145/3719384.3719446,
author = {Lima Carneiro Alves De, Bruno Rucy and Kramer, Merlin and Pinheiro, Victor Henrique Cabral},
title = {Distributed Incremental Ontology Reasoning over Dynamic T-boxes},
year = {2025},
isbn = {9798400717925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719384.3719446},
doi = {10.1145/3719384.3719446},
abstract = {With the advent of Retrieval Augmented Generation (RAG), Knowledge Graphs (KGs) have yet again had a surge in interest in both Academia and Industry, as their use allows for extending the context of Large Language Models (LLMs) by combining traditional vector search with reasoning over Ontologies or Property Graphs encoded as KGs. RAG is a highly dynamic scenario, where the LLM agent might not only retrieve information from a KG or vector store but mutate it as well. This implies eventually there being a greater demand for equally-dynamic KG reasoning systems. We provide a solution to this for the popular ontology language RDF-Schema (RDFS) by showing that computing entailment as a bottom-up query over RDFS graphs with dynamic Terminological Boxes (TBox) and Assertional Boxes (ABox), those where edges and nodes belonging to both boxes can be freely added and removed, can be expressed as an incremental DBSP computation. This computation is then implemented with the distributed computation framework Differential Dataflow (DD), that subsumes DBSP, and compared with a state-of-the-art commercial ontology reasoner. We find that our approach provides more even performance across additions and deletions and a higher potential for scalability across benchmarks with up to 250 GBs of data.},
booktitle = {Proceedings of the 2024 7th Artificial Intelligence and Cloud Computing Conference},
pages = {423–428},
numpages = {6},
keywords = {ontology reasoning, rdfs, stream processing},
location = {
},
series = {AICCC '24}
}

@inproceedings{10.1109/ASE56229.2023.00075,
author = {Huang, Qing and Wan, Zhenyu and Xing, Zhenchang and Wang, Changjing and Chen, Jieshan and Xu, Xiwei and Lu, Qinghua},
title = {Let's Chat to Find the APIs: Connecting Human, LLM and Knowledge Graph through AI Chain},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00075},
doi = {10.1109/ASE56229.2023.00075},
abstract = {API recommendation methods have evolved from literal and semantic keyword matching to query expansion and query clarification. The latest query clarification method is knowledge graph (KG)-based, but limitations include out-of-vocabulary (OOV) failures and rigid question templates. To address these limitations, we propose a novel knowledge-guided query clarification approach for API recommendation that leverages a large language model (LLM) guided by KG. We utilize the LLM as a neural knowledge base to overcome OOV failures, generating fluent and appropriate clarification questions and options. We also leverage the structured API knowledge and entity relationships stored in the KG to filter out noise, and transfer the optimal clarification path from KG to the LLM, increasing the efficiency of the clarification process. Our approach is designed as an AI chain that consists of five steps, each handled by a separate LLM call, to improve accuracy, efficiency, and fluency for query clarification in API recommendation. We verify the usefulness of each unit in our AI chain, which all received high scores close to a perfect 5. When compared to the baselines, our approach shows a significant improvement in MRR, with a maximum increase of 63.9\% higher when the query statement is covered in KG and 37.2\% when it is not. Ablation experiments reveal that the guidance of knowledge in the KG and the knowledge-guided pathfinding strategy are crucial for our approach's performance, resulting in a 19.0\% and 22.2\% increase in MAP, respectively. Our approach demonstrates a way to bridge the gap between KG and LLM, effectively compensating for the strengths and weaknesses of both.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {471–483},
numpages = {13},
keywords = {API recommendation, query clarification, knowledge graph, large language model, out-of-vocabulary},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3733155.3736601,
author = {Bhattacharya, Sukriti and Naudet, Yannick},
title = {Integrating Ontology with Deep Reinforcement Learning: A Formal Framework for Explainability in Robotic Applications},
year = {2025},
isbn = {9798400714023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3733155.3736601},
doi = {10.1145/3733155.3736601},
abstract = {This paper presents a formal framework that integrates an ontology with Deep Reinforcement Learning (DRL) to enhance explainability in AI-driven robotic applications. By mapping DRL components to ontological concepts, our approach generates human-readable explanations for complex model decisions. We demonstrate the framework through a robotic arm pick-and-place task, illustrating improved transparency and interpretability. The integration facilitates domain knowledge incorporation while addressing the critical need for explainable AI systems. Despite challenges in computational overhead and ontological alignment, our contribution advances trustworthy AI systems that support effective human-AI collaboration.},
booktitle = {Proceedings of the 18th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {60–63},
numpages = {4},
keywords = {Ontology, Deep Reinforcement Learning, Explainability, Transparency, Interpretability, Human-AI Collaboration},
location = {
},
series = {PETRA '25}
}

@article{10.1145/3719206,
author = {Cremaschi, Marco and D'Adda, Fabio and Maurino, Andrea},
title = {stEELlm: An LLM for Generating Semantic Annotations of Tabular Data},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3719206},
doi = {10.1145/3719206},
abstract = {The capabilities of LLMs represent a pivotal step in transforming how we manage and interact with information and data. We witness an increasingly pervasive use of such models in various computational tasks. In some preliminary works, attempts to integrate Knowledge Graphs and Large Language Models (LLMs) can be identified, in particular, to perform the classic tasks related to the construction of Knowledge Graphs through semantic annotation of texts. Nowadays, tables are widely used and play a crucial role in creating, organising, and sharing information that could be used to produce factual knowledge to be integrated into a Knowledge Graph. However, table-to-KG techniques through LLM have not been extensively investigated. This paper presents stEELlm, an innovative Semantic Table Interpretation approach obtained by fine-tuning the Mixtral 8x7B model. Conducted experiments demonstrate the capabilities of our model to successfully create semantic annotations of heterogeneous datasets, a scenario where classic approaches based on heuristics tend to fail.},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
keywords = {Large Language Models, Knowledge Graphs, Pre-training, Fine-tuning, Prompt Engineering, Semantic Table Interpretation}
}

@article{10.1145/3582496,
author = {Shafi, Jawad and Adeel Nawab, Rao Muhammad and Rayson, Paul},
title = {Semantic Tagging for the Urdu Language: Annotated Corpus and Multi-Target Classification Methods},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3582496},
doi = {10.1145/3582496},
abstract = {Extracting and analysing meaning-related information from natural language data has attracted the attention of researchers in various fields, such as natural language processing, corpus linguistics, information retrieval, and data science. An important aspect of such automatic information extraction and analysis is the annotation of language data using semantic tagging tools. Different semantic tagging tools have been designed to carry out various levels of semantic analysis, for instance, named entity recognition and disambiguation, sentiment analysis, word sense disambiguation, content analysis, and semantic role labelling. Common to all of these tasks, in the supervised setting, is the requirement for a manually semantically annotated corpus, which acts as a knowledge base from which to train and test potential word and phrase-level sense annotations. Many benchmark corpora have been developed for various semantic tagging tasks, but most are for English and other European languages. There is a dearth of semantically annotated corpora for the Urdu language, which is widely spoken and used around the world. To fill this gap, this study presents a large benchmark corpus and methods for the semantic tagging task for the Urdu language. The proposed corpus contains 8,000 tokens in the following domains or genres: news, social media, Wikipedia, and historical text (each domain having 2K tokens). The corpus has been manually annotated with 21 major semantic fields and 232 sub-fields with the USAS (UCREL Semantic Analysis System) semantic taxonomy which provides a comprehensive set of semantic fields for coarse-grained annotation. Each word in our proposed corpus has been annotated with at least one and up to nine semantic field tags to provide a detailed semantic analysis of the language data, which allowed us to treat the problem of semantic tagging as a supervised multi-target classification task. To demonstrate how our proposed corpus can be used for the development and evaluation of Urdu semantic tagging methods, we extracted local, topical and semantic features from the proposed corpus and applied seven different supervised multi-target classifiers to them. Results show an accuracy of 94\% on our proposed corpus which is free and publicly available to download.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {175},
numpages = {32},
keywords = {Urdu corpus annotation, multi-target classifiers, semantic annotation, semantic tagger}
}

@proceedings{10.1145/3639233,
title = {NLPIR '23: Proceedings of the 2023 7th International Conference on Natural Language Processing and Information Retrieval},
year = {2023},
isbn = {9798400709227},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seoul, Republic of Korea}
}

@inproceedings{10.1145/3704440.3704788,
author = {Lehmann, Ren\'{e}},
title = {Towards Interoperability of APIs - an LLM-based approach},
year = {2024},
isbn = {9798400713545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704440.3704788},
doi = {10.1145/3704440.3704788},
abstract = {Applications that integrate multiple Application Programming Interfaces (APIs) often face challenges due to data and application heterogeneity, complicating the integration process. Our proposed approach leverages Large Language Models (LLMs) to translate API function calls and responses into natural language, enabling communication between API consumers and providers without requiring them to adhere to the same communication protocols or data formats. By abstracting technical complexities, this approach addresses both syntactic and semantic differences. We outline the potential of LLMs to improve API interoperability and discuss strategies for optimizing performance, reducing overhead, and ensuring system reliability through error detection and correction.},
booktitle = {Proceedings of the 25th International Middleware Conference: Demos, Posters and Doctoral Symposium},
pages = {29–30},
numpages = {2},
keywords = {Application Heterogeneity, Application Programming Interface, Data Heterogeneity, Interoperability, Large Language Model},
location = {Hong Kong, Hong Kong},
series = {Middleware '24}
}

@article{10.1145/3729218,
author = {Sun, Jiankai and Zheng, Chuanyang and Xie, Enze and Liu, Zhengying and Chu, Ruihang and Qiu, Jianing and Xu, Jiaqi and Ding, Mingyu and Li, Hongyang and Geng, Mengzhe and Wu, Yue and Wang, Wenhai and Chen, Junsong and Yin, Zhangyue and Ren, Xiaozhe and Fu, Jie and He, Junxian and Wu, Yuan and Liu, Qi and Liu, Xihui and Li, Yu and Dong, Hao and Cheng, Yu and Zhang, Ming and Heng, Pheng Ann and Dai, Jifeng and Luo, Ping and Wang, Jingdong and Wen, Ji-Rong and Qiu, Xipeng and Guo, Yike and Xiong, Hui and Liu, Qun and Li, Zhenguo},
title = {A Survey of Reasoning with Foundation Models: Concepts, Methodologies, and Outlook},
year = {2025},
issue_date = {November 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {11},
issn = {0360-0300},
url = {https://doi.org/10.1145/3729218},
doi = {10.1145/3729218},
abstract = {Reasoning, a crucial ability for complex problem-solving, plays a pivotal role in various real-world settings such as negotiation, medical diagnosis, and criminal investigation. It serves as a fundamental methodology in the field of Artificial General Intelligence (AGI). With the ongoing development of foundation models, there is a growing interest in exploring their abilities in reasoning tasks. In this article, we introduce seminal foundation models proposed or adaptable for reasoning, highlighting the latest advancements in various reasoning tasks, methods, and benchmarks. We then delve into the potential future directions behind the emergence of reasoning abilities within foundation models. We also discuss the relevance of multimodal learning, autonomous agents, and super alignment in the context of reasoning. By discussing these future research directions, we hope to inspire researchers in their exploration of this field, stimulate further advancements in reasoning with foundation models, e.g., Large Language Models (LLMs), and contribute to the development of AGI.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {278},
numpages = {43},
keywords = {Reasoning, foundation models, multimodal, AI agent, artificial general intelligence}
}

@inproceedings{10.1145/3318236.3318246,
author = {Cristea, Daniela-Maria and Trofin, Bogdan-Gabriel},
title = {A Historical Ontology of Semi-automatic Specification Extraction from Romanian Language},
year = {2019},
isbn = {9781450362450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318236.3318246},
doi = {10.1145/3318236.3318246},
abstract = {The increasing need for ontologies with practical results and the difficulties of manual construction guide us towards methods that propose automatic and semi-automated techniques for creating ontologies. The article presents a conceptual method for building ontology for historical documents aiming to support historians and researchers to organize metadata, search for relevant factors, extract information and derive new knowledge for localized metadata from documents.We envision its use for educational purposes and for the benefit of the community through several aspects, such as scientific utility, accessibility to documents from the late medieval period to explore events or personal histories of 14-16 centuries.As a case study, we implemented a semi-automatic method for extracting instances from a category of Romanian documents that generate a historical ontology. The results are evaluated on the basis of the need for an automated tool comparable to those found in the literature for other languages.},
booktitle = {Proceedings of the 2019 2nd International Conference on Geoinformatics and Data Analysis},
pages = {125–129},
numpages = {5},
keywords = {Historical document, historical domain, ontology population, semi-automatic ontology extraction},
location = {Prague, Czech Republic},
series = {ICGDA '19}
}

@article{10.1145/3569927,
author = {Broy, Manfred and Rumpe, Bernhard},
title = {Development Use Cases for Semantics-Driven Modeling Languages},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/3569927},
doi = {10.1145/3569927},
abstract = {Choosing underlying semantic theories and definition techniques must closely follow intended use cases for the modeling language.},
journal = {Commun. ACM},
month = apr,
pages = {62–71},
numpages = {10}
}

@article{10.1145/3676956,
author = {Pei, Jiahuan and Yan, Guojun and De Rijke, Maarten and Ren, Pengjie},
title = {Mixture-of-Languages Routing for Multilingual Dialogues},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {6},
issn = {1046-8188},
url = {https://doi.org/10.1145/3676956},
doi = {10.1145/3676956},
abstract = {We consider multilingual dialogue systems and ask how the performance of a dialogue system can be improved by using information that is available in other languages than the language in which a conversation is being conducted. We adopt a collaborative chair-experts framework, where each expert agent can be either monolingual or cross-lingual, and a chair agent follows a mixture-of-experts procedure for globally optimizing multilingual task-oriented dialogue systems. We propose a mixture-of-languages routing framework that includes four functional components, i.e., input embeddings of multilingual dialogues, language model, pairwise alignment between the representation of every two languages, and mixture-of-languages. We quantify language characteristics of unity and diversity using a number of similarity metrics, i.e., genetic similarity and word and sentence similarity based on embeddings. Our main finding is that the performance of multilingual task-oriented dialogue systems can be greatly impacted by three key aspects, i.e., data sufficiency, language characteristics, and model design in a mixture-of-languages routing framework.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {165},
numpages = {33},
keywords = {multilingual systems, task-oriented dialogue systems, collaborative agents, mixture-of-experts}
}

@inproceedings{10.1145/3550356.3561534,
author = {Cornelis, Milan and Vanommeslaeghe, Yon and Van Acker, Bert and De Meulenaere, Paul},
title = {An ontology DSL for the co-design of mechatronic systems},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3561534},
doi = {10.1145/3550356.3561534},
abstract = {The complexity of mechatronic systems is vastly increasing. Therefore, the design of these systems requires different engineering domains, e.g., the mechanical, electrical, and control domains, to work together. The different domains often work in parallel to gain efficiency in this so-called co-design process. However, the design choices made by engineers in one domain can influence parameters in another domain. Too little or even no knowledge about these cross-domain influences may later lead to system integration problems or to degraded system performance. Solving these problems requires taking steps back in the development process, causing a higher design cost. In order to improve this cross-domain collaboration, we propose using ontologies to assist the co-design process by explicitly capturing the design dependencies, both within and across the engineering domains. However, designing ontologies can be complex and is labor-intensive, especially if one relies on generic ontology languages like the Web Ontology Language 2 (OWL 2). Therefore, we created a Domain Specific Language (DSL) focusing on the essential complexity, which enables engineers to design a cross-domain system ontology in a consistent and straightforward way. We elaborate on the metamodel for this DSL, discuss the realization of a prototype tool, and demonstrate how one can then reason on this ontology to derive new information about the various cross-domain design relationships.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {633–642},
numpages = {10},
keywords = {co-design, domain-specific language, mechatronics, metamodel, ontology},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3742876.3742882,
author = {Esterhuyse, Christopher A. and M\"{u}ller, Tim and van Binsbergen, L. Thomas},
title = {A Stable Model Semantics for eFLINT Norm Specifications and Model Checking Scenarios},
year = {2025},
isbn = {9798400719950},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3742876.3742882},
doi = {10.1145/3742876.3742882},
abstract = {Since its introduction at GPCE2020, the eFLINT norm specification language has been used in academic and industrial applications to specify and automate compliance for various norms, such as privacy regulations and data processing agreements.   The eFLINT interpreter has been used to automate the analysis of real-time or historical cases by computing logical consequences and reporting normative violations.    To support future language and tooling developments, we contribute a formal definition of the language as a translation to first-order logic programming with stable model semantics.   The described semantics aligns with the previous semi-formal descriptions of the language, but resolves issues relating to logical inference with negative antecedent and aggregation operators.   Specifically, we formalise the connection between eFLINT's derivation rules and Horn clauses under the stable model semantics. Secondly, by repurposing the Clingo answer-set solver as a highly-optimised eFLINT interpreter, we extend the toolset for eFLINT with model-checking abstract properties in addition to case analysis.    We evaluate the new semantics and interpreter via an empirical comparison of the existing implementation to our prototype implementation.   We observe that the expected subset of our tests have the equivalent behaviours.},
booktitle = {Proceedings of the 24th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {80–93},
numpages = {14},
keywords = {answer set solving, dynamic semantics, logic programming, model checking, norm, specification languages},
location = {Bergen, Norway},
series = {GPCE '25}
}

@inproceedings{10.5555/3511065.3511090,
author = {Finidori, Helene},
title = {From pattern language to pattern literacy: the biosemiotic underpinnings of "patterning" and "languaging"},
year = {2022},
isbn = {9781941652169},
publisher = {The Hillside Group},
address = {USA},
abstract = {The key inquiry in this paper, to move the discussion forward on innovation and the future of Pattern Language, is about the relationship between patterns (and more precisely our capacities as humans to recognize and use patterns, aka 'patterning') and language (both in its form and in the processes of 'languaging' involved), in order to assess how each can be leveraged in understanding and communication, within and across domains. I dive here deep into the biological and bio-semiotic underpinnings of patterning and languaging, seeking to make a clear distinction between them. I explore the nature and "timeless properties" of patterns as signs and their role in the emergence of human cognition and language from an evolutionary perspective. In particular I examine their involvement in 'habit taking' and in the coordination of unselfconscious action and creative processes, such as evoked by Christopher Alexander.This paper does not provide solutions or answers, it sets a foundation to show how the development of a pattern literacy around patterns seen as basic units for the coordination of action and the understanding of the world, beyond domain knowledge and linguistic divides, could bring new possibilities for the study and orientation of socio-ecological and socio-technological systems. This will open up opportunities to further explore how pattern languages could be understood and applied towards this objective, in order to actually realize their potential as lingua franca.},
booktitle = {Proceedings of the 27th Conference on Pattern Languages of Programs},
articleno = {18},
numpages = {34},
keywords = {action research, boundary objects, complex adaptive modeling, complex systems, participatory inquiry, pattern language, pattern literacy, semiotics},
location = {Virtual Event},
series = {PLoP '20}
}

@inproceedings{10.1145/3516492.3558811,
author = {Ong, Ethel},
title = {How HCI can Inform the Design of Human Language Technologies},
year = {2023},
isbn = {9781450392501},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3516492.3558811},
doi = {10.1145/3516492.3558811},
abstract = {In this paper, we present how practices in human-computer interaction can be used to inform the design of human language technologies. We begin with a description of human language technologies, paying particular attention to the collaborative nature of dialogue systems that necessitates a strong interaction between man and machine. We then share our insights from our application of principles and practices of human computer interaction in the design and evaluation of these conversational interfaces.},
booktitle = {Proceedings of the Asian HCI Symposium 2022},
pages = {44–47},
numpages = {4},
keywords = {Conversational interfaces, Dialogue systems, Human-language technologies},
location = {New Orleans, LA, USA},
series = {Asian HCI '22}
}

@article{10.1145/3592599,
author = {Jin, Dawei and Hu, Yiyi and Chen, Jingyu and Xia, Mengran},
title = {Stock Price Trends Prediction Based on the Classical Models with Key Information Fusion of Ontologies},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3592599},
doi = {10.1145/3592599},
abstract = {An ontology of the financial field can support effective association and integration of financial knowledge. Based on behavioral finance, social media is increasingly applied as one of the data sources for information fusion in stock forecasting to approximate the patterns of market changes. By predicting Tesla (TSLA) stock price trends, this study finds that satisfactory forecasting results can be achieved using classical models and incorporating key information features from the technical indicator ontology class and the investor behavior ontology class, even in the face of the impact of the COVID-19 epidemic. In the post-epidemic period, the back propagation neural network (BPNN) model is used to predict the price trend of TSLA for the next five trading days with an accuracy of up to 91.34\%, an F1 score of 0.91, and a return of up to 268.42\% obtained from simulated trading. This study extends the research on stock forecasting using fused information in the ontology of the financial field, providing a new basis for general investors in the selection of fusion information and the application of trading strategies and providing effective support for organizations to make intelligent financial decisions under uncertainty.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {154},
numpages = {22},
keywords = {Financial ontology, feature fusion, trends prediction, social media, sentiment analysis}
}

@article{10.14778/3742728.3742761,
author = {Chung, Yeounoh and Kakkar, Gaurav T. and Gan, Yu and Milne, Brenton and \"{O}zcan, Fatma},
title = {Is Long Context All You Need? Leveraging LLM's Extended Context for NL2SQL},
year = {2025},
issue_date = {April 2025},
publisher = {VLDB Endowment},
volume = {18},
number = {8},
issn = {2150-8097},
url = {https://doi.org/10.14778/3742728.3742761},
doi = {10.14778/3742728.3742761},
abstract = {Large Language Models (LLMs) have demonstrated impressive capabilities across a range of natural language processing tasks. In particular, improvements in reasoning abilities and the expansion of context windows have opened new avenues for leveraging these powerful models. NL2SQL is challenging in that the natural language question is inherently ambiguous, while the SQL generation requires a precise understanding of complex data schema and semantics. One approach to this semantic ambiguous problem is to provide more and sufficient contextual information.In this work, we explore the performance and the latency tradeoffs of the extended context window (a.k.a., long context) offered by Google's state-of-the-art LLM (gemini-1.5-pro). We study the impact of various contextual information, including column example values, question and SQL query pairs, user-provided hints, SQL documentation, and schema. To the best of our knowledge, this is the first work to study how the extended context window and extra contextual information can help NL2SQL generation with respect to both accuracy and latency cost. We show that long context LLMs are robust and do not get lost in the extended contextual information. Additionally, our long-context NL2SQL pipeline based on Google's Gemini-pro-1.5 achieves strong performance across multiple benchmark datasets without fine-tuning or expensive self-consistency based techniques.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {2735–2747},
numpages = {13}
}

@article{10.1145/3528576,
author = {Bashir, Muhammad Farrukh and Javed, Abdul Rehman and Arshad, Muhammad Umair and Gadekallu, Thippa Reddy and Shahzad, Waseem and Beg, Mirza Omer},
title = {Context-aware Emotion Detection from Low-resource Urdu Language Using Deep Neural Network},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3528576},
doi = {10.1145/3528576},
abstract = {Emotion detection (ED) plays a vital role in determining individual interest in any field. Humans use gestures, facial expressions, and voice pitch and choose words to describe their emotions. Significant work has been done to detect emotions from the textual data in English, French, Chinese, and other high-resource languages. However, emotion classification has not been well studied in low-resource languages (i.e., Urdu) due to the lack of labeled corpora. This article presents a publicly available Urdu Nastalique Emotions Dataset (UNED) of sentences and paragraphs annotated with different emotions and proposes a deep learning (DL)-based technique for classifying emotions in the UNED corpus. Our annotated UNED corpus has six emotions for both paragraphs and sentences. We perform extensive experimentation to evaluate the quality of the corpus and further classify it using machine learning and DL approaches. Experimental results show that the developed DL-based model performs better than generic machine learning approaches with an F1 score of 85\% on the UNED sentence-based corpus and 50\% on the UNED paragraph-based corpus.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {131},
numpages = {30},
keywords = {Emotion detection (ED), Urdu Nastalique Emotions Dataset (UNED), annotated UNED corpus}
}

@inproceedings{10.1145/3627673.3680009,
author = {Agrawal, Sanjay and Merugu, Srujana and Sembium, Vivek},
title = {Boosting Entity Recognition by leveraging Cross-task Domain Models for Weak Supervision},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3680009},
doi = {10.1145/3627673.3680009},
abstract = {Entity Recognition (ER) is a common natural language processing task encountered in a number of real-world applications. For common domains and named entities such as places and organisations, there exists sufficient high quality annotated data and foundational models such as T5 and GPT-3.5 also provide highly accurate predictions. However, for niche domains such as e-commerce and medicine with specialized entity types, there is a paucity of labeled data since manual labeling of tokens is often time-consuming and expensive, which makes entity recognition challenging for such domains. Recent works such as NEEDLE [48] propose hybrid solutions to efficiently combine a small amount of strongly labeled (human-annotated) with a large amount of weakly labeled (distant supervision) data to yield superior performance relative to supervised training. The extensive noise in the weakly labeled data, however, remains a challenge. In this paper, we propose WeSDoM (Weak Supervision with Domain Models), which leverages pretrained encoder models from the same domain but different tasks to create domain ontologies that can enable the creation of less noisy weakly labeled data. Experiments on internal e-commerce and public biomedical NER datasets demonstrate that WeSDoM outperforms existing SOTA baselines by a significant margin. We achieve new SOTA F1 scores on two popular Biomedical NER datasets, BC5CDR-chem 94.27, BC5CDR-disease 91.23.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {4324–4331},
numpages = {8},
keywords = {cross-task domain encoder, entity recognition, ontologies, weak supervision},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3663529.3663861,
author = {Goel, Drishti and Husain, Fiza and Singh, Aditya and Ghosh, Supriyo and Parayil, Anjaly and Bansal, Chetan and Zhang, Xuchao and Rajmohan, Saravan},
title = {X-Lifecycle Learning for Cloud Incident Management using LLMs},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663861},
doi = {10.1145/3663529.3663861},
abstract = {Incident management for large cloud services is a complex and tedious process that requires a significant amount of manual effort from on-call engineers (OCEs). OCEs typically leverage data from different stages of the software development lifecycle [SDLC] (e.g., codes, configuration, monitor data, service properties, service dependencies, trouble-shooting documents, etc.) to generate insights for detection, root cause analysis and mitigation of incidents. Recent advancements in large language models [LLMs] (e.g., ChatGPT, GPT-4, Gemini) have created opportunities to automatically generate contextual recommendations for the OCEs, assisting them in quickly identifying and mitigating critical issues. However, existing research typically takes a silo-ed view of solving a certain task in incident management by leveraging data from a single stage of the SDLC. In this paper, we demonstrate that augmenting additional contextual data from different stages of the SDLC improves the performance of two critically important and practically challenging tasks: (1) automatically generating root cause recommendations for dependency failure related incidents, and (2) identifying the ontology of service monitors used for automatically detecting incidents. By leveraging a dataset of 353 incidents and 260 monitors from Microsoft, we demonstrate that augmenting contextual information from different stages of the SDLC improves the performance over state-of-the-art methods.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {417–428},
numpages = {12},
keywords = {Cloud Services, Large language models, Monitor management, Reliability, Root-cause analysis},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@article{10.1613/jair.1.14061,
author = {Kurucz, Agi and Ryzhikov, Vladislav and Savateev, Yury and Zakharyaschev, Michael},
title = {Deciding FO-rewritability of Regular Languages and Ontology-Mediated Queries in Linear Temporal Logic},
year = {2023},
issue_date = {May 2023},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {76},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.14061},
doi = {10.1613/jair.1.14061},
abstract = {Our concern is the problem of determining the data complexity of answering an ontology-mediated query (OMQ) formulated in linear temporal logic LTL over (Z,&lt;) and deciding whether it is rewritable to an FO(&lt;)-query, possibly with some extra predicates. First, we observe that, in line with the circuit complexity and FO-definability of regular languages, OMQ answering in AC0, ACC0&nbsp;and NC1&nbsp;coincides with FO(&lt;,≡)-rewritability using unary predicates&nbsp;x&nbsp;≡ 0 (mod&nbsp;n), FO(&lt;,MOD)-rewritability, and FO(RPR)-rewritability using relational primitive recursion, respectively. We prove that, similarly to known PSᴘᴀᴄᴇ-completeness of recognising FO(&lt;)-definability of regular languages, deciding FO(&lt;,≡)- and FO(&lt;,MOD)-definability is also PSᴘᴀᴄᴇ-complete (unless ACC0&nbsp;= NC1). We then use this result to show that deciding FO(&lt;)-, FO(&lt;,≡)- and FO(&lt;,MOD)-rewritability of LTL OMQs is ExᴘSᴘᴀᴄᴇ-complete, and that these problems become PSᴘᴀᴄᴇ-complete for OMQs with a linear Horn ontology and an atomic query, and also a positive query in the cases of FO(&lt;)- and FO(&lt;,≡)-rewritability. Further, we consider FO(&lt;)-rewritability of OMQs with a binary-clause ontology and identify OMQ classes, for which deciding it is PSᴘᴀᴄᴇ-, Π2p- and coNP-complete.},
journal = {J. Artif. Int. Res.},
month = may,
numpages = {59}
}

@inproceedings{10.1145/3600046.3600050,
author = {Zhang, Wei and Chen, Perry and Yang, Jian and Tang, Yongping and Su, Jianwen},
title = {A Capability Description Language Design for Data Products},
year = {2023},
isbn = {9798400708466},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600046.3600050},
doi = {10.1145/3600046.3600050},
abstract = {Data has become a crucial factor driving the global economy, from various perspectives, including economics, technological advancements, digitization, and data analytics technologies. Data products are specific, ready-made data sets that have been enriched with metadata and designed to achieve particular outcomes. However, there is currently limited research on how to provide efficient and automated data product generation and sharing services. This paper proposes an approach to extract descriptive data capabilities to automate these processes. We examine real-world datasets from the Internet of Things (IoT) and user scenarios to extract selection capabilities and retrieval capabilities from the working data. A data access model is presented based on these capabilities. These preliminary results form the foundation for automating data product generation.},
booktitle = {Proceedings of the Second ACM Data Economy Workshop},
pages = {21–26},
numpages = {6},
keywords = {Data Capability, Data Economy, Data Product, Data Service},
location = {Seattle, WA, USA},
series = {DEC '23}
}

@inproceedings{10.1145/3613904.3642698,
author = {Liu, Yiren and Chen, Si and Cheng, Haocong and Yu, Mengxia and Ran, Xiao and Mo, Andrew and Tang, Yiliu and Huang, Yun},
title = {How AI Processing Delays Foster Creativity: Exploring Research Question Co-Creation with an LLM-based Agent},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642698},
doi = {10.1145/3613904.3642698},
abstract = {Developing novel research questions (RQs) often requires extensive literature reviews, especially in interdisciplinary fields. To support RQ development through human-AI co-creation, we leveraged Large Language Models (LLMs) to build an LLM-based agent system named CoQuest. We conducted an experiment with 20 HCI researchers to examine the impact of two interaction designs: breadth-first and depth-first RQ generation. The findings revealed that participants perceived the breadth-first approach as more creative and trustworthy upon task completion. Conversely, during the task, participants considered the depth-first generated RQs as more creative. Additionally, we discovered that AI processing delays allowed users to reflect on multiple RQs simultaneously, leading to a higher quantity of generated RQs and an enhanced sense of control. Our work makes both theoretical and practical contributions by proposing and evaluating a mental model for human-AI co-creation of RQs. We also address potential ethical issues, such as biases and over-reliance on AI, advocating for using the system to improve human research creativity rather than automating scientific inquiry. The system’s source is available at: https://github.com/yiren-liu/coquest.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {17},
numpages = {25},
keywords = {Co-creation Systems, Large Language Models, Mixed-initiative Design, Scientifc Discovery},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.1145/3708478,
author = {Meng, Zhixin and Zhan, Shaoxiong and Xu, Ruiqing and Mayer, Wolfgang and Zhu, Ye and Zhang, Hong-Yu and He, Chuan and He, Keqing and Cheng, Debo and Feng, Zaiwen},
title = {Domain Ontology-Driven Knowledge Graph Generation from Text},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708478},
doi = {10.1145/3708478},
abstract = {A knowledge graph serves as a unified and standardized representation for extracting and representing textual information. In the field of knowledge extraction and representation research, named entity recognition and relation extraction provide effective solutions for knowledge graph generation tasks. However, it is a challenge that lies in extracting domain-specific knowledge from the rich and general textual corpora and generating corresponding domain knowledge graphs to support domain-specific reasoning, question-answering, and decision-making tasks. The hierarchical domain knowledge representation model (i.e. domain ontology) provides a solution for this problem. Therefore, we propose an end-to-end approach based on domain ontology embedding and pre-trained language models for domain knowledge graph generation from text, which incorporates domain node recognition and domain relation extraction phases. We evaluated our domain ontology-driven model on the Wikidata-TekGen dataset and the DBpedia-WebNLG dataset, and the results indicate that our approach based on the pre-trained language models with fewer parameters compared with the baseline models has significantly contributed to the domain knowledge graph generation without prompts.},
note = {Just Accepted},
journal = {ACM Trans. Probab. Mach. Learn.},
month = dec,
keywords = {Domain Knowledge Graph, Domain Ontology, Ontology Embedding, Domain Node Recognition, Domain Relation Extraction}
}

@article{10.1145/3468889,
author = {Zhang, Ruqing and Guo, Jiafeng and Chen, Lu and Fan, Yixing and Cheng, Xueqi},
title = {A Review on Question Generation from Natural Language Text},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3468889},
doi = {10.1145/3468889},
abstract = {Question generation is an important yet challenging problem in Artificial Intelligence (AI), which aims to generate natural and relevant questions from various input formats, e.g., natural language text, structure database, knowledge base, and image. In this article, we focus on question generation from natural language text, which has received tremendous interest in recent years due to the widespread applications such as data augmentation for question answering systems. During the past decades, many different question generation models have been proposed, from traditional rule-based methods to advanced neural network-based methods. Since there have been a large variety of research works proposed, we believe it is the right time to summarize the current status, learn from existing methodologies, and gain some insights for future development. In contrast to existing reviews, in this survey, we try to provide a more comprehensive taxonomy of question generation tasks from three different perspectives, i.e., the types of the input context text, the target answer, and the generated question. We take a deep look into existing models from different dimensions to analyze their underlying ideas, major design principles, and training strategies We compare these models through benchmark tasks to obtain an empirical understanding of the existing techniques. Moreover, we discuss what is missing in the current literature and what are the promising and desired future directions.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {14},
numpages = {43},
keywords = {Question generation, natural language generation, survey}
}

@inproceedings{10.5555/3400397.3400443,
author = {Padilla, Jose J and Shuttleworth, David and O'Brien, Kevin},
title = {Agent-based model characterization using natural language processing},
year = {2020},
isbn = {9781728132839},
publisher = {IEEE Press},
abstract = {This paper reports on Natural Language Processing (NLP) as a technique to analyze phenomena towards specifying agent-based models (ABM). The objective of the ABM NLP Analyzer is to facilitate non-simulationists to actively engage in the learning and collaborative designing of ABMs. The NLP model identifies candidate agents, candidate agent attributes, and candidate rules all of which non-simulationists can later evaluate for feasibility. IBM's Watson Natural Language Understanding (NLU) and Knowledge Studio were used in order to annotate, evaluate, extract agents, agent attributes, and agent rules from unstructured descriptions of phenomena. The software, and related agent-attribute-rule characterization, provides insight into a simple but useful means of conceptualizing and specifying baseline ABMs. Further, it emphasizes on how to approach the design of ABMs without the use of NLP by focusing on the identification of agent, attributes and rules.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {560–571},
numpages = {12},
location = {National Harbor, Maryland},
series = {WSC '19}
}

@article{10.1145/3705322,
author = {Zhang, Jianrong and Fan, Hehe and Yang, Yi},
title = {Protein Captioning: Bridging the Gap between Protein Sequences and Natural Languages},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1551-6857},
url = {https://doi.org/10.1145/3705322},
doi = {10.1145/3705322},
abstract = {We introduce the multimodal task of Protein Captioning, which is an easy-to-understand and flexible way for protein analysis. Compared to specific protein recognition or classification tasks, such as enzyme reaction classification and gene ontology term prediction, protein captioning provides comprehensive textural descriptions for proteins, thus playing a key role in bridging the gap between protein sequences and natural languages. To address the problem, we propose a simple yet effective method, Protein-to-Text Generative Pre-trained Transformer (P2T-GPT), to fuse multimodal embeddings and translate the chain of amino acid residues in a protein to a sequence of natural language words, i.e., text. For the evaluation of protein captioning, we collect the ProteinCap dataset that contains 94,454 protein-text pairs. Experiments on ProteinCap demonstrate the effectiveness of the proposed P2T-GPT on protein captioning. For example, our method obtains improvements of 8.74, 10.03, and 11.05 in the BERTScore compared to the baseline model on ProteinCap- (alpha,beta,gamma) , respectively. As minor contributions, first, P2T-GPT provides a way to connect protein science and Large Language Models (LLMs). By appending ChatGPT, our method can interact in a conversational way to answer questions given a protein. Second, we show that protein captioning can be treated as a pre-trained task that can benefit a range of downstream tasks, to a certain extent.},
note = {Just Accepted},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = nov,
keywords = {Protein captioning, Natural language processing, Multimodal learning}
}

@article{10.14778/3746405.3746410,
author = {Sun, Ye and Shi, Lei and Tong, Yongxin},
title = {eXpath: Explaining Knowledge Graph Link Prediction with Ontological Closed Path Rules},
year = {2025},
issue_date = {May 2025},
publisher = {VLDB Endowment},
volume = {18},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/3746405.3746410},
doi = {10.14778/3746405.3746410},
abstract = {Link prediction (LP) is crucial for Knowledge Graphs (KG) completion but commonly suffers from interpretability issues. While several methods have been proposed to explain embedding-based LP models, they are generally limited to local explanations on KG and are deficient in providing human interpretable semantics. Based on real-world observations of the characteristics of KGs from multiple domains, we propose to explain LP models in KG with path-based explanations. An integrated framework, namely eXpath, is introduced which incorporates the concept of relation path with ontological closed path rules to enhance both the efficiency and effectiveness of LP interpretation. Notably, the eXpath explanations can be fused with other single-link explanation approaches to achieve a better overall solution. Extensive experiments across benchmark datasets and LP models demonstrate that introducing eXpath can boost the quality of resulting explanations by about 20\% on two key metrics and reduce the required explanation time by 61.4\%, in comparison to the best existing method. Case studies further highlight eXpath's ability to provide more semantically meaningful explanations through path-based evidence.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {2818–2830},
numpages = {13}
}

@inproceedings{10.1145/3649158.3657036,
author = {Ahmed, Mohiuddin and Wei, Jinpeng and Al-Shaer, Ehab},
title = {Prompting LLM to Enforce and Validate CIS Critical Security Control},
year = {2024},
isbn = {9798400704918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649158.3657036},
doi = {10.1145/3649158.3657036},
abstract = {Proper security control enforcement reduces the attack surface and protects the organizations against attacks. Organizations like NIST and CIS (Center for Internet Security) provide critical security controls (CSCs) as a guideline to enforce cyber security. Automated enforcement and measurability mechanisms for these CSCs still need to be developed. Analyzing the implementations of security products to validate security control enforcement is non-trivial. Moreover, manually analyzing and developing measures and metrics to monitor, and implementing those monitoring mechanisms are resource-intensive tasks and massively dependent on the security analyst's expertise and knowledge. To tackle those problems, we use large language models (LLMs) as a knowledge base and reasoner to extract measures, metrics, and monitoring mechanism implementation steps from security control descriptions to reduce the dependency on security analysts. Our approach used few-shot learning with chain-of-thought (CoT) prompting to generate measures and metrics and generated knowledge prompting for metrics implementation. Our evaluation shows that prompt engineering to extract measures, metrics, and monitoring implementation mechanisms can reduce dependency on humans and semi-automate the extraction process. We also demonstrate metric implementation steps using generated knowledge prompting with LLMs.},
booktitle = {Proceedings of the 29th ACM Symposium on Access Control Models and Technologies},
pages = {93–104},
numpages = {12},
keywords = {account management., critical security control, llm, prompt engineering},
location = {San Antonio, TX, USA},
series = {SACMAT 2024}
}

@inproceedings{10.1145/3670474.3685974,
author = {Wang, Li-C.},
title = {LLM-Assisted Analytics in Semiconductor Test (Invited)},
year = {2024},
isbn = {9798400706998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3670474.3685974},
doi = {10.1145/3670474.3685974},
abstract = {The emergence of Large Language Models (LLMs) has impacted our perspective on applying Machine Learning (ML) in semiconductor test. This paper shares our experience in leveraging the power of LLMs to build an AI agent for test data analytics. We advocate for an end-to-end approach where the Knowledge Graph (KG) plays a central role. Using wafermap analytics as an example, we highlight the key ideas behind developing the LLM-assisted AI agent named IEA-Plot, and discuss its practical applications.},
booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD},
articleno = {38},
numpages = {7},
keywords = {Knowledge Graph, Large Language Model, Machine Learning, Test Data Analytics},
location = {Salt Lake City, UT, USA},
series = {MLCAD '24}
}

@article{10.1145/3690767,
author = {Paul, Soumen and Das, Partha Pratim and Rao, K. Sreenivas},
title = {Ontology in Dance Domain—A Survey},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3690767},
doi = {10.1145/3690767},
abstract = {This article presents a literature review on the domain of dance research that exploits ontology artifacts to manage their domain knowledge. Any dance form around the world is rich in knowledge because of its historical and geographical diversity, movement rules, and interpretive aspects. Researchers found various approaches to manage this knowledge base, among which the ontology development process is preferred by most people owing to its superficial and easy-to-manage characteristics. However, the heterogenic use of ontology in different dance research aspects demands an organized study to understand its contributions to the domain. Our survey approach toward this objective starts with a systematic literature selection and further grouping them into four categories based on ontology involvement. Second, we discuss each group of articles by their contributions and the level of ontology involvement. Third, a novel evaluation framework is proposed, which assesses each selected article based on 19 attributes from ontology quality, development, and applications perspectives. We rank each article into three qualitative measures, i.e., High (H), Medium (M), and Low (L), for our attribute set based on our understanding. Finally, we comprehensively analyze the outcomes of our qualitative assessment to present the current research status and their limitations in the candidate domain. This review aspires to be a cornerstone resource, enlightening researchers about the current landscape and future prospects of ontological involvement in dance research.},
journal = {J. Comput. Cult. Herit.},
month = feb,
articleno = {16},
numpages = {32},
keywords = {Ontology, Domain knowledge, Dance domain, Multimedia, Quality model, Evaluation framework}
}

@article{10.1145/3592601,
author = {Gharagozlou, Hamid and Mohammadzadeh, Javad and Bastanfard, Azam and Ghidary, Saeed Shiry},
title = {Semantic Relation Extraction: A Review of Approaches, Datasets, and Evaluation Methods With Looking at the Methods and Datasets in the Persian Language},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {7},
issn = {2375-4699},
url = {https://doi.org/10.1145/3592601},
doi = {10.1145/3592601},
abstract = {A large volume of unstructured data, especially text data, is generated and exchanged daily. Consequently, the importance of extracting patterns and discovering knowledge from textual data is significantly increasing. As the task of automatically recognizing the relations between two or more entities, semantic relation extraction has a prominent role in the exploitation of raw text. This article surveys different approaches and types of relation extraction in English and the most prominent proposed methods in Persian. We also introduce, analyze, and compare the most important datasets available for relation extraction in Persian and English. Furthermore, traditional and emerging evaluation metrics for supervised, semi-supervised, and unsupervised methods are described, along with pointers to commonly used performance evaluation datasets. Finally, we briefly describe challenges in extracting relationships in Persian and English and dataset creation challenges.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {189},
numpages = {29},
keywords = {Semantic relations, relation extraction, Natural Language Processing (NLP), automatic extraction, Persian text processing, linguistics, dataset, evaluation methods, information extraction}
}

@article{10.1007/s00165-021-00549-0,
author = {Dubslaff, Clemens and Koopmann, Patrick and Turhan, Anni-Yasmin},
title = {Enhancing Probabilistic Model Checking with Ontologies},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {6},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-021-00549-0},
doi = {10.1007/s00165-021-00549-0},
abstract = {Probabilistic model checking (PMC) is a well-established method for the quantitative analysis of state based operational models such as Markov decision processes. Description logics (DLs) provide a well-suited formalism to describe and reason about knowledge and are used as basis for the web ontology language (OWL). We investigate how such knowledge described by DLs can be integrated into the PMC process, introducing ontology-mediated PMC. Specifically, we propose ontologized programs as a formalism that links ontologies to behaviors specified by probabilistic guarded commands, the de-facto standard input formalism for PMC tools such as Prism. Through DL reasoning, inconsistent states in the modeled system can be detected. We present three ways to resolve these inconsistencies, leading to different Markov decision process semantics. We analyze the computational complexity of checking whether an ontologized program is consistent under these semantics. Further, we present and implement a technique for the quantitative analysis of ontologized programs relying on standard DL reasoning and PMC tools. This way, we enable the application of PMC techniques to analyze knowledge-intensive systems.We evaluate our approach and implementation on amulti-server systemcase study,where different DL ontologies are used to provide specifications of different server platforms and situations the system is executed in.},
journal = {Form. Asp. Comput.},
month = dec,
pages = {885–921},
numpages = {37},
keywords = {Probabilisticmodel checking, Ontologies, Description logics, Ontology-mediated verification, Context dependent systems analysis}
}

@proceedings{10.1145/3567512,
title = {SLE 2022: Proceedings of the 15th ACM SIGPLAN International Conference on Software Language Engineering},
year = {2022},
isbn = {9781450399197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 15th ACM SIGPLAN International Conference on Software Language Engineering (SLE), co-located with the ACM SIGPLAN conference on Systems, Programming, Languages, and Applications (SPLASH) in Auckland, a vibrant port city in northern New Zealand, from December 5th to December 10th 2022. Like its predecessors, the this edition of the SLE conference, SLE 2022, is devoted to the principles of software languages: their design, their implementation, and their evolution. As such, SLE brings together researchers united by their common interest in the creation, capture, and tooling of software languages.},
location = {Auckland, New Zealand}
}

@inproceedings{10.1145/3599957.3606210,
author = {Garcia, Rebecca and Harris, Hunter and Beach, Matthew and Couch, Dylan and Khan, Samee U.},
title = {UAS Integration Safety and Security Technology Ontology},
year = {2023},
isbn = {9798400702280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3599957.3606210},
doi = {10.1145/3599957.3606210},
abstract = {Unmanned Aerial Systems (UAS) are a versatile and essential tool for law enforcement, first responders, utility providers, and the public. Integrating the UAS into the National Airspace System (NAS) poses a significant challenge to policymakers and manufacturers. A UAS Integration Safety and Security Technology Ontology (ISSTO) has been developed in the Web Ontology Language (OWL) to aid this integration. ISSTO is a domain ontology covering aviation topics corresponding to flights, aircraft types, manufacturers, temporal/spatial, waivers and authorizations, track data, NAS facilities, air traffic control advisories, weather phenomena, surveillance and security equipment, and events, sensor types, radio frequency ranges, actions, and outcomes. As ISSTO is a domain ontology, it models the current state of UAS integration into the NAS and provides a comprehensive view of every aspect of UAS.},
booktitle = {Proceedings of the 2023 International Conference on Research in Adaptive and Convergent Systems},
articleno = {9},
numpages = {6},
keywords = {Aviation, Ontology, Unmanned Aerial Systems},
location = {Gdansk, Poland},
series = {RACS '23}
}

@inproceedings{10.5555/3716662.3716812,
author = {Zhang, Richard and van Liemt, Erin and Fischella, Tyler},
title = {Ontology of Belief Diversity: A Community-Based Epistemological Approach},
year = {2025},
publisher = {AAAI Press},
abstract = {AI applications across classification, fairness, and human interaction often implicitly require ontologies of social concepts. Constructing these well, especially when there are many relevant categories, is a controversial task but is crucial for achieving meaningful inclusivity. Here, we focus on developing a pragmatic ontology of belief systems, which is a complex and often controversial space. By iterating on our community-based design until mutual agreement is reached, we found that epistemological methods were best for categorizing the fundamental ways beliefs differ, maximally respecting our principles of inclusivity and brevity. We demonstrate our methodology's utility and interpretability via user studies in term annotation and sentiment analysis experiments for belief fairness in language models.},
booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {1735–1743},
numpages = {9},
location = {San Jose, California, USA},
series = {AIES '24}
}

@inproceedings{10.1145/3701716.3715483,
author = {Chhetri, Tek Raj and Halchenko, Yaroslav O. and Jarecka, Dorota and Trivedi, Puja and Ghosh, Satrajit S. and Ray, Patrick and Ng, Lydia},
title = {Bridging the Scientific Knowledge Gap and Reproducibility: A Survey of Provenance, Assertion and Evidence Ontologies},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715483},
doi = {10.1145/3701716.3715483},
abstract = {The rapid growth of scientific publications and evolving experimental paradigms create significant challenges in staying up-to-date with current advances. Assertions are often unstructured and have limited provenance, which hinders reproducibility. Ontologies and knowledge graphs (KGs) offer structured solutions by capturing assertions, evidence, and provenance to support reproducibility. This paper reviews 23 ontologies -- 13 focused on assertions and evidence and 10 on provenance -- providing an overview of the current landscape while highlighting key challenges and opportunities for improvement.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {924–928},
numpages = {5},
keywords = {assertion and evidence ontology, knowledge discovery and integration, knowledge graphs, ontology survey, provenance, reproducibility},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3230833.3232799,
author = {Johnson, Pontus and Lagerstr\"{o}m, Robert and Ekstedt, Mathias},
title = {A Meta Language for Threat Modeling and Attack Simulations},
year = {2018},
isbn = {9781450364485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230833.3232799},
doi = {10.1145/3230833.3232799},
abstract = {Attack simulations may be used to assess the cyber security of systems. In such simulations, the steps taken by an attacker in order to compromise sensitive system assets are traced, and a time estimate may be computed from the initial step to the compromise of assets of interest. Attack graphs constitute a suitable formalism for the modeling of attack steps and their dependencies, allowing the subsequent simulation.To avoid the costly proposition of building new attack graphs for each system of a given type, domain-specific attack languages may be used. These languages codify the generic attack logic of the considered domain, thus facilitating the modeling, or instantiation, of a specific system in the domain. Examples of possible cyber security domains suitable for domain-specific attack languages are generic types such as cloud systems or embedded systems but may also be highly specialized kinds, e.g. Ubuntu installations; the objects of interest as well as the attack logic will differ significantly between such domains.In this paper, we present the Meta Attack Language (MAL), which may be used to design domain-specific attack languages such as the aforementioned. The MAL provides a formalism that allows the semi-automated generation as well as the efficient computation of very large attack graphs. We declare the formal background to MAL, define its syntax and semantics, exemplify its use with a small domain-specific language and instance model, and report on the computational performance.},
booktitle = {Proceedings of the 13th International Conference on Availability, Reliability and Security},
articleno = {38},
numpages = {8},
keywords = {Attack Graphs, Cyber Security, Domain Specific Language, Threat Modeling},
location = {Hamburg, Germany},
series = {ARES '18}
}

@inproceedings{10.1145/3627673.3679915,
author = {Abdi, Samireh},
title = {Enhancing Event Detection with Inter-Event Dependencies in Large Ontologies},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3679915},
doi = {10.1145/3627673.3679915},
abstract = {Event Detection (ED), a crucial component of comprehensive text analysis tools, is a well-established task within the fields of Natural Language Processing (NLP) and Information Extraction (IE). Current state-of-the-art models for ED primarily focus on identifying a limited set of predefined event types. Recently, the challenge of detecting a broad array of predefined event types has garnered increasing interest within the IE community. However, a significant gap in existing research on ED with extensive ontologies is the inadequate exploration of how interactions between event types affect ED model performance. One of the hindrances for this purpose is the lack of resources to encode event-event dependencies for large ontologies. This study introduces a novel approach that leverages existing inter-event dependency resources to provide this information for extensive ontologies. Specifically, a solution based on Optimal Transport is proposed to map event-event dependency from existing resources to a large ontology. We conduct extensive experiments on multiple benchmark datasets to assess the effectiveness of our approach. Our findings, supported by a thorough analysis, demonstrate that this innovative technique significantly enhances the performance of ED models, especially for ontologies with a large number of event types.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {3612–3616},
numpages = {5},
keywords = {event detection, large ontology, optimal transport},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3711403.3711488,
author = {Lin, Jian and Mai, Shanyin and Bu, Bingqian and He, Musheng and Wang, Xiaoyi},
title = {Research on the Application of STEM Practical Teaching Based on RAG Knowledge Graph and Large Models},
year = {2025},
isbn = {9798400717468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711403.3711488},
doi = {10.1145/3711403.3711488},
abstract = {Practical experience plays a pivotal role in STEM education, effectively cultivating students' practical skills, innovation capabilities, and critical thinking. However, the scarcity of domain-specific practical experience data within Large Language Models (LLMs) has not fully met the deep-level practical knowledge demands of STEM education, impacting the learners' application outcomes. This paper proposes a STEM practical teaching and inquiry system based on Retrieval-Augmented Generation (RAG) technology and knowledge graphs, aiming to enhance learners' learning experiences and interdisciplinary learning abilities, achieving an intelligent upgrade of STEM practical teaching.},
booktitle = {Proceedings of the 2024 7th International Conference on Educational Technology Management},
pages = {520–527},
numpages = {8},
keywords = {Knowledge Graph, Large Language Models (LLMs), RAG, STEM Practical Teaching},
location = {
},
series = {ICETM '24}
}

@inproceedings{10.1145/3503229.3547044,
author = {Abbasi, Ebrahim Khalil and Leclercq, Tony and Heymans, Patrick},
title = {A meta-model for product configuration ontologies},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547044},
doi = {10.1145/3503229.3547044},
abstract = {Conceptual modelling of product configuration is an essential step towards improving reuse and configuration knowledge sharing, systems interoperability, and people communication. Among several approaches proposed, ontology-based approaches are known to provide better support for the conceptualization of product configuration knowledge in terms of precision and expressiveness of reasoning and representation. This paper studies product configuration ontologies and presents a meta-model of concepts and their relationships that are used in those ontologies. The proposed meta-model consists of a generalisation hierarchy of configuration types, the compositional structure of a configurable product, the generalisation hierarchy of a component, and constraint types. The meta-model has been designed with the aim of first understanding the current state of product configuration ontologies and then extending it for integrating new concepts.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {166–173},
numpages = {8},
keywords = {knowledge representation, ontology, product configuration},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3178876.3186029,
author = {Cannaviccio, Matteo and Barbosa, Denilson and Merialdo, Paolo},
title = {Towards Annotating Relational Data on the Web with Language Models},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3186029},
doi = {10.1145/3178876.3186029},
abstract = {Tables and structured lists on Web pages are a potential source of valuable information, and several methods have been proposed to annotate them with semantics that can be leveraged for search, question answering and information extraction. This paper is concerned with the specific problem of finding and ranking relations from a given Knowledge Graph (KG) that hold over pairs of entities juxtaposed in a table or structured list. The state-of-the-art for this task is to attempt to link the entities mentioned in the table cells to objects in the KG and rank the relations that hold for those linked objects. As a result, these methods are hampered by the incompleteness and uneven coverage in even the best knowledge graphs available today. The alternative described here does not require entity linking, relying instead on ranking relations using generative language models derived from Web-scale corpora. As such, it can produce quality results even when the entities in the table are missing in the KG. The experimental validation, designed to expose the challenges posed by KG incompleteness, shows that our approach is robust and effective in practice.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {1307–1316},
numpages = {10},
keywords = {generative language models, knowledge graphs, web table understanding},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3503823.3503898,
author = {Krasadakis, Panteleimon and Sakkopoulos, Evangelos and Verykios, Vassilios S.},
title = {A Natural Language Processing Survey on Legislative and Greek Documents},
year = {2022},
isbn = {9781450395557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503823.3503898},
doi = {10.1145/3503823.3503898},
abstract = {Natural Language Processing is developing rapidly alongside the various complex applications that make use of it and they will depend on it even further in the future. It has many challenges that require the attention of both researchers and businesses. The state-of-the-art approaches usually involve the implementation of Deep Learning Neural Networks. Our work serves as a rigorous research of the bibliography on the field focusing on Legal and Greek documents. We also present the current challenges of the field and some future considerations.},
booktitle = {Proceedings of the 25th Pan-Hellenic Conference on Informatics},
pages = {407–412},
numpages = {6},
keywords = {Deep Learning, Information Extraction, Natural Language Processing},
location = {Volos, Greece},
series = {PCI '21}
}

@inproceedings{10.1145/3587259.3627568,
author = {Dobriy, Daniil and Polleres, Axel},
title = {O2WB: A tool enabling ontology reuse in Wikibase},
year = {2023},
isbn = {9798400701412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587259.3627568},
doi = {10.1145/3587259.3627568},
abstract = {The Semantic Web initiative has established standards and practices for publishing interconnected knowledge, where RDF Schema and OWL shall enable the reuse of ontologies as one of these established practices. However, Wikibase, the software behind Wikidata, which is increasingly gaining popularity among data publishers, lacks the functionality to import and reuse existing RDF Schema and OWL ontologies. To facilitate ontology reuse, FAIR data publishing and encourage a tighter connection of existing Linked Data resources with Wikibase instances, we align the Wikibase data model with RDF and present O2WB, a tool for ontology import and export within Wikibase.},
booktitle = {Proceedings of the 12th Knowledge Capture Conference 2023},
pages = {101–104},
numpages = {4},
keywords = {FAIR, Interoperability, Linked Data, Ontology Reuse, Wikibase},
location = {Pensacola, FL, USA},
series = {K-CAP '23}
}

@article{10.1145/3185046,
author = {Purao, Sandeep and Bolloju, Narasimha and Tan, Chuan-Hoo},
title = {A Modeling Language for Conceptual Design of Systems Integration Solutions},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3185046},
doi = {10.1145/3185046},
abstract = {Systems integration—connecting software systems for cross-functional work—is a significant concern in many large organizations, which continue to maintain hundreds, if not thousands, of independently evolving software systems. Current approaches in this space remain ad hoc, and closely tied to technology platforms. Following a design science approach, and via multiple design-evaluate cycles, we develop Systems Integration Requirements Engineering Modeling Language (SIRE-ML) to address this problem. SIRE-ML builds on the foundation of coordination theory, and incorporates important semantic information about the systems integration domain. The article develops constructs in SIRE-ML, and a merge algorithm that allows both functional managers and integration professionals to contribute to building a systems integration solution. Integration models built with SIRE-ML provide benefits such as ensuring coverage and minimizing ambiguity, and can be used to drive implementation with different platforms such as middleware, services, and distributed objects. We evaluate SIRE-ML for ontological expressiveness and report findings about applicability check with an expert panel. The article discusses implications for future research such as tool building and empirical evaluation, as well as implications for practice.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = sep,
articleno = {8},
numpages = {25},
keywords = {Design science, SIRE-ML, conceptual modeling}
}

@article{10.1145/3615864,
author = {Mesmia, Fatma Ben and Mouhoub, Malek},
title = {Semi-Automatic Building and Learning of a Multilingual Ontology},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {11},
issn = {2375-4699},
url = {https://doi.org/10.1145/3615864},
doi = {10.1145/3615864},
abstract = {Most online platforms, applications, and Websites use a massive amount of heterogeneous evolving data. These data must be structured and normalized before integration to improve the search and increase the relevance of results. An ontology can address this critical task by efficiently managing data and providing structured formats through techniques such as the Web Ontology Language (OWL). However, building an ontology can be costly, primarily if conducted manually. In this context, we propose a new methodology for automatically building and learning a multilingual ontology using Arabic as the base language via a corpus collected from Wikipedia. Our proposed methodology relies on Finite-state transducers (FSTs). FSTs are regrouped into a cascade to reduce errors and minimize ambiguity. The produced ontology is extended to English and French and independent language images via a translator we developed using APIs. The rationale for starting with the Arabic corpus to extract terms is that entity linking is more convenient from Arabic to other languages. In addition, many Wikipedia articles in English and French (for instance) do not have associated Arabic articles, but the opposite is true. In addition, dealing with Arabic terms permits us to enrich the Arabic module of the free linguistic platform we use in dictionaries and graphs. To assess the efficiency of our proposed methodology, we conducted performance metrics. The reported results are encouraging and promising.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {242},
numpages = {19},
keywords = {Ontology building and learning, finite sate transducer, transducer cascade, API, Arabic NLP}
}

@proceedings{10.1145/3582768,
title = {NLPIR '22: Proceedings of the 2022 6th International Conference on Natural Language Processing and Information Retrieval},
year = {2022},
isbn = {9781450397629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bangkok, Thailand}
}

@inproceedings{10.1145/3613904.3642436,
author = {Gray, Colin M. and Santos, Cristiana Teixeira and Bielova, Nataliia and Mildner, Thomas},
title = {An Ontology of Dark Patterns Knowledge: Foundations, Definitions, and a Pathway for Shared Knowledge-Building},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642436},
doi = {10.1145/3613904.3642436},
abstract = {Deceptive and coercive design practices are increasingly used by companies to extract profit, harvest data, and limit consumer choice. Dark patterns represent the most common contemporary amalgamation of these problematic practices, connecting designers, technologists, scholars, regulators, and legal professionals in transdisciplinary dialogue. However, a lack of universally accepted definitions across the academic, legislative, practitioner, and regulatory space has likely limited the impact that scholarship on dark patterns might have in supporting sanctions and evolved design practices. In this paper, we seek to support the development of a shared language of dark patterns, harmonizing ten existing regulatory and academic taxonomies of dark patterns and proposing a three-level ontology with standardized definitions for 64 synthesized dark pattern types across low-, meso-, and high-level patterns. We illustrate how this ontology can support translational research and regulatory action, including transdisciplinary pathways to extend our initial types through new empirical work across application and technology domains.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {289},
numpages = {22},
keywords = {dark patterns, deceptive design, ontology, regulation},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3444757.3485108,
author = {Morais, Gabriel and Bork, Dominik and Adda, Mehdi},
title = {Towards an Ontology-driven Approach to Model and Analyze Microservices Architectures},
year = {2021},
isbn = {9781450383141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3444757.3485108},
doi = {10.1145/3444757.3485108},
abstract = {Microservices Architectures (MSAs) are continuously replacing monolithic systems toward achieving more flexible and maintainable service-oriented software systems. However, the shift toward an MSA also requires a technological and managerial shift for its adopters. Architecting and managing MSAs represent unique challenges, including microservices' identification, interoperability, and reuse. To handle these challenges, we propose an Ontology-driven Conceptual Modelling approach, based on the Ontology of Microservices Architecture Concepts (OMSAC), for modelling and analyzing microservices-based systems. We show, how OMSAC-based conceptual models, stocked in a Stardog triple store, support Stakeholder-specific communication, documentation, and reuse. This paper reports on the application of our approach in three open-source MSA systems with a focus on microservices' discovery based on similarity metrics. Eventually, we compare the extracted similarity metrics derived from the application of machine learning techniques to the OMSAC models with a manual analysis performed by experts.},
booktitle = {Proceedings of the 13th International Conference on Management of Digital EcoSystems},
pages = {79–86},
numpages = {8},
keywords = {Microservices, OMSAC, Stardog, machine learning, ontology},
location = {Virtual Event, Tunisia},
series = {MEDES '21}
}

@inproceedings{10.1145/3613904.3642669,
author = {Das, Dipto and Guha, Shion and Brubaker, Jed R. and Semaan, Bryan},
title = {The ``Colonial Impulse" of Natural Language Processing: An Audit of Bengali Sentiment Analysis Tools and Their Identity-based Biases},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642669},
doi = {10.1145/3613904.3642669},
abstract = {While colonization has sociohistorically impacted people’s identities across various dimensions, those colonial values and biases continue to be perpetuated by sociotechnical systems. One category of sociotechnical systems–sentiment analysis tools–can also perpetuate colonial values and bias, yet less attention has been paid to how such tools may be complicit in perpetuating coloniality, although they are often used to guide various practices (e.g., content moderation). In this paper, we explore potential bias in sentiment analysis tools in the context of Bengali communities who have experienced and continue to experience the impacts of colonialism. Drawing on identity categories most impacted by colonialism amongst local Bengali communities, we focused our analytic attention on gender, religion, and nationality. We conducted an algorithmic audit of all sentiment analysis tools for Bengali, available on the Python package index (PyPI) and GitHub. Despite similar semantic content and structure, our analyses showed that in addition to inconsistencies in output from different tools, Bengali sentiment analysis tools exhibit bias between different identity categories and respond differently to different ways of identity expression. Connecting our findings with colonially shaped sociocultural structures of Bengali communities, we discuss the implications of downstream bias of sentiment analysis tools.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {769},
numpages = {18},
keywords = {Algorithmic audit, Bias, Colonial, Identity, Sentiment analysis tools},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3732771.3742719,
author = {Fr\"{o}lich, Damian and Pacciani, Tommaso and van Binsbergen, L. Thomas},
title = {Exploratory, Omniscient, and Multiverse Diagnostics in Debuggers for Non-Deterministic Languages},
year = {2025},
isbn = {9798400718847},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3732771.3742719},
doi = {10.1145/3732771.3742719},
abstract = {Debugging non-deterministic programs is inherently difficult as the compound effects of non-deterministic execution steps is hard to predict and gives rise to a (potentially) vast space of reachable program states such that manual exploration of all reachable states is infeasible.Multiverse debugging addresses these problems by realising a fine-grained, exhaustive and interactive process for state space exploration. At SLE2023, Pasquier et al. presented a generic framework that makes exploration practical through user-defined reductions on program states and by proposing expressive logics for defining and searching for states and traces of interest, generalising the concept of breakpoint. The framework has been validated through the case study language AnimUML designed to make non-deterministic UML specifications executable.In this paper, we perform additional case studies to evaluate the applicability of the framework. We analyse three non-deterministic, domain-specific languages representing three different domains: grammar engineering, formal operational semantics, and norm engineering. The framework is evaluated against requirements extracted from these domains, resulting in the identification of several limitations of the framework. We then propose a modified and extended framework and apply it to develop multiverse debuggers for the case study languages. The result demonstrates a multiverse debugging framework with more general applicability.},
booktitle = {Proceedings of the 18th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {134–147},
numpages = {14},
keywords = {debuggers, domain-specific languages, exploratory programming, multiverse debuggers, parsing},
location = {Koblenz, Germany},
series = {SLE '25}
}

@inproceedings{10.1145/3652620.3687793,
author = {Hou-Liu, Jerry and Jiang, Zhekai and Babikian, Aren A.},
title = {Concretize: A Model-Driven Tool for Scenario-Based Autonomous Vehicle Testing},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687793},
doi = {10.1145/3652620.3687793},
abstract = {To achieve rigorous certification of autonomous vehicles (AVs), testing approaches must handle all possible, practically relevant traffic scenarios. This is achievable through the handling of relevant abstractions within the scenario specification language and throughout the scenario generation process. While many scenario generation approaches exist, they are often limited to generating instances of a fixed (pre-defined) scenario and lack tool support. In this paper, we introduce Concretize, a model-driven AV testing framework. It (1) allows users to define scenario specifications using an abstract domain-specific language, and (2) generates conforming concrete (exact) scenarios, which are (3) visualized via a user-friendly web interface. Scenarios are also (4) executed in simulation, in which case Concretize (5) auto-generates figures depicting the monitored safety behavior of the AV-under-test wrt. scenario components at various abstraction levels.Video demonstration: https://youtu.be/inaD8jd7YxI.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {66–70},
numpages = {5},
keywords = {model-driven engineering, scenario-based autonomous vehicle testing, traffic scenario generation, domain-specific language},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@article{10.1145/3578708,
author = {Belhadi, Asma and Djenouri, Youcef and Srivastava, Gautam and Lin, Jerry Chun-Wei},
title = {Fast and Accurate Framework for Ontology Matching in Web of Things},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3578708},
doi = {10.1145/3578708},
abstract = {The Web of Things (WoT) can help with knowledge discovery and interoperability issues in many Internet of Things (IoT) applications. This article focuses on semantic modeling of WoT and proposes a new approach called Decomposition for Ontology Matching (DOM) to discover relevant knowledge by exploring correlations between WoT data using decomposition strategies. The DOM technique adopts several decomposition techniques to order highly linked ontologies of WoT data into similar groups. The main idea is to decompose the instances of each ontology into similar groups and then match instances of similar groups instead of entire instances of two ontologies. Three main algorithms for decomposition have been developed. The first algorithm is based on radar scanning, which determines the distribution of distances between each instance and all other instances to determine the cluster centroid. The second algorithm is based on adaptive grid clustering, where it focuses on distribution information and the construction of spanning trees. The third algorithm is based on split index clustering, where instances are divided into groups of cells from which noise is removed during the merging process. Several studies were conducted with different ontology databases to illustrate the use of the DOM technique. The results show that DOM outperforms state-of-the-art ontology matching models in terms of computational cost while maintaining the quality of the matching. Moreover, these results demonstrate that DOM is capable of handling various large datasets in WoT contexts.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {147},
numpages = {19},
keywords = {Ontology matching, Web of Things, decomposition}
}

@inproceedings{10.1145/3546118.3546147,
author = {Lekova, Anna and Andreeva, Anna and Tanev, Tanio and Simonska, Miglena and Kostova, Snezhana},
title = {A system for speech and language therapy with a potential to work in the IoT},
year = {2022},
isbn = {9781450396448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546118.3546147},
doi = {10.1145/3546118.3546147},
abstract = {In this study we propose a designed, developed and validated by experiments Speech and Language Therapy (SLT) system to provide SLT intervention for children with communication disorders. In order to help the SLT services in different educational and social context, the system has a potential to work in the Internet of Things (IoT). It can wire different assistive devices, APIs, online services and agents in order to meet the child individual needs for the intervention. Humanoid NAO-type robot, Emotiv EPOC+ brain headset, emotionally-expressive robot EmoSan and Kinect depth sensor are the devices connected using Node-RED. It is a flow-based tool designed for visual programming without a need to write any code and it can run locally or in the IoT. The proposed system is sufficiently general to be applied for other kind of therapy and to support other assistive technologies, cloud services and people in the remote areas.},
booktitle = {Proceedings of the 23rd International Conference on Computer Systems and Technologies},
pages = {119–124},
numpages = {6},
keywords = {Brain Computer Interface, Node-RED, Socially Assistive Robots, Speech Language Therapy},
location = {University of Ruse, Ruse, Bulgaria},
series = {CompSysTech '22}
}

@inproceedings{10.1145/3627673.3680094,
author = {Gubanov, Michael and Pyayt, Anna and Karolak, Aleksandra},
title = {CancerKG.ORG - A Web-scale, Interactive, Verifiable Knowledge Graph-LLM Hybrid for Assisting with Optimal Cancer Treatment and Care},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3680094},
doi = {10.1145/3627673.3680094},
abstract = {Here, we describe one of the first Web-scale hybrid Knowledge Graph (KG)-Large Language Model (LLM), populated with the latest peer-reviewed medical knowledge on colorectal Cancer. It is currently being evaluated to assist with both medical research and clinical information retrieval tasks at Moffitt Cancer Center and Research Institute, which is one of the top Cancer centers in the U.S. and in the world. Our hybrid is remarkable as it serves the user needs better than just an LLM, KG or a search-engine in isolation. LLMs as is are known to exhibit hallucinations and catastrophic forgetting as well as are trained on outdated corpora. The state of the art KGs, such as PrimeKG, cBioPortal, ChEMBL, NCBI, and other require manual curation, hence are quickly getting stale. CancerKG is unsupervised and is capable of automatically ingesting and organizing the latest medical findings. To alleviate the LLMs shortcomings, the verified KG serves as a Retrieval Augmented Generation (RAG) guardrail. CancerKG exhibits 5 different advanced user interfaces, each tailored to serve different data modalities better and more convenient for the user. We evaluated CancerKG on real user queries and report a high NDCG score on a large-scale corpora of approximately 44K publications.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {4497–4505},
numpages = {9},
keywords = {LLM, artificial intelligence (AI), cancer, data management},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3627673.3680090,
author = {Wu, Hao and Cho, Hyunji and Davies, Anna R. and Jones, Gareth J. F.},
title = {LLM-based Automated Web Retrieval and Text Classification of Food Sharing Initiatives},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3680090},
doi = {10.1145/3627673.3680090},
abstract = {Urban and peri-urban (UPU) food systems encounter challenges in sustainability and are fragile and vulnerable to shocks. Addressing these issues is one of the key drivers of food sharing initiatives (FSIs) which focus on collective acts around food across the food system. FSIs range from seed sharing and surplus food redistribution to community composting. We describe our development and deployment of web retrieval and content classification tools designed to provide automated mapping of FSIs at scale to populate databases of FSIs within cities. We present our novel automated system tailored for retrieving, identifying, categorizing and real-time monitoring of FSIs in over 200 European cities. Developed within the European CULTIVATE project, this system not only aids in comprehending the complex dynamics of the food sharing economy, but also enhances its visibility and operational efficiency. The automation of these processes plays a vital role in supporting the goals of the CULTIVATE project, notably in promoting sustainable food practices and resilient local food networks. Our system integrates web search using queries constructed automatically using domain-specific vocabulary resources with Large Language Model (LLM) query writing and classification methods. Experimental results using a collection of data derived from real online FSI content underscore the potential of digital automation to make significant contributions to innovative digital solutions to contemporary sustainability challenges. As such, the findings of this work pave the way for future research and implementation in similar contexts.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {4983–4990},
numpages = {8},
keywords = {automatic query writing, content discovery and classification, food sharing, llm-based retrieval, web search},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3567445.3571103,
author = {Jarwar, Muhammad Aslam and Watson CBE FREng, Jeremy and Ani, Uchenna P Daniel and Chalmers, Stuart},
title = {Industrial Internet of Things Security Modelling using Ontological Methods},
year = {2023},
isbn = {9781450396653},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3567445.3571103},
doi = {10.1145/3567445.3571103},
abstract = {The Industrial Internet of Things (IIoT) trend presents many significant benefits for improving industrial operations. However, its emergence from the convergence of legacy Industrial Control Systems (ICS) and Information and Communication Technologies (ICT) has introduced newer security issues such as weak or lack of end-to-end security. These challenges have weakened the interest of many critical infrastructure industries in adopting IIoT-enabled systems. Implementing security in IIoT is challenging because this involves many heterogeneous Information Technology (IT) and Operational Technology (OT) devices and complex interactions with humans, and the environments in which these are operated and monitored. This article presents the initial results of the PETRAS Secure Ontologies for Internet of Things Systems (SOfIoTS) project, which consists of key security concepts and a modular design of a base security ontology, which supports security knowledge representation and analysis of IIoT security.},
booktitle = {Proceedings of the 12th International Conference on the Internet of Things},
pages = {163–170},
numpages = {8},
keywords = {Cyber physical systems, Industrial Internet of Things, Knowledge modelling, Security attributes, Security ontology},
location = {Delft, Netherlands},
series = {IoT '22}
}

@article{10.1145/3511097,
author = {Nasim, Zarmeen and Haider, Sajjad},
title = {Automatic Labeling of Clusters for a Low-Resource Urdu Language},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3511097},
doi = {10.1145/3511097},
abstract = {Document clustering techniques often produce clusters that require human intervention to interpret the meaning of such clusters. Automatic cluster labeling refers to the process of assigning a meaningful phrase to a cluster as a label. This article proposes an unsupervised method for cluster labeling that is based on noun phrase chunking. The proposed method is compared with four other statistical-based methods, including Z-Order, M-Order, T-Order, and YAKE. In addition to the statistical measures based labeling schemes, the approach is also compared with two graph-based techniques: TextRank and PositionRank. The experiments were performed on the low-resource Urdu language corpus of News Headlines. The proposed approach's effectiveness was evaluated using cosine similarity, the Jaccard index, and feedback received from human evaluators. The results show that the proposed method outperforms other methods. It was found that the labels produced were more relevant and semantically rich in contrast to other approaches.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {93},
numpages = {22},
keywords = {Cluster labeling, low-resource language, urdu language processing}
}

@inproceedings{10.1145/3704137.3704180,
author = {Rzepka, Rafal and Obayashi, Akihiko},
title = {Effectiveness of Security Export Control Ontology for Predicting Answer Type and Regulation Categories},
year = {2025},
isbn = {9798400718014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704137.3704180},
doi = {10.1145/3704137.3704180},
abstract = {In this paper we present results of our experiments investigating if an expert knowledge graph can improve Large Language Models accuracy in predicting correct answer labels and regulations related to the topic of security export control. As the lack of related data prevents machine-learning or fine-tuning approaches, we implement prompt expansion by searching most relevant nodes of the graph and adding the expanded context to the prompt. Results of our experiments show that the addition improved answer type selection but clearly hamper the capability of finding a correct regulation category.},
booktitle = {Proceedings of the 2024 8th International Conference on Advances in Artificial Intelligence},
pages = {156–161},
numpages = {6},
keywords = {Expert Systems, GraphRAG, Large Language Models, Knowledge Graph, Security Export Control},
location = {
},
series = {ICAAI '24}
}

@inproceedings{10.1145/3487553.3524199,
author = {Kuculo, Tin},
title = {Comprehensive Event Representations using Event Knowledge Graphs and Natural Language Processing},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524199},
doi = {10.1145/3487553.3524199},
abstract = {Recent work has utilised knowledge-aware approaches to natural language understanding, question answering, recommendation systems, and other tasks. These approaches rely on well-constructed and large-scale knowledge graphs that can be useful for many downstream applications and empower knowledge-aware models with commonsense reasoning. Such knowledge graphs are constructed through knowledge acquisition tasks such as relation extraction and knowledge graph completion. This work seeks to utilise and build on the growing body of work that uses findings from the field of natural language processing (NLP) to extract knowledge from text and build knowledge graphs. The focus of this research project is on how we can use transformer-based approaches to extract and contextualise event information, matching it to existing ontologies, to build comprehensive knowledge graph-based event representations. Specifically, sub-event extraction is used as a way of creating sub-event-aware event representations. These event representations are then further enriched through fine-grained location extraction and contextualised through the alignment of historically relevant quotes.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {359–363},
numpages = {5},
keywords = {event extraction, event geotagging, event representation, knowledge graph, quotes},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3652620.3688261,
author = {Oakes, Bentley and Gomes, Claudio and Kamburjan, Eduard and Abbiati, Giuseppe and Ecem Bas, Elif and Engelsgaard, Sebastian},
title = {Towards Ontological Service-Driven Engineering of Digital Twins},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688261},
doi = {10.1145/3652620.3688261},
abstract = {The systematic engineering of Digital Twins (DTs) requires the establishment of clear methodologies supported by intelligent tooling. We propose an approach to guide the user in the creation and deployment of services for DTs utilizing ontologies and workflows. In our approach, the user selects a desired DT service from an array of options. This selection is then used to suggest a) enablers and models to place in the DT, and b) development and deployment workflows for the DT service. The aim is to provide DT engineering guidance to assist non-software engineering experts to develop DT services more rapidly with less effort. We describe our initial work on applying this approach to a derived version of an industrial wind turbine generator case study, utilizing openCAESAR for ontology definition and enacting the workflows with Jupyter notebooks.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {464–469},
numpages = {6},
keywords = {digital twins, ontologies, DT services, wind turbine testing, guided software engineering, recommendation, workflows},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3685243.3685291,
author = {Moret, Julian Gutierrez and Samper-Zapater, Jose Javier and Delgado, Ana M. and Rocha, Jose Macario de S and Tena, Gustavo and Soriano, Francisco R.},
title = {A Comprehensive Analysis of Ontologies Related to Safety Related Traffic Information for Road Traffic},
year = {2025},
isbn = {9798400717338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3685243.3685291},
doi = {10.1145/3685243.3685291},
abstract = {Ontologies within the Semantic Web represent a transformative advancement, offering standardized data schemes that seamlessly interconnect with diverse vocabularies and facilitate efficient information exchange globally. This paper explores the intersection of ontologies and road safety, a domain critical to public well-being. The analysis focuses on existing ontologies related to Safety Related Traffic Information, delving into their implications for enhanced road safety, optimized traffic management, and streamlined safety-related data exchange. The study discerns common patterns, best practices, and areas for improvement across various ontological models, thereby contributing to the development of standardized solutions for managing road safety information. Using an ad hoc metrics system for this type of information will assist us in determining which ontology is most suitable and/or the need for its adaptation or utilization. Acknowledging the vital role of ontologies in promoting data interoperability, knowledge sharing, and informed decision-making, this research underscores the potential of integrating Semantic Web technologies and ontological models to revolutionize safety-related information management. The findings advocate for the transformative impact of ontologies in reducing road accidents and safeguarding human lives, emphasizing their significance in advancing the evolution of safer and more efficient road networks.},
booktitle = {Proceedings of the 12th Euro American Conference on Telematics and Information Systems},
articleno = {10},
numpages = {1},
keywords = {Intelligent Transport., Knowledge Representation, Linked Data, Safety Road, Semantic Web, Traffic Ontology},
location = {Praia, Cape Verde},
series = {EATIS 2024}
}

@inproceedings{10.1145/3233027.3233029,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Extracting software product line feature models from natural language specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233029},
doi = {10.1145/3233027.3233029},
abstract = {The specification of a family of software products may include documents written in natural language. Automatically extracting knowledge from these documents is a challenging problem that requires using Natural Language Processing (NLP) techniques. This knowledge can be formalized as a Feature Model (FM), a diagram capturing the key features and the relationships among them.In this paper, we first review previous works that have presented tools for extracting FMs from textual specifications and compare their strengths and limitations. Then, we propose a framework for feature and relationship extraction, which overcomes the identified limitations and is built upon state-of-the-art open-source NLP tools. This framework is evaluated against previous works using several case studies, showing improved results.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {43–53},
numpages = {11},
keywords = {NLTK, feature model extraction, natural language processing, requirements engineering, software product line},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3532213.3532240,
author = {Liu, Yixun and Zou, Chuyi and Wang, Yongheng},
title = {Distil Knowledge from Natural Language},
year = {2022},
isbn = {9781450396110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532213.3532240},
doi = {10.1145/3532213.3532240},
abstract = {Knowledge Distillation (KD) is a machine learning approach for model compression and acceleration, which is suitable for applications with limited computational resources. KD is typically performed by distilling and transferring the knowledge of large teacher models into smaller student models, to enhance the performance of the latter. Current KD models require supervision by a pretrained teacher model, whose training requires additional computational cost. In this work, we first analyze the output of the teacher, then propose a method to distill knowledge from natural language. On this basis, we propose Semantic-knowledge-based Teacher-free KD (ST-KD) to further advance model compression and acceleration methods. Our model was tested on the CIFAR10 and CIFAR100 image classification datasets, and it was experimentally demonstrated that it improved the performance of a variety of deep neural networks with virtually no additional computational cost.},
booktitle = {Proceedings of the 8th International Conference on Computing and Artificial Intelligence},
pages = {181–186},
numpages = {6},
keywords = {Image classification, Knowledge distillation},
location = {Tianjin, China},
series = {ICCAI '22}
}

@inproceedings{10.1145/3589335.3651945,
author = {Ayoub, Michael Antonios Kruse and Su, Zhan and Li, Qiuchi},
title = {A Case Study of Enhancing Sparse Retrieval using LLMs},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3651945},
doi = {10.1145/3589335.3651945},
abstract = {While dense retrieval methods have made significant advancements, sparse retrieval techniques continue to offer advantages in terms of interpretability and generalizability. However, query-document term mismatch in sparse retrieval persists, rendering it infeasible for many practical applications. Recent research has shown that Large Language Models (LLMs) hold relevant information that can enhance sparse retrieval through the application of prompt engineering. In this paper, we build upon this concept to explore various strategies employing LLMs for information retrieval purposes. Specifically, we utilize LLMs to enhance sparse retrieval by query rewriting and query expansion. In query rewriting, the original query is refined by creating several new queries. For query expansion, LLMs are employed to generate extra terms, thereby enriching the original query. We conduct experiments on a range of well-known information retrieval datasets, including MSMARCO-passage, TREC2019, TREC2020, Natural Questions, SCIFACT. The experiments show that LLMs can be beneficial for sparse methods since the added information provided by the LLMs can help diminish the discrepancy between the term frequencies of the important terms in a query and the relevant document. In certain domains, we demonstrate that the effectiveness of LLMs is constrained, indicating that they may not consistently perform optimally, which will be explored in future research.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {1609–1615},
numpages = {7},
keywords = {information retrieval, large language models, query expansion, query writing},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3452021.3458310,
author = {Console, Marco and Kolaitis, Phokion G. and Pieris, Andreas},
title = {Model-theoretic Characterizations of Rule-based Ontologies},
year = {2021},
isbn = {9781450383813},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452021.3458310},
doi = {10.1145/3452021.3458310},
abstract = {An ontology specifies an abstract model of a domain of interest via a formal language that is typically based on logic. Although description logics are popular formalisms for modeling ontologies, tuple-generating dependencies (tgds), originally introduced as a unifying framework for database integrity constraints, and later on used in data exchange and integration, are also well suited for modeling ontologies that are intended for data-intensive tasks. The reason is that, unlike description logics, tgds can easily handle higher-arity relations that naturally occur in relational databases. In recent years, there has been an extensive study of tgd-ontologies and of their applications to several different data-intensive tasks. However, the fundamental question of whether the expressive power of tgd-ontologies can be characterized in terms of model-theoretic properties remains largely unexplored. We establish several characterizations of tgd-ontologies, including characterizations of ontologies specified by such central classes of tgds as full, linear, guarded, and frontier-guarded tgds. Our characterizations use the well-known notions of critical instance and direct product, as well as a novel locality property for tgd-ontologies. We further use this locality property to decide whether an ontology expressed by frontier-guarded (respectively, guarded) tgds can be expressed by tgds in the weaker class of guarded (respectively, linear) tgds, and effectively construct such an equivalent ontology if one exists.},
booktitle = {Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems},
pages = {416–428},
numpages = {13},
keywords = {finite axiomatizability, guardedness, model theory, ontologies, tuple-generating dependencies},
location = {Virtual Event, China},
series = {PODS'21}
}

@inproceedings{10.1145/3469830.3470894,
author = {Wang, Xieyang and Xu, Jianqiu and Lu, Hua},
title = {NALMO: A Natural Language Interface for Moving Objects Databases},
year = {2021},
isbn = {9781450384254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469830.3470894},
doi = {10.1145/3469830.3470894},
abstract = {Moving objects databases (MODs) have been extensively studied due to their wide variety of applications including traffic management, tourist service and mobile commerce. However, queries in natural languages are still not supported in MODs. Since most users are not familiar with structured query languages, it is essentially important to bridge the gap between natural languages and the underlying MODs system commands. Motivated by this, we design a natural language interface for moving objects, named NALMO. In general, we use semantic parsing in combination with a location knowledge base and domain-specific rules to interpret natural language queries. We design a corpus of moving objects queries for model training, which is later used to determine the query type. Extracted entities from parsing are mapped through deterministic rules to perform query composition. NALMO is able to well translate moving objects queries into structured (executable) languages. We support four kinds of queries including time interval queries, range queries, nearest neighbor queries and trajectory similarity queries. We develop the system in a prototype system SECONDO and evaluate our approach using 240 natural language queries extracted from popular conference and journal papers in the domain of moving objects. Experimental results show that (i) NALMO achieves accuracy and precision 98.1 and 88.1, respectively, and (ii) the average time cost of translating a query is 1.47s.},
booktitle = {Proceedings of the 17th International Symposium on Spatial and Temporal Databases},
pages = {1–11},
numpages = {11},
keywords = {moving objects database, natural language interface, query processing, semantic parsing, structured language},
location = {virtual, USA},
series = {SSTD '21}
}

@inproceedings{10.1145/3543873.3587318,
author = {Zaitoun, Antonio and Sagi, Tomer and Hose, Katja},
title = {OntoEval: an Automated Ontology Evaluation System},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587318},
doi = {10.1145/3543873.3587318},
abstract = {Developing semantically-aware web services requires comprehensive and accurate ontologies. Evaluating an existing ontology or adapting it is a labor-intensive and complex task for which no automated tools exist. Nevertheless, in this paper we propose a tool that aims at making this vision come true, i.e., we present a tool for the automated evaluation of ontologies that allows one to rapidly assess an ontology’s coverage of a domain and identify specific problems in the ontology’s structure. The tool evaluates the domain coverage and correctness of parent-child relations of a given ontology based on domain information derived from a text corpus representing the domain. The tool provides both overall statistics and detailed analysis of sub-graphs of the ontology. In the demo, we show how these features can be used for the iterative improvement of an ontology.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {82–85},
numpages = {4},
keywords = {BERT, knowledge engineering, natural language processing, ontology},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@proceedings{10.1145/3528588,
title = {NLBSE '22: Proceedings of the 1st International Workshop on Natural Language-based Software Engineering},
year = {2022},
isbn = {9781450393430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 1st edition of the International Workshop on Natural Language-Based Software Engineering (NLBSE). The potential of Natural Language Processing (NLP) and Natural Language Generation (NLG) to support developers and engineers in a wide number of software engineering-related tasks (e.g., requirements engineering, extraction of knowledge and patterns from the software artifacts, summarization and prioritization of development and maintenance activities, etc.) is increasingly evident. Furthermore, the current availability of libraries (e.g., NLTK, CoreNLP, and fasttext) and models (e.g., BERT) that allow efficiently and easily dealing with low-level aspects of natural language processing and representation, pushed more and more researchers to closely work with industry to attempt to solve software engineers' real-world problems.},
location = {Pittsburgh, Pennsylvania}
}

@inproceedings{10.1145/3704137.3704176,
author = {Dash, Sheetal and Seker, Huseyin and Shahpasand, Maryam},
title = {From Data to Defense: How Ontology Fuels AI in Cyber Threat Detection},
year = {2025},
isbn = {9798400718014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704137.3704176},
doi = {10.1145/3704137.3704176},
abstract = {In today's evolving digital landscape, cybersecurity threats [1] have become increasingly complex and persistent, with attacks like data breaches and ransomware exploiting vulnerabilities in digital systems. As organizations handle growing amounts of data, robust defense strategies depend on comprehensive datasets for detecting and preventing threats. This paper addresses data scarcity in cybersecurity by proposing the development of a dataset ontology tailored to the domain. In data science, ontologies are structured frameworks that define relationships between different concepts within a specific field. A dataset ontology for cybersecurity serves as a cohesive framework for categorizing, organizing, and interconnecting datasets, making it easier for professionals to access and analyze threat-relevant data. The objective is to fill the gap in existing datasets and enable more precise, data-driven cybersecurity strategies. Every cyber-attack generates data, such as network logs, malware metadata, or phishing records. Professionals use this data to analyze threats, understand attack methods, and devise preventive strategies.AI models rely on large volumes of high-quality data to train algorithms, using historical patterns to detect real-time threats and respond before significant damage occurs. Unfortunately, the lack of structured and comprehensive datasets [2] in cybersecurity presents a significant challenge to building these AI models. The data generated by cyber-attacks is often fragmented, inconsistent, or incomplete, making it difficult to develop accurate threat detection systems. Without access to well-organized datasets, AI models are less effective, prone to producing false positives, and unable to adapt to new types of attacks [3]. As a result, cybersecurity defenses are weakened, leaving organizations vulnerable to increasingly sophisticated threats. The dataset ontology proposed in this paper seeks to address the challenge of data scarcity by providing a structured framework for organizing cybersecurity datasets. By categorizing and systematizing data related to various types of cyber-attacks, this ontology facilitates a more comprehensive and detailed understanding of the threat landscape. In the case of cybersecurity, the ontology organizes data based on categories such as attack types, threat actors, vulnerabilities, and attack methods.This proposed well-designed dataset ontology offers several key benefits. First, it makes it easier to identify gaps in existing datasets and create new, targeted datasets that capture the intricacies of real-world cyber-attacks. These datasets can then be used to train AI models, ensuring that they are better equipped to detect and respond to emerging threats. Second, this ontology establishes relationships between different types of cyber-attacks, providing a more holistic view of how these attacks are interconnected. For example, it can link phishing attacks to ransomware infections, allowing AI models to recognize patterns that may otherwise go unnoticed. The ontology-driven approach is particularly valuable because it enables continuous updates and improvements to cybersecurity datasets. As cyber threats evolve, so must the data used to train AI models. A dataset ontology provides the flexibility needed to incorporate new data as it becomes available, ensuring that cybersecurity models remain relevant and effective in the face of changing attack patterns.The research involved the creation of a preliminary draft taxonomy followed by the categorization and classification of the taxonomy into distinct clusters, representing different facets within the realm of cybersecurity. The dataset ontology introduced in this paper offers a solution by ensuring that AI models have access to well-organized, comprehensive datasets that reflect the full spectrum of cyber threats. By systematically categorizing different types of cyber-attacks and the data associated with them, the ontology provides AI models with the information they need to detect both common and emerging threats. This, in turn, enhances the accuracy and reliability of these models, enabling them to detect real-time cyber-attacks with greater precision and fewer false positives.Moreover, the ontology-driven approach ensures that AI models remain adaptable in the face of evolving threats [4]. Cyber-attacks are not static; they continuously evolve as malicious actors develop new techniques to bypass existing defenses. Without access to updated datasets, AI models may become outdated and ineffective. The dataset ontology addresses this issue by allowing for continuous updates to the data used in training, ensuring that AI models remain agile and responsive to new challenges. The practical applications of the proposed dataset ontology extend beyond the realm of theoretical research. By providing a structured and comprehensive framework for organizing cybersecurity data, the ontology enables organizations to develop more effective defense strategies and respond more quickly to emerging threats. For instance, organizations can use dataset ontology to create AI models capable of detecting insider threats, which are particularly difficult to identify due to the complexity of monitoring behavior [5] within trusted environments. These models can also be used to detect zero-day vulnerabilities, which are exploited by attackers before security teams have the chance to patch them.Additionally, dataset ontology facilitates collaboration across the cybersecurity industry by providing a common framework for sharing data. Currently, many organizations struggle to share cybersecurity data due to the lack of standardized formats and structures. The proposed ontology provides a solution to this issue by offering a standardized framework that can be adopted across the industry, making it easier for organizations to collaborate and share data in a meaningful way.In conclusion, the creation of dataset ontology represents a significant advancement in the field of cybersecurity. By addressing the problem of data scarcity and fragmentation [5], the ontology provides a structured framework for organizing and interconnecting diverse datasets, allowing for more effective data analysis and threat detection. This ontology-driven approach enhances the accuracy and adaptability of AI models, enabling them to detect and neutralize cyber threats in real time. Furthermore, the practical applications of dataset ontology extend beyond academic research, empowering organizations to protect their data, infrastructure, and services from a wide range of cyber-attacks.As cyber threats continue to evolve, the need for structured and comprehensive datasets will only grow. The dataset ontology provides a much-needed solution to this challenge, ensuring that AI models remain effective in detecting and responding to new and emerging threats. By improving the way cybersecurity data is organized and analyzed, ontology helps safeguard the interconnected digital ecosystem that underpins modern society.},
booktitle = {Proceedings of the 2024 8th International Conference on Advances in Artificial Intelligence},
pages = {121–133},
numpages = {13},
keywords = {DDoS, Taxonomy, cyber security, datasets, metrics, ontology, real-time cyber threats, type},
location = {
},
series = {ICAAI '24}
}

@inproceedings{10.1145/3711542.3711607,
author = {Al Subhi, Sundos Nasser Said and Sassani, Ardavan and Mikler, Armin Robert},
title = {Navigating Complexity: A Multi-Domain Ontology Evaluation with Cluster Centroids as Hierarchical Representatives},
year = {2025},
isbn = {9798400717383},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711542.3711607},
doi = {10.1145/3711542.3711607},
abstract = {Building on prior research, this study extends and improves an algorithm originally designed to identify flood-related ontology concepts, evaluating its applicability across multiple fields. The primary objectives are to: (1) demonstrate the algorithm’s effectiveness in various domains (urban system, flood with urban system, and flood with fashion) and (2) illustrate how community detection enables each cluster’s “centroid” to anchor the ontology hierarchy and represent core topics. The improved algorithm shows greater efficiency, completing in 0.35 seconds compared to 0.6 seconds for the original algorithm on datasets with up to 1,000 rows, with both algorithms delivering consistent results. The findings confirm the algorithm’s adaptability across different domains and demonstrate the use of cluster centroids, derived through community detection, as representatives of key topics in domain-specific ontologies. This approach enhances information retrieval, supports coherent data integration across systems, and provides a framework to aid decision-making and analysis in complex domains.},
booktitle = {Proceedings of the 2024 8th International Conference on Natural Language Processing and Information Retrieval},
pages = {192–198},
numpages = {7},
keywords = {Multi-Domain Ontology, Community Detection, Co-Occurrence Graphs},
location = {
},
series = {NLPIR '24}
}

@inproceedings{10.1145/3701716.3715309,
author = {Sun, Qiang and Luo, Yuanyi and Zhang, Wenxiao and Li, Sirui and Li, Jichunyang and Niu, Kai and Kong, Xiangrui and Liu, Wei},
title = {Docs2KG: A Human-LLM Collaborative Approach to Unified Knowledge Graph Construction from Heterogeneous Documents},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715309},
doi = {10.1145/3701716.3715309},
abstract = {Enterprises generate vast amounts of unstructured documents, posing challenges for knowledge extraction and representation. Large language models (LLMs) offer strong potential for processing such data but struggle with factual accuracy and provenance. Knowledge graphs (KGs) provide a structured framework to address these limitations [6], yet constructing high-quality KGs from heterogeneous data remains a challenge. To address this issue, we present Docs2KG, a modular framework to build high-quality KGs from diverse unstructured documents. We first employs state-of-the-art document processing techniques to extract textual content, tabular data, and figures. The extracted information is then unified into a multifaceted KG with three aspects: (1) a Layout KG capturing document structural hierarchies, (2) a Metadata KG preserving document properties, and (3) a Semantic KG representing domain-specific entities and relationships. Docs2KG supports multiple construction paradigms for Semantic KG: ontology-based approaches, hybrid NLP pipelines with LLM verification, LLM-guided ontology generation, and specialized models for named entity recognition, event extraction, and causal relationship identification to enhance semantic coverage and accuracy. A key feature of Docs2KG is its human-in-the-loop verification interface, enabling iterative quality assessment and refinement of the resulting KGs. Docs2KG is openly available at https://docs2kg.ai4wa.com, with the aim of advancing knowledge graph construction research and accelerating enterprise applications through high-quality knowledge graph construction.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {801–804},
numpages = {4},
keywords = {heterogeneous data, knowledge graph, unstructured data},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3229345.3229373,
author = {de Almeida Bordignon, Ana Cl\'{a}udia and Thom, Lucin\'{e}ia Heloisa and Silva, Thanner Soares and Dani, Vinicius Stein and Fantinato, Marcelo and Ferreira, Renato Cesar Borges},
title = {Natural Language Processing in Business Process Identification and Modeling: A Systematic Literature Review},
year = {2018},
isbn = {9781450365598},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229345.3229373},
doi = {10.1145/3229345.3229373},
abstract = {Business Process Management (BPM) has been receiving increasing attention in recent years. Many organizations have been adapting their business to a process-centered view since they started noticing its potential to reduce costs, improve productivity and achieve higher levels of quality. However, implementing BPM in organizations requires time, making the automation of process identification and discovery highly desirable. To achieve this expectation, the application of Natural Language Processing (NLP) techniques and tools has emerged to generate process models from unstructured text. In this paper, we provide the results of a systematic literature review conducted in preparation and processing of natural language text aiming the extraction of business processes and process quality assurance. The study presents techniques applied to the BPM life-cycle phases of process identification, process discovery and process analysis as well as tools to support process discovery. This review covered papers from 2009 up to 2016 and identifies 518 articles of which 33 were selected as relevant to our work. The results of the present study may be valuable to support research in extraction of business process models from natural language text.},
booktitle = {Proceedings of the XIV Brazilian Symposium on Information Systems},
articleno = {25},
numpages = {8},
keywords = {Business Process Management, Natural Language Processing, Process Analysis, Process Discovery, Systematic Literature Review},
location = {Caxias do Sul, Brazil},
series = {SBSI '18}
}

@article{10.1145/3722233,
author = {Novak, Justin and Hueca, Angel and Rodman, Christopher and Perl, Samuel and Breaux, Travis and Valdengo, Justin},
title = {Building a Better SOC: Towards the Ontology for Security Operations Center Assistance and Replication (OSCAR)},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
url = {https://doi.org/10.1145/3722233},
doi = {10.1145/3722233},
abstract = {There are many methods for developing a Security Operations Center (SOC) or SOC capability. However, there currently exists no unified approach which comprehensively outlines the people, processes, and technology required for developing a SOC, or how an organization might implement those into an effective SOC capability. This article outlines a data gathering process used to compile knowledge necessary for a proposed Ontology for SOC Creation Assistance and Replication, which can serve as a solution to the gap in the current body of knowledge. An ontology such as the one proposed here would leverage the collective experience of a large cadre of cybersecurity experts with deep knowledge in fields related to Security Operations and the development of SOCs. Using interview methods and analysis, the knowledge of how these experts approach the problem of creating new SOC capabilities within a set of known constraints can be captured and codified. The result is a comprehensive body of structured knowledge outlining what critical decisions are made during the process, and how those decisions affect the implementation of People, Processes, and Technology which become part of a SOC. It is this body of knowledge which can be organized and presented as a formal ontology.},
journal = {Digital Threats},
month = mar,
articleno = {6},
numpages = {22},
keywords = {SOC, Security Operations, Security Operations Center, People, Process, Technology, Incident Response, Ontology}
}

@inproceedings{10.1145/3657054.3657077,
author = {Maratsi, Maria Ioanna and Ahmed, Umair and Alexopoulos, Charalampos and Charalabidis, Yannis and Polini, Andrea},
title = {Towards Cross-Domain Linking of Data: A Semantic Mapping of Cultural Heritage Ontologies},
year = {2024},
isbn = {9798400709883},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657054.3657077},
doi = {10.1145/3657054.3657077},
abstract = {The Linked Open Vocabularies (LOV) registry, designed with the Linked Data principles at core, provides an environment suitable for research which targets domain-specific, but also potentially reusable, information representation. The main purpose of this study is to follow the recommendations pertaining to the utilisation of LOV as a basis for experimentation in order to examine how information within the Cultural Heritage (CH) domain can be improved in terms of reusability and interoperability. The present lack of cross-domain knowledge transfer forms the motivation behind this study, with the aim of facilitating the transition from conventional, domain-specific knowledge representation to reusable and semantically interoperable information. The methodology of this study involves the manual semantic mapping of elements from 12 vocabularies in the LOV registry, reinforced by a small-scale experiment using contemporary large language models (LLMs), particularly GPT, for a preliminary assessment of the mapping process. The findings revealed several key aspects to consider regarding the alignment of semantically adjacent vocabulary elements in the CH domain and beyond, emphasising the potential unveiled by linking domain-focused schemata to standardised, established ones while preserving the conceptual hierarchies inherent to each individual knowledge domain. The contribution of this research pertains to the vision of linking data across different domains by initiating the alignment among representation schemata in CH, with the ultimate aim to expand beyond the boundaries of the in-word knowledge domain, while employing combinatory methodological approaches of technological means and human expertise to facilitate this process.},
booktitle = {Proceedings of the 25th Annual International Conference on Digital Government Research},
pages = {165–176},
numpages = {12},
location = {Taipei, Taiwan},
series = {dg.o '24}
}

@inproceedings{10.1145/3180374.3181333,
author = {Hamdani, Maryum and Butt, Wasi Haider and Anwar, Muhammad Waseem and Azam, Farooque},
title = {A Systematic Literature Review on Interaction Flow Modeling Language (IFML)},
year = {2018},
isbn = {9781450354318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180374.3181333},
doi = {10.1145/3180374.3181333},
abstract = {Design of front-end interfaces in software applications is a complex process. In this context, Interaction Flow Modeling Language (IFML) is an emerging standard introduced in 2013 by Object Management Group (OMG). In this article, a Systematic Literature Review (SLR) is performed to examine the applications of IFML. Particularly, 22 research studies (2014-2017) are identified and analyzed. Consequently, four major areas are recognized where IFML is frequently applied i.e. mobile applications (9 studies), web applications (8 studies), others (4 studies) and desktop applications (1 study). Furthermore, 9 leading IFML tools are presented i.e. Modeling (3) and model transformation (6). It has been concluded that IFML certainly simplifies the design and implementation of front-end interfaces. However, the existing IFML tools are not mature enough to be utilized for complex and large software applications.},
booktitle = {Proceedings of the 2018 2nd International Conference on Management Engineering, Software Engineering and Service Sciences},
pages = {134–138},
numpages = {5},
keywords = {IFML, Interaction flow modeling language, MDA, SLR},
location = {Wuhan, China},
series = {ICMSS 2018}
}

@inproceedings{10.1145/3489088.3489089,
author = {A. Setiawan, Foni and Murdani, Dendy and Riana, Freza and Dwimawati, Eny},
title = {COPOMBOCY: A COVID-19 Pandemic Ontology Model of Bogor City},
year = {2022},
isbn = {9781450385244},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489088.3489089},
doi = {10.1145/3489088.3489089},
abstract = {Coronavirus Disease of 2019 (COVID-19) has become a global health problem along with a declaration by the World Health Organization (WHO) on March 11, 2020, which declared it a pandemic. COVID-19 has spread to almost all countries, including Indonesia. Data related to the COVID-19 pandemic is complex and heterogeneous. To generate and maintain knowledge that is semantically stored in it, knowledge modeling is necessary to be done. This study aims to develop a model of knowledge about the COVID-19 pandemic in Bogor City in the form of ontology. The scope of knowledge includes the distribution of agents, close contact tracing, COVID-19 transmission, diagnosis, disease, type of illness, location, Large-Scale Social Restrictions (Pembatasan Sosial Berskala Besar [PSBB]) implementation, PSBB sanctions, statistics, status, symptoms, test processes, test results, and color zones. Data were obtained from the Bogor City COVID-19 Task Force, COVID-19 Information \&amp; Coordination Center of West Java Province (Pusat Informasi \&amp; Koordinasi COVID-19 Provinsi Jawa Barat [PIKOBAR]), Covid19.go.id, literature review, interview with epidemiologist, and reuse of knowledge from existing ontologies. The result of this study is an ontology in the Web Ontology Language (OWL) format consisting of 88 classes, 29 object properties, 48 property data, 2103 axioms, and 229 individuals. The test results on the built ontology were successful in answering 85.7\% of non-expert questions and 71.4\% of expert questions. Overall, the ontology built successfully answered 78.6\% of the questions about the COVID-19 pandemic in Bogor City. The ontology has been published so that it is publicly available and is still being developed to accommodate the latest data.},
booktitle = {Proceedings of the 2021 International Conference on Computer, Control, Informatics and Its Applications},
pages = {86–90},
numpages = {5},
keywords = {Bogor City, COVID-19, Ontology, Pandemic},
location = {Virtual/online conference, Indonesia},
series = {IC3INA '21}
}

@article{10.1145/3604612,
author = {Dave, Nakul R. and Mehta, Mayuri A. and Kotecha, Ketan},
title = {A Systematic Review of Stemmers of Indian and Non-Indian Vernacular Languages},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3604612},
doi = {10.1145/3604612},
abstract = {The stemming process is crucial and significant in the pre-processing step of natural language processing. The stemmer oversees the stemming process. It facilitates the extraction of morphological variants of a root or base word from the provided word. Over the period, several stemmers for various vernacular languages have been proposed. However, very few research studies have comprehensively investigated these available stemmers. This article makes multifold contributions. First, we discuss the various stemmers of 15 Indian and 17 non-Indian languages describing their key points, benefits, and drawbacks. All the Indian languages for which stemmers have been built are covered in this study. For the non-Indian languages, stemmers of commonly spoken languages have been covered. Second, we present a language-wise comparative analysis of stemmers based on our identified parameters. Third, we discuss the wordnets and dictionaries available for different languages. Fourth, we provide details of the datasets available for various languages. Fifth, we also provide challenges in existing stemmers and future directions for future researchers. The study presented in this article reveals that significant research has been carried out for the stemmers of influential languages such as English, Arabic, and Urdu. On the other hand, languages with d resources, such as Farsi, Polish, Odia, Amharic, and others, have received the least attention for research. Moreover, rigorous analysis reveals that most of the stemmers suffer from over-stemming errors. With a complete catalogue of available stemmers, this study aims at assisting the researchers and professionals working in the areas such as information retrieval, semantic annotation, word meaning disambiguation, and ontology learning.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {18},
numpages = {51},
keywords = {Natural Language Processing (NLP), stemming, rule-based stemmer, dictionary-based stemmer, hybrid stemmer, over-stemming error, under-stemming error}
}

@inproceedings{10.1145/3442442.3452347,
author = {Johnson, Isaac and Gerlach, Martin and S\'{a}ez-Trumper, Diego},
title = {Language-agnostic Topic Classification for Wikipedia},
year = {2021},
isbn = {9781450383134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442442.3452347},
doi = {10.1145/3442442.3452347},
abstract = {A major challenge for many analyses of Wikipedia dynamics—e.g., imbalances in content quality, geographic differences in what content is popular, what types of articles attract more editor discussion—is grouping the very diverse range of Wikipedia articles into coherent, consistent topics. This problem has been addressed using various approaches based on Wikipedia’s category network, WikiProjects, and external taxonomies. However, these approaches have always been limited in their coverage: typically, only a small subset of articles can be classified, or the method cannot be applied across (the more than 300) languages on Wikipedia. In this paper, we propose a language-agnostic approach based on the links in an article for classifying articles into a taxonomy of topics that can be easily applied to (almost) any language and article on Wikipedia. We show that it matches the performance of a language-dependent approach while being simpler and having much greater coverage.},
booktitle = {Companion Proceedings of the Web Conference 2021},
pages = {594–601},
numpages = {8},
keywords = {Wikipedia, language-agnostic, topic classification},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3708468.3711892,
author = {Post, Kevin and Kuchida, Reo and Olapade, Mayowa and Yin, Zhigang and Nurmi, Petteri and Flores, Huber},
title = {ContextLLM: Meaningful Context Reasoning from Multi-Sensor and Multi-Device Data Using LLMs},
year = {2025},
isbn = {9798400714030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708468.3711892},
doi = {10.1145/3708468.3711892},
abstract = {Conventional context awareness and activity recognition models produce abstract outputs that offer limited insights into user behavior and situational context. These can be significantly enhanced by leveraging multi-sensor and multi-device data streams. However, the aggregation and modelling of context sensor data presents complex challenges that require advanced inference capabilities. We introduce ContextLLM, a context-driven solution powered by Large Language Models (LLMs), designed to transform sparse, abstract insights from various sensors and devices into a detailed, descriptive context. Through rigorous experiments using a well-established benchmark dataset for activity recognition, we demonstrate that ContextLLM can significantly enhance context understanding. However, our analysis also highlights how the quality and complexity of sensor data representations impact the LLM's ability to accurately deduce context. Building on these findings, we develop a research agenda that outlines key challenges, and conclude with a discussion on the limitations and practical considerations of LLM-based reasoning in context-aware applications.},
booktitle = {Proceedings of the 26th International Workshop on Mobile Computing Systems and Applications},
pages = {13–18},
numpages = {6},
keywords = {Context modelling, Sensor data assistant, Wearable data},
location = {La Quinta, CA, USA},
series = {HotMobile '25}
}

@article{10.1145/3594724,
author = {Sartini, Bruno and Baroncini, Sofia and van Erp, Marieke and Tomasi, Francesca and Gangemi, Aldo},
title = {ICON: An Ontology for Comprehensive Artistic Interpretations},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3594724},
doi = {10.1145/3594724},
abstract = {In this work, we introduce ICON, an ontology that models artistic interpretations of artworks’ subject matter (i.e., iconographies) and meanings (i.e., symbols, iconological aspects). Developed by conceptualizing authoritative knowledge and notions taken from Panofsky’s levels of interpretation theory, ICON ontology focuses on the granularity of interpretations. It can be used to describe an interpretation of an artwork from the pre-iconographical, icongraphical, and iconological levels. Its main classes have been aligned to ontologies that come from the domains of cultural descriptions (ArCo, CIDOC-CRM, VIR), semiotics (DOLCE), bibliometrics (CITO), and symbolism (Simulation Ontology), to grant a robust schema that can be extendable using additional classes and properties coming from these ontologies. The ontology was evaluated through competency questions that range from simple recognition on a specific level of interpretation to complex scenarios. Data written using this model was compared to state-of-the-art ontologies and schemas to both highlight the current lack of a domain-specific ontology on art interpretation and show how our work fills some of the current gaps. The ontology is openly available and compliant with FAIR principles. With our ontology, we hope to encourage digital art historians working for cultural institutions in making more detailed linked open data about the content of their artifacts, to exploit the full potential of Semantic Web in linking artworks through not only subjects and common metadata but also specific symbolic interpretations, intrinsic meanings, and the motifs through which their subjects are represented. Additionally, by basing our work on theories made by different art history scholars in the last century, we make sure that their knowledge and studies will not be lost in the transition to the digital, linked open data era.},
journal = {J. Comput. Cult. Herit.},
month = aug,
articleno = {59},
numpages = {38},
keywords = {Iconology, iconography, art interpretation, ontology, cultural heritage, Semantic Web}
}

@inproceedings{10.1145/3686397.3686420,
author = {Sun, Yi and Yang, Wanru and Liu, Yin},
title = {The Application of Constructing Knowledge Graph of Oral Historical Archives Resources Based on LLM-RAG},
year = {2024},
isbn = {9798400717345},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686397.3686420},
doi = {10.1145/3686397.3686420},
abstract = {Oral historical archive resources are an emerging archive resource with the rapid development of modern technology. Its "bottom-up" approach to historical research has received widespread attention in the fields of history, archives, and libraries. Under the common knowledge discovery mode, oral historical archives resources are showing a dispersed state. Information technology represented by knowledge graphs can break through the data solidification of oral historical archives, reshape the information stack of oral historical archives, and achieve knowledge association and aggregation of oral historical archive resources. The article attempts to construct a knowledge graph of the oral historical archives resources on the theme of "science and art" in the collection of T.D. Lee Library of Shanghai Jiao Tong University. It uses Large Language Model - Retrieval Augmented Generation (LLM-RAG) for knowledge extraction, and then uses a semantic model for knowledge organization and management. The article attempts to empower humanities with technology, exploring the possibility of combining "digital technology" and "humanities research", extending traditional humanities research methods, breaking down barriers between technology and humanities resources, and providing a new path reference for revealing resource content characteristics, semantic deep correlation, and multi-dimensional knowledge discovery.},
booktitle = {Proceedings of the 2024 8th International Conference on Information System and Data Mining},
pages = {142–149},
numpages = {8},
keywords = {Knowledge Graph, LLM-RAG, Oral History Archives},
location = {
},
series = {ICISDM '24}
}

@inproceedings{10.1145/3672608.3707818,
author = {Mariani, Elisa and Seghouani, Nac\'{e}ra and Ma, Yue},
title = {A Hybrid Self-Correcting Approach for Embedding Ontology-based Knowledge Graphs},
year = {2025},
isbn = {9798400706295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672608.3707818},
doi = {10.1145/3672608.3707818},
abstract = {One significant challenge in Knowledge Graph (KG) Embedding is the generation of negative triples. Negative triples are essential as they enable a training model to distinguish between relationships that exist within the KG and those that do not. In this paper, we propose TransHySeCo approach: a Hybrid and Self-Correcting approach for embedding knowledge graphs. TransHySeCo is based on a hybrid training using both the domain semantics provided in the ontology related to the KG and the topology underlying the graph structure. Moreover, it is self-correcting. It generates new negative triples by leveraging the embeddings from previous training iterations and the (quasi-)true negatives obtained with the ontology-based negative generation method proposed in this paper. The self-correction terminates when no new (quasi-)true negative triple is generated. To evaluate TransHySeCo, we conducted experiments on different benchmark datasets and assessed the embeddings' effectiveness for the link prediction task. The results show that TransHySeCo provides KG embeddings of promising quality for link prediction.},
booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing},
pages = {1156–1163},
numpages = {8},
keywords = {knowledge graph, ontology, translational embedding, link prediction},
location = {Catania International Airport, Catania, Italy},
series = {SAC '25}
}

@article{10.1145/3708520,
author = {Cederbladh, Johan and Cicchetti, Antonio and Jongeling, Robbert},
title = {A Road-Map to Readily Available Early Validation and Verification of System Behaviour in Model-Based Systems Engineering using Software Engineering Best Practices},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708520},
doi = {10.1145/3708520},
abstract = {In this article, we discuss how we can facilitate the growing need for early validation and verification (V&amp;V) of system behaviour in Model-Based Systems Engineering (MBSyE). Several aspects, such as reducing cost and time to market, push companies towards integration of V&amp;V methods earlier in development to support effective decision-making. One foundational methodology seeing increased attention in industry is the use of MBSyE, which brings benefits of models with well-defined syntax and semantics to support V&amp;V activities, rather than relying on natural language text documentation. Despite their promise, industrial adoption of these practices is still challenging.This article presents a vision for readily available early V&amp;V. We present a summary of the literature on early V&amp;V in MBSyE and position existing challenges regarding potential solutions and future investigations towards this vision. We elaborate our vision by means of challenges with a specific emphasis on early V&amp;V of system behaviour. We identify three specific challenge areas: Creating and managing Models, Organisational systems engineering aspects, and early V&amp;V Methods. Finally, we outline a road-map to address these categories of challenges, in which we propose the transfer of established best practices from the software engineering domain to support emerging technologies in the systems engineering domain.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {151},
numpages = {30},
keywords = {Validation, Verification, Models, Early, Systems, Behaviour}
}

@inproceedings{10.1145/3631700.3665226,
author = {Afreen, Neda and Balloccu, Giacomo and Boratto, Ludovico and Fenu, Gianni and Malloci, Francesca Maridina and Marras, Mirko and Martis, Andrea Giovanni},
title = {Learner-centered Ontology for Explainable Educational Recommendation},
year = {2024},
isbn = {9798400704666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631700.3665226},
doi = {10.1145/3631700.3665226},
abstract = {Ontologies form the core of knowledge graphs, which act as faithful, semantic-rich sources for training models in delivering explainable recommendations. These models learn to extract logical paths between learners and resources to be recommended within the knowledge graph, according to behavior- and content-based patterns. Extracted paths are then used not only to provide recommendations, but also to generate accompanying textual explanations. Despite the potential of this approach, current ontologies derived from the traditional learner-resource interaction data fall short in terms of richness from an educational perspective. Conversely, general-purpose ontologies, while comprehensive in educational aspects, are overly complex for recommendation tasks. Unfortunately, a suboptimal ontology might prevent to articulate reasoning paths, and thus explanations, relevant for learners within the knowledge graph. To counter this limitation, in this paper, we propose LOXER, a novel ontology designed to unlock learner-centered logical paths for explainable educational recommendation. Our design integrates insights from diverse sources, including feedback from a local co-design group of learners, observations from specialized traditional large-scale educational recommendation datasets, and connections with well-known vocabularies of other existing ontologies. To validate our ontology, we conducted an evaluation of the explanation types it enables, involving university and lifelong learners and assessing explanation properties like effectiveness, decision-making speed, motivation, satisfaction, and confidence. Results show our ontology’s ability to foster diverse considerations during the learners’ decision-making process and to establish a semantic structure for knowledge graphs for explainable recommendation.},
booktitle = {Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {567–575},
numpages = {9},
keywords = {Explainability., Ontology, Recommendation},
location = {Cagliari, Italy},
series = {UMAP Adjunct '24}
}

@inproceedings{10.1145/3425898.3426958,
author = {van Binsbergen, L. Thomas and Liu, Lu-Chi and van Doesburg, Robert and van Engers, Tom},
title = {eFLINT: a domain-specific language for executable norm specifications},
year = {2020},
isbn = {9781450381741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425898.3426958},
doi = {10.1145/3425898.3426958},
abstract = {Software systems that share potentially sensitive data are subjected to laws, regulations, policies and/or contracts. The monitoring, control and enforcement processes applied to these systems are currently to a large extent manual, which we rather automate by embedding the processes as dedicated and adaptable software services in order to improve efficiency and effectiveness. This approach requires such regulatory services to be closely aligned with a formal description of the relevant norms.  This paper presents eFLINT, a domain-specific language developed for formalizing norms. The theoretical foundations of the language are found in transition systems and in Hohfeld's framework of legal fundamental conceptions. The language can be used to formalize norms from a large variety of sources. The resulting specifications are executable and support several forms of reasoning such as automatic case assessment, manual exploration and simulation. Moreover, the specifications can be used to develop regulatory services for several types of monitoring, control and enforcement. The language is evaluated through a case study formalizing articles 6(1)(a) and 16 of the General Data Protection Regulation (GDPR). A prototype implementation of eFLINT is discussed and is available online.},
booktitle = {Proceedings of the 19th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {124–136},
numpages = {13},
keywords = {GDPR, domain-specific language, executable specifications, normative modeling, policy enforcement},
location = {Virtual, USA},
series = {GPCE 2020}
}

@article{10.1145/3418208,
author = {Ni, Pin and Li, Yuming and Li, Gangmin and Chang, Victor},
title = {A Hybrid Siamese Neural Network for Natural Language Inference in Cyber-Physical Systems},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3418208},
doi = {10.1145/3418208},
abstract = {Cyber-Physical Systems (CPS), as a multi-dimensional complex system that connects the physical world and the cyber world, has a strong demand for processing large amounts of heterogeneous data. These tasks also include Natural Language Inference (NLI) tasks based on text from different sources. However, the current research on natural language processing in CPS does not involve exploration in this field. Therefore, this study proposes a Siamese Network structure that combines Stacked Residual Long Short-Term Memory (bidirectional) with the Attention mechanism and Capsule Network for the NLI module in CPS, which is used to infer the relationship between text/language data from different sources. This model is mainly used to implement NLI tasks and conduct a detailed evaluation in three main NLI benchmarks as the basic semantic understanding module in CPS. Comparative experiments prove that the proposed method achieves competitive performance, has a certain generalization ability, and can balance the performance and the number of trained parameters.},
journal = {ACM Trans. Internet Technol.},
month = mar,
articleno = {33},
numpages = {25},
keywords = {Cyber-physical systems, Natural language inference, Siamese neural networks}
}

@inproceedings{10.1145/3383583.3398545,
author = {Qin, Jian and \v{Z}umer, Maja and Wang, Xiaoguang and Fan, Wei},
title = {Conceptual Models and Ontological Schemas for Semantically Sustainable Digital Libraries},
year = {2020},
isbn = {9781450375856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383583.3398545},
doi = {10.1145/3383583.3398545},
abstract = {Semantic frameworks build foundations for digital libraries and repositories to enable structured data and information representation and interoperability in today's interlinked information systems. Conceptual modeling and ontological schemas provide effective communication and powerful tools for creating shared understanding and sustainable systems in various digital libraries. This panel will present cases in which conceptual modeling and ontologies are used to enrich content representation and reach consensus among communities of practice, especially in fast changing digital society and emerging application domains. Four experts in knowledge organization will first give a brief introduction for their research in conceptual modeling and ontology building and then engage the audience with question answering interactions.},
booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
pages = {441–442},
numpages = {2},
keywords = {conceptual modeling, cultural heritage resource description, knowledge representation, library reference model, ontological modeling, semantic enrichment},
location = {Virtual Event, China},
series = {JCDL '20}
}

@inproceedings{10.1145/3428658.3430973,
author = {Rocha, Bartira Dantas and Silva, Larysse and Batista, Thais and Cavalcante, Everton and Gomes, Porf\'{\i}rio},
title = {An Ontology-based Information Model for Multi-Domain Semantic Modeling and Analysis of Smart City Data},
year = {2020},
isbn = {9781450381963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428658.3430973},
doi = {10.1145/3428658.3430973},
abstract = {Smart city services are typically defined according to domains (e.g., health, education, safety) and supported by different systems. Consequently, the analysis of smart city data is often domain-specific, thus limiting the capabilities of the offered services and hampering decision-making that relies on isolated domain information. To support a suitable analysis across multiple domains, it is necessary having a unified data model able to handle the inherent heterogeneity of smart city data and take into account both geographic and citizen information. This paper presents an ontology-based information model to support multi-domain analysis in smart cities to foster interoperability and powerful automated reasoning upon unambiguous information. The proposed information model follows Linked Data principles and takes advantage of ontologies to define information semantically. The semantic relationships and properties defined in the model also allow inferring new pieces of information that improve accuracy when analyzing multiple city domains. This paper reports an evaluation of the information model through ontological metrics and competence questions.},
booktitle = {Proceedings of the Brazilian Symposium on Multimedia and the Web},
pages = {73–80},
numpages = {8},
keywords = {Inference, Information model, Linked Data, Ontologies, Semantic search, Smart cities},
location = {S\~{a}o Lu\'{\i}s, Brazil},
series = {WebMedia '20}
}

@inproceedings{10.1145/3607827.3616842,
author = {Rossetto, Federico and Dalton, Jeffrey and Murray-Smith, Roderick},
title = {Generating Multimodal Augmentations with LLMs from Song Metadata for Music Information Retrieval},
year = {2023},
isbn = {9798400702839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607827.3616842},
doi = {10.1145/3607827.3616842},
abstract = {In this work we propose a set of new automatic text augmentations that leverage Large Language Models from song metadata to improve on music information retrieval tasks. Compared to recent works, our proposed methods leverage large language models and copyright-free corpora from web sources, enabling us to release the knowledge sources collected. We show how combining these representations with the audio signal provides a 21\% relative improvement on five of six datasets on genre classification, emotion recognition and music tagging, achieving state-of-the-art in three (GTZAN, FMA-Small and Deezer). We demonstrate the benefit of injecting external knowledge sources by comparing them withintrinsic text representation methods that rely only on the sample's information.},
booktitle = {Proceedings of the 1st Workshop on Large Generative Models Meet Multimodal Applications},
pages = {51–59},
numpages = {9},
keywords = {large language models application, multimodal learning, music information retrieval},
location = {Ottawa ON, Canada},
series = {LGM3A '23}
}

@inproceedings{10.1145/3711542.3711583,
author = {Tan, Tee Hean},
title = {Rule-Based vs. AI-Driven: Comparing PolyAQG Framework and Generative AI Models},
year = {2025},
isbn = {9798400717383},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711542.3711583},
doi = {10.1145/3711542.3711583},
abstract = {This comparative analysis examines the PolyAQG framework and Generative AI models (e.g., ChatGPT, Gemini) across ten key criteria for question generation. The PolyAQG framework, a rule-based approach, is well-suited for structured content and excels in generating consistent questions for educational purposes. However, it may be limited in creativity and depth. Generative AI models, while capable of covering broader topics and interpreting complex contexts, require more computational resources and may introduce inaccuracies in specialized domains. The PolyAQG framework offers scalability within specific domains and predictable error handling. Generative AI models, although scalable across topics, may require fine-tuning for accuracy. Furthermore, Generative AI enables dynamic user interaction and fosters critical thinking, while the PolyAQG framework provides a more limited user interface. The choice between PolyAQG and generative AI depends on application needs. PolyAQG is ideal for structured questions and consistency, while generative AI excels in creativity, adaptability, and user interaction.},
booktitle = {Proceedings of the 2024 8th International Conference on Natural Language Processing and Information Retrieval},
pages = {298–303},
numpages = {6},
keywords = {Generative AI model, PolyAQG framework, contextual understanding, domain-specific, questions generation, rule-based, scalability},
location = {
},
series = {NLPIR '24}
}

@inproceedings{10.1145/3437120.3437310,
author = {E. Samaridi, Nikoletta and N. Karanikolas, Nikitas and C. Papakitsos, Evangelos},
title = {Lexicographic Environments in Natural Language Processing (NLP)},
year = {2021},
isbn = {9781450388979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437120.3437310},
doi = {10.1145/3437120.3437310},
abstract = {In this paper, a literature review is presented in reference to the most important lexicographical environments that have been developed in the last decades, with the aim of utilizing them in various knowledge management applications, in the wider area of ​​the Semantic Web (SW). This paper focuses on the semantic networks of lexical information, in order to highlight their value but also the urgent need to design and implement an electronic conceptual dictionary with ontological structure for the modern Greek language, according to international dictionary standards.},
booktitle = {Proceedings of the 24th Pan-Hellenic Conference on Informatics},
pages = {219–222},
numpages = {4},
keywords = {Computational Lexicography, conceptual dictionary, corpus, ontology, semantic networks},
location = {Athens, Greece},
series = {PCI '20}
}

@article{10.1145/3704729,
author = {Asprino, Luigi and Damiano, Rossana and Daquino, Marilena and De Giorgis, Stefano and Gangemi, Aldo and Lieto, Antonio and Sartini, Bruno and Striani, Manuel},
title = {An Ontology Network for Citizen Curation},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1556-4673},
url = {https://doi.org/10.1145/3704729},
doi = {10.1145/3704729},
abstract = {Citizen curation is gaining momentum as a new form of engagement with cultural heritage. Citizen curatorial activities require and produce a wealth of information, ranging from descriptions of the artefacts to visitor experience feedback. Although formalising and integrating such various data is of paramount importance, the domain lacks comprehensive ontologies to enable querying, interpreting and reasoning over the collected data. Social Participation, Cohesion and Inclusion through Cultural Engagement (SPICE) is an EU project dedicated to experimenting with citizen curation activities to foster cultural engagement. SPICE develops technologies that help communities to create and share their own interpretation of cultural artefacts, hence developing a better understanding of, and empathy for, themselves and other communities. Part of the SPICE ecosystem of technologies is the SPICE Ontology Network (SON), which empowers applications with knowledge-level reasoning abilities and supports both applications and users interacting with data involved in citizen curation activities. This article provides an overview of the SON and outlines its main use cases.},
journal = {J. Comput. Cult. Herit.},
month = dec,
articleno = {72},
numpages = {30},
keywords = {citizen curation, ontologies, cultural heritage, semantic web}
}

@article{10.1145/3397874,
author = {Moldovan, Alex and Nicula, Vlad and Pasca, Ionut and Popa, Mihai and Namburu, Jaya Krishna and Oros, Anamaria and Brie, Paul},
title = {OpenUIDL, A User Interface Description Language for Runtime Omni-Channel User Interfaces},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {EICS},
url = {https://doi.org/10.1145/3397874},
doi = {10.1145/3397874},
abstract = {We extend the concept of cross-device user interfaces into the new, more general, concept of omni-channel user interfaces to better reflect the technological variety offered for developing multi-target user interfaces for interactive applications. We present a model-based approach for developing runtime omni-channel user interfaces for multi-target applications, which consists of: (1) OpenUIDL, a user interface description language for describing omni-channel user interfaces with its semantics by a meta-model and its syntax based on JSON, (2) the definition of a step-wise approach for producing runtime interactive applications based onOpenUIDLwith integration into the development life cycle, (3) the development of a cloud-based, OpenUIDL compliant, Interactive Development Environment that supports the application and the enactment of the step-wise approach and its illustration on several multi-target user interfaces.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jun,
articleno = {86},
numpages = {52},
keywords = {model-based user interface, multi-target user interfaces, omni-channel user interfaces, open source, user interface description language}
}

@inproceedings{10.5555/3374138.3374175,
author = {Wagner, Gerd},
title = {Towards a non-proprietary modeling language for processing network simulation},
year = {2019},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Processing networks have been investigated in the mathematical theory of queueing and have been the application focus of most industrial simulation software products, starting with GPSS and SIMAN/Arena. They allow modeling many forms of discrete processing processes, and are mainly used in simulation projects for the manufacturing and services industries. However, there is still no proper vendor-neutral language definition for this paradigm, e.g., in the form of a meta-model defining an abstract syntax for specifying the structure and dynamics of processing networks. We reconstruct the core of this paradigm in the form of a UML-based meta-model and show how to map a processing network specified with this metamodel to an Object Event Simulation model providing its operational semantics.},
booktitle = {Proceedings of the 2019 Summer Simulation Conference},
articleno = {37},
numpages = {12},
keywords = {GPSS, SIMAN, arena, processing networks, queuing networks},
location = {Berlin, Germany},
series = {SummerSim '19}
}

@article{10.1109/TASLP.2018.2852492,
author = {Jang, Youngsoo and Ham, Jiyeon and Lee, Byung-Jun and Kim, Kee-Eung},
title = {Cross-Language Neural Dialog State Tracker for Large Ontologies Using Hierarchical Attention},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2852492},
doi = {10.1109/TASLP.2018.2852492},
abstract = {Dialog state tracking, which refers to identifying the user intent from utterances, is one of the most important tasks in dialog management. In this paper, we present our dialog state tracker developed for the fifth dialog state tracking challenge, which focused on cross-language adaptation using a very scarce machine-translated training data when compared to the size of the ontology. Our dialog state tracker is based on the bi-directional long short-term memory network with a hierarchical attention mechanism in order to spot important words in user utterances. The user intent is predicted by finding the closest keyword in the ontology to the attention-weighted word vector. With the suggested methodology, our tracker can overcome various difficulties due to the scarce training data that existing machine learning-based trackers had, such as predicting user intents they have not seen before. We show that our tracker outperforms other trackers submitted to the challenge with respect to most of the performance measures.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2072–2082},
numpages = {11}
}

@inproceedings{10.1145/3605098.3636153,
author = {Ehrlinger, Lisa and Holzner, Harald and Hemelmayr, Nora and W\"{o}\ss{}, Wolfram},
title = {A News Article Tag Categorization Ontology},
year = {2024},
isbn = {9798400702433},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605098.3636153},
doi = {10.1145/3605098.3636153},
abstract = {In recent years, the demand for personalized and subject-specific news has been growing. Users and organizations alike are requesting the ability to curate individual timelines based on their topics of interest. To enable the easy curation and discovery of such timelines, a suitable news tag categorization is required that is both, news-domain-oriented, and fine-grained enough to cover specific (e.g., regional) use cases. Since existing categorization systems do not sufficiently fulfill both requirements, we developed an ontology that contains news article tags based on media topics by the International Press Telecommunications Council and Wikipedia categories. The ontology has been implemented within the Newsadoo platform, where it improves the topic curation and exploration process.},
booktitle = {Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing},
pages = {1665–1667},
numpages = {3},
keywords = {news articles, tag categorization, ontologies, wikipedia categories},
location = {Avila, Spain},
series = {SAC '24}
}

@article{10.1145/3539732,
author = {Singh, Shashank Sheshar and Srivastava, Vishal and Kumar, Ajay and Tiwari, Shailendra and Singh, Dilbag and Lee, Heung-No},
title = {Social Network Analysis: A Survey on Measure, Structure, Language Information Analysis, Privacy, and Applications},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3539732},
doi = {10.1145/3539732},
abstract = {The rapid growth in popularity of online social networks provides new opportunities in computer science, sociology, math, information studies, biology, business, and more. Social network analysis (SNA) is a paramount technique supporting understanding social relationships and networks. Accordingly, certain studies and reviews have been presented focusing on information dissemination, influence analysis, link prediction, and more. However, the ultimate aim is for social network background knowledge and analysis to solve real-world social network problems. SNA still has several research challenges in this context, including users’ privacy in online social networks. Inspired by these facts, we have presented a survey on social network analysis techniques, visualization, structure, privacy, and applications. This detailed study has started with the basics of network representation, structure, and measures. Our primary focus is on SNA applications with state-of-the-art techniques. We further provide a comparative analysis of recent developments on SNA problems in the sequel. The privacy preservation with SNA is also surveyed. In the end, research challenges and future directions are discussed to suggest to researchers a starting point for their research.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {137},
numpages = {47},
keywords = {Information diffusion, influence maximization, link prediction, community detection, social network analysis}
}

@inproceedings{10.1109/ASE56229.2023.00018,
author = {Morales, Sergio and Claris\'{o}, Robert and Cabot, Jordi},
title = {Automating Bias Testing of LLMs},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00018},
doi = {10.1109/ASE56229.2023.00018},
abstract = {Large Language Models (LLMs) are being quickly integrated in a myriad of software applications. This may introduce a number of biases, such as gender, age or ethnicity, in the behavior of such applications. To face this challenge, we explore the automatic generation of tests suites to assess the potential biases of an LLM. Each test is defined as a prompt used as input to the LLM and a test oracle that analyses the LLM output to detect the presence of biases.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1705–1707},
numpages = {3},
keywords = {testing, ethics, bias, fairness, large language models},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@article{10.1613/jair.1.16401,
author = {Dimartino, Mirko M. and Wood, Peter T. and Cali, Andrea and Poulovassilis, Alexandra},
title = {Efficient Ontology-Mediated Query Answering: Extending DL-liteR and Linear ELH},
year = {2025},
issue_date = {Jun 2025},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {82},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.16401},
doi = {10.1613/jair.1.16401},
abstract = {The OWL 2 QL profile of the OWL 2 Web Ontology Language, based on the family of description logics called DL-Lite, is designed so that data stored in a standard relational database system (RDBMS) can be queried through an ontology via a rewriting mechanism, i.e., by rewriting the query into an SQL query that is then answered by the RDBMS system, without any changes to the data. In this paper we propose a language whose expressive power goes beyond that of DL-Lite while still allowing query answering via rewriting of queries into unions of conjunctive two-way regular path queries (UC2RPQs) instead of SQL queries. Our language is an extension of both OWL 2 QL and linear ELH: OWL 2 QL is extended by allowing qualified existential quantification on the left-hand side of concept inclusion axioms, and linear ELH by allowing inverses in role inclusion axioms. We identify a syntactic property of the extended language that guarantees UC2RPQ-rewritability. We propose a novel rewriting technique for conjunctive queries (CQs) under our ontology language that makes use of nondeterministic finite state automata. We show that CQ answering in our setting is NLOGSPACE-complete with respect to data complexity and NP-complete for combined complexity; we also show that answering instance queries is NLOGSPACE-complete for data complexity and in PTIME for combined complexity.},
journal = {J. Artif. Int. Res.},
month = apr,
numpages = {49}
}

@article{10.1145/3564604,
author = {Bibi, Nazia and Rana, Tauseef and Maqbool, Ayesha and Alkhalifah, Tamim and Khan, Wazir Zada and Bashir, Ali Kashif and Zikria, Yousaf Bin},
title = {Reusable Component Retrieval: A Semantic Search Approach for Low-Resource Languages},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3564604},
doi = {10.1145/3564604},
abstract = {A common practice among programmers is to reuse existing code, accomplished by performing natural language queries through search engines. The main aim of code retrieval is to search for the most relevant snippet from a corpus of code snippets. However, code retrieval frameworks for low-resource languages are insufficient. Retrieving the most relevant code snippet efficiently can be accomplished only by eliminating the semantic gap between the code snippets residing in the repository and the user’s query (natural language description). The primary objective of the research is to contribute to this field by providing a code search framework that can be extended for low-resource languages. The secondary objective is to provide a code retrieval mechanism that is semantically relevant to the user query and provide programmers with the ability to locate source code that they want to use when developing new applications. The proposed approach is implemented using a web platform to search for source code. As code retrieval is a sophisticated task, the proposed approach incorporates a semantic search mechanism. This research uses a semantic model for code retrieval, which generates meanings or synonyms of words. The proposed model integrates ontologies and Natural Language Processing. System performance measures and classification accuracy are computed using precision, recall, and F1-score. We also compare the proposed approach with state-of-the-art baseline models. The retrieved results are ranked, showing that our approach significantly outperforms robust code matching. Our evaluation shows that semantic matching leads to improved source code retrieval. This study marks a substantial advancement in integrating programming expertise with code retrieval techniques. Moreover, our system lets users know when and how it is used for successful semantic searching.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {141},
numpages = {31},
keywords = {Ontologies, web semantics, source code retrieval, source code search, information retrieval}
}

@inbook{10.1145/3672608.3707870,
author = {Adu-Duodu, Kwabena and Wilson, Stanly and Li, Yinhao and Oladimeji, Aanuoluwapo and Huraysi, Talea and Barati, Masoud and Perera, Charith and Solaiman, Ellis and Rana, Omer and Ranjan, Rajiv and Shah, Tejal},
title = {A Circular Construction Product Ontology for End-of-Life Decision-Making},
year = {2025},
isbn = {9798400706295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672608.3707870},
abstract = {Efficient management of end-of-life (EoL) products is critical for advancing circularity in supply chains, particularly within the construction industry where EoL strategies are hindered by heterogenous lifecycle data and data silos. Current tools like Environmental Product Declarations (EPDs) and Digital Product Passports (DPPs) are limited by their dependency on seamless data integration and interoperability which remain significant challenges. To address these, we present the Circular Construction Product Ontology (CCPO), an applied framework designed to overcome semantic and data heterogeneity challenges in EoL decision-making for construction products. CCPO standardises vocabulary and facilitates data integration across supply chain stakeholders enabling lifecycle assessments (LCA) and robust decision-making. By aggregating disparate data into a unified product provenance, CCPO enables automated EoL recommendations through customisable SWRL rules aligned with European standards and stakeholder-specific circularity SLAs, demonstrating its scalability and integration capabilities. The adopted circular product scenario depicts CCPO's application while competency question evaluations show its superior performance in generating accurate EoL suggestions highlighting its potential to greatly improve decision-making in circular supply chains and its applicability in real-world construction environments.},
booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing},
pages = {1943–1952},
numpages = {10}
}

@article{10.1145/3611306,
author = {Vats, Preeti and Sharma, Nonita and Sharma, Deepak Kumar},
title = {HKG: A Novel Approach for Low Resource Indic Languages to Automatic Knowledge Graph Construction},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3611306},
doi = {10.1145/3611306},
abstract = {Knowledge graph (KG), a visual representation of text data as a semantic network, holds enormous promise for the development of more intelligent robots. It leads to significant potential solutions for many tasks like question answering, recommendation, and information retrieval. However, this area is confined to using English text only. Since low-resource languages are now being used in the world of AI, it is necessary to develop a semantic network for them as well. In this research work, the authors provide state-of-the-art techniques for automatic knowledge graph construction for the Hindi language, which is still unexplored in ontology. Constructing a knowledge graph faces several hurdles and obstacles in the linguistic domain, primarily when it deals with the Hindi language. With an emphasis on the Indian perspective, this research intends to introduce a novel approach ‘HKG’ for knowledge graph construction framework for Hindi. It also implements the LSTM model to evaluate the accuracy of newly constructed knowledge graphs and compute different evaluation metrics such as accuracy and F1-score. This knowledge graph evaluates the accuracy of 87.50 using Doc2Vec word embedding with a train-test split of 7:3.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
keywords = {Knowledge graph Construction, Natural Language Processing, Low Resource Indian Languages, Stanza, Link Analysis and Neighbor Nodes, LSTM}
}

@inproceedings{10.1145/3638067.3638080,
author = {Costa, Simone Dornelas and Manso, Carolina De Freitas and Marques, Leonardo Carneiro and Gadelha, Bruno Freitas and Conte, Tayana Uch\^{o}a and Barcellos, Monalessa Perini},
title = {Using Networked Ontologies to support UX Evaluation in Immersive Context},
year = {2024},
isbn = {9798400717154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638067.3638080},
doi = {10.1145/3638067.3638080},
abstract = {Over the past few years, new interactive systems such as immersive technologies have gradually permeated our daily lives and found adoption across various fields. Immersive technologies provide users with immersive experiences. Assessing and modeling the quality of such experiences has become a trending topic in HCI, and UX is a key quality attribute in this context. When it comes to immersive experiences, evaluating UX is particularly challenging because the user should not be interrupted to provide feedback. In this paper, we propose using networked ontologies to support evaluating immersive experiences. We have explored using ontologies from an ontology network addressing the HCI domain to develop a tool that supports UX experts evaluating such experiences based on data recorded in interaction logs. We used the ontology-based tool to evaluate the UX of an immersive application that supports collaborative music composition. The tool extracted data from the application interaction logs applied UX metrics, and provided consolidated data and information in graphs and tables. We conducted a study and collected feedback from the tool developer and three UX experts who used the tool. Results showed that using networked ontologies to develop a tool to support UX evaluation is feasible and valuable. In summary, the ontologies helped at the conceptual level by offering a basis to define the system’s structural model and at the implementation level by assigning semantics to data to make inferences about UX. Based on the UX experts’ perceptions, the tool was considered a promising system, beneficial, helpful, and easy to use.},
booktitle = {Proceedings of the XXII Brazilian Symposium on Human Factors in Computing Systems},
articleno = {69},
numpages = {12},
keywords = {Immersive Experience, Ontology, Ontology Network, UX Evaluation, User Experience},
location = {Macei\'{o}, Brazil},
series = {IHC '23}
}

@inproceedings{10.1145/3652620.3687820,
author = {Moln\'{a}r, Vince and Graics, Bence and V\"{o}r\"{o}s, Andr\'{a}s and Tonetta, Stefano and Cristoforetti, Luca and Kimberly, Greg and Dyer, Pamela and Giammarco, Kristin and Koethe, Manfred and Hester, John and Smith, Jamie and Grimm, Christoph},
title = {Towards the Formal Verification of SysML v2 Models},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687820},
doi = {10.1145/3652620.3687820},
abstract = {Systems Modeling Language (SysML) is the de facto standard in the industry for modeling complex systems. SysML v2 is the new version of the language with reworked fundamentals. In this paper, we explore how the new formal semantics of SysML v2 can enable formal verification and various forms of automated reasoning. Formal verification involves mathematically proving the correctness of a system's design with respect to certain specifications or properties. This rigorous approach ensures that models behave as intended under all possible conditions. Through a detailed examination, we demonstrate how five specific tools - Gamma, MP-Firebird, Imandra, SAVVS, and SysMD - can formally analyze SysML v2 models. We show how these tools support the different concepts in the language, as well as the set of features and technologies they provide to users of SysML v2, such as model checking, theorem proving, contract-based design, or automatic fault injections. We propose a workflow for applying formal methods on SysML v2 models, illustrated by example models and artifacts generated by the above tools.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {1086–1095},
numpages = {10},
keywords = {SysML V2, systems modeling, formal methods, verification and validation, automated reasoning, tools},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3627673.3679582,
author = {Zhu, Yinghao and Ren, Changyu and Wang, Zixiang and Zheng, Xiaochen and Xie, Shiyun and Feng, Junlan and Zhu, Xi and Li, Zhoujun and Ma, Liantao and Pan, Chengwei},
title = {EMERGE: Enhancing Multimodal Electronic Health Records Predictive Modeling with Retrieval-Augmented Generation},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3679582},
doi = {10.1145/3627673.3679582},
abstract = {The integration of multimodal Electronic Health Records (EHR) data has significantly advanced clinical predictive capabilities. Existing models, which utilize clinical notes and multivariate time-series EHR data, often fall short of incorporating the necessary medical context for accurate clinical tasks, while previous approaches with knowledge graphs (KGs) primarily focus on structured knowledge extraction. In response, we propose EMERGE, a Retrieval-Augmented Generation (RAG) driven framework to enhance multimodal EHR predictive modeling. We extract entities from both time-series data and clinical notes by prompting Large Language Models (LLMs) and align them with professional PrimeKG, ensuring consistency. In addition to triplet relationships, we incorporate entities' definitions and descriptions for richer semantics. The extracted knowledge is then used to generate task-relevant summaries of patients' health statuses. Finally, we fuse the summary with other modalities using an adaptive multimodal fusion network with cross-attention. Extensive experiments on the MIMIC-III and MIMIC-IV datasets' in-hospital mortality and 30-day readmission tasks demonstrate the superior performance of the EMERGE framework over baseline models. Comprehensive ablation studies and analysis highlight the efficacy of each designed module and robustness to data sparsity. EMERGE contributes to refining the utilization of multimodal EHR data in healthcare, bridging the gap with nuanced medical contexts essential for informed clinical predictions. We have publicly released the code at https://github.com/yhzhu99/EMERGE.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {3549–3559},
numpages = {11},
keywords = {electronic health record, large language model, multimodal learning, retrieval-augmented generation},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3341105.3373977,
author = {Chang, Doo Soo and Cho, Gun Hee and Choi, Yong Suk},
title = {Ontology-based knowledge model for human-robot interactive services},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3373977},
doi = {10.1145/3341105.3373977},
abstract = {Recently, increasing interest in service robots with artificial intelligence has triggered several studies on developing service robots as human assistants. To perform automated tasks, service robots are not only required to recognize various attributes of the service environment, but they must also determine, which tasks and behaviors to perform, according to their internal system specifications and those of the individual situation. To perform tasks in such a generalized manner efficiently, the service robot must abstractly understand the recognition data obtained from the external environment and plan its tasks by combining the data and existing knowledge. Thus, an abstract and integrated knowledge management of low-level external recognition data and symbolic-level knowledge is indispensable. This study proposes a knowledge model for an integrated robot framework, which is used to provide human-robot interactive services. Our knowledge model is based on an ontology that contains general knowledge which provides clear definitions of common concepts and domain knowledge which provides specific concepts required to understand the information about agents(users and robots), and the environments about human-robot interactive services. An exemplary experiment is given in the context of a social robot service, which shows the usability of our knowledge model.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {2029–2038},
numpages = {10},
keywords = {human-robot interaction, knowledge framework, ontology-based knowledge processing, social robot service},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@article{10.1145/3444689,
author = {Zhao, Liping and Alhoshan, Waad and Ferrari, Alessio and Letsholo, Keletso J. and Ajagbe, Muideen A. and Chioasca, Erol-Valeriu and Batista-Navarro, Riza T.},
title = {Natural Language Processing for Requirements Engineering: A Systematic Mapping Study},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3444689},
doi = {10.1145/3444689},
abstract = {Natural Language Processing for Requirements Engineering (NLP4RE) is an area of research and development that seeks to apply natural language processing (NLP) techniques, tools, and resources to the requirements engineering (RE) process, to support human analysts to carry out various linguistic analysis tasks on textual requirements documents, such as detecting language issues, identifying key domain concepts, and establishing requirements traceability links. This article reports on a mapping study that surveys the landscape of NLP4RE research to provide a holistic understanding of the field. Following the guidance of systematic review, the mapping study is directed by five research questions, cutting across five aspects of NLP4RE research, concerning the state of the literature, the state of empirical research, the research focus, the state of tool development, and the usage of NLP technologies. Our main results are as follows: (i) we identify a total of 404 primary studies relevant to NLP4RE, which were published over the past 36 years and from 170 different venues; (ii) most of these studies (67.08\%) are solution proposals, assessed by a laboratory experiment or an example application, while only a small percentage (7\%) are assessed in industrial settings; (iii) a large proportion of the studies (42.70\%) focus on the requirements analysis phase, with quality defect detection as their central task and requirements specification as their commonly processed document type; (iv) 130 NLP4RE tools (i.e., RE specific NLP tools) are extracted from these studies, but only 17 of them (13.08\%) are available for download; (v) 231 different NLP technologies are also identified, comprising 140 NLP techniques, 66 NLP tools, and 25 NLP resources, but most of them—particularly those novel NLP techniques and specialized tools—are used infrequently; by contrast, commonly used NLP technologies are traditional analysis techniques (e.g., POS tagging and tokenization), general-purpose tools (e.g., Stanford CoreNLP and GATE) and generic language lexicons (WordNet and British National Corpus). The mapping study not only provides a collection of the literature in NLP4RE but also, more importantly, establishes a structure to frame the existing literature&nbsp;through categorization, synthesis and conceptualization of the main theoretical concepts and relationships that encompass&nbsp;both RE and NLP aspects. Our work thus produces a conceptual framework of NLP4RE. The framework is used to identify research gaps and directions, highlight technology transfer needs, and encourage more synergies between the RE community, the NLP one, and the software&nbsp;and systems&nbsp;practitioners. Our results can be used as a starting point to frame future studies according to a well-defined terminology and can be expanded as new technologies and novel solutions emerge.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {55},
numpages = {41},
keywords = {Requirements engineering (RE), natural language processing (NLP), software engineering (SE), systematic mapping study, systematic review}
}

@inproceedings{10.1145/3460210.3493540,
author = {Pernisch, Romana and Dell'Aglio, Daniele and Bernstein, Abraham},
title = {Toward Measuring the Resemblance of Embedding Models for Evolving Ontologies},
year = {2021},
isbn = {9781450384575},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460210.3493540},
doi = {10.1145/3460210.3493540},
abstract = {Updates on ontologies affect the operations built on top of them. But not all changes are equal: some updates drastically change the result of operations; others lead to minor variations, if any. Hence, estimating the impact of a change ex-ante is highly important, as it might make ontology engineers aware of the consequences of their action during editing. However, in order to estimate the impact of changes, we need to understand how to measure them. To address this gap for embeddings, we propose a new measure called Embedding Resemblance Indicator (ERI), which takes into account both the stochasticity of learning embeddings as well as the shortcomings of established comparison methods. We base ERI on (i) a similarity score, (ii) a robustness factor $hatμ $ (based on the embedding method, similarity measure, and dataset), and (iii) the number of added or deleted entities to the embedding computed with the Jaccard index.To evaluate ERI, we investigate its usage in the context of two biomedical ontologies and three embedding methods---GraRep, LINE, and DeepWalk---as well as the two standard benchmark datasets---FB15k-237 and Wordnet-18-RR---with TransE and RESCAL embeddings. To study different aspects of ERI, we introduce synthetic changes in the knowledge graphs, generating two test-cases with five versions each and compare their impact with the expected behaviour. Our studies suggests that ERI behaves as expected and captures the similarity of embeddings based on the severity of changes. ERI is crucial for enabling further studies into impact of changes on embeddings.},
booktitle = {Proceedings of the 11th Knowledge Capture Conference},
pages = {177–184},
numpages = {8},
keywords = {embedding similarity, knowledge graph embeddings, ontology evolution},
location = {Virtual Event, USA},
series = {K-CAP '21}
}

@inproceedings{10.1145/3706598.3713932,
author = {Kretzer, Felix and Kolthoff, Kristian and Bartelt, Christian and Ponzetto, Simone Paolo and Maedche, Alexander},
title = {Closing the Loop between User Stories and GUI Prototypes: An LLM-Based Assistant for Cross-Functional Integration in Software Development},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713932},
doi = {10.1145/3706598.3713932},
abstract = {Graphical user interfaces (GUIs) are at the heart of almost every software we encounter. GUIs are often created through a collaborative effort involving UX designers, product owners, and software developers, constantly facing changing requirements. Historically, problems in GUI development include a fragmented, poorly integrated tool landscape and high synchronization efforts between stakeholders. Recent approaches suggest using large language models (LLMs) to recognize requirements fulfillment in GUIs and automatically propose new GUI components. Based on ten interviews with practitioners, this paper proposes an LLM-based assistant as a Figma plug-in that bridges the gap between user stories and GUI prototyping. We evaluated the prototype with 40 users and 40 crowd-workers, showing that the effectiveness of GUI creation is improved by using LLMs to detect requirements’ completion and generate new GUI components. We derive design rationales to support cross-functional integration in software development, ensuring that our plug-in integrates well into established processes.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {879},
numpages = {19},
keywords = {GUI Prototypes; User Stories; Requirements; Assistance},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3686169.3686200,
author = {Dunbar, Michael and Speed, Chris},
title = {Data Sets for Regenerative Futures: Cultivating Relational Ontologies},
year = {2024},
isbn = {9798400710421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686169.3686200},
doi = {10.1145/3686169.3686200},
abstract = {This short paper examines the limitations of current urban data infrastructures in representing more-than-human perspectives and explores pathways toward more inclusive, regenerative data practices. It argues that prevalent data sets and knowledge repositories reflect anthropocentric ontologies, perpetuating the erasure of non-human subjects and diverse epistemologies. The paper identifies key challenges in developing more inclusive data infrastructures, including ontological incommensurability, the risk of cognitive injustice, and the need for expanded representational repertoires. Drawing on examples from Indigenous knowledge systems, multispecies ethnographies, and arts-based practices, the authors propose strategies for enriching urban datasets. These include centering marginalized perspectives, expanding participatory data governance models, and reimagining urban infrastructure as multi-species sensing apparatuses. The paper concludes by calling for the cultivation of relational data ontologies that can better capture the complex interdependencies between human and more-than-human worlds, essential for envisioning and realizing regenerative urban futures.},
booktitle = {Proceedings of the Halfway to the Future Symposium},
articleno = {39},
numpages = {4},
keywords = {Data sets, More-than-human, Regenerative Futures},
location = {Santa Cruz, CA, USA},
series = {HttF '24}
}

@inproceedings{10.1145/3387940.3392162,
author = {Schlutter, Aaron and Vogelsang, Andreas},
title = {Knowledge Extraction from Natural Language Requirements into a Semantic Relation Graph},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392162},
doi = {10.1145/3387940.3392162},
abstract = {Knowledge extraction and representation aims to identify information and to transform it into a machine-readable format. Knowledge representations support Information Retrieval tasks such as searching for single statements, documents, or metadata. Requirements specifications of complex systems such as automotive software systems are usually divided into different subsystem specifications. Nevertheless, there are semantic relations between individual documents of the separated subsystems, which have to be considered in further processes (e.g. dependencies). If requirements engineers or other developers are not aware of these relations, this can lead to inconsistencies or malfunctions of the overall system. Therefore, there is a strong need for tool support in order to detects semantic relations in a set of large natural language requirements specifications. In this work we present a knowledge extraction approach based on an explicit knowledge representation of the content of natural language requirements as a semantic relation graph. Our approach is fully automated and includes an NLP pipeline to transform unrestricted natural language requirements into a graph. We split the natural language into different parts and relate them to each other based on their semantic relation. In addition to semantic relations, other relationships can also be included in the graph. We envision to use a semantic search algorithm like spreading activation to allow users to search different semantic relations in the graph.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {373–379},
numpages = {7},
keywords = {knowledge extraction, natural language processing, requirement engineering, semantic relation graph, spreading activation},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3637528.3671576,
author = {Chen, Xuanzhong and Mao, Xiaohao and Guo, Qihan and Wang, Lun and Zhang, Shuyang and Chen, Ting},
title = {RareBench: Can LLMs Serve as Rare Diseases Specialists?},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671576},
doi = {10.1145/3637528.3671576},
abstract = {Generalist Large Language Models (LLMs), such as GPT-4, have shown considerable promise in various domains, including medical diagnosis. Rare diseases, affecting approximately 300 million people worldwide, often have unsatisfactory clinical diagnosis rates primarily due to a lack of experienced physicians and the complexity of differentiating among many rare diseases. In this context, recent news such as "ChatGPT correctly diagnosed a 4-year-old's rare disease after 17 doctors failed" underscore LLMs' potential, yet underexplored, role in clinically diagnosing rare diseases. To bridge this research gap, we introduce RareBench, a pioneering benchmark designed to systematically evaluate the capabilities of LLMs on 4 critical dimensions within the realm of rare diseases. Meanwhile, we have compiled the largest open-source dataset on rare disease patients, establishing a benchmark for future studies in this domain. To facilitate differential diagnosis of rare diseases, we develop a dynamic few-shot prompt methodology, leveraging a comprehensive rare disease knowledge graph synthesized from multiple knowledge bases, significantly enhancing LLMs' diagnostic performance. Moreover, we present an exhaustive comparative study of GPT-4's diagnostic capabilities against those of specialist physicians. Our experimental findings underscore the promising potential of integrating LLMs into the clinical diagnostic process for rare diseases. This paves the way for exciting possibilities in future advancements in this field.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4850–4861},
numpages = {12},
keywords = {benchmark for llms, evaluation, rare disease diagnosis},
location = {Barcelona, Spain},
series = {KDD '24}
}

@article{10.1145/3717413.3717417,
author = {Steinert, Alexandro and Ferenz, Stephan and Niesse, Astrid},
title = {Choosing the Right Ontology to Describe Research Data in the Energy Domain},
year = {2025},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
url = {https://doi.org/10.1145/3717413.3717417},
doi = {10.1145/3717413.3717417},
abstract = {As in all disciplines, increasing the FAIRness (findability, accessibility, interoperability, and reusability) of research data and software is a goal in energy research. In order to achieve this, it is important to identify the most appropriate ontology for the description of research data and software. However, despite the importance of this task, it still presents a significant challenge. While there are some comparisons of ontologies, a gap exists in assessing their usefulness according to ontology metadata. This paper fills this gap by defining 21 criteria sorted into four categories to help researchers choose ontologies in the energy domain. The criteria are used to compare eight ontologies for energy research to showcase their use and analyze the ontologies. The analysis reveals the Open Energy Ontology (OEO) as the top-ranked ontology. This underscores the importance of metadata comparison in ontology selection and highlights the benefits of incorporating metadata criteria into ontology terminology services to support researchers.},
journal = {SIGENERGY Energy Inform. Rev.},
month = feb,
pages = {33–48},
numpages = {16},
keywords = {FAIR principles, energy research, energy research data management, ontology, semantic web}
}

@article{10.1145/3533428,
author = {Mundotiya, Rajesh and Kumar, Shantanu and Kumar, Ajeet and Chaudhary, Umesh and Chauhan, Supriya and Mishra, Swasti and Gatla, Praveen and Singh, Anil Kumar},
title = {Development of a Dataset and a Deep Learning Baseline Named Entity Recognizer for Three Low Resource Languages: Bhojpuri, Maithili, and Magahi},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3533428},
doi = {10.1145/3533428},
abstract = {In Natural Language Processing (NLP) pipelines, Named Entity Recognition (NER) is one of the preliminary problems, which marks proper nouns and other named entities such as Location, Person, Organization, Disease and so on. Such entities, without an NER module, adversely affect the performance of a machine translation system. NER helps in overcoming this problem by recognizing and handling such entities separately, although it can be useful in Information Extraction systems also. Bhojpuri, Maithili, and Magahi are low resource languages, usually known as Purvanchal languages. This article focuses on the development of an NER benchmark dataset for Machine Translation systems developed to translate from these languages to Hindi by annotating parts of the available corpora with named entities. Bhojpuri, Maithili, and Magahi corpora of sizes 228,373, 157,468, and 56,190 tokens, respectively, were annotated using 22 entity labels. The annotation considers coarse-grained annotation labels followed by the tagset used in one of the Hindi NER datasets. We also report a Deep Learning baseline that uses an LSTM-CNNs-CRF model. The lower baseline F1-scores from the NER tool obtained by using Conditional Random Fields models are 70.56\% for Bhojpuri, 73.19\% for Maithili, and 84.18\% for Magahi. The Deep Learning-based technique (LSTM-CNNs-CRF) achieved 61.41\% for Bhojpuri, 71.38\% for Maithili, and 86.39\% for Magahi. As the results show, LSTM-CNNs-CRF fails to outperform the lower baseline in the case of Bhojpuri and Maithili, which have more data in terms of the number of tokens, but not in terms of the number of named entities. However, the cross-lingual model training of LSTM-CNNs-CRF for Bhojpuri and Maithili performed better than the CRF.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {29},
numpages = {20},
keywords = {Indo-Aryan languages, low resource languages, Purvanchal languages, Bhojpuri, Maithili, Magahi, named entity recognition, Conditional Random Fields, Deep Learning}
}

@inproceedings{10.1145/3549737.3549765,
author = {Masa, Panagiota and Meditskos, Georgios and Kintzios, Spyridon and Vrochidis, Stefanos and Kompatsiaris, Ioannis},
title = {Ontology-based Modelling and Reasoning for Forest Fire Emergencies in Resilient Societies},
year = {2022},
isbn = {9781450395977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549737.3549765},
doi = {10.1145/3549737.3549765},
abstract = {Every year, thousands of forest fires throughout the world cause disasters. One of the most critical challenges during a wildfire disaster is the effective management of heterogeneous information relative to the crisis to support human operators and authorities. Towards addressing this challenge, this paper presents an ontology-based framework for data representation and interlinking of wildfire events that are being used to foster advanced reasoning, situational awareness and interpretation for decision support. More specifically, we illustrate the capabilities of the ONTO-SAFE ontology to symbolically model contextual information in the domain, addressing application and user requirements promoting the creation of interoperable knowledge graphs. On top of the symbolic knowledge graphs, we define a rule-based framework for infusing expert knowledge in the form of constraints and rules to recognize patterns and situations of interest based on domain knowledge, assisting end-users in taking informed decisions and facilitating advanced decision-making.},
booktitle = {Proceedings of the 12th Hellenic Conference on Artificial Intelligence},
articleno = {24},
numpages = {9},
location = {Corfu, Greece},
series = {SETN '22}
}

@inproceedings{10.1145/3411408.3411410,
author = {Papantoniou, Katerina and Tzitzikas, Yannis},
title = {NLP for the Greek Language: A Brief Survey},
year = {2020},
isbn = {9781450388788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411408.3411410},
doi = {10.1145/3411408.3411410},
abstract = {There is a plethora of methods, tools and resources for processing text in the English language, however this is not the case for other languages, like Greek. Due to the increasing interest in NLP, and since there is a noteworthy number of works related to the processing of the Greek language, in this paper we survey the work related to the processing of Greek language. In particular, we list and briefly discuss related works, resources and tools, categorized according to various processing layers and contexts. This survey can be useful for researchers and students interested in NLP tasks, Information Retrieval and Knowledge Management for the Greek language.},
booktitle = {11th Hellenic Conference on Artificial Intelligence},
pages = {101–109},
numpages = {9},
keywords = {Greek language, Survey, natural language processing},
location = {Athens, Greece},
series = {SETN 2020}
}

@inproceedings{10.1145/3487664.3487700,
author = {de Almeida, Melonie and Samarawickrama, Chamodi and de Silva, Nisansa and Ratnayaka, Gathika and Perera, Shehan},
title = {Identifying Legal Party Members from Legal Opinion Documents using Natural Language Processing},
year = {2022},
isbn = {9781450395564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487664.3487700},
doi = {10.1145/3487664.3487700},
abstract = {Law and order is a field that can highly benefit from the contribution of Natural Language Processing (NLP) to its betterment. An area in which NLP can be of immense help is, information retrieval from legal documents which function as legal databases. The extraction of legal parties from the aforementioned legal documents can be identified as a task of high importance since it has a significant impact on the proceedings of contemporary legal cases. This study proposes a novel deep learning methodology which can be effectively used to find a solution to the problem of identifying legal party members in legal documents. In addition to that, in this paper, we introduce a novel data set which is annotated with legal party information by an expert in the legal domain. The deep learning model proposed in this study provides a benchmark for the legal party identification task on this data set. Evaluations for the solution presented in the paper show that our system has 90.89\% precision and 91.69\% recall for an unseen paragraph from a legal document, thus conforming the success of our attempt.},
booktitle = {The 23rd International Conference on Information Integration and Web Intelligence},
pages = {259–266},
numpages = {8},
keywords = {Legal party identification, NER, Recurrent Neural Networks, coreference resolution},
location = {Linz, Austria},
series = {iiWAS2021}
}

@inproceedings{10.1145/3589334.3645317,
author = {Zhao, Yizheng},
title = {Efficient Computation of Signature-Restricted Views for Semantic Web Ontologies},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589334.3645317},
doi = {10.1145/3589334.3645317},
abstract = {Uniform Interpolation (UI) is an advanced reasoning service used to narrow down an ontology to a restricted view. This new ontology, known as a uniform interpolant, will only consist of the ''relevant names'', yet it will retain their original meanings. UI is immensely promising due to its applicability across various domains where custom views of ontologies are essential. Nonetheless, to unlock its full potential, we need optimized techniques to generate these tailored views. Previous studies suggest that creating uniform interpolants for EL-ontologies is notably challenging. In some instances, it is not even feasible to compute a uniform interpolant; when feasible, the size of the uniform interpolant can be up to triple exponentially larger than the source ontology. Despite these challenges, our paper introduces an improved ''forgetting'' technique specifically designed for computing uniform interpolants of ELI-ontologies. We demonstrate that, with good normalization and inference strategies, such uniform interpolants can be efficiently computed, just as quickly as computing ''modules''. A comprehensive evaluation with a prototypical implementation of the method shows superb success rates over two popular benchmark datasets, demonstrating a clear computational advantage over state-of-the-art approaches.},
booktitle = {Proceedings of the ACM Web Conference 2024},
pages = {1945–1953},
numpages = {9},
keywords = {forgetting, module extraction, ontology, uniform interpolation},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3266237.3266244,
author = {da Silva, Jo\~{a}o Pablo S. and Ecar, Miguel and Pimenta, Marcelo S. and Guedes, Gilleanes T. A. and Rodrigues, Elder M.},
title = {Towards a domain-specific modeling language for self-adaptive systems conceptual modeling},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266244},
doi = {10.1145/3266237.3266244},
abstract = {Self-adaptive Systems (SaSs) are able to adapt their behavior at runtime in response to contextual changes. In this work, we are interested in SaSs conceptual modeling, which is the act of creating models that describe aspects of the world. SaSs modeling is a non-trivial activity because it deals with requirements uncertainty, contextual changes, and behavior adaptation. This complexity can be minimized by using Domain-Specific Modeling Languages (DSMLs), which may be created by extending Unified Modeling Language (UML). In this paper, we propose a UML profile that represents the higher-level abstractions required to provide support for SaSs conceptual modeling. We developed the UML profile by modeling the domain of interest and extending the UML class metaclass. The UML profile was evaluated through the focus group technique, which was performed by software engineering professors. As the outcome, the focus group participants considered the UML profile able to produce SaSs conceptual models with more expressiveness than UML standard.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {208–213},
numpages = {6},
keywords = {UML profiling, conceptual modeling, self-adaptive system},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@inproceedings{10.1145/3688671.3688737,
author = {Iacovou, Marios and Kontogiannis, Stelios and Avgerinakis, Konstantinos},
title = {Ontology Data Insights and Alerts for Wildfire Protection},
year = {2024},
isbn = {9798400709821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3688671.3688737},
doi = {10.1145/3688671.3688737},
abstract = {This paper presents an improved approach to wildfire protection by integrating heterogeneous data sources under a novel fire detection ontology. The introduced technique automatically unifies data from IoT devices into a single Semantic Knowledge Graph (SemKG), improving user experience by removing the need for users to examine several data sources. To make easier quick and well-informed decision-making, the system generates real-time notifications with different degrees of severity. The validation of the introduced technique has already taken place in various pilot sites of the SILVANUS H2020 project, which focus on developing a climate-resilient forest management platform to prevent and suppress wildfires, involving 49 partners from the European Union, Brazil, Indonesia, and Australia. In this paper, we demonstrate the effectiveness of this approach through scenarios of active fire detection and gas leakage.},
booktitle = {Proceedings of the 13th Hellenic Conference on Artificial Intelligence},
articleno = {41},
numpages = {6},
keywords = {Fire Detection, Health Impact Monitoring, IoT, Ontology, SILVANUS, Semantic Knowledge Graph, Wildfire Protection},
location = {
},
series = {SETN '24}
}

@inproceedings{10.1145/3689492.3689812,
author = {Aish, Robert and Fisher, Al and Orchard, Dominic and Torry, Jay},
title = {Programming Languages for the Future of Design Computation},
year = {2024},
isbn = {9798400712159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689492.3689812},
doi = {10.1145/3689492.3689812},
abstract = {Design Computation is the use of programming in the design of physical systems such as buildings and infrastructure. This involves embedding both general-purpose textual languages and domain-specific visual languages within geometry modelling and engineering applications in the construction industry. A unique form of entry-level end-user programming has emerged in Design Computation. However, there are significant usability and representational issues; general-purpose languages present barriers to adoption, whilst visual languages do not scale to complex design problems.    In this essay, we explore how advances in programming language research could be harnessed in future Design Computation languages to address these pedagogic, representational and scaling issues so as to improve human readable program structure and semantics and to enable machine-readable program verification.},
booktitle = {Proceedings of the 2024 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {241–265},
numpages = {25},
keywords = {Cognitive Dimensions, Collaborative Coding, Collection Types, Design Computation, End-User Programming, Program Verification, Programming Languages, Type Systems, Units of Measure, Usability, Visual Languages},
location = {Pasadena, CA, USA},
series = {Onward! '24}
}

@inproceedings{10.1145/3270112.3270124,
author = {Gilson, Fabian},
title = {Teaching software language engineering and usability through students peer reviews},
year = {2018},
isbn = {9781450359658},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3270112.3270124},
doi = {10.1145/3270112.3270124},
abstract = {Modelling techniques have been part of the software engineering practice for decades. Despite Model-Driven Software Engineering (Mde) being spread in the industry, effective teaching through meaningful use cases remains a challenge. Some guidelines and teaching experiences have been reported that either focus on the content of an Mde course, or on tooling environments designed for students. Furthermore, little emphasis has been put on the usability in the context of Software Language Engineering (Sle), which may lead to technically valid languages, but which are difficult to use. In this paper, we report on our attempt to combine a practical use case for Sle and a structured peer review focusing on the language usability and its tooling environment. By putting students in both the seats of language designers and users, we believe to empower students with suitable technical Sle skills as well as making them aware that a good software language requires more than theoretical validity to be adopted.},
booktitle = {Proceedings of the 21st ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {98–105},
numpages = {8},
keywords = {domain specific language, language usability, model-driven software engineering, software language engineering},
location = {Copenhagen, Denmark},
series = {MODELS '18}
}

@article{10.1145/3586078,
author = {Antonini, Alessio and Adamou, Alessandro and Su\'{a}rez-Figueroa, Mari Carmen and Benatti, Francesca},
title = {Experiential Observations: An Ontology Pattern-Based Study on Capturing the Potential Content within Evidences of Experiences},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3586078},
doi = {10.1145/3586078},
abstract = {Modelling the knowledge behind human experiences is a complex process: it should take into account, among others, the activities performed, human observations and the documentation of the evidence. To represent this knowledge in a declarative way means to support data interoperability in the context of cultural heritage artefacts, as linked datasets on experience documentation have started to appear. With this objective in mind, we describe a study based on an ontology design pattern for modelling experiences through observations, which are considered indirect evidence of a mental process (i.e., the experience). This pattern highlights the structural differences between types of experiential documentation, such as diaries and social media, providing a guideline for the comparability between different domains and for supporting the construction of heterogeneous datasets based on an epistemic compatibility. We have performed not only a formal evaluation over the pattern but also an assessment through a series of case studies. This approach includes (a) the analysis of interoperability among two case studies (reading through social media and historical sources); (b) the development of an ontology for collecting evidences of reading, which reuses the proposed pattern; and (c) the inspection of experience in humanities datasets.},
journal = {J. Comput. Cult. Herit.},
month = aug,
articleno = {58},
numpages = {30},
keywords = {ICT technologies in support of creating new cultural experiences or digital artefacts; metadata, classification schema, ontologies and semantic processing for CH multimedia repositories; knowledge patterns; digital humanities; intangible cultural heritage; human experience studies}
}

@inproceedings{10.1145/3708036.3708046,
author = {Liao, Fei},
title = {Detecting Public Perceptions of Apollo Go on Chinese Social Media Based on Topic Modeling and Sentiment analysis},
year = {2025},
isbn = {9798400709999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708036.3708046},
doi = {10.1145/3708036.3708046},
abstract = {As one representative of autonomous vehicles (AVs), Apollo Go, Baidu's self-driving cab service, has been operated in 11 cities in China and aroused heated discussion on Chinese social media after its crash with one pedestrian in Wuhan in July, 2024. It's significant to explore the public's opinions and sentiment for a company to promote its products and expand its market. Chinese social media, Weibo, provides a vast amount of data to dig out the public attitudes and perspectives. In this paper, 11,882 Weibo posts from May 29th to August 15th 2024 were crawled for topic modelling and sentiment analysis by using LDA algorithm and sentiment dictionaries. It is found that the public mainly focus on five topics related with Apollo Go, including unemployment, accident and privacy, rumor response and market expansion, industry development and riding experience as well as price. While some people embrace Apollo Go which, they think, stands for the future means of transportation, some of them show their concern about the liability for the accident, riding safety, possible privacy invasion as well as the potential unemployment. At the end this study, several measures are put forward to tackle those negative sentiments to promote the popularity of AVs. Meanwhile, the limitation of this study were also proposed at the conclusion of this paper.},
booktitle = {Proceedings of the 2024 5th International Conference on Computer Science and Management Technology},
pages = {61–65},
numpages = {5},
keywords = {Apollo Go, Autonomous vehicles, Sentiment analysis, Topic modelling, Weibo},
location = {
},
series = {ICCSMT '24}
}

@article{10.1145/3715069,
author = {Varshney, Deeksha and Behera, Niranshu and Katari, Prajeet and Ekbal, Asif},
title = {MedProm: Bridging Dialogue Gaps in Healthcare with Knowledge-Enhanced Generative Models},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715069},
doi = {10.1145/3715069},
abstract = {In medical dialogue systems, recent advancements underscore the critical role of incorporating relevant medical knowledge to enhance performance. However, existing knowledge bases often lack completeness, posing a challenge in sourcing pertinent information. We present MedProm, a novel generative model tailored for medical dialogue generation to address this gap. Motivated by the need for comprehensive and contextually relevant responses, MedProm leverages state-of-the-art language models such as BioGPT. Our model is designed to integrate extensive medical knowledge into conversations, facilitating effective communication between patients and healthcare providers. At the core of MedProm lies the MediConnect Graph, a meticulously constructed knowledge graph capturing intricate relationships among medical entities extracted from dialogue contexts. By employing a KnowFusion encoder with a pretraining objective and masked multi-head self-attention, MedProm effectively processes the MediConnect graph, enabling precise control over information flow to capture its underlying structure. Furthermore, MedProm incorporates a sophisticated Curriculum Knowledge Decoder, leveraging transformer-based decoding to generate response utterances conditioned on input representations from the KnowFusion Encoder. The training process is guided through curriculum learning, gradually increasing optimization difficulty based on a coherence-based criterion. Experimental results on two datasets demonstrate the efficacy of MedProm in generating accurate and contextually relevant responses compared to state-of-the-art models.},
note = {Just Accepted},
journal = {ACM Trans. Comput. Healthcare},
month = jan,
keywords = {Medical Dialogue Systems (MDS), Generative Neural Model, MediConnect Graph, KnowFusion Encoder, Curriculum Knowledge Decoder}
}

@inproceedings{10.1145/3173574.3173851,
author = {Huber, Bernd and McDuff, Daniel and Brockett, Chris and Galley, Michel and Dolan, Bill},
title = {Emotional Dialogue Generation using Image-Grounded Language Models},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173851},
doi = {10.1145/3173574.3173851},
abstract = {Computer-based conversational agents are becoming ubiquitous. However, for these systems to be engaging and valuable to the user, they must be able to express emotion, in addition to providing informative responses. Humans rely on much more than language during conversations; visual information is key to providing context. We present the first example of an image-grounded conversational agent using visual sentiment, facial expression and scene features. We show that key qualities of the generated dialogue can be manipulated by the features used for training the agent. We evaluate our model on a large and very challenging real-world dataset of conversations from social media (Twitter). The image-grounding leads to significantly more informative, emotional and specific responses, and the exact qualities can be tuned depending on the image features used. Furthermore, our model improves the objective quality of dialogue responses when evaluated on standard natural language metrics.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {computer vision, conversation, conversational agents, dialogue, emotion},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/3685243.3685295,
author = {Samper-Zapater, J. Javier and Martinez-Dura, Juan J. and Martinez-Plume, Javier and Rodr\'{\i}guez, David Garc\'{\i}a and Moret, Julian Guti\'{e}rrez and Baeza, Ernesto L\'{o}pez},
title = {VAS. A Semantic Model for Earth Observation data},
year = {2025},
isbn = {9798400717338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3685243.3685295},
doi = {10.1145/3685243.3685295},
abstract = {This paper discusses a segment of the "Artificial Intelligence and Semantics of Earth Observation (EO) data for the establishment of the Valencia Anchor Station as a supersite of the CEOS LPV Program (ASOTVAS)." The ASOTVAS project aims to apply AI and data semantics algorithms to the Valencia Anchor Station’s control zone (10 km x 10 km). A data semantics analysis explores concepts such as vocabularies, ontologies, and knowledge maps, with an emphasis on the reuse of existing models. Crucially, an ontological model with clear data definitions is developed to correlate Copernicus Program data with station parameters (e.g., FAPAR, LST, soil moisture, etc.). Using the ontology, the actual data are aggregated to create a knowledge map, which includes both an assertion model (concepts and properties) and an instance model (specific instances for ontological relationships). The main task is to develop an ontology-based representation scheme for terms of the application domain, which facilitates knowledge capture, distribution, and maintenance. The scheme enables computer processing for tasks related to information handling and supporting information exchange. Widely accepted ontology specification methodologies are employed. To avoid unnecessary efforts, emphasis is put on the reuse of existing ontologies or vocabularies, expanded, and adapted to the described case study. The optimal granularity is chosen for efficient ontology operation. The conceptual and technological requirements of the representation scheme are discussed. The methodology is based on software quality standards.},
booktitle = {Proceedings of the 12th Euro American Conference on Telematics and Information Systems},
articleno = {9},
numpages = {8},
keywords = {earth observation data, ontology, remote sensing, semantic.},
location = {Praia, Cape Verde},
series = {EATIS 2024}
}

@inproceedings{10.1145/3340555.3356100,
author = {Li, Hao and Liu, Chen and Zhu, Su and Yu, Kai},
title = {Robust Spoken Language Understanding with Acoustic and Domain Knowledge},
year = {2019},
isbn = {9781450368605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340555.3356100},
doi = {10.1145/3340555.3356100},
abstract = {Spoken language understanding (SLU) converts user utterances into structured semantic forms. There are still two main issues for SLU: robustness to ASR-errors and the data sparsity of new and extended domains. In this paper, we propose a robust SLU system by leveraging both acoustic and domain knowledge. We extract audio features by training ASR models on a large number of utterances without semantic annotations. For exploiting domain knowledge, we design lexicon features from the domain ontology and propose an error elimination algorithm to help predicted values recovered from ASR-errors. The results of CATSLU challenge show that our systems can outperform all of the other teams across four domains.},
booktitle = {2019 International Conference on Multimodal Interaction},
pages = {531–535},
numpages = {5},
keywords = {Robustness, Spoken Language Understanding},
location = {Suzhou, China},
series = {ICMI '19}
}

@article{10.1145/3583592,
author = {Clavaud, Florence and Francart, Thomas and Charbonnier, Pauline},
title = {RiC-O Converter: A Software to Convert EAC-CPF and EAD 2002 XML Files to RDF Datasets Conforming to Records in Contexts Ontology},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3583592},
doi = {10.1145/3583592},
abstract = {RiC-O Converter is an open source command-line tool to convert EAD finding aids and EAC-CPF authority records to RDF files conforming to ICA Records in Contexts ontology (RiC-O) in a robust manner. It was developed for the Archives nationales of France but is aimed to be reused by other archival institutions, and to this aim is fully documented in English. It is based on XSLT stylesheets that consider the variability of EAD content. It enabled the Archives nationales of France to convert 15,400 EAC-CPF files and 31,000 EAD files into a homogeneous knowledge graph, and to start a more specific project aiming to provide end users with an intuitive search interface for a significant subset of this graph, opening new perspectives for navigating and linking from/to archival metadata.},
journal = {J. Comput. Cult. Herit.},
month = aug,
articleno = {42},
numpages = {13},
keywords = {Records in Contexts (RiC), RiC Ontology (RiC-O), RDF, XML EAD, XML EAC-CPF, open source software, archival finding aids, archival authority records}
}

@inproceedings{10.1145/3428502.3428638,
author = {Chichkova, Natalia and Begler, Alena and Vlasov, Vitaly},
title = {Modeling city land use with an ontology},
year = {2020},
isbn = {9781450376747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428502.3428638},
doi = {10.1145/3428502.3428638},
abstract = {Urban planning developers, city government and citizens need a digital city model for sustainable development and city planning. Such models are based on main regulatory documents that differ by the city. In the current project, we aimed to develop a Land Use Ontology (LUO) for St. Petersburg as well as to expand general approaches to create such ontologies. The project is at its initial stage, i.e. development of the Land Use and Development Rules model that is a crucial document for the city land use system. We propose the ontological model of the city land use. The model includes classes for city land sites description, their coordinates, and permissions for construction works. The model is implemented in Web Ontology Language that assures the five-star rating of open data. The model source code can be found at https://github.com/inxaoc/cityont-public.},
booktitle = {Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance},
pages = {851–854},
numpages = {4},
keywords = {City digital model, city open data, land use ontology, smart city},
location = {Athens, Greece},
series = {ICEGOV '20}
}

@inproceedings{10.1145/3330204.3330253,
author = {Chagas, Michael William and Farias, Kleinner and Gon\c{c}ales, Lucian and Kupssinsk\"{u}, Lucas and Gluz, Jo\~{a}o Carlos},
title = {Hermes: A Natural Language Interface Model for Software Transformation},
year = {2019},
isbn = {9781450372374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330204.3330253},
doi = {10.1145/3330204.3330253},
abstract = {Software maintenance is a costly task and error-prone for both software developers and users as well. By knowing how and what software requirements need to be changed, end users could perform maintenance assisted by tools. However, current literature lacks for tools that support automated maintenance in real-world scenarios and allow users interaction via natural language. Even worse, the current tools are unable to understand the semantic of requests, as well as perform the necessary transformations in the maintenance software. This paper, therefore, proposes Hermes, a natural language interface model for software transformation. It combines computational linguistics techniques and logic programming to perform automated maintenance requests in software. Hermes interacts with end user through state of the art language parsers and domain ontologies by interpreting the semantics of changes requests to build a typed graph that change the software. Hermes was evaluated through an empirical study with 8 participants to investigate its performance, the level of acceptance, and usability. The collected data show that Hermes was accurate, producing a high elevated correctness number of hits by finding correct transformations and has been highly accepted by the users. The results are encouraging and show the potential for using Hermes to properly produce software maintenance requests.},
booktitle = {Proceedings of the XV Brazilian Symposium on Information Systems},
articleno = {43},
numpages = {8},
keywords = {Natural Language Processing, Ontologies, Parsing},
location = {Aracaju, Brazil},
series = {SBSI '19}
}

@inproceedings{10.1145/3360901.3364431,
author = {Wiens, Vitalis and Lohmann, Steffen and Auer, S\"{o}ren},
title = {GizMO -- A Customizable Representation Model for Graph-Based Visualizations of Ontologies},
year = {2019},
isbn = {9781450370080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360901.3364431},
doi = {10.1145/3360901.3364431},
abstract = {Visualizations can support the development, exploration, communication, and sense-making of ontologies. Suitable visualizations, however, are highly dependent on individual use cases and targeted user groups. In this article, we present a methodology that enables customizable definitions for the visual representation of ontologies.The methodology describes visual representations using the OWL annotation mechanisms and separates the visual abstraction into two information layers. The first layer describes the graphical appearance of OWL constructs. The second layer addresses visual properties for conceptual elements from the ontology. Annotation ontologies and a modular architecture enable separation of concerns for individual information layers. Furthermore, the methodology ensures the separation between the ontology and its visualization.We showcase the applicability of the methodology by introducing GizMO, a representation model for graph-based visualizations in the form of node-link diagrams. The graph visualization meta ontology (GizMO) provides five annotation object types that address various aspects of the visualization (e.g., spatial positions, viewport zoom factor, and canvas background color). The practical use of the methodology and GizMO is shown using two applications that indicate the variety of achievable ontology visualizations.},
booktitle = {Proceedings of the 10th International Conference on Knowledge Capture},
pages = {163–170},
numpages = {8},
keywords = {annotation ontology, customization, ontology visualization, visual notation, visual representation, visualization framework},
location = {Marina Del Rey, CA, USA},
series = {K-CAP '19}
}

@inproceedings{10.1145/3701716.3715240,
author = {Liang, Lei and Bo, Zhongpu and Gui, Zhengke and Zhu, Zhongshu and Zhong, Ling and Zhao, Peilong and Sun, Mengshu and Zhang, Zhiqiang and Zhou, Jun and Chen, Wenguang and Zhang, Wen and Chen, Huajun},
title = {KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715240},
doi = {10.1145/3701716.3715240},
abstract = {The recently developed Retrieval-Augmented Generation (RAG) technology has enabled the efficient construction of domain-specific applications. The key technologies of RAG are retrieval based on similarity and reasoning based on next-token prediction. However, this approach differs significantly from how humans solve problems. Humans typically follow certain analytical logic, reasoning while retrieving relevant information, and then connecting the clues to serve as references, ultimately generating an answer. In this process, the focus is on the semantic type and clear relationships between the keywords rather than similarity and co-occurrence. This difference in methodology results in the answers generated by RAG technology being insufficiently accurate or valuable.In this work, we concentrate on establishing semantic relationships between keywords to enable a more precise expression of knowledge and propose the Knowledge Augmented Generation(KAG) framework. KAG performs semantic parsing and reasoning on both documents and questions, involving three specific strategies: In the indexing phase, we complete the semantic information of keywords and the semantic relationships between them through information extraction and semantic reasoning; in the reasoning phase of question answering, we leverage semantic parsing to transform questions into Logical Forms with clear semantic types and relationships; in the retrieval phase, we predict the semantic relationships between Logical Form elements and structured index, thereby obtaining the required references.We compared KAG with existing RAG methods in three multi-hop QA datasets and the results show that KAG significantly outperforms existing methods, achieving a new state-of-the-art. We also applied KAG to real E-Government Q&amp;A business scenario, and achieving significant improvements in professionalism compared to traditional RAG methods. Meanwhile, to help developers easily build accurate and efficient domain knowledge QA services, our KAG natively supports the open-source KG engine OpenSPG.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {334–343},
numpages = {10},
keywords = {information retrieval, kbqa, knowledge graph, knowledge reasoning, rag},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3358501.3361235,
author = {Roy Chaudhuri, Subhrojyoti and Natarajan, Swaminathan and Banerjee, Amar and Choppella, Venkatesh},
title = {Methodology to develop domain specific modeling languages},
year = {2019},
isbn = {9781450369848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358501.3361235},
doi = {10.1145/3358501.3361235},
abstract = {Domain Specific Modeling Languages (DSML) significantly improve productivity in designing Computer Based System (CBS), by enabling them to be modeled at higher levels of abstraction. It is common for large and complex systems with distributed teams, to use DSMLs, to express and communicate designs of such systems uniformly, using a common language. DSMLs enable domain experts, with no or minimal software development background, to model solutions, using the language and terminologies used in their respective domains. Although, there are already a number of DSMLs available for modeling CBSs, their need is felt strongly across multiple domains, which still are not well supported with DSMLs. Developing a new DSML, however, is non trivial, as it requires (a) significant knowledge about the domain for which the DSML needs to be developed, as well as (b) skills to create new languages. In the current practice, DSMLs are developed by experts, who have substantial understanding of the domain of interest and strong background in computer science. One of the many challenges in the development of DSMLs, is the collection of domain knowledge and its utilization, based on which the abstract syntax, the backbone of the DSML is defined. There is a clear gap in the current state of art and practice, with respect to overcoming this challenge. We propose a methodology, which makes it easier for people with different backgrounds such as domain experts, solution architects, to contribute towards defining the abstract syntax of the DSML. The methodology outlines a set of steps to systematically capture knowledge about the domain of interest, and use that to arrive at the abstract syntax of the DSML. The key contribution of our work is in abstracting a CBS from a domain into a Domain Specific Machine, embodied in domain specific concepts. The methodology outlines, how the Domain Specific Machine, when coupled with guidelines from current practices of developing DSMLs, results in the definition of the abstract syntax of the intended DSML. We discuss our methodology in detail, in this paper.},
booktitle = {Proceedings of the 17th ACM SIGPLAN International Workshop on Domain-Specific Modeling},
pages = {1–10},
numpages = {10},
keywords = {domain specific language, domain specific modeling language, language engineering, modeling},
location = {Athens, Greece},
series = {DSM 2019}
}

@inproceedings{10.1145/3732945.3733004,
author = {Yang, Cuicui and Zhu, Min},
title = {Research on Robotics Diffusion Transformer in Robot Foundation Models Based on Diffusion Model},
year = {2025},
isbn = {9798400715204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3732945.3733004},
doi = {10.1145/3732945.3733004},
abstract = {In recent years, diffusion models have shown great potential in the field of robotics. This paper focuses on the research of robot foundation models based on diffusion models, with a particular emphasis on the Robotics Diffusion Transformer (RDT). First, it reviews the technological frontier of robot basic action models, including visual language models and robot action learning. Then, it details the RDT model, covering its design for an ALOHA dual - arm robot, core methods such as diffusion model design, feature coding, regularization, non - linear action output, and Alternating Condition Injection (ACI). The RDT model is trained on a large number of datasets and fine - tuned on specific data. Experimental results demonstrate that RDT outperforms previous models in tasks such as generalization to unseen objects and scenes, few - shot learning, and dexterous control. Ablation experiments further verify the effectiveness of its key design elements. Overall, RDT proves the significance of generative models in embodied intelligence, showing that appropriate model design and data - driven training can achieve excellent results.},
booktitle = {Proceedings of the 2025 4th International Conference on Intelligent Systems, Communications and Computer Networks},
pages = {402–409},
numpages = {8},
keywords = {Diffusion model, Robot action learning, Robot foundation model, Robotics Diffusion Transformer (RDT), Visual language model},
location = {
},
series = {ISCCN '25}
}

@inproceedings{10.1145/3716554.3716555,
author = {Mendes, Mara and Ali, Mohsan and Charalabidis, Yannis},
title = {Analyzing the Freedom of Information Requests Using Topic Modeling: Towards User-driven Open Government Data Ecosystems},
year = {2025},
isbn = {9798400713170},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716554.3716555},
doi = {10.1145/3716554.3716555},
abstract = {Open data has become an important player in promoting government transparency and accountability around the globe. Governments have established policies to provide citizens or users with access to data. One way to obtain such data is through Freedom of Information Act (FOIA) requests. Considering users' requests for data and responding with the required information makes open data ecosystems user-driven. This paper explores freedom of information (FOI) requests received by Germany's Ministry of Health as a user-driven approach to open government data (OGD). By applying advanced natural language processing (NLP) techniques, specifically Latent Dirichlet Allocation (LDA) and BERTopic, we analyze a large corpus of FOI requests to identify key topics and trends in public inquiries. The findings reveal valuable insights into citizens' information demands and demonstrate the strengths of these NLP methods in extracting actionable patterns. In this way, governments can easily understand user needs in health-related datasets (this study, in particular, focuses on requests made by citizens related to COVID-19 data). This study lays the groundwork in two ways: first, by understanding and thematically categorizing user needs related to open data, and second, by improving government data transparency, informing the prioritization of dataset releases, and supporting evidence-based policymaking.},
booktitle = {Proceedings of the 28th Pan-Hellenic Conference on Progress in Computing and Informatics},
pages = {1–10},
numpages = {10},
keywords = {Freedom of Information, Open Data, Open Data Ecosystems, Open Government Data, Topic Modeling},
location = {
},
series = {PCI '24}
}

@inproceedings{10.1145/3610978.3640642,
author = {Saracco, Alessandro and Lillo, Alberto and Stranisci, Marco and Gena, Cristina},
title = {Human Robot Interaction through an Ontology-based Dialogue Engine},
year = {2024},
isbn = {9798400703232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610978.3640642},
doi = {10.1145/3610978.3640642},
abstract = {This paper outlines the evolution of the Sugar, Salt, \&amp; Pepper project for high level functioning children affected by autism, focusing on the development of a dialogue system that relies on an ontology-based knowledge base. The ontology offers a formal representation of knowledge and interrelationships within the movie domain. The dialogue system addresses issues related to predefined answers, emphasizing adaptability for multi-platform use, particularly in the context of the social robot Pepper. The research covers detailed phases of construction and development, highlighting implementation choices and challenges faced.},
booktitle = {Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {940–944},
numpages = {5},
keywords = {dialog, human-robot interaction, machine learning, ontology},
location = {Boulder, CO, USA},
series = {HRI '24}
}

@inproceedings{10.1145/3592813.3592893,
author = {Campos, J\'{u}lio Gon\c{c}alves and De Almeida, Vitor Pinheiro and De Armas, Elvismary Molina and Da Silva, Geiza Maria Hamazaki and Corseuil, Eduardo Thadeu and Gonzalez, Fernando Rodrigues},
title = {INSIDE: an Ontology-based Data Integration System Applied to the Oil and Gas Sector},
year = {2023},
isbn = {9798400707599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3592813.3592893},
doi = {10.1145/3592813.3592893},
abstract = {Context: Data integration remains a major challenge facing organizations in the information age. Despite the advances made in recent decades, new approaches have become necessary to deal with new challenges such as Big Data. Problem: Semantic heterogeneity is a significant problem faced by companies in the oil and gas sector, as it makes it difficult to exchange information with other companies. Furthermore, there is a shortage of data integration systems that use open source technologies to deal with semantics, interoperability and scalability. Solution: INSIDE - Semantic Interoperability for Engineering Data Integration - an information system based on ontologies for data integration developed for an oil and gas company. SI Theory: This work is influenced by Representation Theory, based on the idea that an information system is a faithful representation of certain phenomena in the real world. Method: Review of state of the art on system architectures for data integration and use of methodologies for elaborating ontologies that represent the knowledge base of the information system. Summary of Results: Implementation of a prototype that allows querying heterogeneous data sources using a vocabulary familiar to the user, removing ambiguities from data with semantics. Contributions and Impact in IS area: The development of a solution for data integration using open source technologies tested with real-world data from a company in the oil and gas sector that can serve as a reference for developing new applied systems to other sectors.},
booktitle = {Proceedings of the XIX Brazilian Symposium on Information Systems},
pages = {94–101},
numpages = {8},
keywords = {Data Integration, Information Systems, Ontology., Semantic Web},
location = {Macei\'{o}, Brazil},
series = {SBSI '23}
}

@inproceedings{10.1145/3652620.3688208,
author = {Weber, Thomas and Ojha, Monalisha and Sadeghi, Mohammad and K\"{o}nig, Lars and Armbruster, Martin and Lange, Arne and Burger, Erik and Atkinson, Colin},
title = {Towards Deep Reactions in Multi-Level, Multi-View Modeling},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688208},
doi = {10.1145/3652620.3688208},
abstract = {As the scale, complexity, and scope of software-intensive systems continue to grow, so does the importance of synergistically integrating two important emerging paradigms in software engineering - multi-level modeling and multi-view modeling. While stable tooling for both has been developed by research institutions in recent years, to date no tool has attempted to integrate the two at a fundamental level. In this paper, we describe some first steps we have taken in this direction by integrating the Vitruvius V-SUM-based multi-view environment with the Melanee multi-level modeling environment. In particular, we show how Vitruvius's Reactions language, which allows different models in Vitruvius V-SUMs to be kept consistent, can be extended to support multi-level V-SUMs and views represented in Melanee's dialect of multi-level modeling.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {760–769},
numpages = {10},
keywords = {multi-level modeling, V-SUM, view-based modeling, vitruvius, consistency},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@article{10.1145/3744754,
author = {Ahmad, Pir Noman and Ullah, Inam and M. Salim, Nagwa and Kumar Singh, Sushil and Jiang, Weiwei and Al-Khasawneh, Mahmoud Ahmad and Daradkeh, Yousef Ibrahim},
title = {Deep Neural Network-Based Feature Encoding for Automated Health Monitoring Using Large AI Models in Online Communication Systems},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3744754},
doi = {10.1145/3744754},
abstract = {The hybrid model combines deep neural networks (DNN) and large AI models, such as large language models (LLM), for enhanced clinical information retrieval (CIR) from electronic clinical records (ECR). While LLMs show promise for encoding complex medical data, they face challenges in user-dependent information, such as patient reports with encoded knowledge, accessing real-time data, and requiring extensive fine-tuning for clinical decision-making in online communication systems. To overcome these limitations, we introduce a Transformer-based Sequence (TBS) multimodal method that integrates representation learning with human expertise to encode and analyze intricate relationships within clinical data. This model improves predictive tasks and medical search accuracy, achieving F1-scores of 0.83-0.80, and outperforms baseline methods. Integrating AI-driven methodologies in healthcare has the potential to transform medical record analysis and utilization, resulting in enhanced patient outcomes and more personalized healthcare solutions.},
note = {Just Accepted},
journal = {ACM Trans. Internet Things},
month = jun,
keywords = {Deep neural network, communication systems, healthcare, large AI models, transformer-based sequence}
}

@inproceedings{10.1145/3723366.3723370,
author = {Wang, Yukang and Yang, Mengyu and Chen, Yu and Yu, Mingjie},
title = {Research on Intelligent Course Q&amp;A Systems Based on NLP Models, Knowledge Graphs, and Deep Learning Methods},
year = {2025},
isbn = {9798400718298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3723366.3723370},
doi = {10.1145/3723366.3723370},
abstract = {With the increasing demand for personalized and interactive learning in online education environments. intelligent question-answering (Q&amp;A) systems have emerged as powerful tools to enhance student engagement and support independent learning. However, traditional educational Q&amp;A systems face significant limitations, such as reliance on simple keyword matching, inability to provide contextually rich answers, and challenges in handling complex, procedural queries. These limitations often result in incomplete or irrelevant responses, highlighting the need for more sophisticated solutions that can understand and interpret diverse student queries accurately. This research presents the design and implementation of an intelligent course Q&amp;A system that integrates Natural Language Processing (NLP), semantic analysis, and knowledge graph traversal to deliver context-aware, real-time responses to student queries. The system is built upon advanced deep learning techniques, leveraging BERT for answer retrieval, a Bi-LSTM + CRF model for Named Entity Recognition (NER), and a dynamically constructed knowledge graph for multi-step reasoning. The system was evaluated on a dataset of student queries and course content, demonstrating strong performance with an overall accuracy of 88.1\%. The results highlight the system's effectiveness in handling factual, conceptual, and procedural questions, offering personalized and relevant answers to enhance the learning experience. However, challenges remain in addressing ambiguous queries, scaling the knowledge graph, and providing detailed procedural explanations. The paper concludes by discussing potential future improvements, such as the integration of interactive dialogue mechanisms, optimized knowledge graphs, and enhanced explanatory models. This intelligent Q&amp;A system represents a step forward in educational technology, offering scalable and adaptable solutions for modern digital learning environments.},
booktitle = {Proceedings of the 2024 4th International Symposium on Big Data and Artificial Intelligence},
pages = {21–28},
numpages = {8},
keywords = {Deep Learning, Educational Technology, Knowledge Graph, Natural Language Processing, Question-Answering System},
location = {
},
series = {ISBDAI '24}
}

@inproceedings{10.1145/3318464.3383128,
author = {undefinedzcan, Fatma and Quamar, Abdul and Sen, Jaydeep and Lei, Chuan and Efthymiou, Vasilis},
title = {State of the Art and Open Challenges in Natural Language Interfaces to Data},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3383128},
doi = {10.1145/3318464.3383128},
abstract = {Recent advances in natural language understanding and processing resulted in renewed interest in natural language based interfaces to data, which provide an easy mechanism for non-technical users to access and query the data. While early systems only allowed simple selection queries over a single table, some recent work supports complex BI queries, with many joins and aggregation, and even nested queries. There are various approaches in the literature for interpreting user's natural language query. Rule-based systems try to identify the entities in the query, and understand the intended relationships between those entities. Recent years have seen the emergence and popularity of neural network based approaches which try to interpret the query holistically, by learning the patterns. In this tutorial, we will review these natural language interface solutions in terms of their interpretation approach, as well as the complexity of the queries they can generate. We will also discuss open research challenges.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2629–2636},
numpages = {8},
keywords = {conversation systems, natural language interfaces, natural language query},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3383313.3418478,
author = {Twomey, Niall and Fain, Mikhail and Ponikar, Andrey and Sarraf, Nadine},
title = {Towards Multi-Language Recipe Personalisation and Recommendation},
year = {2020},
isbn = {9781450375832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383313.3418478},
doi = {10.1145/3383313.3418478},
abstract = {Multi-language recipe personalisation and recommendation is an under-explored field of information retrieval in academic and production systems. The existing gaps in our current understanding are numerous, even on fundamental questions such as whether consistent and high-quality recipe recommendation can be delivered across languages. Motivated by this need, we consider the multi-language recipe recommendation setting and present grounding results that will help to establish the potential and absolute value of future work in this area. Our work draws on several billion events from millions of recipes, with published recipes and users incorporating several languages, including Arabic, English, Indonesian, Russian, and Spanish. We represent recipes using a combination of normalised ingredients, standardised skills and image embeddings obtained without human intervention. In modelling, we take a classical approach based on optimising an embedded bi-linear user-item metric space towards the interactions that most strongly elicit cooking intent. For users without interaction histories, a bespoke content-based cold-start model that predicts context and recipe affinity is introduced. We show that our approach to personalisation is stable and scales well to new languages. A robust cross-validation campaign is employed and consistently rejects baseline models and representations, strongly favouring those we propose. Our results are presented in a language-oriented (as opposed to model-oriented) fashion to emphasise the language-based goals of this work. We believe that this is the first large-scale work that evaluates the value and potential of multi-language recipe recommendation and personalisation.},
booktitle = {Proceedings of the 14th ACM Conference on Recommender Systems},
pages = {708–713},
numpages = {6},
keywords = {information retrieval, personalisation, recipes and food modelling, recommendation},
location = {Virtual Event, Brazil},
series = {RecSys '20}
}

@inproceedings{10.5555/3716662.3716767,
author = {Rathje, William},
title = {Learning When Not to Measure: Theorizing Ethical Alignment in LLMs},
year = {2025},
publisher = {AAAI Press},
abstract = {LLMs and other forms of generative AI have shown immense promise in producing highly accurate epistemic judgements in domains as varied as law, education, and medicine - with GPT notably passing the legal Bar exam and various medical licensing exams. The safe extension of LLMs into safety-critical professional domains requires assurance not only of epistemic but ethical alignment. This paper adopts a theoretical and philosophical approach, drawing from metaethical theories to argue for a distinction hinging around quantitative, axiological comparability that separates Kantian ethics from not only the utilitarianism it is well-known to oppose, but from just distribution theories as well, which are key to debiasing LLM models. It presents the novel hypothesis that LLM ethical acquisition from both corpus induction and RLHF may encounter value conflicts between Kantian and just distribution principles that intensify as they come into improved alignment with both theories, hinging around the variability by which self-attention may statistically attend to the same characterizations as more person-like or more resource-like under distinct prompting strategies.},
booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {1190–1199},
numpages = {10},
location = {San Jose, California, USA},
series = {AIES '24}
}

@article{10.1145/3555719,
author = {Park, Eun Hee and Storey, Veda C.},
title = {Emotion Ontology Studies: A Framework for Expressing Feelings Digitally and its Application to Sentiment Analysis},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3555719},
doi = {10.1145/3555719},
abstract = {Emotion ontologies have been developed to capture affect, a concept that encompasses discrete emotions and feelings, especially for research on sentiment analysis, which analyzes a customer's attitude towards a company or a product. However, there have been limited efforts to adapt and employ these ontologies. This research surveys and synthesizes emotion ontology studies to develop a Framework of Emotion Ontologies that can be used to help a user select or design an appropriate emotion ontology to support sentiment analysis and increase the user's understanding of the roles of affect, context, and behavioral information with respect to sentiment. The framework, which is derived from research on emotion ontologies, psychology, and sentiment analysis, classifies emotion ontologies as discrete emotion or one of two hybrid ontologies that are combinations of the discrete, dimensional, or componential process emotion paradigms. To illustrate its usefulness, the framework is applied to the development of an emotion ontology for a sentiment analysis application.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {181},
numpages = {38},
keywords = {Ontology, sentiment analysis, affect, emotion, Framework of Emotion Ontologies, discrete emotion ontology, dimensional emotion ontology, componential process ontology}
}

@article{10.1145/3594719,
author = {Chakraborty, Chinmay and Wan, Shaohua and Khosravi, Mohammad R.},
title = {Editorial: Ontology-based Knowledge Presentation and Computational Linguistics for Semantic Big Social Data Analytics in Asian Social Networks},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3594719},
doi = {10.1145/3594719},
abstract = {Data-driven ontology-based knowledge (OK) presentation and computational linguistics for evolving semantic Asian social networks (ASNs) can make one of the most important platforms that provide robust and real-time data mapping in massive access across the heterogeneous big data sources in the web that is named OK-ASN. It benefits from computational intelligence, web-of-things (WoT) architecture, semantic features, statistical learning and pattern recognition, database management, computer vision, cyber-security, and language processing. OK-ASN is a critical strategy for WoT big data mining and enterprises from social media to medical and industrial sectors.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {136},
numpages = {3},
keywords = {Social data, ontology, knowledge graph, computational linguistics, Asian social networks}
}

@inproceedings{10.1145/3401895.3402064,
author = {Henarejos-Blasco, Jos\'{e} and Garc\'{\i}a-D\'{\i}az, Jos\'{e} Antonio and Apolinario-Arzube, \'{O}scar and Valencia-Garc\'{\i}a, Rafael},
title = {CNL-RDF-query: a controlled natural language interface for querying ontologies and relational databases},
year = {2021},
isbn = {9781450377119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3401895.3402064},
doi = {10.1145/3401895.3402064},
abstract = {The most commonly used search engines do not always show the information that the user needs, because they do not take into account factors such as the context or natural language ambiguity. Therefore, other types of search engines that considered these factors emerged in last few years, such as question-answering systems or semantic web-based search engines. In this work, we present CNL-RDF-Query, a controlled natural language interface for querying rdf-based ontologies and relational databases. The system guides users in the construction of queries with the knowledge of domain ontology. Our proposal has been tested in the domain of the IMDb movie repository.},
booktitle = {Proceedings of the 10th Euro-American Conference on Telematics and Information Systems},
articleno = {35},
numpages = {5},
keywords = {SPARQL, natural language interface, ontologies, user interfaces},
location = {Aveiro, Portugal},
series = {EATIS '20}
}

@inproceedings{10.1145/3723178.3723230,
author = {Arnob, Arjun Kumar Bose and Naim, Mostaque Ahammed and Rezwan, Md Tahmid and Hasan, Mohammad Mahmudul},
title = {Utilizing Kidney Ontology for Data-Driven Exploration of Potential Biomarkers in Kidney Diseases: Introducing the Kidney Diseases Biomarker Ontology (KDBO)},
year = {2025},
isbn = {9798400713828},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3723178.3723230},
doi = {10.1145/3723178.3723230},
abstract = {This study proposes a new method of identifying possible kidney disease biomarkers using Kidney Diseases Biomarker Ontology (KDBO). This study classically merges clinical imaging, biopsy data, proteomics, and genomic data derived from various sources through the Kidney Development Subontology (KDSO) of Gene Ontology (GO). Machine learning algorithms, network analysis, and statistical approaches are used to identify novel biomarkers that have implications for renal impairment at different stages of the disease have been employed here. A comparison between the new upstart markers found and the available right markers was made in patient cohorts to confirm their accuracy. The study showed that employing kidney ontology improves diagnosis, prognosis, and treatment by facilitating earlier identification of dysfunction, precise evaluation of disease severity levels, tailored choice therapy, addressing issues related to renal ontologies, and presenting suggestions for future studies.},
booktitle = {Proceedings of the 3rd International Conference on Computing Advancements},
pages = {391–398},
numpages = {8},
keywords = {Kidney Ontology, Biomarkers, Data Integration, KDBO, Renal Disease, Chronic Kidney Disease (CKD)},
location = {
},
series = {ICCA '24}
}

@inproceedings{10.1145/3569902.3569947,
author = {Cerqueira, Jorsiele},
title = {An Ontology for Context-aware Middleware for Dependable Medical Systems},
year = {2023},
isbn = {9781450397377},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3569902.3569947},
doi = {10.1145/3569902.3569947},
abstract = {In healthcare systems, there is an ecosystem of heterogeneous biosensors. A middleware is required for transmitting and establishing dependable communication with multiple integrations and information exchange through messages. However, in environments in which a distributed and dynamic network exists, a purely traditional middleware would not minimize the effect failures at data transmission and network congestion can cause. To preserve data integrity and provide relevant services as the environment changes, the use of context-aware middleware is recommended. This article describes an ontology for context-aware middleware to handle challenges faced by medical system networks and environment changes.},
booktitle = {Proceedings of the 11th Latin-American Symposium on Dependable Computing},
pages = {79–83},
numpages = {5},
keywords = {context aware middleware, dependable, modelling, ontology},
location = {Fortaleza/CE, Brazil},
series = {LADC '22}
}

@inproceedings{10.1145/3191697.3214330,
author = {Sharpe, Oli},
title = {Semprola: a semiotic programming language},
year = {2018},
isbn = {9781450355131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3191697.3214330},
doi = {10.1145/3191697.3214330},
abstract = {Most people interested in developing new programming languages or programming environments are looking at how to improve the syntax and semantics of the program text or at tools that help make programmers more productive at crafting the program text. What we need is a more fundamental change to the conception of what a program is. This paper introduces a new, Semiotic Programming environment in which we program with signs in a context, rather than with symbols in a text file and where we treat dialogue rather than functions as the dominant organising principle of our code. All of the information held in this environment is managed in a distributed, semiotic graph that is organized into multiple ontological spaces. Taken together these enable our programs and data to have greater semantic depth. Finally the paper gives a brief introduction to Semprola, a Semiotic Programming Language that can be used in this Semiotic Programming environment.},
booktitle = {Companion Proceedings of the 2nd International Conference on the Art, Science, and Engineering of Programming},
pages = {202–213},
numpages = {12},
keywords = {Compile time semantics, computational referent, context, dialogue, distributed graph, messaging, multiple ontologies, nodedge, programming languages, referent, semantic depth, semantics, semiotic programming, semiotics, semprola, sign, signified, signifier, spuid},
location = {Nice, France},
series = {Programming '18}
}

@inproceedings{10.1145/3459637.3482491,
author = {Xie, Chenhao and Huang, Wenhao and Liang, Jiaqing and Huang, Chengsong and Xiao, Yanghua},
title = {WebKE: Knowledge Extraction from Semi-structured Web with Pre-trained Markup Language Model},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482491},
doi = {10.1145/3459637.3482491},
abstract = {The World Wide Web contains rich up-to-date information for knowledge graph construction. However, most current relation extraction techniques are designed for free text and thus do not handle well semi-structured web content. In this paper, we propose a novel multi-phase machine reading framework, called WebKE. It processes the web content on different granularity by first detecting areas of interest at DOM tree node level and then extracting relational triples for each area. We also propose HTMLBERT as an encoder the web content. It is a pre-trained markup language model that fully leverages the visual layout information and DOM-tree structure, without the need of hand engineered features. Experimental results show that the proposed approach outperforms state-of- the-art methods by a considerable gain. The source code is available at https://github.com/redreamality/webke.},
booktitle = {Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management},
pages = {2211–2220},
numpages = {10},
keywords = {htmlbert, knowledge extraction, knowledge graph construction, pre-trained markup language model, relation extraction, semi-structured web extraction, webke},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3340555.3356097,
author = {Huang, Heyan and Mao, Xianling and Yang, Puhai},
title = {Streamlined Decoder for Chinese Spoken Language Understanding},
year = {2019},
isbn = {9781450368605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340555.3356097},
doi = {10.1145/3340555.3356097},
abstract = {As a critical component of Spoken Dialog System (SDS), spoken language understanding (SLU) attracts a lot of attention, especially for methods based on unaligned data. Recently, a new approach has been proposed that utilizes the hierarchical relationship between act-slot-value triples. However, it ignores the transfer of internal information which may record the intermediate information of the upper level and contribute to the prediction of the lower level. So, we propose a novel streamlined decoding structure with attention mechanism, which uses three successively connected RNN to decode act, slot and value respectively. On the first Chinese Audio-Textual Spoken Language Understanding Challenge (CATSLU), our model exceeds state-of-the-art model on an unaligned multi-turn task-oriented Chinese spoken dialogue dataset provided by the contest.},
booktitle = {2019 International Conference on Multimodal Interaction},
pages = {516–520},
numpages = {5},
keywords = {attention mechanisms, long short term memory networks, pointer network, spoken dialog system, spoken language understanding, streamlined decoder},
location = {Suzhou, China},
series = {ICMI '19}
}

@inproceedings{10.1145/3368691.3368700,
author = {Joy, Jeevamol and Raj, Nisha S and G, Renumol V},
title = {An ontology model for content recommendation in personalized learning environment},
year = {2019},
isbn = {9781450372848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368691.3368700},
doi = {10.1145/3368691.3368700},
abstract = {Personalized Learning Environments (PLEs) are expected to enhance the learning experience by providing tailor-made services based on learner preferences. It is of utmost importance to provide a personalized system which can automatically adapt to learners' learning styles, knowledge level and intelligently recommend resources that would favor and improve the learning. The existing PLEs still exhibit cold-start problems and other issues related with mapping of learning style and learning object. To solve these issues and to improve the dynamicity of the PLEs, an appropriate learner/learning object model is very essential. In this paper, we introduce an ontology model which encapsulates both learner profile and learning object attributes, which can be used for the content recommendation in an e-learning platform. Learner profile is the representation of learner data which includes both static and dynamic characteristics of the learner. The static data is gathered directly from the learner using forms and questionnaires and dynamic data is collected by tracking the behavior of learners, while interacting through a learning management system. The proposed ontology also holds space for learning topics and their corresponding Learning Object (LO) characteristics. The elements which come under the educational category of IEEE LOM standard, is considered for tagging the selected learning objects in the ontology. The JENA API of Java programming language is used for developing the ontology. The data is described using Resource Description Framework (RDF) tools. We have developed an ontology model consisting of an adaptive learner profile and standard LO characteristics which can be used in content recommender systems of e-learning environment.},
booktitle = {Proceedings of the Second International Conference on Data Science, E-Learning and Information Systems},
articleno = {9},
numpages = {6},
keywords = {content recommendation, learner profile, learning object, ontology, personalized learning environment},
location = {Dubai, United Arab Emirates},
series = {DATA '19}
}

@article{10.1145/3712008,
author = {Burgue\~{n}o, Lola and Di Ruscio, Davide and Sahraoui, Houari and Wimmer, Manuel},
title = {Automation in Model-Driven Engineering: A Look Back, and Ahead},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3712008},
doi = {10.1145/3712008},
abstract = {Model-Driven Engineering (MDE) provides a huge body of knowledge of automation for many different engineering tasks, especially those involving transitioning from design to implementation. With the huge progress made in AI, questions arise about the future of MDE, such as how existing MDE techniques and technologies can be improved or how other activities that currently lack dedicated support can also be automated. However, at the same time, it has to be revisited where and how models should be used to keep the engineers in the loop for creating, operating, and maintaining complex systems. To trigger dedicated research on these open points, we discuss the history of automation in MDE and present perspectives on how automation in MDE can be further improved and which obstacles have to be overcome in both the medium and long-term.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {122},
numpages = {25},
keywords = {Model-Driven Engineering (MDE), automation}
}

@inproceedings{10.1145/3374587.3374603,
author = {Xiaohui, Chen and Yinzhen, Liu and Li, Xu and Lei, Ge and Yiwei, Ma},
title = {The Construction Method of Geographic Knowledge Graph Ontology Model Based on GML},
year = {2020},
isbn = {9781450376273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374587.3374603},
doi = {10.1145/3374587.3374603},
abstract = {Geographic ontology model is the conceptual model of geographic knowledge graph and the logical basis for constructing the pattern layer of geographic knowledge graph. In the classification of geographic ontology research, geographic ontology model is in the category of domain ontology. It is a set of abstract structures to express ontology according to the spatial location, attribute characteristics and relational characteristics of geographic data. This paper discussed the logical components and architecture of geographic ontology, designed the geographic ontology model reference to GML, described the model using OWL language, and constructed the geographic ontology model based on GML. The geographic ontology model comprises three sub-models: element model, geometric model and spatial relation model. Finally, based on Prot\'{e}g\'{e} ontology construction tool, this paper designed the semantic description of geographic entity and realized the construction of geographic ontology system.},
booktitle = {Proceedings of the 2019 3rd International Conference on Computer Science and Artificial Intelligence},
pages = {138–143},
numpages = {6},
keywords = {GML, Geographic knowledge graph, geographic ontology, logical composition, ontology model construction},
location = {Normal, IL, USA},
series = {CSAI '19}
}

@inproceedings{10.1145/3410992.3410996,
author = {Noura, Mahda and Heil, Sebastian and Gaedke, Martin},
title = {Natural language goal understanding for smart home environments},
year = {2020},
isbn = {9781450387583},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410992.3410996},
doi = {10.1145/3410992.3410996},
abstract = {One of the main challenges of the Internet of Things (IoT) is to enable end-users without technical experience to use, control or monitor smart devices. However, enabling end-users to interact with these smart devices in an intuitive and natural way becomes increasingly important as they become more pervasive in our homes, workplaces and public environments. Voice-based interfaces are the emerging trend to provide a more natural human-device interaction in smart environments. Such interfaces require Natural Language Understanding (NLU) approaches to identify the meaning of end-users' voice inputs. Designing voice interfaces that are not limited to a small, fixed set of pre-defined commands is far from trivial. Existing voice-based solutions in the smart home domain either restrict the end-users to follow a strict language pattern, do not support indirect goals, require a large training dataset, or need a voice assistant located in the cloud. In this paper, we propose an approach for understanding end-users goals from voice inputs in smart homes. Our approach alleviates the need for end-users to learn or remember concrete operations of the devices and specific words/pattern structures rather it enables them to control their smart homes based on the desired goals (effects). We evaluate the approach through application to a collection of 253 goals from real end-users and report on quality metrics. The results demonstrate that our solution provides a good accuracy, high precision and acceptable recall for understanding end-users goals in the smart home domain.},
booktitle = {Proceedings of the 10th International Conference on the Internet of Things},
articleno = {1},
numpages = {8},
keywords = {goal recognition, internet of things, natural language understanding, smart home, voice interface},
location = {Malm\"{o}, Sweden},
series = {IoT '20}
}

@inproceedings{10.1145/3613372.3613380,
author = {Santos J\'{u}nior, Paulo S\'{e}rgio and Almeida, Jo\~{a}o Paulo A. and Barcellos, Monalessa},
title = {Towards Federated Ontology-Driven Data Integration in Continuous Software Engineering},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613372.3613380},
doi = {10.1145/3613372.3613380},
abstract = {Organizations have adopted Continuous Software Engineering (CSE) practices aiming at making software development faster, iterative, integrated, continuous, and aligned with the business. In this context, they often use different applications (e.g., project management tools, source repositories, and quality assessment tools) that store valuable data to support daily activities and decision-making. However, data items often remain spread in different applications that adopt different data and behavioral models, posing a barrier to integrated data usage. As a consequence, data-driven software development is uncommon, missing valuable opportunities for product and process improvement. In this paper, we explore an ontology network addressing CSE aspects to develop a data integration solution in which networked ontologies are the basis to build reusable and autonomous software components that work together in a system federation to provide meaningful integrated data. We achieve a comprehensive and flexible solution that can be used as a whole or partially, by extracting only the components related to the subdomains of interest.},
booktitle = {Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
pages = {31–36},
numpages = {6},
keywords = {Continuous Software Engineering, Data Integration, Ontology},
location = {Campo Grande, Brazil},
series = {SBES '23}
}

@inbook{10.1145/3191315.3191321,
author = {De Cat, Broes and Bogaerts, Bart and Bruynooghe, Maurice and Janssens, Gerda and Denecker, Marc},
title = {Predicate logic as a modeling language: the IDP system},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan \&amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191321},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {279–323},
numpages = {45}
}

@inproceedings{10.1145/3699682.3728348,
author = {Sampaio de Alencar, Rafaella and Demirtas, Mehmet Arif and Saha, Adittya Soukarjya and Shi, Yang and Brusilovsky, Peter},
title = {Integrating Expert Knowledge With Automated Knowledge Component Extraction for Student Modeling},
year = {2025},
isbn = {9798400713132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3699682.3728348},
doi = {10.1145/3699682.3728348},
abstract = {Knowledge tracing is a method to model students’ knowledge and enable personalized education in many STEM disciplines such as mathematics and physics, but has so far still been a challenging task in computing disciplines. One key obstacle to successful knowledge tracing in computing education lies in the accurate extraction of knowledge components (KCs), since multiple intertwined KCs are practiced at the same time for programming problems. In this paper, we address the limitations of current methods and explore a hybrid approach for KC extraction, which combines automated code parsing with an expert-built ontology. We use an introductory (CS1) Java benchmark dataset to compare its KC extraction performance with the traditional extraction methods using a state-of-the-art evaluation approach based on learning curves. Our preliminary results show considerable improvement over traditional methods of student modeling. The results indicate the opportunity to improve automated KC extraction in CS education by incorporating expert knowledge into the process.},
booktitle = {Proceedings of the 33rd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {307–312},
numpages = {6},
keywords = {knowledge components, computing education, student modeling, intelligent tutoring systems, learning curves},
location = {
},
series = {UMAP '25}
}

@inproceedings{10.1145/3688671.3688735,
author = {Dimitropoulos, Konstantinos and Hatzilygeroudis, Ioannis},
title = {An Ontology-Knowledge Graph Based Context Representation Scheme for Robotic Problems},
year = {2024},
isbn = {9798400709821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3688671.3688735},
doi = {10.1145/3688671.3688735},
abstract = {Context representation is a crucial part of a variety of robotic and other applications. Context refers to the environment, the (robotic or other) tasks as well as human-machine/environment interactions. One of the most utilized methods for representing context knowledge is ontologies. An ontology offers among others a highly descriptive structured representation and capabilities for inconsistencies checking and querying. Another well-known method is knowledge graphs, which offers a relation-based visualizable structure and querying capability, which are more convenient than those in ontologies. However, knowledge graphs cannot check inconsistencies. Therefore, we present an approach for context representation development, where an ontology is first created, checked and evaluated, and afterwards is converted into a knowledge graph. This approach assures design and implementation of more convenient and consistent context representations. The approach is applied to the creation of a robotics related ontology, where Prot\'{e}g\'{e} is used for ontology creation and logical consistency checking (via HermiT reasoner), OOPS! is used for ontology evaluation, and Neo4j is used for converting the ontology to a knowledge graph. Example queries in both representations show the preferability to knowledge graph, while an example shows the inability of knowledge graph to trace inconsistencies.},
booktitle = {Proceedings of the 13th Hellenic Conference on Artificial Intelligence},
articleno = {39},
numpages = {7},
keywords = {knowledge graphs, knowledge representation, ontologies, robot context representation},
location = {
},
series = {SETN '24}
}

@inproceedings{10.1145/3404663.3406876,
author = {Rencis, Edgars},
title = {Knowledge Extraction from Healthcare Data Using User-Adaptable Keywords-Based Query Language},
year = {2020},
isbn = {9781450377652},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404663.3406876},
doi = {10.1145/3404663.3406876},
abstract = {Nowadays, the volume of the information gathered by any organization increases more and more rapidly. It is essential to be able to use this information efficiently for it to benefit the operation of the organization. There is no point of gathering the information if it is not converted into knowledge. The knowledge extraction process becomes the backbone of any successful organization. Moreover, the extraction of the knowledge must be quick and efficient, so that the newly-obtained knowledge can be put in use at once. The problem addressed in this paper is how to allow the domain expert to extract the knowledge from their information systems themselves without involving the third party in the form of an IT specialist. This goal is of utmost importance for the domain experts, e.g. hospital managers and physicians, because they need to make decisions based on the available knowledge and to do it rapidly and efficiently. We propose a system in this paper that allows formulating queries in the natural language and that also adapts to the specifics of the user. Our experiments show that such kind of querying could provide an improvement in the decision-making process of healthcare professionals.},
booktitle = {Proceedings of the 2020 the 4th International Conference on Information System and Data Mining},
pages = {128–131},
numpages = {4},
keywords = {Natural language processing, hospital management, keywords-containing text, knowledge extraction, query language, query translation},
location = {Hawaii, HI, USA},
series = {ICISDM '20}
}

@article{10.1145/3439735,
author = {Hiebel, Gerald and Asp\"{o}ck, Edeltraud and Kopetzky, Karin},
title = {Ontological Modeling for Excavation Documentation and Virtual Reconstruction of an Ancient Egyptian Site},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3439735},
doi = {10.1145/3439735},
abstract = {In this article we introduce our semantic modeling approach for data from over 50 years of excavations at Tell el-Daba in Egypt. The CIDOC CRM with some of its extensions is used as an ontological framework to provide the semantics for creating a knowledge graph containing material remains, excavated areas, and documentation resources. An objective of the project A Puzzle in 4D is to digitize the documentation and create metadata for analog and digital resources in order to provide the data to the research community and facilitate future work for this important archaeological site. Using an example of 3D reconstruction of a tomb, we show how the knowledge graph linked to digital resources can be exploited for a specific task to encounter available information that is essential for a virtual reconstruction. Moreover, we show an approach of modeling to represent the interpretations supporting reconstructions as well as relate them to the sources used, thus providing transparency for the model and provenance data. Modeling for excavation documentation as well as virtual reconstruction has been tailored to the large amount of data processed from the project. The goal is to propose a semantic modeling feasible even on a large scale while still preserving the basic underlying ontological structures.},
journal = {J. Comput. Cult. Herit.},
month = jul,
articleno = {32},
numpages = {14},
keywords = {ARIADNE, CIDOC CRM, Cultural heritage, archaeology, interpretation, virtual reconstruction}
}

@article{10.1145/3719291,
author = {Diamantini, Claudia and Khan, Tarique and Potena, Domenico and Storti, Emanuele},
title = {Semantic Models of Performance Indicators: A Systematic Survey},
year = {2025},
issue_date = {August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3719291},
doi = {10.1145/3719291},
abstract = {Performance indicators and metrics are essential management tools. They provide synthetic objective measures to monitor the progress of a process, set objectives, and assess deviations, enabling effective decision-making. They can also be used for communication purposes, facilitating the sharing of objectives and results or improving the awareness of certain phenomena, thus motivating more responsible and sustainable behaviors. Given their strategic role, it is of paramount importance, as well as challenging, to guarantee that the intended meaning of an indicator is fully shared among stakeholders and that its implementation is aligned with the definition provided by decision makers, as this is a precondition for data quality and trustworthiness of the information system. Formal models, such as ontologies, have been long investigated in the literature to address the issues. This article proposes a comprehensive survey on semantic approaches aimed to specify conceptual definitions of indicators and metrics, illustrating also the advantages of these formal approaches in relevant use cases and application domains.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {202},
numpages = {37},
keywords = {KPI, performance indicators, indicators, semantic models, challenges}
}

@inproceedings{10.1145/3397056.3397083,
author = {Liu, Zhengjun and Sun, Zhi and Chen, Jianfeng and Zhou, Yujin and Yang, Tao and Yang, Hui and Liu, Jie},
title = {STIX-based Network Security Knowledge Graph Ontology Modeling Method},
year = {2020},
isbn = {9781450377416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397056.3397083},
doi = {10.1145/3397056.3397083},
abstract = {Network security incidents are complex and unstructured, making them difficult to understand and share. In this paper, we analyzes the commonality between structured threat information representation (STIX) and network security domain knowledge, and proposes a knowledge graph ontology modeling method of network security based on STIX. With the architecture knowledge of STIX, this method generates an ontology schema of network security knowledge graph, through classifying the concepts in the field of network security, describing the attributes of concepts and combing the relationships between concepts. The ontology schema has small redundancy and strong structural hierarchy, and can clearly display the structure of the attack activity and the mutual relationship. Therefore, it can help decision makers to understand security incidents more deeply, and help them make reasonable decisions and share cyber threat intelligence.},
booktitle = {Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis},
pages = {152–157},
numpages = {6},
keywords = {Knowledge Graph, Network Security, Ontology, STIX},
location = {Marseille, France},
series = {ICGDA '20}
}

@inproceedings{10.1145/3350546.3352558,
author = {Noura, Mahda and Gaedke, Martin},
title = {WoTDL: Web of Things Description Language for Automatic Composition},
year = {2019},
isbn = {9781450369343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350546.3352558},
doi = {10.1145/3350546.3352558},
abstract = {Enabling end users to take a proactive role in the development of Web of Things (WoT) applications that achieves their goals is a challenge for End User Development (EUD) in the context of WoT. This can be achieved through Artificial Intelligence (AI) planning algorithms if the relevant WoT concepts and relationships are described in an interoperable way. Although similar, existing service description languages like WSDL or ontologies like OWL-S are not sufficient to represent all required characteristics of concrete WoT planning scenarios. To address these limitations, in this paper we present the Web of Things Description Language (WoTDL) ontology which is an extension of existing WoT models to describe the key concepts of AI planning for automatic WoT composition. To demonstrate the feasibility of our approach, we follow the best practices recommended by the semantic web community and describe the physical devices of our smart home testbed in an AI planning scenario using WoTDL.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {413–417},
numpages = {5},
keywords = {AI Planning, Automatic Composition, Internet of Things, Ontology, Semantic Web, Web of Things},
location = {Thessaloniki, Greece},
series = {WI '19}
}

@inproceedings{10.1145/3696410.3714573,
author = {Tsai, Elisa and Mangaokar, Neal and Zheng, Boyuan and Zheng, Haizhong and Prakash, Atul},
title = {Harmful Terms and Where to Find Them: Measuring and Modeling Unfavorable Financial Terms and Conditions in Shopping Websites at Scale},
year = {2025},
isbn = {9798400712746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696410.3714573},
doi = {10.1145/3696410.3714573},
abstract = {Terms and conditions for online shopping websites often contain terms that can have significant financial consequences for customers. Despite their impact, there is currently no comprehensive understanding of the types and potential risks associated with unfavorable financial terms. Furthermore, there are no publicly available detection systems or datasets to systematically identify or mitigate these terms. In this paper, we take the first steps toward solving this problem with three key contributions.First, we introduce TermMiner, an automated data collection and topic modeling pipeline to understand the landscape of unfavorable financial terms. Second, we create ShopTC-100K, a dataset of terms and conditions from shopping websites in the Tranco top 100K list, comprising 1.8 million terms from 8,251 websites. Consequently, we develop a taxonomy of 22 types from 4 categories of unfavorable financial terms---spanning purchase, post-purchase, account termination, and legal aspects. Third, we build TermLens, an automated detector that uses Large Language Models (LLMs) to identify unfavorable financial terms.Fine-tuned on an annotated dataset, TermLens achieves an F1 score of 94.6\% and a false positive rate of 2.3\% using GPT-4o. When applied to shopping websites from the Tranco top 100K, we find that 42.06\% of these sites contain at least one unfavorable financial term, with such terms being more prevalent on less popular websites. Case studies further highlight the financial risks and customer dissatisfaction associated with unfavorable financial terms, as well as the limitations of existing ecosystem defenses.},
booktitle = {Proceedings of the ACM on Web Conference 2025},
pages = {990–1003},
numpages = {14},
keywords = {consumer protection, deceptive content, terms and conditions dataset, topic modeling, unfavorable financial terms},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3631700.3665234,
author = {Carta, Salvatore and Giuliani, Alessandro and Manca, Marco Manolo and Piano, Leonardo and Tiddia, Sandro Gabriele},
title = {Towards Zero-shot Knowledge Graph building: Automated Schema Inference},
year = {2024},
isbn = {9798400704666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631700.3665234},
doi = {10.1145/3631700.3665234},
abstract = {In the current Digital Transformation scenario, Knowledge Graphs are essential for comprehending, representing, and exploiting complex information in a structured form. The main paradigm in automatically generating proper Knowledge Graphs relies on predefined schemas or ontologies. Such schemas are typically manually constructed, requiring an intensive human effort, and are often sensitive to information loss due to negligence, incomplete analysis, or human subjectivity or inclination. Limiting human bias and the resulting information loss in creating proper Knowledge Graphs is paramount, particularly for user modeling in various sectors, such as education or healthcare. To this end, we propose a novel approach to automatically generating a proper entity schema. The devised methodology combines the language understanding capabilities of LLM with classical machine learning methods such as clustering to properly build an entity schema from a set of documents. This solution eliminates the need for human intervention and fosters a more efficient and comprehensive knowledge representation. The assessment of our proposal concerns adopting a state-of-the-art entity extraction model (UniNER) to estimate the relevance of the extracted entities based on the generated schema. Results confirm the potential of our approach, as we observed a negligible difference between the topic similarity score obtained with the ground truth and with the automatically generated schema (less than 1\% on average on three different datasets). Such an outcome confirms that the proposed approach may be valuable in automatically creating an entity schema from a set of documents.},
booktitle = {Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {467–473},
numpages = {7},
keywords = {Large Language Models, Named Entity Recognition, Ontology Learning},
location = {Cagliari, Italy},
series = {UMAP Adjunct '24}
}

@inproceedings{10.1145/3652620.3688567,
author = {Cederbladh, Johan and Zimmermann, Thomas C.},
title = {How does one Model Appropriately in Systems Engineering? An Initial Conceptual Model Framing Model Appropriateness},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688567},
doi = {10.1145/3652620.3688567},
abstract = {Appropriateness of models through modelling languages, tools, and methods is at the core of systems modelling. A concrete example is that useful abstraction depends on the contextual case of modelling purpose and need. Naturally, this results in different modelling formalisms and languages being appropriate at different stages of a common development process and for different stakeholders. In Model-Based Systems Engineering (MBSE) many stakeholders are involved in procedural modelling activities. Consequently, there is a need to identify appropriate modelling approaches at each modelling activity and development stage. Current MBSE adoption and application is still in early stages, and consequently lacking in overall modelling contextualisation. In this paper we discuss what facilitates an appropriate systems engineering model and how practitioners can reason about models in industrial contexts by providing an initial conceptual model of how model artefacts support governing systems engineering concerns.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {930–934},
numpages = {5},
keywords = {appropriate, model-based, systems engineering, decision-making},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3654522.3654607,
author = {Nguyen, Dien Thanh and Do, Nhon Van and Tran, Tung Hoang},
title = {Ontology-Based Solution for Designing knowledge Retrieval Systems in the Field of Artificial Intelligence},
year = {2024},
isbn = {9798400716713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654522.3654607},
doi = {10.1145/3654522.3654607},
abstract = {Currently, using domain knowledge and semantics in the field of artificial intelligence to conduct designing knowledge retrieval system has attracted great attention from researchers in many different communities. In this paper, we have improved the previous CK_ONTO model to improve the semantic representation technique of documents more effectively. This improved model is used for designing the knowledge querying system based on the ontology-base in education. Its foundation includes a model of concept relations between concepts and rules of the knowledge domain. Besides, we have proposed techniques and algorithms on New_Onto that are presented to solve the problems in this paper. Experimental results show that the knowledge query system works more effectively than the previous knowledge query system and is suitable for students who want to search for artificial intelligence documents for their learning and studying and is emerging for use in the real-world.},
booktitle = {Proceedings of the 2024 9th International Conference on Intelligent Information Technology},
pages = {558–563},
numpages = {6},
keywords = {Additional Key Words and Phrases: Ontology-base, artificial intelligence, document representation, graph matching, semantic search},
location = {Ho Chi Minh City, Vietnam},
series = {ICIIT '24}
}

@inproceedings{10.1145/3587259.3627574,
author = {Ram\'{o}n-Ferrer, Virginia and Badenes-Olmedo, Carlos and Corcho, Oscar},
title = {Automatic Topic Label Generation using Conversational Models},
year = {2023},
isbn = {9798400701412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587259.3627574},
doi = {10.1145/3587259.3627574},
abstract = {In probabilistic topic models, a topic is characterised by a set of words, with a probability associated to each of them. Even though it is not necessary to understand the meaning of topics to perform common downstream tasks where topic models are used, such as topic inference or document similarity, there have been attempts to uncover the semantics of topics by providing labels to them, consisting in a couple of concepts. In this paper we propose a methodology, Conversational Probabilistic Topic Labelling (CPTL), to study whether conversational models can be used to generate labels that describe probabilistic topics given their most representative keywords. We evaluate and compare the performance of a selection of conversational models for the topic label generation task with the performance of a task-specific language model trained to generate topic labels.},
booktitle = {Proceedings of the 12th Knowledge Capture Conference 2023},
pages = {17–24},
numpages = {8},
keywords = {conversational model, probabilistic topic labelling, topic label, topic label generation},
location = {Pensacola, FL, USA},
series = {K-CAP '23}
}

@inproceedings{10.1145/3551902.3551983,
author = {Waseeb, Shakirullah and Vrani\'{c}, Valentino},
title = {Toward Organizational Pattern Ontology},
year = {2023},
isbn = {9781450395946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551902.3551983},
doi = {10.1145/3551902.3551983},
abstract = {Organizational patterns of agile software development are proven practices for dealing organizational principles. Finding and selecting the right pattern is difficult. One way to select a pattern is to follow the sequence and compositions given in a pattern language. However, in general, pattern languages do not always reveal every reliable connection. Patterns are described in informal and unstructured text, making it difficult to understand their connections. This work attempts to provide a conceptual ontology model for organizational patterns – describing the collection of concepts (terms) and their relations. This contribution can be an attempt to express, expose, and share semantic knowledge between patterns using ontology. The resulting ontology can be used on top of the organizational patterns repositories to help retrieve patterns based on their logical connections.},
booktitle = {Proceedings of the 27th European Conference on Pattern Languages of Programs},
articleno = {20},
numpages = {8},
keywords = {knowledge base, ontology, organizational patterns, semantics},
location = {Irsee, Germany},
series = {EuroPLop '22}
}

@inproceedings{10.1145/3664476.3670916,
author = {Jorquera Valero, Jos\'{e} Mar\'{\i}a and L\'{o}pez Mart\'{\i}nez, Antonio and S\'{a}nchez S\'{a}nchez, Pedro Miguel and Navarro Mart\'{\i}nez, Daniel and Varas L\'{o}pez, Rodrigo and Rojo Lacal, Javier Ignacio and L\'{o}pez Vivar, Antonio and Sotelo Monge, Marco Antonio and Gil P\'{e}rez, Manuel and Mart\'{\i}nez P\'{e}rez, Gregorio},
title = {Unlocking the Potential of Knowledge Graphs: A Cyber Defense Ontology for a Knowledge Representation and Reasoning System},
year = {2024},
isbn = {9798400717185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664476.3670916},
doi = {10.1145/3664476.3670916},
abstract = {In today’s dynamic and complex warfare landscape, characterized by the convergence of traditional and emerging threats, the significance of cybersecurity in shaping modern conflicts cannot be overstated. Such trend presents a challenging paradigm shift in how military organizations approach mosaic warfare in the digital age since new attack vectors and targets appear in their landscapes. In this vein, it is pivotal for military teams to have a clear and concise roadmap for cybersecurity incidents linked to potential mosaic warfare. This manuscript introduces a novel approach to bolstering mosaic warfare strategies by integrating an advanced Knowledge Representation and Reasoning system and a tailored ontology. Motivated by the critical role of cybersecurity in contemporary warfare, the proposed system aims to enhance situational awareness, decision-making capabilities, and operational effectiveness in the face of evolving cyber threats. In this sense, this manuscript entails a new ontology that not only covers the cybersecurity realm but also introduces key concepts related to strategic and operational military levels at the same time. The ad-hoc ontology is also compared against other well-known ones, such as MITRE, NATO, or UCO approaches and manifests a significant performance by employing standardized quality metrics for ontologies. Lastly, a realistic mosaic warfare scenario is contextualized to demonstrate the deployment of the proposed system and how it can properly represent all information gathered from heterogeneous data sources.},
booktitle = {Proceedings of the 19th International Conference on Availability, Reliability and Security},
articleno = {73},
numpages = {9},
keywords = {Cyber defense, Knowledge graph, Knowledge representation, Mosaic warfare, Ontology, Reasoning},
location = {Vienna, Austria},
series = {ARES '24}
}

@article{10.1145/3626254,
author = {Bikakis, Antonis and Ferrario, Roberta and Jean, St\'{e}phane and Markhoff, B\'{e}atrice and Mosca, Alessandro and Asmundo, Marianna Nicolosi},
title = {Editorial: Special Issue on Semantic Web and Ontology Design for Cultural Heritage},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3626254},
doi = {10.1145/3626254},
journal = {J. Comput. Cult. Herit.},
month = nov,
articleno = {54},
numpages = {5},
keywords = {Ontologies, Knowledge Graphs, Cultural Heritage, Digitial Humanities}
}

@proceedings{10.1145/3708319,
title = {UMAP Adjunct '25: Adjunct Proceedings of the 33rd ACM Conference on User Modeling, Adaptation and Personalization},
year = {2025},
isbn = {9798400713996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@article{10.1145/3609484,
author = {Wei, Tong and Chen, Yuqi},
title = {A Ding Ontology of Chinese Bronze},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1556-4673},
url = {https://doi.org/10.1145/3609484},
doi = {10.1145/3609484},
abstract = {Ding is a significant type of Chinese bronze that holds key cultural value. Traditional humanists have primarily focused on dating and classifying Ding. However, in the context of Digital Humanities, the research perspective of humanities scholars is gradually shifting towards data-driven research, with linked data emerging as a popular topic. A well-defined and standard ontology representing the complete domain knowledge is essential for linked Ding data. Unfortunately, most existing ontology cannot represent fine-grained knowledge of Ding or is too restrictive to represent partial knowledge of bronze Ding. In this context, we propose a fine-grained Ding ontology to represent the bronze Ding knowledge. In this paper, we present in detail the Ding ontology of Chinese bronze during the Shang and Zhou dynasties (from 1600 BC to 256 BC). We provide a detailed exposition of the Ding ontology and evaluate its effectiveness using OOPS!, OntoMetrics, and by answering competency questions in SPARQL. The building methodology of Ding ontology follows the ISO principles (ISO 1087 and ISO 704). The objective of this paper is to develop an open ontology of Ding during the Shang and Zhou dynasties, which can serve as a valuable resource for bilingual terminology dictionaries. The Ding ontology was published at .},
journal = {J. Comput. Cult. Herit.},
month = aug,
articleno = {69},
numpages = {12},
keywords = {Cultural heritage, Ontology, Terminology, Digital humanities, Semantic Web}
}

@article{10.1145/3650041,
author = {Sekuli\'{c}, Ivan and Alinannejadi, Mohammad and Crestani, Fabio},
title = {Analysing Utterances in LLM-Based User Simulation for Conversational Search},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3650041},
doi = {10.1145/3650041},
abstract = {Clarifying underlying user information needs by asking clarifying questions is an important feature of modern conversational search systems. However, evaluation of such systems through answering prompted clarifying questions requires significant human effort, which can be time-consuming and expensive. In our recent work, we proposed an approach to tackle these issues with a user simulator, USi. Given a description of an information need, USi is capable of automatically answering clarifying questions about the topic throughout the search session. However, while the answers generated by USi are both in line with the underlying information need and in natural language, a deeper understanding of such utterances is lacking. Thus, in this work, we explore utterance formulation of large language model (LLM)–based user simulators. To this end, we first analyze the differences between USi, based on GPT-2, and the next generation of generative LLMs, such as GPT-3. Then, to gain a deeper understanding of LLM-based utterance generation, we compare the generated answers to the recently proposed set of patterns of human-based query reformulations. Finally, we discuss potential applications as well as limitations of LLM-based user simulators and outline promising directions for future work on the topic.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {62},
numpages = {22},
keywords = {User simulation, conversational search, mixed-initiative}
}

@inproceedings{10.1145/3534678.3539258,
author = {Liu, Fenglin and Yang, Bang and You, Chenyu and Wu, Xian and Ge, Shen and Woicik, Adelaide and Wang, Sheng},
title = {Graph-in-Graph Network for Automatic Gene Ontology Description Generation},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539258},
doi = {10.1145/3534678.3539258},
abstract = {Gene Ontology (GO) is the primary gene function knowledge base that enables computational tasks in biomedicine. The basic element of GO is a term, which includes a set of genes with the same function. Existing research efforts of GO mainly focus on predicting gene term associations. Other tasks, such as generating descriptions of new terms, are rarely pursued. In this paper, we propose a novel task: GO term description generation. This task aims to automatically generate a sentence that describes the function of a GO term belonging to one of the three categories, i.e., molecular function, biological process, and cellular component. To address this task, we propose a Graph-in-Graph network that can efficiently leverage the structural information of GO. The proposed network introduces a two-layer graph: the first layer is a graph of GO terms where each node is also a graph (gene graph). Such a Graph-in-Graph network can derive the biological functions of GO terms and generate proper descriptions. To validate the effectiveness of the proposed network, we build three large-scale benchmark datasets. By incorporating the proposed Graph-in-Graph network, the performances of seven different sequence-to-sequence models can be substantially boosted across all evaluation metrics, with up to 34.7\%, 14.5\%, and 39.1\% relative improvements in BLEU, ROUGE-L, and METEOR, respectively.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1060–1068},
numpages = {9},
keywords = {bioinformatics, gene ontology, graph representations, natural language generation, sequence-to-sequence learning},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3316615.3316688,
author = {Marurngsith, Worawan and Weawsawangwong, Pakorn},
title = {Applying Formal Logic Validation to Enhance Natural Language Understanding},
year = {2019},
isbn = {9781450365734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316615.3316688},
doi = {10.1145/3316615.3316688},
abstract = {Inconsistencies and ambiguities of annotation can cause vagueness in the results obtained by natural language understanding (NLU). The quality of the type systems used for annotation affects the quality of annotation. To achieve highly accepted sets of annotated documents, the Fleiss' kappa score has been widely used to observe the level of agreement from annotated results, submitted by different human annotators. The challenge is that the kappa score cannot be used to validate the type systems nor to identify any incorrect annotations. Thus, we proposed an application of formal logic for validating type systems and annotations against expert rules. Experiments have been done by using four different type systems and annotation sets created by an expert and three novices. Our proposed formal logic model was used to validate the novice type systems and annotations against the expert rules. The results show that the technique could help identifying inconsistencies between expert and novice annotations, by using a model checker. The number of detected inconsistencies impacts the level of achieved F1 score. Thus, the proposed formal logic technique could be used to guide novice annotators to develop accepted type systems. This will help to enhance the performance of the generated machine learning models used by the NLU.},
booktitle = {Proceedings of the 2019 8th International Conference on Software and Computer Applications},
pages = {380–384},
numpages = {5},
keywords = {IBM Watson, Inconsistency detection, Natural language understanding, Validation},
location = {Penang, Malaysia},
series = {ICSCA '19}
}

@inproceedings{10.1145/3424616.3424715,
author = {Brutzman, Don and Floty\'{n}ski, Jakub},
title = {X3D Ontology for Querying 3D Models on the Semantic Web},
year = {2020},
isbn = {9781450381697},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424616.3424715},
doi = {10.1145/3424616.3424715},
abstract = {The Semantic Web offers significant capabilities that transform the current Web into a global knowledge base including various cross-linked multimedia content with formal descriptions of its semantics understandable to humans and processable by computers. Content on the Semantic Web can be subject to reasoning and queries with standardized languages, methods and tools, which opens new opportunities for collaborative creation, use and exploration of web repositories. However, these opportunities have not been exploited so far by the available 3D formats and modeling tools, which limits the possibilities of search and reuse of 3D content as part of the Semantic Web. This work contributes a semantic development pipeline of the X3D Ontology, with corresponding conversion of X3D models into triple forms suitable for formal query. The ontology design reflects experience accompanying the development of the Extensible 3D (X3D) Graphics International Standard, in particular, the X3D Unified Object Model (X3DUOM). This approach combines semantic and syntactic elements of X3D models and metadata to support integration with the Semantic Web. The pipeline enables automatic generation of the X3D Ontology, thereby providing an up-to-date 3D representation with semantics during X3D specification development. By extending commonplace model conversions from other formats to X3D, the ontology presents the potential to enable integration of most forms of 3D content with the Semantic Web.},
booktitle = {Proceedings of the 25th International Conference on 3D Web Technology},
articleno = {14},
numpages = {6},
keywords = {Semantic 3D, Semantic Web, Web3D, X3D Ontology, X3DUOM},
location = {Virtual Event, Republic of Korea},
series = {Web3D '20}
}

@inproceedings{10.5555/3492252.3492254,
author = {Garc\'{e}s, Lina and Sena, Bruno and Nakagawa, Elisa Y.},
title = {Towards an architectural patterns language for systems-of-systems},
year = {2021},
publisher = {The Hillside Group},
address = {USA},
abstract = {Systems-of-Systems (SoS) architectures are inherently dynamic; hence, they must support continuous modification in the behaviour and configuration of these systems at runtime as a result of changes in the environment, new SoS missions, and failures or unavailability of constituents. Modifications should occur without affecting the integrity of constituents, the accomplishment of SoS missions, neither their reliability, security, safety, nor other quality attributes. Architecting SoS requires then important investments in human, time, and economic resources, bringing big challenges. Architectural patterns have been widely used to improve software architecture quality, decreasing costs of design and promoting reuse of good practices and well-known solutions. Nowadays, a great amount of architectural patterns is available; most of them are domain-independent, which harness their selection in specific software projects. The main goal of this paper is to contribute to reduce the work of architects during the choice of better architectural patterns for their SoS. For this, we present a set of patterns that are commonly used to construct such architectures. Additionally, benefits of adopting these patterns are described. To demonstrate their feasibility, we present HSH-SoS, a pattern-base architecture for SoS that presents how different patterns can interact to create a solution for SoS. As results, the architectural patterns reported in this work are feasible candidates to compose a language for recurrent problems in SoS architectures. However, formalization of such language is an open issue that we intend to address in future works.},
booktitle = {Proceedings of the 26th Conference on Pattern Languages of Programs},
articleno = {1},
numpages = {24},
keywords = {architectural pattern, architectural patterns, patterns language, software architecture, systems-of-systems},
location = {Urbana, Illinois},
series = {PLoP '19}
}

@article{10.1145/3673226,
author = {Uhrmacher, Adelinde M and Frazier, Peter and H\"{a}hnle, Reiner and Kl\"{u}gl, Franziska and Lorig, Fabian and Lud\"{a}scher, Bertram and Nenzi, Laura and Ruiz-Martin, Cristina and Rumpe, Bernhard and Szabo, Claudia and Wainer, Gabriel and Wilsdorf, Pia},
title = {Context, Composition, Automation, and Communication: The C2AC Roadmap for Modeling and Simulation},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {1049-3301},
url = {https://doi.org/10.1145/3673226},
doi = {10.1145/3673226},
abstract = {Simulation has become, in many application areas, a sine qua non. Most recently, COVID-19 has underlined the importance of simulation studies and limitations in current practices and methods. We identify four goals of methodological work for addressing these limitations. The first is to provide better support for capturing, representing, and evaluating the context of simulation studies, including research questions, assumptions, requirements, and activities contributing to a simulation study. In addition, the composition of simulation models and other simulation studies’ products must be supported beyond syntactical coherence, including aspects of semantics and purpose, enabling their effective reuse. A higher degree of automating simulation studies will contribute to more systematic, standardized simulation studies and their efficiency. Finally, it is essential to invest increased effort into effectively communicating results and the processes involved in simulation studies to enable their use in research and decision making. These goals are not pursued independently of each other, but they will benefit from and sometimes even rely on advances in other sub-fields. In this article, we explore the basis and interdependencies evident in current research and practice and delineate future research directions based on these considerations.},
journal = {ACM Trans. Model. Comput. Simul.},
month = aug,
articleno = {23},
numpages = {51},
keywords = {Modeling, simulation, state of the art, open challenges, reuse, composition, communication, reproducibility, automation, intelligent modeling and simulation lifecycle}
}

@inproceedings{10.1145/3325917.3325948,
author = {Rencis, Edgars},
title = {Natural Language-Based Knowledge Extraction in Healthcare Domain},
year = {2019},
isbn = {9781450366359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325917.3325948},
doi = {10.1145/3325917.3325948},
abstract = {There is a growing amount of data in the databases of hospitals. These data could be exploited to alleviate the decision-making process of hospital managers, physicians and researchers. However, these types of end-users often lack the expertise necessary for extracting those data from the database. Several approaches exist in the field of how to allow non-programmers writing queries in a convenient manner, but none of them has yet reached fully satisfactory results. This paper sketches a solution to this problem by introducing means for writing queries in a keywords-containing natural language thus alleviating the query writing process for the end-user. Introducing this approach in the knowledge management system of the organization would greatly benefit the domain experts by allowing them to carry out the decision-making process in a more rapid and less erroneous manner.},
booktitle = {Proceedings of the 2019 3rd International Conference on Information System and Data Mining},
pages = {138–142},
numpages = {5},
keywords = {Natural language processing, hospital management, keywords-containing text, knowledge extraction, query language, query translation},
location = {Houston, TX, USA},
series = {ICISDM '19}
}

@inproceedings{10.1145/3638380.3638430,
author = {Barcham, Manuhuia},
title = {Decolonizing Computing and the Quest for Ontological Justice - Putting Fourth-Wave HCI/IxD Into Practice},
year = {2024},
isbn = {9798400717079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638380.3638430},
doi = {10.1145/3638380.3638430},
abstract = {This paper takes as its starting point the emergent literature on issues of justice and decolonization in the HCI/IxD field and their focus on the ways in which current computing systems unwittingly (but perhaps knowingly) perpetuate existing forms of systemic violence by ignoring oppressive histories and sustained negative impacts against certain groups of people. Linking this to the ontological turn underway in a range of disciplines the paper then looks at how these ideas open up the possibility for the achievement of ontological justice for groups marginalized by the colonial nature of computing. The paper then explores these ideas through a discussion of the experiences of New Zealand Mundefinedori hap\={u} (clans) building out computing infrastructures as part of their resurgence as groups. The paper ends by discussing the ways in which a distinction between upstream and downstream design can provide greater purchase of how we might be able to bring out the shifts that are required to achieve a space of ontological justice through a shift into Fourth-Wave HCI/IxD.},
booktitle = {Proceedings of the 35th Australian Computer-Human Interaction Conference},
pages = {486–492},
numpages = {7},
keywords = {Decolonization, Fourth-Wave HCI, Indigenous, Ontological Justice},
location = {Wellington, New Zealand},
series = {OzCHI '23}
}

@inproceedings{10.1145/3486713.3486730,
author = {Koutsomitropoulos, Dimitrios},
title = {Validating Ontology-based Annotations of Biomedical Resources using Zero-shot Learning},
year = {2021},
isbn = {9781450385107},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486713.3486730},
doi = {10.1145/3486713.3486730},
abstract = {Authoritative thesauri in the form of web ontologies offer a sound representation of domain knowledge and can act as a reference point for automated semantic tagging. On the other hand, current language models achieve to capture contextualized semantics of text corpora and can be leveraged towards this goal. We present an approach for injecting subject annotations using query term expansion against such ontologies in the biomedical domain. For the user to have an indication of the usefulness of these suggestions we further propose an online method for validating the quality of annotations using NLI models such as BART and XLM-R. To circumvent training barriers posed by very large label sets and scarcity of data we rely on zero-shot classification and show that semantic matching can contribute above-average thematic annotations. Also, a web-based validation service can be attractive for human curators vs. the overhead of pretraining large, domain-tailored classification models.},
booktitle = {The 12th International Conference on Computational Systems-Biology and Bioinformatics},
pages = {37–43},
numpages = {7},
keywords = {MeSH, Thesaurus, biomedical indexing, classification, language models, machine learning, semantic matching},
location = {Virtual (GMT+7 Bangkok Time), Thailand},
series = {CSBio2021}
}

@article{10.1145/3597304,
author = {Lambrix, Patrick},
title = {Completing and Debugging Ontologies: State-of-the-art and Challenges in Repairing Ontologies},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3597304},
doi = {10.1145/3597304},
abstract = {As semantically enabled applications require high-quality ontologies, developing and maintaining ontologies that are as correct and complete as possible is an important although difficult task in ontology engineering. A key task is ontology debugging and completion. In general, there are two steps: detecting defects and repairing defects. In this article, we discuss the state-of-the-art regarding the repairing step. We do this by formalizing the repairing step as an abductive reasoning problem and situating the state-of-the-art with respect to this framework. We show that there are still many open research problems and show opportunities for further work and advancing the field.},
journal = {J. Data and Information Quality},
month = nov,
articleno = {41},
numpages = {38},
keywords = {Ontology engineering, ontology debugging, ontology completion, ontology alignment}
}

@inproceedings{10.1145/3410352.3410789,
author = {Aitim, A. K. and Satybaldiyeva, R. Zh. and Wojcik, W.},
title = {The construction of the Kazakh language thesauri in automatic word processing system},
year = {2020},
isbn = {9781450377362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410352.3410789},
doi = {10.1145/3410352.3410789},
abstract = {In the paper presents an overview of existing electronic Kazakh-language thesauri and their automatic methods of construction and application. The author analyzed the main characteristics of open access thesauri for scientific research, evaluated the dynamics of their development and effectiveness in solving problems of natural language processing. Statistical and linguistic methods of thesaurus construction were studied, which allow to automate the development and reduce the labor costs of expert linguists. It is considered algorithms for selecting key terms from texts and semantic thesaurus links of all types, as well as the quality of application of the resulting thesauri. For illustrate the features of various methods of building thesaurus links, a combined method was developed that generates a specialized thesaurus completely automatically based on the corpus of domain texts and several existing linguistic resources.},
booktitle = {Proceedings of the 6th International Conference on Engineering \&amp; MIS 2020},
articleno = {53},
numpages = {4},
keywords = {Kazakh language, Thesaurus, agglutinative languages, morphological thesauri, semantics},
location = {Almaty, Kazakhstan},
series = {ICEMIS'20}
}

@inproceedings{10.1145/3424953.3426540,
author = {Faria, Carolinne Roque e and de Barbosa, Cinthyan Renata Sachs C.},
title = {System for identifying pests and diseases in soybean crop through natural language},
year = {2020},
isbn = {9781450381727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424953.3426540},
doi = {10.1145/3424953.3426540},
abstract = {The presence of technologies in the agronomic field has the purpose of proposing the best solutions to the challenges found in agriculture, especially to the problems that affect cultivars. One of the obstacles found is to apply the use of your own language in applications that interact with the user in Brazilian Agribusiness, which would bring gains in time, money and accuracy of the analyzes to be performed. This paper proposes the use of Natural Language Processing techniques for the development of an effective system to assist in the identification of pests and diseases in the soybean crop, stored in a database repository to facilitate access to information and diagnosis of the professional.},
booktitle = {Proceedings of the 19th Brazilian Symposium on Human Factors in Computing Systems},
articleno = {58},
numpages = {6},
keywords = {Agriculture 4.0, human-computer interaction, natural language processing},
location = {Diamantina, Brazil},
series = {IHC '20}
}

@inproceedings{10.1145/3316615.3316683,
author = {Yanuarifiani, Amarilis Putri and Chua, Fang-Fang and Chan, Gaik-Yee},
title = {Automating Business Process Model Generation from Ontology-based Requirements},
year = {2019},
isbn = {9781450365734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316615.3316683},
doi = {10.1145/3316615.3316683},
abstract = {Requirements elicitation process faces major challenges about how stakeholders can easily verify requirements. Requirements document allows developers to visualize requirements using modeling language to ensure stakeholders have the same perspective as them. It is also effective to give presentations to stakeholders about how business processes will be carried out after the requirements are implemented. Issues are raised in building requirements modeling as business users generally do not have enough knowledge to build requirements models in specific notations. Transforming requirements (natural language) into semi-formal notation (BPMN) manually lead to inconsistency of elements structure. The need to automatically generate requirements model become crucial because it will be the basis for the programming process. Existing studies are mostly concerned on auto-completion of modeling language using domain ontology as basic knowledge, and let the stakeholders building initial requirements model with limited knowledge. The idea of this paper is to propose a methodology for building business process model in semi-formal language (BPMN) to represent future business processes using ontology approach. This research continues from previous study which transform requirements list into requirements ontology to formalize the elements such as problem, actor and process. By using requirements ontology as input, rule-based mapping method is proposed to map ontology instances to BPMN elements.},
booktitle = {Proceedings of the 2019 8th International Conference on Software and Computer Applications},
pages = {205–209},
numpages = {5},
keywords = {Semi-formal modeling, auto-generate BPMN, ontology-based requirements},
location = {Penang, Malaysia},
series = {ICSCA '19}
}

@article{10.1145/3567594,
author = {Mahlaza, Zola and Keet, C. Maria},
title = {Surface Realization Architecture for Low-resourced African Languages},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3567594},
doi = {10.1145/3567594},
abstract = {There has been growing interest in building surface realization systems to support the automatic generation of text in African languages. Such tools focus on converting abstract representations of meaning to a text. Since African languages are low-resourced, economical use of resources and general maintainability are key considerations. However, there is no existing surface realizer architecture that possesses most of the maintainability characteristics (e.g., modularity, reusability, and analyzability) that will lead to maintainable software that can be used for the languages. Moreover, there is no consensus surface realization architecture created for other languages that can be adapted for the languages in question. In this work, we solve this by creating a novel surface realizer architecture suitable for low-resourced African languages that abides by the features of maintainable software. Its design comes after a granular analysis, classification, and comparison of the architectures used by 77 existing NLG systems. We compare our architecture to existing architectures and show that it supports the most features of a maintainable software product.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = mar,
articleno = {84},
numpages = {26},
keywords = {Natural language generation, software architecture, low-resourced languages, surface realisation}
}

@inproceedings{10.1145/3614321.3614367,
author = {Silva-Aguilar, Jairo H. and Torres T., Rommel and Estevez, Elsa},
title = {Design of an ontology to represent the elaboration of the annual operational plan in the area of public sector planning},
year = {2023},
isbn = {9798400707421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3614321.3614367},
doi = {10.1145/3614321.3614367},
abstract = {The Semantic Web helps to represent knowledge in such a way that it can be easily processed by machines. Ontologies are a tool the Semantic Web uses to define concepts, establish their hierarchy, outline attributes, and determine relationships, which capture knowledge within a specific domain. In the public sector, institutions generate a substantial volume of information. To foster transparency and openness, such information is published on the web. However, in many cases, it is necessary to incorporate semantic content that helps to organize the information so that it can be processed automatically, by government authorities and interested users. One of the fundamental activities in public institutions is planning, which includes a set of processes that contribute to the achievement of previously defined objectives. In this article, we design an ontology whose domain is the elaboration of the Annual Operational Plan in Ecuador. An operational plan is a process that belongs to the Planning area of the Decentralized Autonomous Governments of Ecuador. We propose a set of activities and their respective tasks for developing the ontology based on the Methontology methodology. After its development, we formalized the proposed ontology using Prot\'{e}g\'{e}. The main theoretical contribution of this paper is the definition of an ontology in a specific government area. In addition, from a practical perspective, we developed a tool that facilitates the analysis and processing of data for budget planning in Ecuador.},
booktitle = {Proceedings of the 16th International Conference on Theory and Practice of Electronic Governance},
pages = {340–346},
numpages = {7},
keywords = {Government budget, Ontology, Open Government, Planning, Prot\'{e}g\'{e}, Semantic Web},
location = {Belo Horizonte, Brazil},
series = {ICEGOV '23}
}

@article{10.1145/3626307.3626308,
author = {Baader, Franz},
title = {Relating Optimal Repairs in Ontology Engineering with Contraction Operations in Belief Change},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {1559-6915},
url = {https://doi.org/10.1145/3626307.3626308},
doi = {10.1145/3626307.3626308},
abstract = {The question of how a given knowledge base can be modified such that certain unwanted consequences are removed has been investigated in the area of ontology engineering under the name of repair and in the area of belief change under the name of contraction. Whereas in the former area the emphasis was more on designing and implementing concrete repair algorithms, the latter area concentrated on characterizing classes of contraction operations by certain postulates they satisfy. In the classical setting, repairs and contractions are subsets of the knowledge base that no longer have the unwanted consequence. This makes these approaches syntax-dependent and may result in removal of more consequences than necessary. To alleviate this problem, gentle repairs and pseudo-constractions have been introduced in the respective research areas, and their connections have been investigated in recent work. Optimal repairs preserve a maximal amount of consequences, but they may not always exist. We show that, if they exist, then they can be obtained by certain pseudo-contraction operations, and thus they comply with the postulates that these operations satisfy. Conversely, under certain conditions, pseudo-contractions are guaranteed to produce optimal repairs. Recently, contraction operations have also been defined for concepts rather than for whole knowledge bases. We show that there is again a close connection between such operations and optimal repairs of a restricted form of knowledge bases.},
journal = {SIGAPP Appl. Comput. Rev.},
month = sep,
pages = {5–18},
numpages = {14},
keywords = {belief change, description logic, ontology repair}
}

@inproceedings{10.1145/3340555.3356098,
author = {Zhu, Su and Zhao, Zijian and Zhao, Tiejun and Zong, Chengqing and Yu, Kai},
title = {CATSLU: The 1st Chinese Audio-Textual Spoken Language Understanding Challenge},
year = {2019},
isbn = {9781450368605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340555.3356098},
doi = {10.1145/3340555.3356098},
abstract = {Spoken language understanding (SLU) is a key component of conversational dialogue systems, which converts user utterances into semantic representations. The previous works almost focus on parsing semantic from textual inputs (top hypothesis of speech recognition and even manual transcripts) while losing information hidden in the audio. We herein describe the 1st Chinese Audio-Textual Spoken Language Understanding Challenge (CATSLU) which introduces a new dataset with audio-textual information, multiple domains and domain knowledge. We introduce two scenarios of audio-textual SLU in which participants are encouraged to utilize data of other domains or not. In this paper, we will describe the challenge and results.},
booktitle = {2019 International Conference on Multimodal Interaction},
pages = {521–525},
numpages = {5},
keywords = {datasets, spoken language understanding},
location = {Suzhou, China},
series = {ICMI '19}
}

@inproceedings{10.1145/3614321.3614327,
author = {Avgerinos Loutsaris, Michalis and Alexopoulos, Charalampos and Maratsi, Maria Ioanna and Charalabidis, Yannis},
title = {Semantic Interoperability for Legal Information: Mapping the European Legislation Identifier (ELI) and Akoma Ntoso (AKN) Ontologies},
year = {2023},
isbn = {9798400707421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3614321.3614327},
doi = {10.1145/3614321.3614327},
abstract = {The legislative landscape, characterized by overwhelming amounts of legal data which, on many occasions, is only accessible by legal experts, the fragmented nature of information and the ever-increasing number of disparate systems, have given more impetus to the interoperability realm of legal data. The semantic interoperability of Linked Open Legal Data (LOLD) requires rich and well-defined metadata, as well as the establishment of standards, in order to be able to connect and link these scattered legal data resources. This is usually achieved through the transformation of legal information into structured format and through the utilization of legal ontologies whose main purpose is to connect the legal basis of two or more countries by allowing for reusability and the ability to adequately represent legal information, which is understood and retrieved across borders. Within the context of this study, the European Legislation Identifier (ELI) and Akoma Ntoso (AKN) ontologies are mapped in order to make legal data compatible and reusable in as many contexts as possible and to support the Linked Open Legal Data (LOLD) concept. The mapping of these two widely used legal ontologies was evaluated by domain experts and strongly validated by tools. The usefulness of the produced mapping is proven through its real-life context application, although one thing to consider regarding possible future perspectives, could be the inclusion and mapping of more legal ontologies to expand the application domain and improve the semantic interoperability of legal information. These mappings could be either achieved using similar methodological approaches or applications of automated and AI-based ontology mapping techniques.},
booktitle = {Proceedings of the 16th International Conference on Theory and Practice of Electronic Governance},
pages = {41–53},
numpages = {13},
keywords = {OGD, Open Government Data, legal data, legal information, linked open legal data (LOLD), ontology mapping, semantic interoperability},
location = {Belo Horizonte, Brazil},
series = {ICEGOV '23}
}

@inproceedings{10.1145/3342428.3342662,
author = {Afacan, Yasemin and Surer, Elif},
title = {Modeling a User-Oriented Ontology on Accessible Homes for Supporting Activities of Daily Living (ADL) in Healthy Aging},
year = {2019},
isbn = {9781450362610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342428.3342662},
doi = {10.1145/3342428.3342662},
abstract = {Inaccessibility of the buildings is the most common obstacle which presents barriers for older adults with different motor abilities. An inclusive design process, where elderly and designers work together, is required to overcome this obstacle. To do so, this study proposes a user-oriented model (i) to define a knowledge presentation for designers; (ii) to assist them during the development of accessible homes and (iii) to accommodate exemplary home attributes for activities of daily living (ADL). The ontology for this model was first constructed by collecting user information through LEGO® Serious Play® on the four subdomains of motor abilities: (1) strength; (2) balance; (3) locomotion; and (4) endurance. The findings of this study are significant for future aging studies and mobile computing researches in terms of indicating that diverse motor ability difficulties are associated with different requirements of accessibility attributes, and structured knowledge is required to diagrammatize their association with ADL.},
booktitle = {Proceedings of the 5th EAI International Conference on Smart Objects and Technologies for Social Good},
pages = {67–71},
numpages = {5},
keywords = {Accessible Home, Activities of Daily Living (ADL), Assisted Living, Ontology},
location = {Valencia, Spain},
series = {GoodTechs '19}
}

@inproceedings{10.1145/3698322.3698326,
author = {Almeida, Francisca and Pinho, Daniel and Aguiar, Ademar},
title = {Validating Pattern Languages: A systematic literature review},
year = {2024},
isbn = {9798400716836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698322.3698326},
doi = {10.1145/3698322.3698326},
abstract = {The concept of patterns and pattern languages, although very common in software nowadays, was first approached by Christopher Alexander, in the area of architecture, in the book A pattern language: towns, buildings, construction. However, it was only in 1980 that the term was adapted for software development, gaining its popularity in 1994. Despite the fact that the concept of patterns has been used in the area of software development for more than 40 years, there is still no consensus on the best method to validate patterns and patterns languages, and the existing methods are scattered in several different papers and research across the scientific community. As such, in this paper, we conduct a systematic literature review about the existing methods in the scientific community to validate patterns and pattern languages.},
booktitle = {Proceedings of the 29th European Conference on Pattern Languages of Programs, People, and Practices},
articleno = {34},
numpages = {8},
keywords = {Patterns, pattern languages, pattern validation, software development},
location = {
},
series = {EuroPLoP '24}
}

@inbook{10.1145/3477322.3477328,
author = {Pieraccini, Roberto},
title = {Natural Language Understanding in Socially Interactive Agents},
year = {2021},
isbn = {9781450387200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3477322.3477328},
booktitle = {The Handbook on Socially Interactive Agents: 20 Years of Research on Embodied Conversational Agents, Intelligent Virtual Agents, and Social Robotics Volume 1: Methods, Behavior, Cognition},
pages = {147–172},
numpages = {26}
}

@inproceedings{10.1145/3550356.3561553,
author = {T\"{o}pel, Daniel and Kaczmarek-He\ss{}, Monika},
title = {Towards flexible creation of multi-level models: bottom-up change support in the modeling and programming environment XModeler},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3561553},
doi = {10.1145/3550356.3561553},
abstract = {A process of a multi-level model creation follows typically the top-down approach, i.e., it requires first defining concepts and relations on the highest classification levels, which only then can be used to create concepts on the lower ones. Empirical insights into the process of multi-level model creation suggest however, that this strategy may be counter-intuitive and challenging, especially for non-experts. This paper addresses this problem by focusing on the idea of flexible multi-level model creation, understood as an intertwined application of top-down and bottom-up strategies. As a first step towards realizing this vision for multi-level models in general, and those created with the XModeler and Flexible Meta-Modeling and Execution Language (FMMLx) in particular, in this paper, we select a set of relevant multi-level refactoring patterns, adapt them to our approach, and implement them in the supporting tool. We illustrate the flexible creation process using an exemplary scenario.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {404–413},
numpages = {10},
keywords = {XModeler, bottom-up modeling, flexible meta-modeling and execution language FMMLx, flexible modeling process, multi-level modeling, multi-level refactoring patterns},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3292500.3330710,
author = {Weng, Wei-Hung and Chung, Yu-An and Szolovits, Peter},
title = {Unsupervised Clinical Language Translation},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330710},
doi = {10.1145/3292500.3330710},
abstract = {As patients' access to their doctors' clinical notes becomes common, translating professional, clinical jargon to layperson-understandable language is essential to improve patient-clinician communication. Such translation yields better clinical outcomes by enhancing patients' understanding of their own health conditions, and thus improving patients' involvement in their own care. Existing research has used dictionary-based word replacement or definition insertion to approach the need. However, these methods are limited by expert curation, which is hard to scale and has trouble generalizing to unseen datasets that do not share an overlapping vocabulary. In contrast, we approach the clinical word and sentence translation problem in a completely unsupervised manner. We show that a framework using representation learning, bilingual dictionary induction and statistical machine translation yields the best precision at 10 of 0.827 on professional-to-consumer word translation, and mean opinion scores of 4.10 and 4.28 out of 5 for clinical correctness and layperson readability, respectively, on sentence translation. Our fully-unsupervised strategy overcomes the curation problem, and the clinically meaningful evaluation reduces biases from inappropriate evaluators, which are critical in clinical machine learning.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {3121–3131},
numpages = {11},
keywords = {consumer health, machine translation, representation learning, unsupervised learning},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@article{10.1145/3686981,
author = {Zhu, Mengxiao and Wang, Xin and Wang, Xiantao and Chen, Zihang and Huang, Wei},
title = {Application of Prompt Learning Models in Identifying the Collaborative Problem Solving Skills in an Online Task},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW2},
url = {https://doi.org/10.1145/3686981},
doi = {10.1145/3686981},
abstract = {Collaborative problem solving (CPS) competence is considered one of the essential 21st-century skills. To facilitate the assessment and learning of CPS competence, researchers have proposed a series of frameworks to conceptualize CPS and explored ways to make sense of the complex processes involved in collaborative problem solving. However, encoding explicit behaviors into subskills within the frameworks of CPS skills is still a challenging task. Traditional studies have relied on manual coding to decipher behavioral data for CPS, but such coding methods can be very time-consuming and cannot support real-time analyses. Scholars have begun to explore approaches for constructing automatic coding models. Nevertheless, the existing models built using machine learning or deep learning techniques depend on a large amount of training data and have relatively low accuracy. To address these problems, this paper proposes a prompt-based learning pre-trained model. The model can achieve high performance even with limited training data. In this study, three experiments were conducted, and the results showed that our model not only produced the highest accuracy, macro F1 score, and kappa values on large training sets, but also performed the best on small training sets of the CPS behavioral data. The application of the proposed prompt-based learning pre-trained model contributes to the CPS skills coding task and can also be used for other CSCW coding tasks to replace manual coding.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {442},
numpages = {23},
keywords = {automatic coding, collaborative problem solving, natural language processing, prompt-based learning}
}

@inproceedings{10.1145/3487664.3487789,
author = {Mohamed, Aya and Auer, Dagmar and Hofer, Daniel and K\"{u}ng, Josef},
title = {Extended XACML Language and Architecture for Access Control in Graph-structured Data},
year = {2022},
isbn = {9781450395564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487664.3487789},
doi = {10.1145/3487664.3487789},
abstract = {The rapidly increasing use of graph databases for a wide variety of applications demands flexible authorization and fine-grained access control at the level of attributes associated with the basic entities (i.e., accessing subject, requested resource, performed action, and environmental conditions) but also the vertices and edges along a particular access path. We present a solution for authorization policy specification and enforcement in a graph database to apply fine-grained path-specific constraints on graph-structured data. Therefore, we extend the well-established declarative policy definition language eXtensible Access Control Markup Language (XACML) and its architecture to describe path patterns and enforce the policies using the standard functional components of XACML. Our approach, XACML for Graph-structured data (XACML4G), defines an extended XACML grammar for the authorization policy and access request. To enforce XACML4G policies, we relied on the extensibility points of the XACML architecture and added proprietary extensions. We show the significance of our approach by means of a demonstration prototype in the university domain. Finally, we provide an initial evaluation of the expressiveness and performance of XACML4G with regard to XACML.},
booktitle = {The 23rd International Conference on Information Integration and Web Intelligence},
pages = {367–374},
numpages = {8},
keywords = {Access Control, Authorization Policy, Graph Database, Graph-structured Data, XACML, XACML4G},
location = {Linz, Austria},
series = {iiWAS2021}
}

@inproceedings{10.1145/3357419.3357442,
author = {Tapia-Leon, Mariela and Aveiga, Carlos and Chicaiza, Janneth and Su\'{a}rez-Figueroa, Mari Carmen},
title = {Ontological Model for the Semantic Description of Syllabuses},
year = {2019},
isbn = {9781450371889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357419.3357442},
doi = {10.1145/3357419.3357442},
abstract = {The syllabus is a relevant document to organize how the teaching-learning process will be carried out during an academic course in Higher Education Institutions (HEI). Usually, this document is written in a human-readable format that do not enable automatic processing through intelligent services to support teaching and learning. Therefore, we created OntoSyllabus ontology for the representation of syllabuses applying the NeOn methodology. The semantic model of a syllabus will allow the comprehension for both: machines and humans, and it will facilitate the interchange of data between different services and applications. The ontology was created based on the results of our three previous studies, which helped us to determinate the terms and relations in the syllabus ontology. The documentation and the computable model are available on the Internet for their reuse.},
booktitle = {Proceedings of the 9th International Conference on Information Communication and Management},
pages = {175–180},
numpages = {6},
keywords = {Higher Education Institution, NeOn Methodology, Ontology, Semantic Web, Syllabus},
location = {Prague, Czech Republic},
series = {ICICM '19}
}

@inproceedings{10.5555/3237383.3237828,
author = {Pomarlan, Mihai and Bateman, John},
title = {Robot Program Construction via Grounded Natural Language Semantics \&amp; Simulation},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Robots acting in semi-structured, human environments need to understand the effects of their actions and the instructions given by a human user. Simulation has been considered a promising reasoning technique to help tackle both problems. In this paper, we present a system that constructs an executable robot program from a linguistic semantic specification produced by parsing a natural language sentence; in effect, our system grounds the semantic specification into the produced robot plan. The plan can then be run in a simulated environment, which allows one to infer more about the plan than was present in the initial semantic specification. Our system allows modeling how actions can be modified by subclauses, which we showcase by a transport action. Simulation runs allow discovery of better parameters, either locally for a subtask or such that the entire task is better performed; simulation reveals these parameterizations may differ.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {857–864},
numpages = {8},
keywords = {cognitive robotics, human-robot interaction, language grounding, robotic agent languages},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@article{10.5555/3722479.3722530,
author = {Kollapally, Navya Martin and Geller, James and Morreale, Patricia and Kwak, Daehan},
title = {An Ontology for Social Determinants of Education (SDoEd) Based on Human-AI Collaborative Approach},
year = {2024},
issue_date = {October 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {3},
issn = {1937-4771},
abstract = {The use of computational ontologies is well-established in the field of Medical Informatics. The topic of Social Determinants of Health (SDoH) has also received extensive attention. Work at the intersection of ontologies and SDoH has been published. However, a standardized framework for Social Determinants of Education (SDoEd) is lacking. In this paper, we are closing the gap by introducing an SDoEd ontology for creating a precise conceptualization of the interplay between life circumstances of students and their possible educational achievements. The ontology was developed utilizing suggestions from ChatGPT-3.5-010422 and validated using peer-reviewed research articles. The first version of developed ontology was evaluated by human experts in the field of education and validated using standard ontology evaluation software. This version of the SDoEd ontology contains 231 domain concepts, 10 object properties, and 24 data properties.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {191–203},
numpages = {13}
}

@article{10.1145/3375547,
author = {Abulaish, Muhammad and Kamal, Ashraf and Zaki, Mohammed J.},
title = {A Survey of Figurative Language and Its Computational Detection in Online Social Networks},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1559-1131},
url = {https://doi.org/10.1145/3375547},
doi = {10.1145/3375547},
abstract = {The frequent usage of figurative language on online social networks, especially on Twitter, has the potential to mislead traditional sentiment analysis and recommender systems. Due to the extensive use of slangs, bashes, flames, and non-literal texts, tweets are a great source of figurative language, such as sarcasm, irony, metaphor, simile, hyperbole, humor, and satire. Starting with a brief introduction of figurative language and its various categories, this article presents an in-depth survey of the state-of-the-art techniques for computational detection of seven different figurative language categories, mainly on Twitter. For each figurative language category, we present details about the characterizing features, datasets, and state-of-the-art computational detection approaches. Finally, we discuss open challenges and future directions of research for each figurative language category.},
journal = {ACM Trans. Web},
month = feb,
articleno = {3},
numpages = {52},
keywords = {Social network analysis, figurative language, humor recognition, hyperbole detection, irony detection, metaphor detection, sarcasm detection, satire detection, simile detection}
}

@inproceedings{10.5555/3373669.3373682,
author = {Knote, Robin and S\"{o}llner, Matthias and Leimeister, Jan Marco},
title = {Towards a pattern language for smart personal assistants},
year = {2020},
publisher = {The Hillside Group},
address = {USA},
abstract = {Supporting users in their daily activities, thus, making their lives more comfortable, has long been a goal for consumer-oriented systems development. With the rise of smart personal assistants (SPAs), however, we have reached a new milestone along the path towards this goal. These systems assist their owners by providing personalized and context-dependent information and service. Today's implementations reach from conversational agents, such as Siri, Cortana or Google Assistant, over chatbots, which are primarily text-based, to cognitive assistants, which assist according to a user's current cognitive or emotional state. However, although both research and practice proceed with full pace, recurring design elements of SPAs have not yet been investigated. We hence propose a pattern language for smart personal assistants to guide further empirical and design efforts. Therefore, we review existing information systems, computer science and human-computer interaction literature to find recurring design characteristics among 115 different assistants. The resulting pattern language contains 22 patterns that specify the interaction behavior and the intelligence of smart personal assistants.},
booktitle = {Proceedings of the 25th Conference on Pattern Languages of Programs},
articleno = {14},
numpages = {16},
keywords = {pattern language, smart personal assistants},
location = {Portland, Oregon},
series = {PLoP '18}
}

@inproceedings{10.1145/3626772.3657815,
author = {Joko, Hideaki and Chatterjee, Shubham and Ramsay, Andrew and de Vries, Arjen P. and Dalton, Jeff and Hasibi, Faegheh},
title = {Doing Personal LAPS: LLM-Augmented Dialogue Construction for Personalized Multi-Session Conversational Search},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657815},
doi = {10.1145/3626772.3657815},
abstract = {The future of conversational agents will provide users with personalized information responses. However, a significant challenge in developing models is the lack of large-scale dialogue datasets that span multiple sessions and reflect real-world user preferences. Previous approaches rely on experts in a wizard-of-oz setup that is difficult to scale, particularly for personalized tasks. Our method, LAPS, addresses this by using large language models (LLMs) to guide a single human worker in generating personalized dialogues. This method has proven to speed up the creation process and improve quality. LAPS can collect large-scale, human-written, multi-session, and multi-domain conversations, including extracting user preferences. When compared to existing datasets, LAPS-produced conversations are as natural and diverse as expert-created ones, which stays in contrast with fully synthetic methods. The collected dataset is suited to train preference extraction and personalized response generation. Our results show that responses generated explicitly using extracted preferences better match user's actual preferences, highlighting the value of using extracted preferences over simple dialogue history. Overall, LAPS introduces a new method to leverage LLMs to create realistic personalized conversational data more efficiently and effectively than previous methods.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {796–806},
numpages = {11},
keywords = {conversational search, dialogue collection, personalization},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@article{10.1145/3605910,
author = {Koch, In\^{e}s and Teixeira Lopes, Carla and Ribeiro, Cristina},
title = {Moving from ISAD(G) to a CIDOC CRM-based Linked Data Model in the Portuguese Archives},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1556-4673},
url = {https://doi.org/10.1145/3605910},
doi = {10.1145/3605910},
abstract = {Archives are facing numerous challenges. On the one hand, archival assets are evolving to encompass digitized documents and increasing quantities of born-digital information in diverse formats. On the other hand, the audience is changing along with how it wishes to access archival material. Moreover, the interoperability requirements of cultural heritage repositories are growing. In this context, the Portuguese Archives started an ambitious program aiming to evolve its data model, migrate existing records, and build a new archival management system appropriate to both archival tasks and public access. The overall goal is to have a fine-grained and flexible description, more machine-actionable than the current one. This work describes ArchOnto, a linked open data model for archives, and rules for its automatic population from existing records. ArchOnto adopts a semantic web approach and encompasses the CIDOC Conceptual Reference Model and additional ontologies, envisioning interoperability with datasets curated by multiple communities of practice. Existing ISAD(G)-conforming descriptions are being migrated to the new model using the direct mappings provided here. We used a sample of 25 records associated with different description levels to validate the completeness and conformity of ArchOnto to existing data. This work is in progress and is original in several respects: (1) it is one of the first approaches to use CIDOC CRM in the context of archives, identifying problems and questions that emerged during the process and pinpointing possible solutions; (2) it addresses the balance in the model between the migration of existing records and the construction of new ones by archive professionals; and (3) it adopts an open world view on linking archival data to global information sources.},
journal = {J. Comput. Cult. Herit.},
month = nov,
articleno = {71},
numpages = {21},
keywords = {Cultural heritage, archives, archival description, linked open data, semantic web, data migration}
}

@inproceedings{10.1145/3670013.3670022,
author = {Sitthisak, Onjira and Pradubsuwun, Denduang},
title = {Analysis of Generated Assessment Items from the COMBA Competency Model},
year = {2024},
isbn = {9798400717062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3670013.3670022},
doi = {10.1145/3670013.3670022},
abstract = {This paper proposes an item analysis of generated assessment items from the COMBA competency model. Two metrics are used including difficulty and discrimination. We generate assessment items by COMBA for evaluating the learner's competence in the Python programming class and use them to examine the cognitive level of learners. The item analysis is applied to check conformance between the capability level of the generated assessment items and the cognitive level of learners. Experimenting with the generated assessment items, the capability level affects the difficulty and discrimination of their items.},
booktitle = {Proceedings of the 2024 15th International Conference on E-Education, E-Business, E-Management and E-Learning},
pages = {117–122},
numpages = {6},
keywords = {Adaptive Assessment, Competency Model, Item Analysis, Ontology},
location = {Fukuoka-shi, Japan},
series = {IC4E '24}
}

@inproceedings{10.1145/3343485.3343500,
author = {Zhomartkyzy, Gulnaz and Kumargazhanova, Saule and Popova, Galina and Suleimenova, Laura},
title = {Development of University Scientific Knowledge Ontological Model},
year = {2019},
isbn = {9781450371681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343485.3343500},
doi = {10.1145/3343485.3343500},
abstract = {An ontology is a link between objects of knowledge and a connecting bridge between various steps of Knowledge Processes. Ontology development is an important aspect of knowledge management solution support (KM-solutions). In this paper, we consider a university scientific knowledge ontological model which is one of the knowledge management systems tools. The main functions of the university scientific knowledge ontology are given. The main classes, properties and relations of ontology for maintaining the knowledge base of educational resources are described.},
booktitle = {Proceedings of the 2019 2nd International Conference on Mathematics and Statistics},
pages = {40–45},
numpages = {6},
keywords = {Electronic scientific resources, Knowledge base, Knowledge management, Ontological engineering, Scientific knowledge ontology},
location = {Prague, Czech Republic},
series = {ICoMS '19}
}

@inproceedings{10.1145/3737609.3747119,
author = {Hughes, Margaret and DeSota, Elianna and Victor, Matthew and Lynn, Stuart and Stormonth-Darling, John M, John and Barry, Liz},
title = {Towards Interoperability: Pursuing an ontology for data exchange between deliberative democratic platforms},
year = {2025},
isbn = {9798400719684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3737609.3747119},
doi = {10.1145/3737609.3747119},
abstract = {In response to the fragmented state of civic engagement tools and the urgent challenges facing democratic systems, this paper introduces a shared, contributor-driven ontology to connect diverse civic tech platforms, emerging from the work of the Interoperable Deliberative Tool cohort at Metagov. By integrating platforms like Voice to Vision, Assemblis, and Decidim, we enable the flow of deliberative data across contexts, supporting more cohesive decision-making. This approach helps bridge gaps between input, analysis, and action, enhancing democratic resilience in crisis moments. Through our work, we demonstrate how interoperability can strengthen civic engagement and provide a foundation for more responsive, collaborative governance.},
booktitle = {Adjunct Proceedings of the Sixth Decennial Aarhus Conference: Computing X Crisis},
articleno = {19},
numpages = {5},
keywords = {Governance, Interoperability, Deliberative Democracy, Data Commons, Civic Technology, Digital Civics},
location = {
},
series = {AAR Adjunct '25}
}

@inproceedings{10.1145/3550356.3561577,
author = {Delabeye, Romain and Penas, Olivia and Plateaux, R\'{e}gis},
title = {Scalable ontology-based V&amp;V process for heterogeneous systems and applications},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3561577},
doi = {10.1145/3550356.3561577},
abstract = {This work focuses on ongoing research within the EU-funded EnerMan project aiming at improving the energy efficiency of manufacturing systems. Industrial use cases are generally too constrained to easily proceed to the verification and validation (V&amp;V) of the scientific approaches tackling their challenges. In this context, we propose an ontology-based framework with a methodology assessing the scalability of heterogeneous systems, environments, and missions in a V&amp;V context. Indeed, projecting these industrial and laboratory applications onto a meaningful ontology allows them to be flattened out to the same scale from a semantic point of view. Reasoning is used to evaluate the extent to which a given scientific approach can be verified on a laboratory use case different from the industrial scenario on which it has to be validated. The framework has been implemented using Prot\'{e}g\'{e} and Owlready2, and applied to a scientific approach focused on a blind source separation technique used to identify system operating modes in a black box manner, tested on a coffee machine and two industrial case studies (a vehicle testbed's heating ventilation and air conditioning system, and a chocolate production line).},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {341–350},
numpages = {10},
keywords = {heterogeneous systems, ontology, requirements analysis, scalability, verification \&amp; validation},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3638837.3638882,
author = {Yu, Rongdong and Zhong, Yaoyi and Xu, Yunliang and Wen, Jie and Pan, Quanhong and Sha, Wanli and Wang, Zhan},
title = {Leveraging Graph Databases for Automated OPC UA Information Model Construction},
year = {2024},
isbn = {9798400709265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638837.3638882},
doi = {10.1145/3638837.3638882},
abstract = {To address the OPC UA transformation requirements of conventional factory data collection systems, we propose an innovative method for the automated construction of OPC UA information models utilizing graph databases. This method not only streamlines the process but also minimizes the need for extensive manual labor and specialized expertise. Drawing from the resemblance between OPC UA information models and OWL ontologies, we have devised mapping rules that facilitate the translation of knowledge graph data into OPC UA format. Leveraging the existing Neo4j database within the factory as the primary information source, we’ve developed an OWL ontology construction module for exporting ontology files. In parallel, an information model construction module has been designed to convert the ontology file into an OPC UA XML file, complete with the OPC UA information model. This XML file adheres to the official OPC UA specification, ensured through the use of the UA-ModelCompiler tool. To validate the effectiveness and viability of this construction method, we conducted functional tests and evaluations using a publicly available database. The results of these tests confirm the feasibility of our approach, marking a significant advancement in OPC UA information model construction for factory data systems.},
booktitle = {Proceedings of the 2023 12th International Conference on Networks, Communication and Computing},
pages = {294–299},
numpages = {6},
keywords = {Neo4j, OPC UA information model, OWL ontology, graph database},
location = {Osaka, Japan},
series = {ICNCC '23}
}

@inproceedings{10.5555/3466184.3466386,
author = {Ramzy, Nour and Martens, Christian James and Singh, Shreya and Ponsignon, Thomas and Ehm, Hans},
title = {First steps towards bridging simulation and ontology to ease the model creation on the example of semiconductor industry},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {With diverse product mixes in fabs, high demand volatility, and numerous manufacturing steps spread across different facilities, it is impossible to analyze the combined impacts of multiple operations in semiconductor supply chains without a modeling tool like simulation. This paper explains how ontologies can be used to develop and deploy simulation applications, with interoperability and knowledge sharing at the semantic level. This paper proposes a concept to automatically build simulations using ontologies and its preliminary results. The proposed approach seeks to save time and effort expended in recreating the information for different use cases that already exists elsewhere. The use case provides first indications that with an enhancement of a so-called Digital Reference with Semantic Web Technologies, modeling and simulation of semiconductor supply chains will not only become much faster but also require less modeling efforts because of the reusability property.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1789–1800},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@inproceedings{10.1145/3543873.3587613,
author = {Li, Huanyu and Abd Nikooie Pour, Mina and Li, Ying and Lindecrantz, Mikael and Blomqvist, Eva and Lambrix, Patrick},
title = {A Survey of General Ontologies for the Cross-Industry Domain of Circular Economy},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587613},
doi = {10.1145/3543873.3587613},
abstract = {Circular Economy has the goal to reduce value loss and avoid waste by extending the life span of materials and products, including circulating materials or product parts before they become waste. Circular economy models (e.g., circular value networks) are typically complex and networked, involving different cross-industry domains. In the context of a circular value network, multiple actors, such as suppliers, manufacturers, recyclers, and product end-users, may be involved. In addition, there may be various flows of resources, energy, information and value throughout the network. This means that we face the challenge that the data and information from cross-industry domains in a circular economy model are not built on common ground, and as a result are difficult to understand and use for both humans and machines. Using ontologies to represent domain knowledge can enable actors and stakeholders from different industries in the circular economy to communicate using a common language. The knowledge domains involved include circular economy, sustainability, materials, products, manufacturing, and logistics. The objective of this paper is to investigate the landscape of current ontologies for these domains. This will enable us to in the future explore what existing knowledge can be adapted or used to develop ontologies for circular value networks.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {731–741},
numpages = {11},
keywords = {Circular Economy, Cross-Industry Domain, Ontology, Standard},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{10.1145/3360901.3364433,
author = {Lieber, Sven and De Meester, Ben and Dimou, Anastasia and Verborgh, Ruben},
title = {MontoloStats - Ontology Modeling Statistics},
year = {2019},
isbn = {9781450370080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360901.3364433},
doi = {10.1145/3360901.3364433},
abstract = {Within ontology engineering concepts are modeled as classes and relationships, and restrictions as axioms. Reusing ontologies requires assessing if existing ontologies are suited for an application scenario. Different scenarios not only influence concept modeling, but also the use of different restriction types, such as subclass relationships or disjointness between concepts. However, metadata about the use of such restriction types is currently unavailable, preventing accurate assessments for reuse. We created the RDF Data Cube-based dataset MontoloStats, which contains restriction use statistics for 660 LOV and 565 BioPortal ontologies. We analyze the dataset and discuss the findings and their implications for ontology reuse. The MontoloStats dataset reveals that 94\% of LOV and 95\% of BioPortal ontologies use RDFS-based restriction types, 49\% of LOV and 52\% of BioPortal ontologies use at least one OWL-based restriction type, and different literal value-related restriction types are not or barely used. Our dataset provides modeling insights, beneficial for ontology reuse to discover and compare reuse candidates, but can also be the basis of new research that investigates novel ontology engineering methodologies with respect to restrictions definition.},
booktitle = {Proceedings of the 10th International Conference on Knowledge Capture},
pages = {69–76},
numpages = {8},
keywords = {axioms, ontology engineering, rdf, restrictions, statistics},
location = {Marina Del Rey, CA, USA},
series = {K-CAP '19}
}

@inproceedings{10.1145/3652620.3688209,
author = {Hinkel, Georg},
title = {Modeling a Warehouse system using refinements and decomposition: A contribution to the MULTI Warehouse challenge},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688209},
doi = {10.1145/3652620.3688209},
abstract = {Traditional two-level modeling has limitations when it comes to represent domain-specific, non-transitive instantiation relationships. Despite many approaches tackle this problem, there is no consensus on how to models these cases best. To have a common benchmark to discuss the different approaches, the MULTI workshop has set the warehouse model challenge. In this paper, we present and discuss a solution of this challenge applying deep modeling through refinements and structural decomposition - a technology that supports deep modeling with just few extensions to EMOF [12].},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {770–779},
numpages = {10},
keywords = {deep modeling, NMF, refinements and decomposition},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3652620.3688340,
author = {Fiyouzisabah, Zahra and Galasso, Jessie and Fokaefs, Marios and Famelis, Michalis},
title = {Towards Rapid Design of Compartmental Models},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688340},
doi = {10.1145/3652620.3688340},
abstract = {In times of crisis, epidemiologists can come under great pressure to model rapidly evolving diseases and to produce analyses about the effects of potential public health interventions. Taking previously developed, tested, and validated model components as the base on which to prototype new infectious disease models can save precious time and effort. However, there is currently no systematic process for quickly navigating a corpus of existing epidemiological models or identifying and reusing their most useful components. In this paper, we propose a vision to accelerate the creation of prototype compartmental models for infectious diseases. We outline a semi-automated process that epidemiologists can use to create prototypes that have been partially completed with reused fragments from existing models. Epidemiologists can thus focus on modelling the novel aspects of an ongoing public health crisis, as opposed to aspects of it that are already more or less well understood in previous work. Our approach comprises five steps in total, including identifying useful components in a corpus of infectious disease models, generating potential candidate prototypes, and organizing them in a formal data structure that allows navigation and exploration by the modellers. We outline 13 challenges ahead and discuss potential solutions based on formal modelling techniques.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {1041–1045},
numpages = {5},
keywords = {domain specific modelling, reuse, evolution, prototyping, formal concept analysis, scientific computing, compartmental models},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3570991.3571058,
author = {Gupta, Akshay and Kumar, Suresh and Kumar P, Sreenivasa},
title = {Solving age-word problems using domain ontology and BERT},
year = {2023},
isbn = {9781450397971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570991.3571058},
doi = {10.1145/3570991.3571058},
abstract = {An age word problem (ageWP) typically involves sentences that express relationships between the age of the agents and asks for the age of one of them. Automatically solving ageWPs is a challenging task as we need to tackle temporal relationships between the agent’s ages, frame and solve the equations for the required unknowns. To the best of our knowledge, there exists only one ageWP dataset consisting of just 124 examples. The dataset is too small to employ a learning-based solver, mainly consisting of ageWPs with simple temporal relationships. To address this issue, in our earlier work, we designed a description-logic based ontology (ageWP-ont) for the domain of age word problems and utilized it to automatically generate a large number of ageWPs. Sentences in these ageWPs relate the ages of agents in a temporally complex manner. In this paper, we focus on solving these problems. We analyzed an existing learning-based solver of algebraic word-problems that uses a traditional machine learning approach and found that the solver can be adapted to our domain. But we found that this approach does not seem to perform well, perhaps due to the complex nature of the ageWPs. As we have the ontology of the domain on hand, we propose a new approach of utilizing it in the deep-learning based NLU component of the solution. We annotate parts of the ageWP sentences with class-names from ageWP-ont and train a BERT-based language model (LM) that learns to predict the instances for these classes in the given sentences. An RDF graph is populated with these values and serves as a concrete problem-specific instance of the ontology. The dataset for training the LM is automatically generated with the help of ageWP-ont. Finally, for the actual solving of a given ageWP, we make use of its RDF graph and employ Semantic Web Rule Language (SWRL) rules. We implemented the proposed system and achieved 68.8\% accuracy. The work demonstrates that combining deep learning with ontologies can give impressive results.},
booktitle = {Proceedings of the 6th Joint International Conference on Data Science \&amp; Management of Data (10th ACM IKDD CODS and 28th COMAD)},
pages = {95–103},
numpages = {9},
keywords = {Age-word problem solver, BERT, OWL-DL ontology, SWRL},
location = {Mumbai, India},
series = {CODS-COMAD '23}
}

@article{10.1145/3745021,
author = {Xu, Heng-Da and Mao, Xian-Ling and Sun, Fanshu and Che, Tian-Yi and Xu, Chun and Huang, Heyan},
title = {AgentTOD: A Task-Oriented Dialogue Agent with a Flexible and Adaptive API Calling Paradigm},
year = {2025},
issue_date = {September 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {5},
issn = {1046-8188},
url = {https://doi.org/10.1145/3745021},
doi = {10.1145/3745021},
abstract = {Task-oriented dialogue (TOD) systems play a vital role in numerous assistance and service scenarios, significantly improving people’s daily lives. Conventionally, a TOD system adheres to a fixed paradigm, where it must first extract user goals and query external databases before it can generate the final response. However, this fixed extract-and-query paradigm is not always optimal for all dialogue turns, which is redundant for the simple turns that do not need external information, and is inadequate for the complex turns that need to interact with the external world multiple times. To address the limitations, in this article, we propose AgentTOD, a novel TOD framework that uses a large language model (LLM) as the intelligent agent to achieve a flexible dialogue paradigm. AgentTOD deprecates the traditional modular architecture (including dialogue state tracking and dialogue policy) by utilizing an LLM as the controller brain to determine when and how to call the provided APIs to obtain external information. It can choose to call APIs any number of times with various parameters until it’s enough to reply to the user. Besides, to train AgentTOD, we construct a large and comprehensive TOD dataset, called TrajsTOD (Trajectories of TODs), which consists of 66k+ user-agent dialogue trajectories converted from eight popular TOD datasets covering 60 domains. TrajsTOD is constructed with minimal dialogue annotations where only the API calling logs are needed and can empower AgentTOD with the general ability to call APIs and generate responses according to the task definition. Extensive experimental results on the MultiWOZ-series and SGD datasets demonstrate AgentTOD has superior performance on TODs as well as a superior adaptability to new task scenarios.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {136},
numpages = {32},
keywords = {Task-Oriented Dialogue, Large Language Models, Intelligent Agents}
}

@inproceedings{10.1145/3313831.3376315,
author = {Strengers, Yolande and Qu, Lizhen and Xu, Qiongkai and Knibbe, Jarrod},
title = {Adhering, Steering, and Queering: Treatment of Gender in Natural Language Generation},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376315},
doi = {10.1145/3313831.3376315},
abstract = {Natural Language Generation (NLG) supports the creation of personalized, contextualized, and targeted content. However, the algorithms underpinning NLG have come under scrutiny for reinforcing gender, racial, and other problematic biases. Recent research in NLG seeks to remove these biases through principles of fairness and privacy. Drawing on gender and queer theories from sociology and Science and Technology studies, we consider how NLG can contribute towards the advancement of gender equity in society. We propose a conceptual framework and technical parameters for aligning NLG with feminist HCI qualities. We present three approaches: (1) adhering to current approaches of removing sensitive gender attributes, (2) steering gender differences away from the norm, and (3) queering gender by troubling stereotypes. We discuss the advantages and limitations of these approaches across three hypothetical scenarios; newspaper headlines, job advertisements, and chatbots. We conclude by discussing considerations for implementing this framework and related ethical and equity agendas.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {feminist hci, natural language generation},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3711896.3736926,
author = {Wu, Xuan and Zhao, Yizheng},
title = {A Neuro-Symbolic Approach to Symbol Grounding for ALC-Ontologies},
year = {2025},
isbn = {9798400714542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711896.3736926},
doi = {10.1145/3711896.3736926},
abstract = {Neuro-symbolic computing aims to integrate neural learning with symbolic reasoning to address the fundamental challenge of symbol grounding. While neural networks excel at pattern recognition, they struggle to maintain logical consistency. Conversely, symbolic systems provide formal reasoning capabilities but lack mechanisms for handling perceptual uncertainty. This paper introduces EmALC , a novel neuro-symbolic framework that bridges neural perception with symbolic logic through differentiable fuzzy semantics. Our approach addresses a key limitation of existing methods: while previous neuro-symbolic approaches like Logic Tensor Networks employ first-order fuzzy logic, where key reasoning problems are undecidable, EmALC ensures decidable reasoning by leveraging a fuzzy variant of ALC -- a decidable fragment of first-order logic. Unlike previous approaches that often compromise logical soundness for learning capability, EmALC maintains provable semantic consistency through a hierarchical loss function while mitigating reasoning shortcuts via rule-based revision strategies. Experimental evaluation demonstrates EmALC's effectiveness: on ontology revision tasks, it achieves 100\% success rate in correcting masked groundings while preserving semantic integrity; on semantic image interpretation tasks, it improves object classification F1-scores by up to 5.56\% through ontology-guided knowledge revision.},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
pages = {3240–3249},
numpages = {10},
keywords = {description logic, differentiable fuzzy semantics, neuro-symbolic computing, symbol grounding},
location = {Toronto ON, Canada},
series = {KDD '25}
}

@inproceedings{10.1145/3633637.3633645,
author = {Wang, Changlong and Gan, Tingting and Li, Xingyu and Zhang, Linghan and Wang, Xijie},
title = {An Ontology-enhanced Knowledge Graph Embedding Method},
year = {2024},
isbn = {9798400707988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3633637.3633645},
doi = {10.1145/3633637.3633645},
abstract = {Knowledge graph embedding maps entities and relationships of the graph into low dimensional dense vectors to expresse the semantic information, meantime provides effective support for downstream tasks such as link prediction. However, the existing knowledge graph embedding methods mainly focus on the explicit structured information in the graph and rarely use the entailed rich ontological knowledge. Therefore in the paper, a method for injecting ontology information into the embedding model is proposed, ontology information including class hierarchy information and relationship attribute constraints,especially symmetry attributes are considerd. By taking ontology information as extra constraints, the loss function is further refined.the generation of training samples is optimized and the number of false negative samples is limited. Experiments on the two datasets of DBpedia15K and NELL show that the embedding model can be further optimized by injecting ontology information. Specially, the hit rate of triple prediction is 63.70\% for the no-type, and for type-triples the MR and the H@10 are 13.51 and 96.59\% respectively. The proposed model has better performance than the basic model, which further confirms the effectiveness of the prior knowledge of ontology in knowledge graph embedding learning.},
booktitle = {Proceedings of the 2023 12th International Conference on Computing and Pattern Recognition},
pages = {51–57},
numpages = {7},
keywords = {Knowledge embedding, Knowledge graph, Ontology information, Reasoning, Symmetry},
location = {Qingdao, China},
series = {ICCPR '23}
}

